[
  {
    "title": "How Wi-Fi Signals Can Be Used to Detect Your Heartbeat",
    "link": "https://spectrum.ieee.org/wi-fi-signal-heartbeat-detection",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore . Wi-Fi signals today primarily transmit data. But these signals can also be used for other innovative purposes. For instance, one California-based team has proposed using ambient Wi-Fi signals to monitor a person’s heart rate. The new approach, called Pulse-Fi, offers advantages over existing heart-rate-monitoring methods. It’s low-cost and easily deployable, and it sidesteps the need for people to strap a device to their body. Katia Obraczka is a professor at the University of California, Santa Cruz , who led the development of Pulse-Fi. She notes that continuous tracking of vital signs, including heat rate, can help flag health concerns such as stress, dehydration, cardiac disease, and other illnesses. “But using wearables to monitor vitals can be uncomfortable, have weak adherence, and have limited accessibility due to cost,” she says. Camera-based methods are one option for remote, contactless tracking of a person’s heart rate without a wearable device. However, these approaches may be compromised in poor lighting conditions and may also raise privacy concerns. In the search for a better option, Obraczka, along with postdoc student Nayan Sanjay Bhatia and high-school intern Pranay Kocheta working in her lab, sought to create Pulse-Fi. “Pulse-Fi uses ordinary Wi-Fi signals to monitor your heartbeat without touching you. It captures tiny changes in the Wi-Fi signal waves caused by heartbeats,” says Obraczka. How Can Wi-Fi Signals Measure Someone’s Pulse? Specifically, the team designed Pulse-Fi to filter out background noise and detect the changes in signal amplitude brought about by heartbeats. They developed an AI model —capable of running on a simple computing device, such as a Raspberry Pi —which then reads the filtered signals and estimates heart rate in real time. The team tested their approach in two different experiments, which are described in a study published in August at the 2025 International Conference on Distributed Computing in Smart Systems and the Internet of Things . First, the researchers had seven volunteers sit in a chair at various distances of 1, 2, and 3 meters from two ESP32 microcontrollers that used Pulse-Fi to estimate the volunteers’ heart rates, comparing these data to heart-rate measurements taken by a pulse oximeter. In the second experiment, Pulse-Fi was used on Raspberry Pi devices to monitor the heart rates of more than 100 participants in different positions, including walking, running in place, sitting down, and standing up. The results show that the system performs on par with other reference sensors, and Pulse-Fi’s less than 1.5-beats-per-minute error rate compares favorably to other vital-sign-monitoring technologies. Pulse-Fi also maintained sufficient accuracy despite the person’s posture (for example, sitting or walking) or distance from the recording device (up to 10 feet away). Based on these results, Obraczka says the team plans to establish a company to commercialize the technology. Prof. Katia Obraczka and her Ph.D. student Nayan Sanjay Bhatia—both of the University of California, Santa Cruz—discuss research on their Pulse-Fi technology, which remotely measures people’s pulse rates using Wi-Fi signals. Erika Cardema/UC Santa Cruz Obraczka adds that Pulse-Fi can work in new environments that its underlying AI model hadn’t trained for. “The model generalized well in [a new] setting, showing it’s not just memorizing, but actually learning patterns that transfer to new situations,” she says. Obraczka also notes that the devices that Pulse-Fi runs on are affordable–with ESP32 chips costing about US $5 to $10, with Raspberry Pis costing about $30. As yet, the researchers have tested Pulse-Fi on only a single user in the room at a time. The team is now beginning to pilot the approach on multiple users simultaneously. “In addition to working on multiuser environments, we are also exploring other wellness and health care applications for Pulse-Fi,” Obraczka says, citing sleep apnea and breathing rates as examples.",
    "published": "Sun, 05 Oct 2025 13:00:04 +0000",
    "author": "Michelle Hampson",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "The Story of Engineering Is the Story of Scale",
    "link": "https://spectrum.ieee.org/engineering-scale",
    "summary": "Engineers are masters of scale . They harness energy from the sun, wind, rivers, atoms, and ores. They manipulate electrons, photons, and crystals to compute and communicate. They devise instruments that detect perturbations in the fabric of space-time . And they grapple with challenges—anticipated or not—that are presented by the scale of the problem they are trying to solve. The articles in this issue describe engineers who think about, interact with, and create things at very precise and often mind-boggling scales. They took the point-contact transistor and scaled it over the course of decades into a product manufactured in almost unimaginably large quantities ( 13 sextillion, or 13,000,000,000,000,000,000,000, between 1947 and 2018, by one estimate ) and involving one of the most complex, yet crazily efficient workflows on the planet . They’re sequencing the genomes of 1.8 million species . They’re modeling and mitigating a potential catastrophe—the Kessler syndrome—that threatens to decimate satellites in low Earth orbit [p. 58]. Everywhere you look, engineering ingenuity is pushing against the limits of scale. That ingenuity extends to creating scales for what has yet to be measured. How will we know when AI has achieved human-level general intelligence ? How do we precisely measure the absence of matter in a vacuum ? Then there are the complexities of scaling a technology for mass adoption. Why, for example, have some humanoid robot makers announced overly optimistic deployment targets and boosted production capacity well ahead of specific humanoid robot safety standards, high reliability, decent battery life, or demand for hordes of humanoids ? And how can onshore wind turbines continue to scale up unless there’s a proven way to transport them ? “Infographics let readers grasp at a glance what would take paragraphs of explanation.” —Eliza Strickland In this issue, our editors and artists flex their data-visualization powers through compelling infographics, to help readers appreciate the scale of hundreds of gigatonnes of carbon dioxide and the immense interstellar distances we could traverse with a swarm of tiny, laser-powered space kites. “While we wanted every article to include some visual element, a few topics called for special treatment. You could tell the story of carbon capture or interstellar travel in words, but the real impact comes when you see the gaps, the scales, the leaps involved,” says Senior Editor Eliza Strickland, who curated this issue. “Infographics let readers grasp at a glance what would take paragraphs of explanation, whether it’s the ballooning demand for AI or the long journey from raw quartz to finished computer chips.” Several of these infographics, as well as the cover, were created by renowned graphic designer Carl De Torres, owner of Optics Lab. We also commissioned an essay by the nature writer Paul Bogard, who approached his topic from the human scale. Who among us has not gazed at the stars and marveled at how our eyes are absorbing light that traveled thousands of years to reach us? Bogard ventured to Chile to see how light pollution is encroaching on astronomy and changing our sense of place in the universe , perhaps irrevocably. We hope this issue sparks wonder, and conveys our appreciation for the people who measure the unmeasurable, build the unbuildable, and solve the unsolvable.",
    "published": "Thu, 02 Oct 2025 16:00:03 +0000",
    "author": "Harry Goldstein",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "The Long Strange Trip from Silica to Smartphone",
    "link": "https://spectrum.ieee.org/the-long-strange-trip-from-silica-to-smartphone",
    "summary": "This article is part of our special report The Scale Issue . If you want to get a sense of the truly global scale of the electronics industry, look no further than your smartphone. The processor that powers it started as a humble rock, and by the time it found its way into your device, it had probably seen more of the world than you have. Along the way it was subjected to some of the most technologically sophisticated and closely guarded processes on the planet. Come along as we retrace that incredible 30,000-kilometer ride. 1. Quartz Your smartphone processor began its journey in the northwest corner of Spain, at Mina Serrabal , a quartz mine near the city of Santiago de Compostela. Quartz—or more technically, silicon dioxide or silica—is the main component of sand. But at Serrabal it can come in huge pieces twice the width of a smartphone. Mine operator Ferroglobe runs an automated system to sort the silica by size. After the pieces are washed and treated, the big ones head to the Atlantic coast for the next step in the journey. Fact: According to consultant Thunder Said Energy , 350 million tonnes of silica was mined in 2024. 2. Silicon Metal After an hour by truck , the quartz mini-boulders arrive at Sabón , Ferroglobe’s 125,000-square-meter factory in the coastal province of A Coruña. Here the quartz will be mixed with dehydrated wood chips and heated to 1,500 to 2,000 °C in a trio of electric-arc furnaces that use massive electrodes invented at this plant in the 1990s . Inside the furnace, a reaction takes place that rips the oxygen from the silica and sticks it to the carbon from the wood. The result is silicon metal and carbon monoxide. Fact: 3.8 million tonnes of silicon metal was produced in 2023, according to the U.S. Geological Survey . 3. Purified Polysilicon The resulting silicon metal is about 98 percent pure, and that’s not good enough. It will need to be at least 99.9999999 percent pure to become a microprocessor, which will require some pretty powerful chemistry. So it’s off to Wacker Chemie , in Burghausen, Germany. Here, the metal undergoes what’s called the Siemens process: It’s bathed in hydrochloric acid and reacts to form hydrogen gas and a liquid called trichlorosilane. Any impurities will be in the liquid, which is then run through a multistep distillation process that separates the pure trichlorosilane from anything unwanted. Once the needed purity is reached, the reaction is reversed: At 1,150 °C, the trichlorosilane is reacted with hydrogen to deposit multiple crystals of silicon, called polysilicon, and the resulting hydrochloric acid gas is sucked away. The polysilicon forms thick rods around heating elements. Once it’s cooled and removed from the reaction chamber, the polysilicon is smashed up for shipping. Fact: According to consultant Thunder Said Energy , 1.7 million tonnes of polysilicon was produced in 2024, most of that for solar-cell production. 4. Silicon Wafers The ultrapure silicon is made up of many crystals at different orientations. But microprocessors must be made from a single crystal. So the material might migrate to Sherman, Texas, where GlobalWafers recently opened a US $3.5 billion silicon-wafer plant. Here the polysilicon is put through what’s called the Czochralski (Cz) method . In a high-purity quartz crucible, the polysilicon is heated to about 1,425 °C and melts. Then a seed crystal with a precise crystal orientation is dipped into the melt, slowly drawn upwards, and rotated. Do all that exactly right, and you will pull up an ingot of pure, crystalline silicon that’s 300 millimeters across and several meters tall. Specialized saws then slice this pillar of semiconducting purity into wafers less than 1 millimeter thick. The wafers are cleaned, polished, and sometimes further processed, before heading to wafer fabs. Fact: According to industry association SEMI, manufacturers have shipped nearly 43 million square meters of silicon wafers in the last five years. That’s enough to cover two-thirds of the island of Manhattan. 5. Processed Wafers Now it’s off to Tainan, in southern Taiwan, where TSMC’s Fab 18 will turn these wafers into the latest smartphone processors. It’s an exceedingly intricate process, involving some of the most complex and expensive equipment on the planet, including EUV lithography systems that can cost upward of $300 million each. In Fab 18, each wafer will go through months of exquisitely precise torture to produce the transistors and wiring that make up the processors. Extreme ultraviolet radiation will print patterns onto it, hot ions will ram into its surface, precision chemical reactions will build up some parts one atomic layer at a time, acids will etch away nanometer-scale structures, and metals will electrochemically plate parts and be polished away in others. The result: a wafer full of identical processors. Fact: The maximum size of a silicon die is 858 mm 2 . Within a chip, there are more than 160 kilometers of wiring. Apple’s M3 Max processor contains 92 billion transistors. 6. Packaged Chips As amazing as these processors are, you can’t use them in this form. They first need to be packaged. For our silicon, that’s going to happen at ASE’s facility in Penang , Malaysia. A package provides the chip with mechanical protection, a way for heat to be removed, and a way of connecting the chip’s micrometer-scale parts to a circuit board’s millimeter-scale ones. To do this, the wafers are first diced into chips. Then tiny balls of solder, some only tens of micrometers across, are attached to the chips. The solder bumps are aligned to corresponding parts of the package, and the two parts are melted together. It’s becoming more common for multiple pieces of silicon to be integrated within the same package, either stacked on top of each other or positioned next to each other on a separate piece of silicon called an interposer. Other steps to the process follow, and the packaged part is now ready for its next step. Fact: Stacking multiple chips within a package could lead to GPUs with 1 trillion transistors by 2033 7. Smartphones Our packaged chip arrives next in southern India, at Foxconn’s new $2.56 billion assembly plant on the outskirts of Bengaluru. The 1.2-square-kilometer site includes dormitories to house 30,000 workers, who will turn the chip, printed circuit board, touchscreen, battery, and a multitude of other components into an iPhone—one of some 25 million the company expects to produce per year at this and three other plants . Fact: Processors and other logic chips made up 12 percent of the $463 billion cost of smartphones and other mobile devices in 2022, according to the Yole Group Global Trade Acknowledgment: This journey was inspired by a chapter in Ed Conway’s Material World: The Six Raw Materials That Shape Modern Civilization (Alfred A. Knopf, 2023). This article appears in the October 2025 print issue as “From Silica to Smartphone.”",
    "published": "Tue, 23 Sep 2025 13:00:04 +0000",
    "author": "Samuel K. Moore",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Tech Keeps Chatbots From Leaking Your Data",
    "link": "https://spectrum.ieee.org/homomorphic-encryption-llm",
    "summary": "Your chatbot might be leaky. According to recent reports , user conversations with AI chatbots such as OpenAI ’s ChatGPT and xAI’s Grok “have been exposed in search engine results.” Similarly, prompts on the Meta AI app may be appearing on a public feed . But what if those queries and chats can be protected, boosting privacy in the process? That’s what Duality , a company specializing in privacy-enhancing technologies, hopes to accomplish with its private large language model (LLM) inference framework. Behind the framework lies a technology called fully homomorphic encryption , or FHE, a cryptographic technique enabling computing on encrypted data without needing to decrypt it. Duality’s framework first encrypts a user prompt or query using FHE, then sends the encrypted query to an LLM. The LLM processes the query without decryption, generates an encrypted reply, and transmits it back to the user. “They can decrypt the results and get the benefit of running the LLM without actually revealing what was asked or what was responded,” says Kurt Rohloff , cofounder and chief technology officer at Duality. As a prototype, the framework supports only smaller models , particularly Google ’s BERT models. The team tweaked the LLMs to ensure compatibility with FHE, such as replacing some complex mathematical functions with their approximations for more efficient computation. Even with these slight alterations, however, the AI models operate just like a normal LLM would. “Whatever we do on the inference does not require retraining. In our approach, we still want to make sure that training happens the usual way, and it’s the inference that we essentially try to make more efficient,” says Yuriy Polyakov , vice president of cryptography at Duality. The Challenges of FHE LLM Inference FHE is considered a quantum-computer-proof encryption . Yet despite its high level of security, the cryptographic method can be slow. “Fully homomorphic encryption algorithms are heavily memory bound,” says Rashmi Agrawal , cofounder and chief technology officer at CipherSonic Labs , a company that spun out of her doctoral research at Boston University on accelerating homomorphic encryption. She explains that FHE relies on lattice-based cryptography , which is built on math problems around vectors in a grid. “Because of that lattice-based encryption scheme, you blow up the data size,” she adds. This results in huge ciphertexts (the encrypted version of your data) and keys requiring lots of memory. Another computational bottleneck entails an operation called bootstrapping, which is needed to periodically remove noise from ciphertexts, Agrawal says. “This particular operation is really expensive, and that is why FHE has been slow so far.” To overcome these challenges, the team at Duality is making algorithmic improvements to an FHE scheme known as CKKS (Cheon-Kim-Kim-Song) that’s well-suited for machine learning applications. “This scheme can work with large vectors of real numbers, and it achieves very high throughput,” says Polyakov. Part of those improvements involves integrating a recent advancement dubbed functional bootstrapping. “That allows us to do a very efficient homomorphic comparison operation of large vectors,” Polyakov adds. All of these implementations are available on OpenFHE , an open-source library that Duality contributes to and helps maintain. “This is a complicated and sophisticated problem that requires community effort. We’re making those tools available so that, together with the community, we can push the state of the art and enable inference for large language models,” says Polyakov. Hardware acceleration also plays a part in speeding up FHE for LLM inference, especially for bigger AI models. “They can be accelerated by two to three orders of magnitude using specialized hardware acceleration devices,” Polyakov says. Duality is building with this in mind and has added a hardware abstraction layer to OpenFHE for switching from a default CPU backend to swifter ones such as GPUs and application-specific integrated circuits ( ASICs ). Agrawal agrees that GPUs, as well as field-programmable gate arrays ( FPGAs ), are a good fit for FHE-protected LLM inference because they’re fast and connect to high-bandwidth memory. She adds that FPGAs in particular can be tailored for fully homomorphic encryption workloads. For Duality’s next steps, the team is progressing their private LLM inference framework from prototype to production. The company is also working on safeguarding other AI operations, including fine-tuning pretrained models on specialized data for specific tasks, as well as semantic search to uncover the context and meaning behind a search query rather than just using keywords. Encrypted LLMs Are the Future FHE forms part of a broader privacy-preserving toolbox for LLMs, alongside techniques such as differential privacy and confidential computing. Differential privacy introduces controlled noise or randomness to datasets, obscuring individual details while maintaining collective patterns. Meanwhile, confidential computing employs a trusted execution environment—a secure, isolated area within a CPU for processing sensitive data. Confidential computing has been around longer than the newer FHE technology, and Agrawal considers it as FHE’s “head-to-head competition.” However, she notes that confidential computing can’t support GPUs, making them an ill match for LLMs. “FHE is strongest when you need noninteractive end-to-end confidentiality because nobody is able to see your data anywhere in the whole process of computing,” Agrawal says. A fully encrypted LLM using FHE opens up a realm of possibilities. In health care , for instance, clinical results can be analyzed without revealing sensitive patient records. Financial institutions can check for fraud without disclosing bank account information. Enterprises can outsource computing to cloud environments without unveiling proprietary data. User conversations with AI assistants can be protected, too. “We’re entering into a renaissance of the applicability and usability of privacy technologies to enable secure data collaboration,” says Rohloff. “We all have data. We don’t necessarily have to choose between exposing our sensitive data and getting the best insights possible from that data.”",
    "published": "Tue, 23 Sep 2025 12:00:04 +0000",
    "author": "Rina Diane Caballar",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Hollowing Out Fiber Speeds It Up and Keeps Signals Moving",
    "link": "https://spectrum.ieee.org/hollow-core-fiber",
    "summary": "Fiber-optic cables are very fast—achieving data speeds of up to a couple of hundred terabits per second . However, AI data centers today demand more bandwidth still. This insatiable appetite has spurred a global race among researchers to develop hollow-core fiber —because light travels faster through air than glass. Now, researchers in England have created a new type of hollow-core fiber-optic cable that can reduce signal loss and increase propagation speed through the fiber. The researchers have doubled the fiber’s glass layers, adding a second ring of nested glass tubes. They report that in addition to increasing bandwidth by as much as three times over standard fibers, the new design boosts transmission speeds up to 45 percent and extends the distance to 33 kilometers, compared to about 15 to 20 km for standard fiber. “We can deliver signals to the recipient with much fewer distortions and in a faster time,” says Francesco Poletti, chief scientist at Microsoft’s Azure Fiber , and professor in fiber optics at the University of Southampton, in England. “This new record is well below the 0.14 decibel loss that even the purest glass can achieve—so less energy is consumed to transmit data.” Polettti says the original fiber design used a single antireflective glass tube, or linked glass rings. A second tube was then nested inside it, and later an even smaller tube was nested inside the second tube, leading to the term double nested antiresonant nodeless hollow-core fiber (DNANF). The glass rings each rely on antiresonance­ —reflecting the signal wavelength back into the core to decrease signal attenuation and to confine light to the center. Five of these resonator assemblies are arranged symmetrically around the central core of air, forming the guiding structure of a fiber strand as thin as a human hair. “As a result, over 99.995 percent of the light propagates through the air, and with a significant reduction in the distortion, delay, and loss of power that is normally associated with propagation through glass,” says Poletti. Nested glass tubes, developed by Microsoft and University of Southampton engineers, will help bring about a new era of data connectivity. Gregory Jasion and Francesco Poletti/University of Southampton How Will Hollow-Core Fiber Cables Be Deployed? Hollow-core fiber cables, says Francesco Tani, a researcher in optics at the National Centre for Scientific Research in Lille, France, “have advanced beyond imagination over the past 10 years…. These test results represent an impressive breakthrough for data transmission, making [hollow-core fibers] a real and competitive alternative to standard telecom fiber.” Nevertheless, with over 5 billion kilometers of standard fiber-optic cable installed around the world, it won’t be an easy task to get the industry to take up a new kind of cable that will require its own ecosystem, including connectors, splicers, and signal amplifiers. Poletti says Microsoft is working with partners to develop such an ecosystem and has already installed an earlier generation of DNANF connecting two Azure data centers in Europe. The test installation uses two routes for geographic diversity, each over 20 km long, using hybrid cables that include 32 hollow-core fibers (HCFs) and 48 single-mode fiber strands. “The project brings the fast propagation speed of HCF cables to the data center interconnection space,” says Poletti. “And with 1,280 kilometers of hollow-core fiber now deployed and carrying live traffic, it demonstrates the technology is not just viable—it is ready for commercial adoption.” However, scaling up HCF manufacturing and deployment is still another matter, says Tani. “Compared to standard optical fiber, drawing long lengths—tens or hundreds of kilometers—is more challenging for HCF,” he says. “As far as I know, a significant part of the fabrication is still manual.” Azure Fiber must also contend with mounting competition from companies lik e China’s Yangtze Optical Fibre and Cable and Linfiber Technology, both of which have reported comparable performances using similar HCF technologies. Moreover, Yingying Wang, the founder of Linfiber, announced at this year’s Optical Fiber Communications Conference in March , that Linfiber has achieved “a continuous drawing of a 47.5-kilometer hollow-core fiber with loss of 0.1 dB per kilometer.”",
    "published": "Sat, 20 Sep 2025 13:00:02 +0000",
    "author": "John Boyd",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "How Cryptocurrency Captured the Dream of the Decentralized Web",
    "link": "https://spectrum.ieee.org/web3-hardware-security",
    "summary": "The term Web3 was originally coined by Ethereum cofounder Gavin Wood as a secure, decentralized, peer-to-peer version of the Internet. The idea was to build an Internet based on blockchain technology and a peer-to-peer network, without the need for large data centers or third-party providers. These days, however, blockchain is most famous as the tool enabling cryptocurrencies . Most recently , the Trump administration has taken on a pro-cryptocurrency stance, boosting blockchain’s popularity and media prominence. Cryptography is central to the functioning of blockchains, whether for a decentralized Web or for cryptocurrencies. Every time a cryptocurrency transaction is initiated, all parties involved in the transaction need to securely prove that they agree to the transfer. This is done via a digital signature : a cryptographic protocol that generates a secret, private key that is unique to each user and a public key that the user shares. Then, the private key is used to generate a unique signature for each transaction. The public key can be used to verify that, indeed, the signature was created by the holder of the private key. In this way, Web3 in every incarnation relies heavily on cryptography. To learn more about the evolution of Web3, and cryptography’s role, we caught up with Riad Wahby , assistant professor of electrical and computer engineering at Carnegie Mellon University, in Pittsburgh, and a cofounder and CEO of hardware-backed Web3 security platform Cubist . Wahby explained what Web3 was meant to be, what it’s become, and how hardware-backed cryptography will enable its future. Web3 Began as a Response to What Came Before IEEE Spectrum : What is Web3? Riad Wahby: That’s the hardest question that you’re going to ask by far, because I don’t know how to answer it in a way that satisfies everyone. The term Web3 was coined around 2014, by people who looked at the way that the Web had developed. Web 1.0 was the first Web bubble, the dot-com bubble. Web 2.0, roughly speaking, is Google and Facebook and Microsoft and Apple and Netflix, and the like. And the perception, especially from folks who originally coined this term Web3, was that these companies had basically taken the Web in the wrong direction, because your privacy is gone, and you’re the product, so to speak. You use Gmail for free because Google is mining your emails to sell things like better advertising. Web3 was originally a reaction to this. Early proponents of Web3 basically said, “We don’t want that. We want to take back control of our stuff. I want to own my own data, and maybe cryptocurrencies and blockchains are the way there.” So that’s where the term originally came from. What does the term mean now? Wahby: Now it doesn’t mean anything like that at all. Now Web3 is the broader ecosystem around cryptocurrencies and blockchain-based technologies. And I think basically all of that revolutionary spirit has gone away in favor of building financial products and making a lot of money doing it. As far as I can tell, the term has really transformed from a reaction to a lack of privacy and a lack of sovereignty in my own data to “Hey, this is a technology that has something to do with blockchains.” Maybe you can buy some kind of speculative meme coin and make a bunch of money doing it. So I don’t know, maybe that took a dark turn at the end. That’s how things go. How are those two definitions connected? Wahby: Cryptography really fits into the revolutionary spirit, in the sense that the folks who want to cast off the chains of things like Google and Facebook, one of the tenets was—”The way that we’ll do that is we’ll build this technology that’s sort of amazing and that gives us all these great properties.” And they were going to do that using some advanced cryptographic technologies. This is the reason that there’s so many people who are cryptography researchers at universities that also are involved deeply in some kind of cryptocurrency. Because it’s like this is a sea change in the way that cryptography gets used in the world. Twenty years ago, it used to be that if you were working on really any kind of cryptography, regardless of how theoretical or how practical you intended it to be, you knew that there was not much of a chance that any of it was going to get really used in the world, unless it was extremely practical and extremely focused on solving some immediate problem. And it just used to be the case that people were extremely conservative about what kind of cryptography they used. Basically, everyone thought, “We don’t need any of this crazy stuff. That’s all theory. Nobody cares. The only stuff we need is what lets you connect to Amazon and safely buy stuff.” The rise of cryptocurrencies brought with it this whole shift in the way that cryptography gets deployed in the world, where now if you can come up with some interesting functionality that’s enabled by some advanced cryptography, probably somewhere somebody is going to try and turn that into a product that they can sell. Web3 Is Both Good and Bad for Cryptography What effect has this had on the cryptographic community? Wahby: It’s both good and bad. It’s good in that this means that there’s a lot of motivation to build interesting, cool stuff. And as a researcher in cryptography, I love it because it means that there’s tons more research money being poured into cryptography. That’s the good side. The bad side is that the reason that people were so conservative about deploying new cryptography is that it’s easy to get the security mechanism wrong. The default state of cryptography is [to assume everything is] broken. You have to be very, very careful that each change that you make isn’t returning your cryptography to the default state. I’m not saying that people in Web3 aren’t careful. They are. It’s just by the nature of things, since it’s a much faster time frame, there’s much more pressure to just push stuff into production. And I think the downside is that we have seen a little bit of brokenness. It’s hopefully not causing people to lose oodles of money. And I think the historical record bears this out: People lose oodles of money because other people are really dishonest, not because the cryptography is broken for the most part. But the cryptography can also be broken, and that can also be worrisome. But I’d say from the perspective of somebody who’s doing research in cryptography, the impact of Web3 on the cryptographic community has generally been a good thing. Now you’re focused on hardware security . Can you explain what that is? Wahby: Any cryptocurrency has this property that if I hold some token, and I want to send it to somebody else, the way that I do that is by producing a digital signature that says, I want to spend this token. The secret key is what lets you generate a signature. So if you have 10 E TH [cryptocurrency coins], and they’re all protected by this key, and somebody takes a copy of your key, then life is bad. With a digital signature key it could just be sitting on your hard drive, and then you get some malware, and now somebody has silently stolen your key. There have been these big, broadly targeted malware campaigns where millions and millions of people have all had their keys stolen. So now the criminals are just like sitting there counting up all the money that they’ve stolen, and there’s no reversing transactions, unlike at a bank. Here’s where hardware comes in. This is not really a Web3 technology, this is kind of old, good stuff. There are these devices called hardware security modules, and they’ve been used for multiple decades. This is a physical device, and this device can run certain cryptographic algorithms. And it knows enough that when you tell it, “Hey, please generate me a key,” it can generate you a key securely. And when you tell it, “Please give me a signature,” it can give you a signature securely. But the important thing is the way that it’s designed, the key never leaves this piece of hardware. It turns what was a piece of data into a physical object. And we know how to secure a physical object. You’re working on extending hardware security for more use cases. Can you explain what you’re doing? Wahby: There are two issues with the standard hardware security module. No. 1, you need more cryptography support, so you need to be able to apply digital signatures to transactions very quickly if you’re actively trading. And No. 2, you need a way of expressing that it’s not just a key that can generate any signature. It’s a key that also has attached to it some kind of policy that says these are the kinds of signatures that are okay to generate, and everything else is not allowed, to add extra security. These are the two directions that we have that our technology enables within traditional hardware security modules. We start with the security that’s provided by the traditional hardware security module, and we extend it using this, actually another piece of trusted hardware called the Trusted Execution Environment. We extend it to support the actual kinds of cryptography that are needed for Web3 and to support this rich programmable policy layer that lets you say, “This key is only intended for this specific kind of use,” or “anytime somebody tries to make a payment from this key, first I have to check whether the recipient is subject to sanctions,” or any other rule. So in the end, we have, not only a hardware security module, we have also this Trusted Execution Environment and this policy layer, and all this other cryptographic stuff that together gives us a hardware security module that’s really designed for the Web3 use case.",
    "published": "Mon, 08 Sep 2025 13:00:03 +0000",
    "author": "Dina Genkina",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "More-Sophisticated Codes to Track Deep-Space Probes",
    "link": "https://spectrum.ieee.org/deep-space-communication-tech",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. As deep-space probes continue to explore throughout the solar system and beyond , scientists on the ground are racing to keep in touch with the many increasingly far-flung missions. To that end, researchers in China have developed deep-space-communications processing algorithms that will allow scientists to pinpoint probes with remarkable, meter-level precision—as far as 180 million kilometers from Earth. Xiaoyu Dang—a professor at Hangzhou Dianzi University ’s Space Information Research Institute (SIRI), in China—says that communicating with a spacecraft millions or billions of kilometers away is incredibly difficult. “Signals get very weak, and existing ranging codes struggle to pinpoint a spacecraft’s exact distance precisely over these enormous gaps,” he says. To maintain contact with space probes, the radio signals sent back and forth between Earth and deep space contain special ranging codes, which act as a kind of measuring tape or ruler. The code contains mathematical “notches” in it that enable mission planners to track individual signals as they travel to the probe and back. The travel time of the signal is then used to calculate the probe’s distance from Earth. Dang and his colleagues have developed a set of eight new codes (called Legendre Sequence Ranging Codes, or LS codes) which offer a much longer “measuring tape.” The team’s LS codes offer extended pattern length in deep-space signals compared with existing deep-space-communications codes. And that means the new codes can track probes increasingly farther away. Testing Deep-Space Signal Codes In simulations, the new LS codes reliably measured distances that are between 12 and 2,375 times as far as the distances that existing measuring codes can measure. As a result, the codes could enable precision communication with probes as far as 180 million km away from Earth, or 1.2 times the Earth–sun distance. On the other hand, the deepest deep-space probes travel billions of kilometers and often require specialized technologies and codes to maintain contact. For example, Voyager 1 , which launched in 1977, has traveled nearly 25 billion km from Earth. Remarkably, NASA still communicates with the probe, as recently as earlier this year . But communication with Voyager 1 is also very complex, Dang says, noting that scientists must combine Voyager’s original ranging code with additional low-frequency signals. This technique, he adds, is like using multiple rulers of different sizes and frequencies. By contrast, the new LS codes will enable precise, long-distance communication without complicating the signals and signal processing. “This is invaluable for faster, simpler navigation during critical operations,” Dang says. He also notes that the new LS codes will offer more flexibility than previous ranging codes, allowing for fine-tuning of the codes to suit specific mission needs. Scientists can also choose to modify the codes to achieve optimal long-distance precision, or other desirable goals. For instance, the code can be optimized for faster signal “locking” times when making contact with the probe, depending on the nature of the mission. Dang says the researchers aim to test their codes using hardware in laboratories. Then, if the tests are successful, the team expects to propose the code to space agencies for use in future missions. He says the team also plans to further develop and harden the codes to ensure they are robust against the harsh realities of space. The researchers’ deep-space signal-communications codes are described in a study published 11 August in IEEE Communications Letters .",
    "published": "Wed, 03 Sep 2025 13:09:00 +0000",
    "author": "Michelle Hampson",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Fiber to the Home",
    "link": "https://spectrum.ieee.org/steve-searcy-poem",
    "summary": "The crew has come: caravan of trucks stationed on the street, unstacking cones and digging ditches, deft and efficient. Here in our yards, long years of earth have been hefted by hand and heaped up on tarps. A pneumatic mole emerges from the trailer, and a heavy hose is hauled into place. With a pop, the pumping compressor wakes with startling strength. The strata are threaded, pierced by the pounding power that forges a buried boulevard. This burrow will convey packets with payloads, pulses of light modulated with meaning in marks and spaces, carrying commerce and conversation. The uproar ebbs by afternoon. Machines are shut down and shovels return, covering conduits with clods of soil. The sod is reset and soaked thoroughly. It’s late now. They load the last of the gear. The dirt-girded duct is dark and untapped. The glass-road will run to reach the houses after fees are paid, when the final strands will mate with modems and make connections.",
    "published": "Sun, 31 Aug 2025 15:40:02 +0000",
    "author": "Steven Searcy",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Teach 5G Hands-On with TIMS Lab Experiments",
    "link": "https://content.knowledgehub.wiley.com/unravelling-5g-complexity-engaging-students-with-tims-powered-hands-on-education/",
    "summary": "Boost Student Comprehension in Telecoms with Interactive 5G Labs. Teaching complex 5G and telecommunications concepts can be challenging – students often struggle to connect theory with real-world applications. Traditional lecture-based methods may fail to engage, leaving gaps in understanding critical technologies like OFDM, channel coding, and signal modulation. The Telecommunications Instructional Modelling System (TIMS) bridges this gap by transforming abstract concepts into tangible, hands-on experiments. Designed for EE/ECE/EET educators, TIMS enables students to model 5G systems, measure real signals, and validate theory through interactive labs – boosting engagement and retention. Download this free whitepaper now!",
    "published": "Thu, 28 Aug 2025 16:10:47 +0000",
    "author": "Emona Instruments",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "It’s the End of the Line for AOL’s Dial-Up Service",
    "link": "https://spectrum.ieee.org/end-of-aol-dial-up",
    "summary": "The last time I used a dial-up modem came sometime around 2001. Within just a few years, dial-up had exited my life, never to return. I haven’t even had a telephone line in my house for most of my adult life. But I still feel a strong tinge of sadness to know that AOL is finally retiring the ol’ hobbyhorse. At the end of September, it’s gone. The timeline is almost on-the-nose fitting : The widespread access to the Internet AOL’s service brought in the 1990s is associated with a digital phenomenon called the Eternal September. Before AOL allowed broad access to Usenet—a precursor to today’s online discussion forums—most new users appeared each September, when new college students frequently joined the platform. Thanks to AOL, they began showing up daily, starting around September 1993. The fact that AOL’s dial-up is still active in the first place highlights a truism of technology: Sometimes the important stuff sticks around well after it’s obsolete. Why AOL is ditching dial-up now It’s no surprise that dial-up has lingered for close to a quarter-century. Despite not having needed a dial-up modem myself since the summer of 2001, I was once so passionate about dial-up that I begged to get a modem for my 13th birthday. Modems are hard to shake, and not just because we fondly remember waiting so long for them to do their thing. Originally, the telephone modem was a hack. It was pushed into public consciousness partly by deaf users who worked around the phone industry’s monopolistic regulations to develop the teletypewriter , a system to communicate over phone lines via text. Along the way, the community invented technologies like the acoustic coupler. To make that hack function, modems had to do multiple conversions in real time—from data to audio and back again, in two directions. As I put it in a piece that compared the modem to the telegraph : The modem, at least in its telephone-based forms, represents a dance between sound and data. By translating information into an aural signal, then into current, then back into an aural signal, then back into data once again, the modulation and demodulation going on is very similar to the process used with the original telegraph, albeit done manually. Modems like this one from U.S. Robotics work by converting data to audio and back again. Jphill19/Wikimedia Commons With telegraphs, the information was input by a person, translated into electric pulses, and received by another person. Modems work the same way, just without human translators. The result of all this back-and-forth was that modems had to give up a hell of a lot of speed to make this all work. The need to connect over a medium built for audio meant that data was at risk of getting lost over the line. (This is why error correction was an essential part of the modem’s evolution; often data needed to be shared more than once to ensure it got through. Without error correction, dial-up modems would be even slower.) Remember that sound? It marked many users’ first experience getting online. AdventuresinHD/YouTube Telephone lines were a hugely inefficient system for data because they were built for voice and heavily compressed audio. Voices are still clear and recognizable after being compressed, but audio compression can wreak havoc on data connections. Plus, there was the problem of line access. With a call, you could not easily share a connection. That meant you couldn’t make phone calls while using dial-up, leading to some homes getting a second line. And at the Internet service provider level, having multiple lines got very complex, very fast. The phone industry knew this, but its initial solution, ISDN , did not take off among mainstream consumers. (A later one, DSL , had better uptake and is likely one of the few Internet options rural users currently have.) In some areas of the United States, dial-up remains the best option—the result of decades of poor investment in Internet infrastructure. So the industry moved to other solutions to get consumers Internet— coaxial cable , which was already widespread because of cable TV, and fiber, which wasn’t. The problem is, coax never reached quite as far as telephone wires did, in part because cable television wasn’t technically a utility in the way electricity or water was. In recent years, many attempts have been made to classify Internet access as a public utility , though the most recent one was struck down by an appeals court earlier this year. The public utility regulation is important. The telephone had struggled to reach rural communities in the 1930s and only did so after a series of regulations, including one that led to the creation of the Federal Communications Commission, was put into effect. So too did electricity, which needed a dedicated law to expand its reach. But the reach of broadband is frustratingly incomplete, as highlighted by the fact that many areas of the country are not properly covered by cellular signals. And getting new wires hung can be an immensely difficult task, in part because companies that sell fiber, like Verizon and Google, often stop investing due to the high costs (though, to Google’s credit, it started expanding again in 2022 after a six-year rollback). So, in some areas of the United States, dial-up remains the best option—the result of decades of poor investment in Internet infrastructure. This, for years, has propped up companies like AOL, which has evolved numerous times since it foolishly merged with Time Warner a quarter-century ago. The first PC-based client, called America Online, appeared on the graphical operating system GeoWorks. This screenshot shows the DOS AOL client that was distributed with GeoWorks 2.01. Ernie Smith But AOL is not the company it was. After multiple acquisitions and spin-outs, it is now a mere subsidiary of Yahoo, and it long ago transitioned into a Web-first property. Oh, it still has subscriptions , but they’re effectively fancy analogs for unnecessary security software. And their email client, while having been defeated by the likes of Gmail years ago, still has its fans. When I posted the AOL news on social media, about 90 percent of the responses were jokes or genuine notes of respect. But there was a small contingent, maybe 5 percent, that talked about how much this was going to screw over far-flung communities. I don’t think it’s AOL’s responsibility to keep this model going forever. Instead, it looks like the job is going to fall to two companies: Microsoft, whose MSN Dial-Up Internet Access costs US $179.95 per year, and the company United Online , which still operates the longtime dial-up players Juno and NetZero. Satellite Internet is also an option, with older services like HughesNet and newer ones like Starlink picking up the slack. It’s not AOL’s fault. But AOL is the face of this failing. AOL dropping dial-up is part of a long fade-out As technologies go, the dial-up modem has not lasted quite as long as the telegram, which has been active in one form or another for 181 years. But the modem, which was first used in 1958 as part of an air-defense system , has stuck around for a good 67 years. That makes it one of the oldest pieces of computer-related technology still in modern use. To give you an idea of how old that is: 1958 is also the year that the integrated circuit , an essential building block of any modern computer, was invented. The disk platter, which became the modern hard drive , was invented a year earlier. The floppy disk came a decade later. (It should be noted that the modem itself is not dying—your smartphone has one—but the connection your landline has to your modem, the really loud one, has seen better days.) The news that AOL is dropping its service might be seen as the end of the line for dial-up, but the story of the telegram hints that this may not be the case. In 2006, much hay was made about Western Union sending its final telegram . But Western Union was never the only company sending telegrams, and another company picked up the business. You can still send a telegram via International Telegram in 2025. (It’s not cheap: A single message, sent the same day, is $34, plus 75 cents per word.) In many ways, AOL dropping the service is a sign that this already niche use case is going to get more niche. But niche use cases have a way of staying relevant, given the right audience. It’s sort of like why doctors continue to use pagers. As a Planet Money episode from two years ago noted , the additional friction of using pagers worked well with the way doctors functioned, because it ensured that they knew the messages they were getting didn’t compete with anything else. Dial-up is likely never going to totally die, unless the landline phone system itself gets knocked offline, which AT&T has admittedly been itching to do . It remains one of the cheapest options to get online, outside of drinking a single coffee at a Panera and logging onto the Wi-Fi. But AOL? While dial-up may have been the company’s primary business earlier in its life, it hasn’t really been its focus in quite a long time. AOL is now a highly diversified company whose primary focus over the past 15 years has been advertising. It still sells subscriptions, but those subscriptions are about to lose their most important legacy feature. AOL is simply too weak to support the next generation of Internet service themselves. Their inroad to broadband was supposed to be Time Warner Cable; that didn’t work out, so they pivoted to something else, but kept around the legacy business while it was still profitable. It’s likely that emerging technologies, like Microsoft’s Airband Initiative , which relies on distributing broadband over unused “white spaces” on the television dial, stand a better shot. 5G connectivity will also likely improve over time (T-Mobile already promotes its 5G home Internet as a rural option), and perhaps more satellite-based options will emerge. Technologies don’t die. They just slowly become so irrelevant that they might as well be dead. The monoculture of the AOL log-in experience When I posted the announcement, hidden in an obscure link on the AOL website sent to me by a colleague, it immediately went viral on Bluesky and Mastodon. That meant I got to see a lot of people react to this news in real time. Most had the same comment: I didn’t even know it was still around. Others made modem jokes or talked about AOL’s famously terrible customer service . What was interesting was that most people said roughly the same thing about the service. That is not the case with most online experiences, which usually reflect myriad points of view. I think it speaks to the fact that while the Internet was the ultimate monoculture killer, the experience of getting online for the first time was largely monocultural. Usually it started with a modem connecting to a phone number and dropping us into a single familiar place. We have lost a lot of Internet service providers over the years. Few spark the passion and memories of America Online, a network that somehow beat out more innovative and more established players to become the on-ramp to the Information Superhighway, for all the good and bad that represents. AOL must be embarrassed of that history. It barely even announced its closure.",
    "published": "Wed, 27 Aug 2025 14:00:04 +0000",
    "author": "Ernie Smith",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "In Nigeria, Why Isn’t Broadband Everywhere?",
    "link": "https://spectrum.ieee.org/broadband-internet-in-nigeria",
    "summary": "Under the shade of a cocoa tree outside the hamlet of Atan, near Ibadan, Nigeria, Bolaji Adeniyi holds court in a tie-dyed T-shirt. “In Nigeria we see farms as father’s work,” he says. Adeniyi’s father taught him to farm with a hoe and a machete, which he calls a cutlass. These days, he says, farming in Nigeria can look quite different, depending on whether the farmer has access to the Internet or not. Not far away, farmers are using drones to map their plots and calculate their fertilizer inputs. Elsewhere, farmers can swipe through security camera footage of their fields on their mobile phones. That saves them from having to patrol the farm’s perimeter and potentially dangerous confrontations with thieves. To be able to do those things, Adeniyi notes, the farmers need broadband access, at least some of the time. “Reliable broadband in Atan would attract international cocoa dealers and enable access to agricultural extension agents, which would aid farmers,” he says. Adeniyi has a degree in sociology and in addition to growing cocoa trees, works as a criminologist and statistician. When he’s in Ibadan, a city of 4 million that’s southeast of Atan, he uses a laptop and has good enough Internet. But at his farm in Atan, he carries a candy-bar mobile phone and must trek to one of a few spots around the settlement if he wants better odds of getting a signal. “At times,” Adeniyi says, “it’s like wind bringing the signal.” RELATED: Surf Africa: What to Do With a Shiny New Fiber-Optic Undersea Cable On paper, Nigeria has plenty of broadband capacity. Eight undersea cables bring about 380 terabits of capacity to Nigeria’s coast. The first undersea cable to arrive, SAT-3/WASC , made land in 2001; the most recent is 2Africa, which landed in 2024. They’re among the 75 cables that now connect coastal Africa to the rest of the world. Nigeria’s big telecom operators continue to build long-distance, high-capacity fiber-optic networks from the cables to the important commercial nodes in the cities. But distribution to the urban peripheries and to rural places such as Atan is still incomplete. Incomplete is an understatement: Less than half of the country’s 237 million people have regular access to broadband, with that access mostly happening through mobile devices rather than more stable fixed connections. Nigeria’s Federal Ministry of Communications, Innovation, and Digital Economy has set a goal to almost double the length of the country’s fiber-optic backbone and for broadband to reach 70 percent of the population by the end of this year. But the ministry also claimed in 2024 that it would connect Nigeria’s 774 local governments to the broadband backbone; as of February 2025, it had reached only 51. The broadband buildout has been seriously hampered by Nigeria’s unreliable power grid . Beyond the mere inconvenience of frequent outages, the poor quality of electricity drives up costs for operators and customers alike. During a visit to Nigeria earlier this year, I talked to dozens of people about broadband’s impact on their lives. For more than two decades, the country has possessed an incredible portal to the world, and so I had hoped to hear stories of transformation. In some cases, I did. But that experience was far from uniform, with much work left to do. Where Nigeria’s broadband has arrived Broadband is enabling all kinds of changes in Nigeria, Africa’s most populous country. All eight undersea cables make landfall in Lagos, the cultural, commercial, and one-time federal capital of Nigeria, and one of the cables also lands near Port Harcourt to the southeast. The country’s fiber-optic backbones—which in early 2025 consisted of about 50,000 to 60,000 kilometers of fiber-optic cable—connect the undersea links to the cities. From 2008 to 2025, Nigeria has experienced extraordinary growth in both the number of undersea high-speed cables landing on its shores and the buildout of broadband networks, especially in its cities. Still, fixed-line broadband is unaffordable for most Nigerians, and about half of the population has no access. Africa Bandwidth Maps “Virtually everywhere in Nigeria is covered with long-haul cables,” says Abdullateef Aliyu , general manager for projects at Phase3 Telecom , which is responsible for perhaps 10,000 km of those cables. Most Nigerian cities have at least one fiber-optic backbone, and the biggest have more than half a dozen. The result is that the most densely populated areas enjoy competing Internet service providers offering fiber optics or satellite to the home. Connecting the other half of Nigerians, the rural majority, will become profitable someday, says Stanley Jegede , executive chairman of Phase3 Telecom, but it had better be “patient money.” A Phase3 Telecom worker [left] installs fiber-optic cables on power poles in Abuja, Nigeria. Abdullateef Aliyu [right], Phase3’s general manager for projects, says the country is using only around 25 percent of the capacity of its undersea cables. Andrew Esiebo Unsurprisingly, the customers that got broadband first were those with impatient money, those that could offer the best return to the telecom firms: the oil companies that dominate Nigerian exports, the banks that have since boomed, the Nollywood studios that compete with Bollywood and Hollywood. The impatient money showed up first in flash Victoria Island in Lagos. If you want to serve international customers or do high-speed stock trading, you need a reliable link to the outside world, and in Nigeria that means Victoria Island. Here, the fiber-optic cables rise like thick vines in gray rooms on the ground floors or in the basements of the office towers that house the banks powering Nigerian finance. Between the towers, shopping plazas host foreign fast-food franchises and cafés. From their perch near the submarine network, the banks realized that mobile broadband would allow them to reach exponentially more customers, especially once those customers could take advantage of Nigeria’s instant-payment system , launched by the central bank in 2011. Using mobile payments, bank apps, and other financial apps, Nigerians can conduct convenient cellphone transactions for anything from street food to airplane tickets. The central bank’s platform was such a success that until recently, it handled more money than its U.S. equivalents. RELATED: As Nigeria’s Cashless Transition Falters, POS Operators Thrive Just as important as convenience is trust. Nigerians trust each other so little that a university guesthouse I stayed in had its name printed on the wall-mounted air conditioner units to discourage theft. But Nigerians trust mobile payments. Uber drivers think nothing of sharing their bank account numbers with passengers, so that the passengers can pay their fares via instant payment. A Nigerian engineer explained to me that many people prefer that to disclosing their bank-card information on the Uber platform. Broadband has also brought change to Nollywood , Nigeria’s vast film industry, second only to India’s Bollywood in terms of worldwide film output. On the one hand, broadband transformed Nollywood’s distribution model from easily pirated DVDs to paywalled streaming platforms . On the other hand, streaming platforms made it easier for Nigerians to access foreign video content, cutting into local producers’ market share. The platforms also empowered performers and other content producers to bypass the traditional Nollywood gatekeepers. Instead, content creators can publish straight to YouTube, which will pay them if they achieve enough views. Emmanuella Njoku , a computer science major at the University of the People, an online school, is interested in a graphics or product-design job when she graduates. But a broadband-enabled side hustle is starting to look like a viable alternative, she told me in January. She edits Japanese anime recaps and publishes them to her YouTube channel . “I have 49,000 followers right now, but I need 100,000 followers and 10 million views in the last 90 days to monetize,” Njoku said. Computer science student Emmanuella Njoku has found a broadband-enabled side gig: creating YouTube videos. Andrew Esiebo A friend of hers had recently crossed the 100,000-follower threshold with YouTube videos focused on visits to high-end restaurants around Lagos. The friend expected restaurants and other companies to start paying her for visits, in addition to collecting her tiny cut of YouTube’s ad revenue. Both women said they’d prefer jobs that allow them to telecommute, a more realistic prospect in Nigeria in the last few years thanks to the availability of broadband. More companies are open to remote work and hybrid work, says telecom analyst Fola Odufuwa . That’s especially true in Lagos, where fuel shortages and world-class traffic jams encourage people to minimize the number of days they commute. For academics, broadband can make it easier to collaborate on research. In 2004, IEEE Spectrum reported on a Federal University of Technology researcher in Owerri carrying handwritten messages to a contact, who had a computer with an Internet connection and would type up the messages and send them as emails. Today researchers at the Federal University of Technology campus in Minna collaborate virtually with colleagues in Europe on an Internet of Things demonstration project. While some events take place in person, the collaborators also exchange emails, meet by videoconference, and work on joint publications via the Internet. Why broadband rollout in Nigeria has been so slow The undersea cables and fiber-optic backbones have also been a boon for Nigeria’s telecom industry, which now accounts for 14 percent of GDP, third only to agriculture (23 percent) and international trade (15 percent). Computer Village in Lagos is Nigeria’s main hub for electronics. Andrew Esiebo Alcatel (now part of Nokia) connected SAT-3 to Nigeria’s main switching station in December 2001, just a couple of years into the first stable democratic government since independence in 1960. The state-run telephone monopoly, Nigerian Telecommunications (Nitel), was mainly responsible for the rollout of SAT-3 within the country. Less than 1 percent of the 130 million Nigerians had phone lines in 2002, so the government established a second carrier, Globacom, to try to accelerate competition in the telecom market. But a mixture of mismanagement and wider difficulties contributed to the sluggish spread of broadband, as Spectrum reported in 2004 . Broadband access has soared since then, and yet Aliyu of Phase3 Telecom estimates that the country is using only around 25 percent of the total capacity of its undersea cables. Nigeria’s unreliable electricity drives up telecom prices, making it harder for poor Nigerians to afford broadband. The spotty power grid means that standard telecom equipment needs backup power. But battery or diesel-powered cellphone towers attract theft, which in turn undermines network reliability. Power outages occur with such frequency that even when the lights and air conditioning go out during in-person meetings, it arouses no comment. RELATED: Nigerians Look to Get Out From Under the Nation’s Grid A visit to Nitel’s former headquarters, a 32-story skyscraper with antennas and a lighthouse perched on top, is revealing. Telecom consultant Jubril Adesina leads the way into the once-grand entrance, where armed guards wave visitors past inoperative turnstiles. NTEL’s chief information officer, Anthony Adegbola, inspects broadband equipment at the company’s data center in Lagos, which still houses obsolete coaxial cable boxes [top]. Andrew Esiebo Our destination is NTEL, a private firm that inherited much of Nitel’s mantle , on the 17th floor. Adesina is explaining how a recent mobile tariff increase will improve mobile penetration, but when we reach the elevator lobby, he stops talking. The power is out again. His eyes turn to the unlit indicator alongside the shut elevators, then he looks at the stairs and whispers, “We can’t.” Instead, Adesina walks around to the back of the building and greets NTEL chief information officer Anthony Adegbola , who along with a small team of engineers and technicians guards another relic of Nigeria’s telecom past. We walk along a hallway past rooms with empty desks and old desktop computers and down a short staircase. Cables snake along the ceiling and above a door. Beyond the door, the men point proudly to SAT-3, Nigeria’s first high-speed undersea cable, rising alongside an electrical grounding cable from the tiled floor. Server racks house obsolete coaxial cable boxes, displayed as if in a museum, next to today’s fiber-optic boxes. Since the last time Spectrum visited, engineers have expanded SAT-3’s capacity from 120 gigabits per second to 1.4 terabits per second, Adegbola says, thanks to improvements in data transmission via different wavelengths, and better receiving boxes in the room. NTEL backs up the grid electricity with a battery bank and two generators. In Nigeria, mobile broadband is popular What is often missing in Nigeria is the local connection, the last few kilometers leading to customers. In the developed world, that connection works like this: Internet service providers (ISPs) plug into the nearest backbone via one of several technologies and deliver a small slice of bandwidth to their business and residential customers. A switching station called a point of presence (PoP) serves as an on- and off-ramp between the backbone and the ISPs. The ISPs are responsible for installing the fiber-optic cables that lead to their customers; they may also use microwave antennas to beam a signal to customers. But in Nigeria, fiber-optic ISPs have been sluggish to capture market share. Of the country’s 300,000 or so fixed-line broadband subscribers —just 0.1 percent of Nigerians—about a third are served by the leading ISP, Spectranet . By comparison, the average fixed broadband penetration rate among countries in the Organisation for Economic Co-operation and Development (OECD) was 42.5 percent in 2023 , led by South Korea, with 89.6 percent penetration. Starlink’s satellite-based service, introduced in Nigeria in 2023 , is now the second biggest broadband ISP, with about 60,000 subscribers. That’s almost triple the third biggest ISP, FiberOne . Satellite is outcompeting fiber because it’s more reliable and has higher speeds and tolerable latency , even though it costs more. A Starlink satellite terminal can serve up to 200 subscribers and retails for about US $200 plus a $37 monthly fee. A comparable fiber-to-the-home plan in Abuja, where the median monthly take-home pay is $280, costs about $19 a month. In Lagos’s Computer Village, you can buy or sell a mobile phone or computer, or get yours repaired. Andrew Esiebo Meanwhile, Nigeria has 142 million cellular subscriptions , and so most Internet users access the Internet wirelessly, via a mobile network. In other words, Nigeria’s mobile market is nearly 500 times as big as the market for fixed broadband. The mobile networks also rely on the fiber-optic backbones, but instead of using PoP gateways, they link to cellular base stations, each of which can reach up to thousands of mobile devices but may not offer ideal quality of service. Mobile Internet is a good thing for people who can afford it, which is most Nigerians , according to the International Telecommunication Union . The cost of fixed-line broadband is still around five times as much, which explains why its market share is so tiny. But mobile Internet isn’t enough to run many businesses, nor do mobile network operators guarantee network speeds or low latency, which are crucial factors for high-frequency trading, telemedicine, and e-commerce, and for white-collar jobs requiring streaming video calls. Nigeria is 129th in the world in Internet speeds Internet speeds across Nigeria vary, but broadband tester Ookla’s spring 2025 median for fixed broadband was 28 megabits per second for downloads and 15 Mb/s for uploads, with latency of 25 milliseconds. That puts Nigeria 129th in the world for fixed broadband. In May, Starlink delivered download speeds between 44 and 50 Mb/s, uploads of around 12 Mb/s, and latency of around 61 ms. The top country, Singapore , averaged 393 Mb/s down and 286 Mb/s up, with 4 ms latency. And those numbers for Nigeria don’t capture the effect of unpredictable electricity cuts. Steve A. Adeshina , a computer engineering professor and machine-vision expert at Nile University , in the capital city of Abuja, says he routinely runs up against the limits of Nigeria’s broadband network. That’s why he keeps two personal cellular modems on his desk. His university contracts with several Internet providers, but the broadband in his lab is still intermittent. For machine-vision research, with its huge datasets, failing to upload data stored on his local machine to the more powerful cloud processor where he runs his experiments means failing to work. “We have optical fiber, but we are not getting value for money,” Adeshina says. If he wakes up to a failed overnight data upload, he has to start it all over again. RELATED: The Engineer Who Secured Nigeria’s Democracy Fiber-optic cable spills from an open manhole in Lagos. Local gangs may cut the cables or steal components. Andrew Esiebo There are many causes for the slow Internet, but chief among them are frequent cable cuts— 50,000 in 2024 , according to the federal government. The problem is so bad that in February, the government established a committee to prevent network blackouts due to cable cuts during road construction, which it blamed for 60 percent of the incidents. “The challenge is reaching the hinterland,” Aliyu of Phase3 Telecom says, and keeping lines intact once there. To make his point, Aliyu, dressed in a snappy three-piece suit and red tie, drives a company pickup truck from Phase3’s well-appointed offices in a leafy part of Abuja to a nearby ring road. He pulls over in the shade of an overpass and steps onto the dirt shoulder. A concrete manhole cover sits perched along one edge of an open manhole, looking like the lid of a sarcophagus. Pointing at the hole, Aliyu explains how easy it is for local gangs, called area boys, to steal components or cut the cables, forcing backbone providers and ISPs to strike unofficial security deals with the boys, or the more powerful, shadowy men behind them. Of course, part of the problem is self-inflicted: Sloppy work crews leave manholes open and expose the cables to potential damage from nesting animals or a stray cigarette butt that ignites tumbleweed and melts the cables. Phase3 and other telecom companies are also contending with the expense of replacing the first generation of fiber-optic cables, now about 20 years old, as well as upgrading PoP hardware to increase capacity. They’re spending money not just to reach new customers, but also to provide competitive service to existing customers. For mobile operators such as Globacom, there’s the additional challenge of ensuring reliable power for their base stations. They often rely on diesel or gasoline generators to back up grid power, but fuel scarcity, infrastructure theft, and supply chain issues can undermine base station reliability. How Nigeria’s offline half lives The hamlet of Tungan Ashere is 3 km northwest of the major international airport serving Abuja. To get here, you leave the highway and drive past cinder-block huts with traditional reed roofs. The side of the dirt road is adorned with concrete pylons waiting to be strung with power lines but still naked as the day they were installed in 2021. People here farm cassava, watermelon, yam, and corn. Some keep small herds of goats and cattle. To get to market, they can ride on one of a handful of dirt-bike taxis. In Tungan Ashere, the Internet hub operated by the Centre for Information Technology and Development attracts residents. Andrew Esiebo When someone in Tungan Ashere wants to make an announcement, they stroll to a prominent tree and ring a green bar of scrap metal wedged at about head height in the tree’s branches. The metal resonates, not quite like a church bell, but it serves a similar purpose. “The bell, it’s to tell everybody to go to sleep, to wake up, if there’s an announcement. It’s an ancient way of communicating,” explains Lukman Aliu, a telecom engineer who drove me here. The concept of connectivity in the village differs from just a few kilometers away at the airport, where passengers can enjoy free high-speed Wi-Fi in the comfort of a café. Yet the potential benefits of affordable broadband access for people living in places like Tungan Ashere are enormous. Usman Isah Dandari is trying to meet that need. He is a technical assistant at the Centre for Information Technology and Development (CITAD) , a nonprofit based in Kano, Nigeria. Dandari coordinates a handful of community networking projects, including one in Tungan Ashere. Better broadband here would help farmers track market prices, help students complete their homework, and make it easier for farmers and craftspeople to advertise their goods. CITAD uses a mixture of hardware, including Starlink terminals and cellular modems, to offer relatively reliable broadband to areas neglected by commercial operators. The group is also considering using Nigeria’s national satellite operator, NigComSat , and working with the Nigerian Communications Commission to lower the costs. Usman Isah Dandari [standing] coordinates several projects like the one in Tungan Ashere, to provide affordable broadband access. Andrew Esiebo A few meters away from the scrap-metal bell in Tungan Ashere is a one-story building painted rust red, topped with a pastel green corrugated metal roof and eight solar panels, which power a computer lab inside. There’s no grid electricity here, but the solar panels are enough to run a CITAD-provided cellular modem, a few desktop computers, and a formidable floor fan some of the time. Many of the people in the village once lived where the airport is now. The Nigerian government displaced them when it chose the region as the new federal capital territory in 1991. Since then, successive local governments have provided services piecemeal, usually in the runup to elections. The result is a string of communities like Tungan Ashere—10,000 people in all—that still lack running water, paved roads, grid electricity, and reliable Internet. These people may live on the edge of Nigeria’s broadband backbone, but they reap few of its benefits. A private undersea cable shows how to do it Not every undersea cable rollout has been fraught. In 2005, electrical engineer Funke Opeke was working at Verizon Communications in the United States. MTN , an African telecom company, hired her to help it build its submarine cables. Then Nitel hired her to help manage its privatization. There, she saw up close how the organization was failing to get the Internet from SAT-3 into Nigerians’ lives. Funke Opeke founded MainOne to build Nigeria’s first private undersea fiber-optic cable. George Osodi/Bloomberg/Getty Images “I don’t think it was a question of capital or return on investment, policy, or interest,” Opeke says. Instead, officials favored suppliers offering kickbacks over those with competent bids. Seeing an opportunity for a well-managed submarine cable, Opeke approached private investors about developing a cable of their own. The result is the MainOne cable , which arrived in Lagos in 2010 and is operated by the company of the same name. MainOne offered the first private competition to Nitel’s SAT-3 and Globacom’s Glo-1 , which began service in 2010. (MTN’s two cables landed in Nigeria in 2011.) At first, the MainOne cable suffered the same problem as the others—its capacity wasn’t reaching users. “After we built, there was no distribution,” Opeke, who’s now an advisor with MainOne, says. So the company got its own ISP license and began building fiber links into major metro areas—eventually more than 1,200 km in states near its undersea-cable landing site. It ended up offering a more complete service than originally intended, bringing the Internet from overseas, onshore, across Nigeria, and the last kilometers into businesses and homes, and it attracted more than 800 business clients. MainOne’s success forced the publicly held telecoms and the mobile providers to compete. “The mobile networks were built for voice, and they were not investing fast enough” in data capacity, Opeke says. MainOne did invest, helping to create the broadband capacity needed for Nigeria’s first data centers. It then diversified into data centers, and in 2022 sold its whole business to American data-center giant Equinix . Other companies, including the major mobile operators, also began building fiber between Nigerian cities, duplicating each other’s infrastructure. The problem is they didn’t offer competitive prices to independent ISPs that wanted to piggyback on those new fiber-optic links, says the telecom analyst Odufuwa. And neither the public sector nor the private sector is meeting the needs of Nigerians at the bottom of the market, especially in rural communities such as Tungan Ashere and Atan. A crucial first step will be to improve the reliability of the electrical grid, Opeke says, which will help drive down costs for telecom operators and other businesses, and create a virtuous cycle for further growth. Almost everyone Spectrum interviewed for this story said security is another challenge: If Nigerian states and the federal government could ensure the security of the infrastructure, telecom operators would invest more in expanding their networks. Building telecom infrastructure is well within the reach of Nigerian engineers. “Nigeria doesn’t have a skill problem,” Opeke says. “It has an opportunity problem.” If the bureaucrats, businesspeople, and engineers can overcome those policy and technical hurdles, the unconnected half of Nigerians stand to gain a lot. Reliable broadband in Atan would draw more young people to agriculture, says the farmer and sociologist Bolaji Adeniyi: “It will provide jobs.” Then, like Adeniyi, maybe those young connected Nigerians will reconsider whether farming is just father’s work—perhaps it could be their future, too. Special thanks to IEEE Senior Member John Funso-Adebayo for his assistance with the logistics and reporting for this story. This article was updated on 18 September 2025.",
    "published": "Wed, 06 Aug 2025 14:00:04 +0000",
    "author": "Lucas Laursen",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Two-Factor Authentication Just Got Simpler",
    "link": "https://spectrum.ieee.org/two-factor-authentication-sandia-labs",
    "summary": "Two-factor authentication is a cornerstone of modern digital security, protecting banking, email, and many other kinds of accounts worldwide. Now scientists at Sandia National Laboratories in Albuquerque have developed a new, simpler form of two-factor authentication that, unlike conventional methods, does not require generating authentication codes based on the current time. They say it could help bring two-factor authentication to many new types of devices that currently cannot support it, including drones, remote sensors, farm equipment and industrial control systems. Two-factor authentication (TFA) is a security routine that requires both a password and an additional, temporary code to log into an account, typically sent via text, email, or an authenticator app. This kind of authentication is more difficult to crack than one that uses only a password because it requires a combination of different types of information. Usually, TFA generates these codes based on the current time . Banks might get that from their servers, while remote devices often get it from GPS . All kinds of applications could, in principle, use TFA, such as smart electric meters that require users to log in to change their settings. However, many devices lack the processing power, network bandwidth, or GPS connection to support it, leaving them vulnerable to potential cyberattacks, says Chris Jenkins , a cybersecurity researcher at Sandia. Moreover, as simple as TFA might seem, it often requires a complex set of transactions behind the scenes. For example, authentication codes from banks often come from third-party vendors, which in turn rely on telecom providers to send codes to phones, Jenkins says. Simplifying Two-Factor Authentication Jenkins and his colleagues have devised a simpler variation of TFA that does not require a time stamp and can work directly between two devices without third parties or extensive telecom infrastructures. They suggest it could enable a device as basic as a thermostat to generate its own authentication code. Chris Jenkins, a Sandia cybersecurity researcher, developed a new, simpler two-factor authentication method that does not depend on the current time to generate an additional verification code. Craig Fritz “When this work first started, it was focused on military weapons systems , which can be in GPS-denied environments ,” Jenkins says. “So we wanted TFA or something similar where knowing the time wasn’t going to be a requirement.” Instead of using the current time, the new TFA variant uses a random number generator . “Nothing about TFA in and of itself requires using time,” Jenkins says. “It was just easy, when implementing TFA systems, to use time to generate one-time passwords because of the infrastructure that was already in place.” This new work “doesn’t imply that we should abandon current TFA,” says Eric Vugrin , a senior cybersecurity scientist at Sandia. “It’s just that current TFA does not work for everything.” The new system uses minimal computing resources, and so may prove ideal for devices designed to minimize size, weight, and power use. Such electronics typically lack the kind of processing power needed to run complex security software, Jenkins says. “Our system can be used in resource-constrained devices,” Jenkins says. “Conventional TFA is always generating new codes as time passes—say, every minute or so. Our system performs the computation once. So for systems that might, say, save energy by waking up just once a day, they don’t have to burn energy every minute performing computations. They can just do everything up front.” That said, “there’s no reason our system can’t be used for all kinds of applications,” Jenkins says. Jenkins says that Sandia has a copyright on the code, so anyone interested in using it for TFA would have to go through Sandia’s Licensing and Tech Transfer department to discuss licensing it or establishing a Cooperative Research and Development Agreement . The scientists found their new system resisted machine-learning-based attacks . In the future, they hope to make it more robust with codes that can be updated dynamically during authentication. “When hackers steal passwords, those secrets don’t change if leaked,” Jenkins says. “So we’re looking for ways to update those secrets in the event that databases get compromised.”",
    "published": "Tue, 05 Aug 2025 12:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "The Stratosphere Will Be Telecom’s Next Frontier",
    "link": "https://spectrum.ieee.org/high-altitude-platform-station-softbank",
    "summary": "With more than 8,000 Starlink satellites in the sky today , low Earth orbit may seem like the place to be to connect the next generation of Internet and cellphone customers. However, some players are placing their bets slightly closer to the ground. Starting next year, Tokyo’s SoftBank Corp. will be beaming a prototype 4G and 5G phone and broadband service from the stratosphere to Japanese end users. Floating 20 kilometers above the Earth, the company’s airship-based mast will be using energy-regeneration tech and newly allocated spectrum. And the tech could ultimately pose a real, competitive threat to satellite-based platforms like Starlink. The Japanese telecom giant announced last month it had secured exclusive rights to deploy stratospheric, lighter-than-air craft over Japan. SoftBank’s p recommercial airship “tower” delivering 4G and 5G cellphone service, the company said, will be coming in 2026. The solar-powered airship, developed by the Moriarty, N.M.–based Sceye , has already completed more than 20 successful test flights. In the same press announcement, SoftBank also described its plans to also use heavier-than-air, fixed-wing uncrewed aerial vehicles that the Japanese company has developed. A Technical Blueprint for the Stratosphere Unlike the SoftBank system’s fixed-wing signal repeaters, Sceye’s airship will be an autonomously piloted cell tower operating below outer space but still above the weather. The airship will carry the same type of base station used in terrestrial cell towers (called 4G eNodeB / 5G g NodeB ), which will comply with global broadband standards, as overseen by the Third Generation Partnership Project , or 3GPP. “The mobile phone doesn’t know the difference between our platform and a tower,” says Mikkel Vestergaard Frandsen , Sceye’s CEO. “We just plug into existing infrastructure and operate under the same 3GPP protocol.” Sceye’s airship uses advanced antenna systems that enable precision steering of the signal. Also known as beamforming , this 5G tech helps a network cover wide areas or, conversely, focus bandwidth down to a tighter cone, depending on demand. The company reports that its system’s latency is below 20 milliseconds. That would put it ahead of Starlink, which delivers today a network latency of 45 ms , according to a recent survey. “This is not a relay system. We are the base station, able to respond to network demand from the stratosphere,” says Frandsen. With a payload capacity of 250 kilograms and 10 kilowatts of onboard solar power capacity, the airship can power its telecom suite but also station-keep—something that neither balloons (which drift with the wind) nor fixed-wing UAVs (constrained by limited payload and power) can achieve. Which is why Sceye’s advances in materials have been crucial for high-altitude endurance flights. According to the company, the fabric comprising the airship’s hull is five times as strong per unit mass as conventional high-altitude platform system (HAPS) materials. Sceye’s material is also 1,500 times more gas-tight, as well as being more resistant to both UV and ozone damage. “There’s a lot of overlap between extreme sports like the America’s Cup or Formula One and our work on HAPS,” said Frandsen, who recruited engineers from both sectors. “It’s all about pushing materials to the limit, safely.” But even using such a supermaterial for the airship’s skin, staying aloft at an altitude of 20 kilometers demands further innovations toward greater efficiency. “On this kind of machine, about 30 percent of the weight goes to the structure, and another 30 percent to the energy system,” says Vincenzo Rosario Baraniello , Head of the Earth Observation Systems Unit at the Italian Aerospace Research Centre ( CIRA ). “Improving those technologies gives a competitive advantage”. Sceye’s silvery dirigibles are built for endurance, capable of pointing into the wind, and remaining in their area of operation for months at a time. Ultralightweight and flexible solar skins and high-density battery packs keep the equipment running overnight. While the system’s temperature- and UV-shielded payload compartment can withstand extreme stratospheric conditions. The airship can reach altitude in less than 30 minutes, with a single craft able to replace up to 25 ground towers . Building on New Spectrum The time has come, says Nikolai Vassiliev, chief of the Terrestrial Services Department at the International Telecommunication Union , for stratospheric systems like Sceye’s and SoftBank’s prototype network. “We have established power limits, coordination rules, and allocated harmonized bands,” Vassiliev says. “Now it’s up to operators to deploy.” Until recently, high-altitude platforms like Sceye’s and SoftBank’s helium-filled airship relied primarily on millimeter-wave spectrum , including bandwidth between 47- and 48-gigahertz frequencies. Millimeter waves, though, have limited range and are notoriously vulnerable to rain and other inclement weather . Which is why, in part, the World Radiocommunication Conference in 2023 opened up a number of microwave bands between 700 megahertz and 2.6 GHz for HAPS. These lower-frequency bands effectively opened the way for direct-to-device connections from stratospheric airships and other high-altitude platforms. “The availability of harmonized, low-band spectrum for direct-to-device HAPS has fundamentally changed the business case,” said Toshiharu Sumiyoshi of SoftBank’s Ubiquitous Network Planning Division. “We can now deliver service with commercially available handsets.” Unlike earlier high-altitude platforms that acted like signal relays , Sceye’s high-altitude towers will ultimately allow users to cross coverage zones without losing service, thanks to handovers between ground and aerial nodes. And that could look and feel to the end user much like everyday terrestrial 4G and 5G coverage. SoftBank is still weighing how best to deploy Sceye’s stratospheric platforms, whether as always-on infrastructure or as on-demand responders during emergencies and other periods of anticipated high demand. “Our current plan aims for one aircraft to stay in the air for one year,” says Sumiyoshi. “But both scenarios, continuous flight or launch in response to a disaster, are conceivable. And operational details will be finalized after precommercial testing in 2026, taking cost-effectiveness and multiuse options like remote sensing into account.” Baraniello says whatever form the deployment ultimately takes, it marks an important step forward. “The partnership between Sceye and SoftBank is significant,” he says. “It shows that these platforms have reached a level of technological maturity that allows them to be deployed operationally. From an aerospace-engineering standpoint, that’s a big deal, and the market’s interest will further push research, industry, and development forward.” UPDATE 1 Aug. 2025: The story was updated to clarify the ITU’s position on the harmonized bands it’s allocated (the ITU’s addition in bold ) and to note that SoftBank and Sceye’s airships are helium-filled. This article appears in the October 2025 print issue as “Stratospheric Cell Sites.”",
    "published": "Thu, 31 Jul 2025 13:00:03 +0000",
    "author": "Maurizio Arseni",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Silicon Is Coming to Smartphone Batteries for a Big Energy Boost",
    "link": "https://spectrum.ieee.org/enovix-silicon-anode-battery-smartphone",
    "summary": "A novel lithium-ion battery that uses silicon in its anodes may have the highest energy density of any battery currently commercially available. Its manufacturer, Enovix, says it has shipped the new battery to a leading smartphone company for a debut in mobile phones later this year. Many of the lithium-ion batteries that power everything from mobile devices to electric cars use graphite in their anodes. However, for decades, researchers have investigated silicon as a replacement for this graphite. In theory, silicon offers roughly 10 times the energy density of graphite in lithium-ion batteries. “Basically, graphite holds on to lithium using holes in its structure,” says Raj Talluri , CEO of Enovix . “In contrast, with silicon in the anodes—usually a silicon oxide or a silicon carbide—lithium actually chemically combines with the silicon to form a new material. This lets a silicon-based anode hold on to much more lithium than graphite during charging. When the battery discharges, the silicon material goes back to its original state.” One significant problem researchers have faced when using silicon in lithium-ion batteries is that it bulges during charging, breaking the batteries open. “If you put silicon in a small cellphone battery, stacking the anodes and cathodes like a deck of cards, the expansion that you would see from the silicon during charging would generate enough force to lift up a 2,000-pound truck,” Talluri says. Enovix’s Energy Density On 7 July, Enovix launched its AI-1 battery , which it says overcomes silicon’s major swelling problems. Its energy density, exceeding 900 watt-hours per liter in internal tests, is estimated to be roughly 20 percent greater than that of current major smartphone-model batteries, the company says. The company also says the AI-1 battery, which is about 29.3 cubic centimeters large, possesses an energy capacity of 7,350 milliampere-hours, about 70 percent more than that of current major smartphone-model batteries. It also recharges quickly, reaching 20 percent charge in 5 minutes and 50 percent charge in 15 minutes. The strategy that made this new battery possible was using microscopically thin silicon anode strips. “Then you stack the anodes in such a way that they only expand along their thin side,” Talluri says. “This way they only generate 200 pounds of force when they expand, which can be constrained just by holding them in a metal case.” Improving a smartphone’s battery may improve other critical aspects of its performance. “I spent 25 years working on smartphones,” Talluri says. “One reason I moved to Enovix is that processors got faster and faster, and memories got bigger and bigger, but I found that you could not use either at their full performance because the battery would go down too fast. So by solving the battery problem, it can help everyone.” A key application Enovix is targeting is use in AI-powered smartphones. “AI applications have become very prominent on phones, and they demand much more battery life, especially if they don’t want to process data on the cloud because of user privacy issues,” Talluri says. “Our goal is to have an all-day battery life on smartphones, despite them running the latest AI applications.” Other potential applications for the new battery include Internet of Things (IoT) devices and augmented- or virtual-reality (AR or VR) glasses, Talluri says. Enovix is not currently manufacturing batteries for electric vehicles but may license its technology to other companies in the market. “We’re just at the beginning,” Talluri says. “We’re nowhere close to using the full potential of silicon.” This story was updated on 30 July 2025 to correct the estimated energy density of Enovix’s AI-1 battery compared to other smartphone batteries—it is 20 percent higher, not 50 percent.",
    "published": "Wed, 30 Jul 2025 12:00:02 +0000",
    "author": "Charles Q. Choi",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Over-the-Air Lasers Aim to Solve the Internet’s “Middle Mile”",
    "link": "https://spectrum.ieee.org/free-space-optical-communication-taara",
    "summary": "Twenty years ago, Web-savvy folks were focused on solving the Internet’s “last-mile” problem . Today, by contrast, one of the biggest bottlenecks to expanding Internet access is rather around a “middle-mile” problem —crossing cities and tough terrain, not just driveways and country roads. Taara , a spin-off of X (formerly Google X), is promoting a simple alternative to fiber-optic cables : Free-space optical lasers . Using over-the-air infrared C-band lasers, Taara is rolling out tech that the company says reliably delivers 20-gigabit-per-second bandwidth across distances up to 20 kilometers. However, what happens to open-air laser signals on a rainy or foggy day ? What about a flock of birds or stray tree branch blocking a tower’s signal? Plus, C-band communications tech is decades old . So why haven’t other innovators tried Taara’s approach before? IEEE Spectrum spoke with Taara’s CEO Mahesh Krishnaswamy about the company’s X pedigree (and its Google Fiber and Google Project Loon alumni) as well as upcoming new technologies, set to roll out in 2026, that’ll expand Taara towers’ bandwidth and range. Plus, the fledgling company’s wagering its industry footprint might get a tiny boost too. What does Taara do, and what problem or problems is the company working to solve? Mahesh Krishnaswamy, CEO of Taara, says the Internet’s “middle-mile” problem presents an outsize opportunity. Taara Mahesh Krishnaswamy: Taara is a project that incubated over the last seven years at [Google/Alphabet] X Development , and we recently graduated . We’re now an independent company. It is a technology that uses eye-safe lasers to connect between two line-of-sight points, using beams of light, without having to dig trench fiber. The problem we are really solving is that of global connectivity. Today, as we speak, close to 3 billion people are still not on the Internet. And even the 5 billion that are connected are running into challenges associated with speed, affordability, or reliability. It’s really a global problem that affects not just millions but billions of people. So Taara is addressing the digital divide problem? Krishnaswamy: Some of the ways our customers and partners have deployed [Taara's tech] is they use it for redundancy or to cross difficult terrain. A river, a railroad crossing, a mountain, anywhere the land is difficult to dig and traverse through, we are able to reach. One example is the Congo River , which is the world’s deepest river and one of the fastest flowing rivers. It separates Brazzaville [in the Republic of the Congo ] and Kinshasa [in the Democratic Republic of the Congo ]. Two separate countries on either side. But they’ve not been able to run fiber optic cables underneath the river. Because the Congo River is very fast-flowing. And so the only alternative is to go about 400 km, to where they’re able to safely navigate it. But we were able to connect these two countries very easily, and as a result, bring bandwidth parity. One side had five times higher bandwidth cost than the other side. The Road to New Free Space Optical Internet Tech What is Taara doing today that couldn’t have been done 5 or 10 years ago? Krishnaswamy: We’ve been slowly but steadily building up the improvements to this technology. This started with improvements in the optics, electronics, software algorithms, as well as pointing and tracking. We have enough margin to tackle most of the challenges that typically were limiting this technology up until recently, and we are one of the world’s largest manufacturers of terrestrial, free-space optics. We are live right now in more than 12 countries around the world—and growing every day. What is your company’s main technological product? Krishnaswamy: Today, the technology that we have is called Taara Lightbridge . This is our first-generation product, which is capable of doing 20 Gbps, bidirectionally, at up to 20 km distance. It’s roughly the size of a traffic light and weighs about 13 kilograms. Taara’s traffic-light-size Lightbridge terminal serves as the hub for the company’s free-space Internet tech—with fingernail-size components being promised for 2026. Taara But we are now about to embark on a significant sea change in our technology. We are going to take some of the core photonics and electronics components and shrink it down to the size of my fingernail. And it will be able to point, track, send, and receive light at tens of gigabits per second. We have this Taara chip in a prototype form, which is already communicating indoors at 60 meters as well as outdoors at 1 km. That is a big reveal, and this is going to be the platform by which we’re going to be building future generations of products. When will you be launching that? Krishnaswamy : It’ll be the end of 2026. The Internet’s Middle-Mile and Last-Mile Problems How does all of this relate to the tech being “middle mile” rather than what used to be called “last mile”? How much distinction is there between the two? Krishnaswamy: I f you were to follow the path of data all the way from a subsea fiber , where you have Internet landing points , there’s this very vast capacity fiber that’s bringing it all the way from the edge of the coast into some main city. That’s a longhaul fiber. These are the national backbones, usually laid by the countries. But once you bring it to the town, then the operators, the data centers, start to take it and distribute the bandwidth from there. They start down what we call the middle mile. That’s anywhere from a few kilometers to 20 kilometers of fiber. Now in some cases they will be passing very close to a home. In some cases, they’re a little bit further out. That’s the last mile. Which is not necessarily a mile. In some cases, it’s as short as 50 meters. Does Taara cover the whole length of the middle mile? Krishnaswamy: Today Taara operates where we are able to bridge connections from a few kilometers to up to 20 km. That’s the middle mile that we operate in. And almost 50 percent of the world today is within 25 km of a fiber point of presence. So it’s very much accessible for us to reach most of those communities. Now the next generation technology that I’m talking about, the photonics chip, will allow us to go even shorter distances and will allow us to close the gap on the last mile as well. So today we are mostly operating in the middle mile, and in some cases we can connect the last mile. But with the next-generation chip, we’ll be operating both in the middle mile as well as the last mile . What about the X background? Do you have people from Project Loon or from Google Fiber now working at Taara? Krishnaswamy: Yes. I was personally working on Project Loon, and I was leading up the manufacturing, the supply chain, and some of the operational aspects of it. But my passion was always to solve the connectivity problem. And at X we always say, fall in love with the problem, not the solution per se. So you started using Project Loon’s open-air signaling tech that connects one Internet balloon to another, but you just did it between fixed stations on the ground? Krishnaswamy: Yes, the idea was very simple. What if we were to bring the technology connecting balloons in the stratosphere down to the ground, and start connecting people quickly? It was a quick and dirty way of getting started on connecting and closing out the digital gap. And little did I know that across the street, Google Access was also working on similar technology to cross freeways. So I pulled together a team from Google Access and then from Project Loon. And today the Taara team includes people from various parts of Google who worked on this technology and other connectivity projects. So it’s a team that is really passionate about connectivity globally. The Challenges Ahead for Free-Space Optical Tech OK, so what about foggy days? What about rain and snow? How does Taara technology send over-the-air infrared data traffic through inclement weather? Krishnaswamy: Our biggest challenge is weather, particularly particulates in weather that disperse light. Fog is our biggest nemesis. And we try to avoid deploying in foggy areas. So we built a planning tool that allows us to actually predict the anticipated availability. As long as it’s light rain, and it doesn’t disperse [optical signals], then it’s fine. A simple rule of thumb is if you can see the other side, then you should be able to close the link. We’re also exploring some smart rerouting algorithms , using mesh . Ultimately, we are subject to some environmental degradations. And it’s really how you overcome that is what we’ve been focusing on. Why 20 km? Is Taara trying to extend that to greater distances today? Krishnaswamy: The honest truth is it started out with one of our first customers in rural India who said, “I have many of these access points which are up to 20 km away.” And as we started to dig deeper, we realized we can connect a vast majority of the unconnected places within 20 km of a fiber point of presence. So that ended up becoming our initial specification. How about pointing? If you’re beaming a laser out over 20 km, that’s a tiny target to aim at. Krishnaswamy: When we deployed first in India, we ran into a lot of monkeys that we had to deal with who are territorial. There would be like 20 or 30 of these monkeys jumping and shaking the tower, and our link would always oscillate. So we can’t physically drive them away. But we could actually improve our pointing and tracking, which is exactly what we did. So we have gyroscopes and accelerometers built in. We are constantly monitoring the other side. There’s also a camera inside the terminal. So if you are really out of alignment, we can always repoint it again. But basically we have made a significant amount of improvements in our pointing and tracking. That’s one of our secret sauces. What are the near-term hurdles for the company? Near-term ambitions? Krishnaswamy: I used to work at Apple, so I brought some of the best practices from there as well to make this technology manufacturable. We want physics to be the upper bound of what is capable, and we don’t want any compromises. And the last thing I’ll say is we are really pioneering the light generation. This is a complete relook at how light can be used for communication purposes, which is where we’re starting out. When you have something this small, that could deliver such high speeds at such low latencies, you could put it into robots and into self-driving cars. And it could change the landscape of communications. But if you were to not just use it for communication, it could go into lidar or biomedical devices that scan and sense. You could do a lot more using the underlying technology of phased arrays in a silicon photonics chip. There’s so much more to be done.",
    "published": "Tue, 29 Jul 2025 12:00:02 +0000",
    "author": "Margo Anderson",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "The Telecommunications Pioneer Who Helped Connect the World",
    "link": "https://spectrum.ieee.org/telecommunication-pioneer-seizo-onoe",
    "summary": "Without Seizo Onoe , cellular phone networks would not be the source of global connectivity we know today. The IEEE life member was instrumental in driving the standardization for 3G and 4G mobile networks . The first-generation networks that launched in the late 1970s and early 1980s were largely country-specific, designed for making only domestic or regional calls. There was no way to send text messages or other data over 1G networks. And interference from other radio signals made 1G coverage unreliable; there were plenty of dropped calls. Plus, without encryption, eavesdropping was a persistent problem. Seizo Onoe Employer: International Telecommunication Union Title: Director of the ITU’s Telecommunication Standardization Bureau Member Grade: Life member Alma Mater: Kyoto University In 1991 2G networks signaled the fledgling industry’s switch to digital—which improved security and broadened the range of features. Basic text messages became possible. Individual countries and carriers had built their own telecommunications infrastructures, however, using different technologies and protocols (although Europe had established a common regional standard). An industry-wide, standardized cellular telecommunications infrastructure was needed. Onoe answered that call, helping to align companies’ and countries’ infrastructures as 3G networks took shape. For his efforts, Onoe has been awarded the IEEE Jagadish Chandra Bose Medal in Wireless Communications . The medal, bestowed for the first time this year, is named for an Indian scientist who pioneered radio and millimeter-wave research . It is sponsored by physicist and IEEE Fellow Mani Lal Bhaumik . Onoe is currently director of the Telecommunication Standardization Bureau at the International Telecommunication Union . The medal is designed to commemorate contributions to wireless communications technologies with a global impact. “It is the highest honor for me,” Onoe says, “especially as I am the first recipient.” Learning early digital mobile radio transmission Onoe grew up in Akashi, in southwestern Japan. He says he was drawn to the “directness” of STEM subjects. He majored in engineering when he enrolled at Kyoto University in 1976. “Of course, my parents also suggested engineering because it was more advantageous for employment,” he says. He earned a bachelor’s degree in 1980 and an engineering master’s degree two years later. As a graduate student, he worked on early digital mobile radio transmission. The primitive equipment on which he cut his teeth—repurposed (1.544 megabits per second) fixed-line hardware donated by industry—sent out mobile radio signals at data rates that would be comparable to those of 3G someday, foreshadowing the digital mobile future. The debate behind the 3G standard Japan’s Nippon Telegraph and Telephone in 1979 launched the world’s first 1G network . In 1982 Onoe joined NTT’s Yokosuka Electrical Communication Laboratory, in Yokosuka City. Starting his NTT career as a researcher, he helped develop the control signals necessary for call setup and other controls over an analog network. “At the time, NTT’s mobile services was a very small division,” he recalls. Things really started changing in the early 1990s, he says. In 1992 the company spun off its mobile division, Docomo (do communications over the mobile network). The name was popularly interpreted as a play on the Japanese word dokomo , which means everywhere . Onoe was transferred to Docomo in 1992, when the company was founded, and was later promoted to executive engineer and director. NTT reacquired Docomo in 2020. He contributed to 3G development, including work on a rapid cell search algorithm, which proved critical for network performance. The algorithm lets mobile devices quickly identify the nearest base station in a cellular network. And it didn’t rely on other systems, like GPS, to locate the correct cell in a network—making the process easier, faster, and less expensive. His most challenging effort throughout the 1990s, he says, was including emerging digital cellular stakeholders worldwide—including governments, telecom companies, and regulators—to begin envisioning the infrastructure on which a truly global cellular network could be built. That meant developing a single standard. “There were many, many heated debates all around the world,” Onoe says. According to history articles published online by Ericsson , the debates were complex and contentious . They involved entities from inside and outside the industry, including phone manufacturers, mobile service providers, standards boards, and government officials. Europe alone was considering five different telecom infrastructures across the continent’s numerous cellular networks, Onoe says, highlighting the divide around the world. Some companies and countries supported time-division multiple access (TDMA), which would split the available network bandwidth into time slots and assign users specific slots for transmission. Others were pushing a different access technology that is partly competitive with TDMA and partly complementary to it : code-division multiple access (CDMA), which uses unique codes to allow multiple users to share both bandwidth and time. As if that emerging standards landscape weren’t complicated enough, Sony championed yet another technology based on orthogonal frequency division multiple access (OFDMA). In December 1997 the European Telecommunications Standards Institute met in Madrid. At issue would be who controlled the standards for, at the time, the 3G future . And that is when the fur really flew. The Nordic mobile manufacturers Ericsson and Nokia squared off in what were, according to Ericsson’s account at least, “increasingly warlike circumstances.” Britain’s prime minister, Tony Blair , who the above account said “regarded Ericsson as a British company,” took Ericsson’s side in the squabbles. Other ETSI disputes aired at that meeting found their way into lawsuits years later . None of the standards under consideration garnered enough votes to pass. A second vote would be held the following month in Paris—and so the lobbying began anew. At the Paris meeting in January 1998, the ETSI voted on W-CDMA as the dominant standard for the world’s 3G networks. But in the spirit of compromise, the standards body also allocated a limited amount of 3G spectrum to TD-CDMA, a combination of the time-division and code-division methods. Following that, in the final stage of the 3G standardization battle—a debate between wideband CDMA and a similar access technology, CDMA2000—Onoe emerged as a major player to help broker an agreement, as Japan at least hedgingly supported the push for W-CDMA. “I decided to step in and join the war, so to speak,” Onoe says. “Across all these countries and vendors and individuals fighting, it was clear we were going to have to come up with some compromises to finally agree.” Onoe helped lead an operators’ harmonization group to do just that. It proposed changing the chip rate—the frequency at which the smallest units of 3G code are transmitted. With 3G politics addressed, the hard 3G engineering work then began in earnest. “We had to start the development of the commercial system,” Onoe says. “I don’t think I fully appreciated just how challenging that was going to be.” From 1999 until 2001, he says, he worked every day including weekends. “I would start meetings at midnight, summarize the day’s activities, and plan for the next day,” he says. “It’s hard to imagine all these years later, but as a young and excited engineer, it was easy for me to do.” NTT became the first company to launch 3G commercially , in October 2001. The new wireless standard vastly outstripped 2G’s data rates. Ultimately, average 2G download speeds were about 40 kilobits per second , while 3G eventually would boast up to 8 mega bits per second. The 2G to 3G switch represented a night-and-day difference in speed, making 3G the first global standard to enable the first wave of mobile video calls, Internet browsing, online games, and streaming video content. 4G and telecom’s rapid bandwidth growth With his contributions to 3G secured, Onoe continued to look forward. In 2009 Ericsson and Sweden’s TeliaSonera launched the world’s first 4G/LTE network . Five times faster than 3G, it unlocked high-definition video streaming, lag-free online gaming, and a new range of mobile apps including FaceTime, Snapchat, and Uber . Onoe also played a key role in the global 4G standardization process. At the time, he was Docomo’s managing director of R&D strategy. He went on to become the company’s chief technology officer, as well as an executive vice president who served on the board of directors. When he left the company in 2022, he was NTT’s chief standardization strategy officer. That year he was elected to his current role: director of the ITU’s Telecommunication Standardization Bureau. He began his four-year term at the beginning of 2023. “The ITU’s fundamental mission is to connect the unconnected,” he says. “One-third of the world’s population is still not connected. And common specifications help, because when they’re adopted widely, they create economies of scale. Competition increases, and the price drops. It’s a positive cycle.” Collaborating with IEEE Onoe joined IEEE early in his career—following company policy at NTT encouraging membership. He says he continues to renew his membership because he values the networking opportunities it provides, as well as chances to talk about the industry with fellow engineers. He works closely with IEEE leaders in his current position at ITU. In December the organizations convened the IEEE-ITU Symposium on Achieving Climate Resilience , which aims to shape a technology-driven road map to confront the climate crisis. “We also hold joint workshops and meetings and share thoughts informally,” Onoe says. “As I’ve seen throughout my career, it’s critically important that standards bodies actively collaborate if we hope to advance global technology.”",
    "published": "Fri, 25 Jul 2025 18:00:02 +0000",
    "author": "Julianne Pepitone",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Here Comes the World Wide Web of Everything",
    "link": "https://spectrum.ieee.org/spatial-web-standard",
    "summary": "When it was invented in 1991, the World Wide Web connected an Internet that was overrun with many thousands of individual, fragmented digital documents. HTML, Hypertext Markup Language, represented a daring leap. It combined the age-old idea of hypertext with the Internet’s global reach. Tim Berners-Lee ’s new language offered up a lingua franca for interconnected information. Today, following the social media revolution , a new phase of the Internet is emerging. The Spatial Web promises to connect a physical world full of devices, phones, wearables, robots, drones, and even AI agents. In May, the IEEE Standards Association [which shares a parent organization with IEEE Spectrum ] ratified a set of standards ( IEEE 2874-2025 ) that defines the Spatial Web. The original World Wide Web introduced the idea of URLs that point to HTML files, which are accessed remotely via the HTTP standard . Now the Spatial Web puts forward a new set of defining principles. HSML (Hyperspace Modelling Language) behaves like nouns and verbs on the Spatial Web, describing what an entity is and what it does. HSTP (Hyperspace Transaction Protocol) behaves like the Spatial Web’s grammar, defining how each entity functions and how it can interact with others. And the UDG (Universal Domain Graph) acts as the directory that keeps track of every entity along with its activities and relationships. [See chart.] The Spatial Web defines new ways for devices to interact with the physical world Protocol Purpose Mnemonic HSML (Hyperspace Modelling Language) Describes what a Spatial Web entity is and what it does Nouns and verbs HSTP (Hyperspace Transaction Protocol) Governs how Spatial Web entities negotiate and enforce system policies Grammar rules UDG (Universal Domain Graph) Catalogs and links registered entities, activities, and permissions Continuously updated directory We’ll come to some of the practicalities of the Spatial Web a little later. (For instance, where these various files might be stored, and how each entity can stay up-to-date with everything else in its network.) But for now, let’s first consider a few specific ways devices and AI agents can use the Spatial Web—via HSML, HSTP, and UDG standards—to more seamlessly interact with people, objects, and physical spaces. EcoNet Gives Thermostats and Batteries the Power to Make Deals Verses , the Los Angeles–based AI company where we work, recently collaborated with researchers at University College London on a project called EcoNet, a test home where two AI agents —one controlling a thermostat, the other a wall-mounted energy-storage battery system—worked together to keep the space comfortable while saving money and cutting emissions. Every 10 minutes, the AI agents evaluated 729 possible strategies to balance comfort, cost, and carbon footprint. The thermostat prioritized occupant safety and warmth. The battery agent focused on charging during off-peak hours and using stored energy during expensive periods. It used HSML to describe a set of competing goals. One goal involved keeping the living room between 22 and 25 °C. A second goal involved avoiding discharging the house’s energy storage below 50 percent during peak hours. Here’s how that looks in HSML code: A new Spatial Web standard defines Hyperspace Modelling Language (HSML), above, which helps define how devices can interact with the physical world. Spatial Web Foundation The Spatial Web’s shared digital network protocol, the UDG , helped the smart devices work together in real time. Then, its secure communication method (via the HSTP standard) enabled decisions that followed the system’s rules and commands. The system adjusted automatically to changing weather conditions and energy prices—and cut both energy costs and carbon emissions by 15 to 20 percent. Verses demonstrated EcoNet in March at the 2025 AI UK exhibition at the Turing Institute in London. At scale, an EcoNet-like architecture might enable entire neighborhoods to act a little like intelligent organisms, optimizing collective energy use and accelerating the shift to a more resilient, renewable grid. Coordinated Mobility Standards Show Autonomous Vehicles the Way When an ambulance rushes to an emergency, the ambulance driver still depends on surrounding traffic to notice and react to the siren. But autonomous vehicles may not know which direction the ambulance is coming from or how to properly respond to the ambulance in time, because autonomous vehicles operate without shared context. The Spatial Web can address this shortcoming via HSML. A shared HSML document describes the state and relationships of things in a given neighborhood or at a given intersection. Properties being recorded in the HSML document might include the color, location, and behavior of a given traffic light. With this shared context, an ambulance can issue a Spatial Web query like “Find all autonomous vehicles and traffic infrastructure within 200 meters of my route.” Using the HSTP, it can request green lights, reroute cars, and alert pedestrians through connected devices. How Drones Can Use HSML to Read the Same Map Altitude limits, flight windows, and no-fly zones for drones today are difficult to enforce, in part because most drones follow static rules coded at the factory. They cannot respond to changing conditions or dynamic policies. The Spatial Web provides drones with the necessary context to navigate responsibly. Regulators can use HSML to define constraints like “no flights above 120 meters after sunset and within 500 meters of a hospital.” Those constraints would then be published to the UDG, where drones operating within the relevant airspace can apply these constraints in real time. Before takeoff, a drone might issue a Spatial Web query such as “What restrictions apply to my delivery route?” HSTP allows the drone to confirm its airspace authorization, share its intended path, and adjust mid-flight if conditions or regulations change. The same Spatial Web infrastructure can also be used in emergencies. After a natural disaster, drones could be temporarily authorized to enter restricted zones to assist with search and rescue or deliver supplies—all within a secure, trackable framework. Lunar Rovers Will Bring the Spatial Web to the Moon Coordinating autonomous systems in the air is difficult. In space, it is even more difficult. NASA’s Jet Propulsion Laboratory frequently collaborates with multiple agencies, universities, and contractors, with each using different simulation environments and proprietary platforms. Testing how multiple teams and rovers will one day cooperate on the moon requires a shared language and a common model of the rovers and environment. The Spatial Web makes this possible. In one demo, rover teams from The Jet Propulsion Laboratory in Pasadena, Calif., and California State University, Northridge , each operated their own digital twin and simulation environments using HSML to coordinate a simulated lunar rescue. When one virtual rover got stuck in a crater, HSML allowed the stuck rover to send out real-time geometry, sensor observations, and activity data to the other rovers nearby. The virtual rovers also shared internal models from different physics modeling engines , including parameters like position, velocity, acceleration, and mass. The rover simulation, in other words, demonstrated how HSML-powered digital twins can assist in autonomous collaboration over challenging environments—even on the (virtual) moon. Digital Orchards Use the Spatial Web for Zero-Waste Supply Chains Roughly one-third of global produce spoils before it ever reaches a plate, driving up emissions, reducing profits, and contributing to global hunger. However, using Spatial Web standards, for instance, a peach orchard could use HSML to describe the ripeness, temperature, and shelf life of each crate. These descriptions are published to the local UDG, where retailers can query live inventory across regions. Using the Spatial Web, a buyer might query their local network, “What peaches are ready to harvest within 500 kilometers and meet my freshness criteria?” HSTP can simplify the negotiation, delivery, and policy verification of such a query. If a buyer rejects a shipment, the grower can redirect it to a new buyer, such as a juicer or a nearby store, before the fruit goes to waste. Instead of rigid logistics and guesswork, Spatial Web supply chains have the potential to become more adaptive, intelligent, and responsive to both external demand and internal conditions. The result will be less spoilage, better margins, faster payments, and fresher food. The Road from Protocol to Practice The Spatial Web Standard is still in a very early phase. HTML was published in 1991, but the first browser didn’t arrive until 1993. Additional Web standards on top of that, like cascading style sheets (CSS), didn’t come in until 1996 . IEEE 2874 is similarly rolling out in stages. Ultimately the foundation we are laying in place paves the way for a Spatial Web that spans not so much pages and data files, but rather people, places, and things. Standards succeed only when they disappear into the background. No one thinks about TCP/IP standards when reading email, although email relies on these standards in every message that is sent or received. Similarly, no layperson will need to understand how standards like HSML, HSTP, or UDG work. These components of the Spatial Web will all simply, like other protocols and standards before it, just do the hard communication and computation work behind the scenes. Where, then, do HSML, HSTP, and UDG assets ultimately reside? Do they all sit on some cloud server somewhere? Or perhaps are these various digital files all scattered across individual devices and Internet of Things nodes? Unfortunately, there is no single answer to these pertinent questions. On the other hand, the World Wide Web didn’t launch fully formed either. Its earliest days often tested out trial implementations of new standards and technologies—because nothing like the truly widespread, instantaneous, global scale of the Web had ever been rolled out before. For the Spatial Web, simple agents like IoT devices , for instance, could host HSML files and other Spatial Web assets on-device. In more complex settings, like smart cities or industrial systems, cloud servers or shared storage systems would provide a more remote and cloud-based kind of HSTP, HSML, and UDG deployment. But no matter the Spatial Web implementation, whether fully remote or fully localized, cybersecurity will remain a key priority. HSML, HSTP, and UDG standards embed identity, access, and policy enforcement, via decentralized identifiers. Furthermore, the HSTP standard ensures that all transactions can be signed and auditable. Ultimately, too, another aspect of any Spatial Web deployment will be the registries that must scale to manage billions of entities and agents. That is a larger, later-stage question to be tackled, no doubt, in future implementations of the Spatial Web. Nevertheless, even in the Spatial Web’s earliest incarnations today, we have already abstracted these complex concerns behind a secure, standards-based interface. The standards that defined the World Wide Web connected information. The Spatial Web will begin to interconnect the physical world and the many devices and AI agents operating in it. And with the new Spatial Web standards—and trial runs in homes, streets, skies, and on the (virtual) moon—an increasingly interconnected Spatial Web future is no longer theoretical. A standardized Spatial Web is today as actual, and as actualizable, as HTML.",
    "published": "Thu, 24 Jul 2025 14:00:02 +0000",
    "author": "Gabriel René",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "The Engineer Who Secured Nigeria’s Democracy",
    "link": "https://spectrum.ieee.org/modernizing-nigerian-voting-system",
    "summary": "In October 2000, when electrical engineer Steve A. Adeshina joined Nigeria’s Independent National Electoral Commission (INEC) as director of information and communication technology, the country had just held its first successful democratic general elections in 17 years. The 1999 elections were generally peaceful, if not entirely reliable, according to independent observers . They were also technologically old-school: “When I arrived, things were done essentially manually,” Adeshina recalls, with some voters being registered by hand and others by typewriter. Adeshina, who had been running his own information technology firm, oversaw the transition to machine-readable voter registration forms across 120,000 polling units, many in rural, hard-to-reach places. To complete these forms, applicants fill in bubbles, the way it’s done on many standardized tests. Steve A. Adeshina Employer: Nile University of Nigeria Occupation: Professor of computer vision and engineering Education: Bachelor’s degree in electrical and electronics engineering, University of Ilorin; Ph.D. in computer vision and engineering, University of Manchester Over his decade-long tenure at the electoral commission, Nigeria conducted multiple elections with increasing technological sophistication. The 2015 presidential elections, the first to take place after Adeshina had left the electoral commission, earned positive reviews from independent observers and resulted in the first democratic transition of power between political parties in Nigeria. Now Adeshina, 63, is a professor of computer vision and engineering at Nile University of Nigeria , in Abuja, and his three sons are at the start of their own careers, all in engineering. Like many people his age, Adeshina has reached the point of dispensing advice to younger engineers, his sons included, based on his own long career. “The advice I have for them is to keep their minds open and be creative and innovative,” he says. That’s because surprises have cropped up throughout Adeshina’s own career. Keeping an open mind allowed him to take advantage of those surprises. Adeshina came to public service from the private sector, having run his own hardware and later software service company, Logica Solutions Limited, for about a decade. When INEC offered him a job, he “didn’t have an open mind about the public sector,” he says. “I didn’t think they did anything or that I would stay more than a few years. But I stayed 10 years.” Discovering Electrical Engineering Career surprises go back to Adeshina’s university days. Like many engineers, he recalls trying to fix everything that broke at home when he was young. So, he enrolled at the University of Ilorin , also in Nigeria, as a civil engineering student in 1981. That’s where the hot jobs were at the time. “Nigeria was being built; civil engineering was more popular,” he says. “The advice I have for [younger engineers] is to keep their minds open and be creative and innovative.” Alongside other aspiring mechanical engineers, Adeshina built a culvert and some small bridges. But on a rotation through the electrical engineering department, a standard component of his course, his professors challenged him to build his own power-supply unit and then design and build the cabling for an entire house on a circuit board, including distribution boards and household wall outlets, all by himself. He was surprised by how much he liked it. “That really, really excited me, and that’s what made up my mind,” Adeshina recalls. He switched to electrical engineering. Adeshina’s first job involved working on time-sharing computing on an early computer produced by North Star Computers . After three years there, he left to start Logica, where he began by adapting software designed for mainframes to work on less powerful but more affordable microcomputers suitable for the Nigerian market. But he was always looking for new problems to solve. Modernizing Nigeria’s Voting System By the time the INEC called Adeshina in to modernize voting in 2000, Nigeria was on the verge of big changes. The military that had ruled the country off and on between 1966 and 1999 had given way to democracy at the same time the Internet was gaining a tenuous foothold across Africa. Adeshina and others saw the potential to use the Internet to reinforce the fledgling civil society. INEC asked him if polling units could report preliminary results in real time, while poll workers finalized and certified ballot counts. The idea was to make the results more trustworthy by making it harder to manipulate results, or at least raise red flags. By the time Adeshina left INEC, he had helped enable real-time election results through cellular networks. Here, preliminary results are sent on a mobile phone during the 2011 parliamentary election. George Osodi/Panos Pictures/Redux At the time, 2G cellular networks in Nigeria “hadn’t really penetrated very far, but we were able to deploy radios that had the capacity to send email attachments, even [connect with] fax machines,” Adeshina says. Aid organizations donated Inmarsat satellite terminals for the hardest-to-reach polling units. “There are places that you cannot get to by a car. They use camels and maybe motorbikes to get to those places,” Adeshina says. In such places, voting occurs over several days to allow more participation. That adds to the challenge: Voting machines must have batteries to handle constant electrical-grid failures . It was a race against time to build the infrastructure for the 2002 elections. “We were posting [collated results] on the Internet and the results were available to anybody,” Adeshina says. By the time of those first off-peak elections, INEC was receiving real-time results from perhaps 80 percent of Nigerians, Adeshina estimates, and thus had pioneered a new technology. INEC’s new chairman then asked Adeshina to embark on a fresh registration drive. The challenge was to see if Adeshina and his team could improve the accuracy of voting rolls using fingerprints and photos. They discovered as many as 10 million duplicate registrations at a time when the entire population was around 126 million. He also came up with the predecessor to the country’s current voter-identification cards, which included photos of the voter and were machine readable. From Public Service to Academia By the time his INEC term ended in 2011, Adeshina found a perch at Nile University of Nigeria, in Abuja, the federal capital. There he has worked on a wide range of problems, including using inexpensive medical imaging to diagnose COVID-19 and exploring standards for 6G telecommunications. “He’s a respected voice in the digital world in Nigeria,” says Biodun Omoniyi, CEO of the broadband company VDT Communications and a former university classmate of Adeshina’s. Even years after leaving INEC, Adeshina finds himself thinking about the challenges of elections. Due to its similar infrastructure and literacy levels, he looks to India for how to incorporate fully electronic voting one day in Nigeria. “The time to start preparing for the 2031 election is now.… You need to build trust, to have several off-peak elections and see that it works,” Adeshina says. He now advises his sons and any young engineers to consider how they can apply their skills for their own country’s improvement. “I don’t want everyone to leave Nigeria,” he says. “I would like to have a world-class lab so we can keep some of our students.” If they are lucky, those students may get to apply their own engineering skills to the range of problems Adeshina has wrestled with. With a wealth of experience across subjects and sectors, Adeshina continues to find fulfillment in his work. “It seems to me I’ve lived three kinds of life: private sector, public sector, and now academia,” he says. “Looking back, I’m really very happy, but I’m not done yet.”",
    "published": "Wed, 23 Jul 2025 14:00:04 +0000",
    "author": "Lucas Laursen",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "The Internet of Things Gets a 5G Update",
    "link": "https://spectrum.ieee.org/internet-of-things-5g-mit",
    "summary": "A new chip component designed by MIT researchers promises to expand the reach of the Internet of Things into 5G. The discovery represents a broader push for 5G-based IoT tech—using the telecom standard’s low latency , energy efficiency , and capacity for massive device connectivity . The new research also signals an important step toward applications that include smaller, low-power health monitors, smart cameras, and industrial sensors, for instance. More broadly, the prospect of moving the IoT onto 5G means more things can connect more quickly with potentially greater data speeds and less battery drain. It also means trickier and more complicated circuits will need to be toiling away behind the data streams. And doing all this using 5G standards rather than equivalent 4G/LTE or Wi-Fi networks arguably means IoT is expanding its range and scope. It’s moving beyond relatively modest-sized IoT deployments to broader networks boasting the potential for hundreds of nodes or more. To clarify, however, says Soroush Araei , a Ph.D. candidate at MIT in electrical engineering and computer science , IoT-over-5G doesn’t mean that every node in a network will suddenly be getting its own phone number. “The main goal here is that you have a single radio receiver that can be reused for different applications,” Araei says. “You have a single piece of hardware which is flexible, and you can tune it across a wide frequency range in software.” Using 5G standards rather than 5G wireless networks allows IoT devices to frequency hop, to sip their battery power, and to use massive-connectivity tricks that allow for up to 1 million devices per square kilometer . How to Make a 5G IoT Chip On the other hand, the fact that IoT developers have to date been slow to adopt 5G underscores just how difficult the hardware challenge is. “For IoT, power efficiency is critical,” says Eric Klumperink, associate professor of IC design at the University of Twente in Enschede, Netherlands. “You want a decent radio performance for very low power—[using] a small battery or even energy harvesting.” But with more and more devices connecting to more and more networks, 5G or otherwise, other concerns rear their heads too. “In a world increasingly saturated with wireless signals, interference is a major problem,” says Vito Giannini , a technical fellow at Austin, Texas–based L&T Semiconductor Technologies . (Neither Giannini nor Klumperink were involved with the MIT group’s research.) Using 5G standards potentially addresses both issues, Araei says. Specifically, he says, the MIT group’s new tech relies on a slimmed-down version of 5G that’s already been developed for IoT and other applications. It’s called 5G reduced capacity (or 5G RedCap). “5G RedCap IoT receivers can hop across frequencies,” he says. “But they’re not required to be as low-latency as the top-tier 5G applications [including smartphones].” By contrast, the simplest IoT chip that uses Wi-Fi would rely on a single frequency band—perhaps 2.5 or 5 gigahertz—and could potentially seize up if too many other devices were using the same channel. Frequency hopping, however, requires robust radio communications hardware that can quickly switch between frequency channels as directed by the network and then ensure the frequency hops align with network instructions and timing. That’s a lot of hardware and software smarts packed into a tiny chip that might be just one of hundreds of minuscule devices affixed to pallets across an entire warehouse. But features like that are just the appetizers, Araei says. The centerpiece of any viable 5G RedCap chip is the hardware that can flexibly work across a range of frequencies, while still keeping to a tiny power budget and a modest overall cost for the device. (The MIT group’s tech can be used only for receiving incoming signals; other chip components would be needed to transmit across a similarly wide range of frequencies.) Here the researchers pulled a few tricks from the world of analog circuits and power electronics. But rather than bulk components layered and stacked like ceramic capacitors, the present work integrates these tricks into an on-chip system to miniaturize RF frequency hopping cheaply and efficiently. The researchers presented their work last month at the IEEE Radio Frequency Integrated Circuits Symposium in San Francisco. “This is kind of a switched-capacitor network,” Araei says. “You’re turning on and off these capacitors in a periodic manner sequentially, which is called ‘N-path structure.’ That generally gives you a low-pass filter.” Which means that rather than using a single capacitor in the circuit, the team used a miniaturized bank of capacitors to flick on and off in tune with the needs of the frequency range being received at the circuit. And because they could put all this frequency-filtering wizardry at the front end of the circuit, before the amplifier touches the signal, the team reports high efficiency at blocking out interference. Compared to conventional IoT receivers, they report , their circuit can filter out 30 times as much interference, while doing so using only single-digit milliwatts of power. In other words, the group appears to have designed some pretty effective low-power 5G IoT receiver circuitry. So who can design a similarly clever transmitter? Do both of those, and someone someday will be in business, says Klumperink. “There are arguments to be made for IoT-over-5G (or 6G),” he says. “Because spectrum is allocated and managed better than ad hoc Wi-Fi connections.” Running the Internet of Things over 5G realistically means operating with very low power requirements. The MIT team’s chip consumes less than a milliwatt while still filtering out extraneous signals. Soroush Araei Is This the Stuff of 5G IoT Chips to Come? The MIT group’s circuitry, Klumperink says, could conceivably be manufactured at a mainstream chip fab. “I don’t see big hurdles as the circuit is implemented in mainstream CMOS technology,” Klumperink says. (The group’s circuits demand only a 22-nanometer fabrication process, so it wouldn’t need to be a bleeding-edge foundry by any stretch.) Araei says the team aims next to work on eliminating a need for a battery or other dedicated power supply. “Is it possible to get rid of that power supply and basically harness the power from the existing electromagnetic waves in the environment?” Araei asks. He says they also hope to extend the frequency range for their receiver tech to cover the whole frequency range of 5G signals. “In this prototype we were able to achieve low frequencies of 250 megahertz up to 3 GHz,” he says. “So is it possible to extend that frequency range, let’s say, up to 6 GHz, to cover the entire 5G range?” If these various upcoming hurdles can be cleared, says Giannini, a range of applications probably appear on the near-term horizon. “It offers an advantage for mobility, scalability, and secure wide-area coverage in midrange and mid-bandwidth scenarios,” he says of the MIT group’s work. He adds that the new circuit’s 5G IoT adaptability could make the tech well suited for “industrial sensors, some wearables, and smart cameras.”",
    "published": "Wed, 09 Jul 2025 14:48:50 +0000",
    "author": "Margo Anderson",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Future Wireless Comms Could Process Data in Midair",
    "link": "https://spectrum.ieee.org/wireless-communication-over-air-processing",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore . It’s easy to take for granted the seamless way information is preprocessed, transmitted wirelessly, and nicely processed on another device. But the future of wireless communications may be even more sophisticated, as scientists work toward a concept in which data isn’t just transmitted wirelessly but also processed in midair. The concept, called over-the-air computation—or AirComp for short—is still in its early stages. A team of researchers from China and Singapore has demonstrated through simulations that their approach allows multiple clusters of data to be processed in midair simultaneously. The results are described in a study published 30 May in IEEE Transactions on Wireless Communications . Xiao Tang is an associate professor at Xi’an Jiaotong University, in China. He says that AirComp is particularly useful when data needs to be collected from multiple devices at the same time, for example in the context of environmental sensors, smart cities, or to efficiently update AI models across multiple user devices. “Instead of sending data from each device separately, which can be slow and use a lot of resources, AirComp cleverly allows devices to transmit data at the same time using the same wireless channel,” Tang says. Efficient Data Processing With AirComp As the signals are transmitted, they naturally combine or superpose in the air. In a mathematical sense, it’s akin to a sum or average of signals, which is then picked up by the receiver. As a result of these superimposed signals, there’s no need to individually process each of the original signals. “This makes collecting and processing data from a huge number of devices incredibly fast and efficient,” Tang says. In their study, Tang and colleagues wanted to create an AirComp approach that could transmit and process multiple clusters of data—each completing a distinct calculation—in midair simultaneously. The challenge here is to effectively alleviate the interference among the clusters. Tang compares it to crosstalk on a phone line. To overcome this issue, the researchers combined two different machine learning techniques to achieve their novel AirComp approach. The first one, called an unfolding algorithm, is used to methodically identify optimized calculations for AI models, while the second one, called a graph neural network, is useful for learning to identify and reduce interference of signals across clusters. In their study, the researchers tested their novel approach through simulations with varying amounts of antennas, devices, and clusters, and across distances of 100 to 1,000 meters. They also compared the performance of their novel unfolded deep graph learning method against traditional AirComp methods. “Our proposed model consistently outperformed the other methods, achieving a higher overall computation rate,” says Tang. “This advantage was especially evident in scenarios with heavy interference, where traditional methods struggled.” He adds that the novel model, once trained, was able to “produce a highly effective solution instantly,” and was able to handle more data clusters than it was initially trained to handle. For example, even though the model was trained to handle five clusters of data, it still performed well when handling 10 clusters. The research team has several different goals to build upon this work moving forward, including testing the approach in real-world settings using hardware. Tang says his team is also interested in extending the model to handle a wider variety of midair calculations. “ While this study focused primarily on the ‘sum’ function, we plan to extend the framework to handle a wider variety of more complex mathematical functions over the air, which would broaden its applications,” he says. This article appears in the September 2025 print issue as “ Data Processing in Midair .”",
    "published": "Tue, 01 Jul 2025 14:00:03 +0000",
    "author": "Michelle Hampson",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Exploring the Science and Technology of Spoken Language Processing",
    "link": "https://spectrum.ieee.org/international-conferences-sydney",
    "summary": "This is a sponsored article brought to you by BESydney . Bidding and hosting an international conference involves great leadership, team support, and expert planning. With over 50 years’ experience, Business Events Sydney (BESydney) supports academic leaders with bidding advice, professional services, funding, and delegate promotion to support your committee to deliver a world-class conference experience. Associate Professor Michael Proctor from Macquarie University’s Department of Linguistics recently spoke about his experience of working on the successful bid to host the Interspeech 2026 Conference in Sydney, on behalf of the Australasian Speech Science and Technology Association (ASSTA). Why Bid for a Global Event? Interspeech is the world’s largest and most comprehensive conference on the science and technology of spoken language processing. The conference will feature expert speakers, tutorials, oral and poster sessions, challenges, exhibitions, and satellite events, and will draw around 1,200 participants from around the world to Sydney. Interspeech conferences emphasize interdisciplinary approaches addressing all aspects of speech science and technology. Associate Professor Proctor is Director of Research in the Department of Linguistics at Macquarie University, where he leads the Phonetics Laboratories. Under the leadership of Professor Felicity Cox at Macquarie University, Associate Professor Proctor worked in partnership with Associate Professor Beena Ahmed and Associate Professor Vidhya Sethu at the University of NSW (UNSW) to prepare the bid on behalf of ASSTA. Every breakthrough begins with a conversation. Become a Global Conference Leader and be the voice that starts it all. BESydney’s Global Conference Leaders share their voice and leadership vision to bid and host for a global conference that drives change and shapes the future of academic and industry sectors, with BESydney’s trusted advice, guidance and support at every step of the way. BESydney “Organizing a major international conference is an important service to the scientific community,” says Associate Professor Proctor. A primary motivation for bringing Interspeech 2026 to Sydney was to highlight the rich multilingual landscape of Australasia and refocus the energies of speech researchers and industry on under-resourced languages and speech in all its diversity. These themes guided the bid development and resonated with the international speech science community. “Australasia has a long tradition of excellence in speech research but has only hosted Interspeech once before in Brisbane in 2008. Since then, Australia has grown and diversified into one of the most multilingual countries in the world, with new language varieties emerging in our vibrant cities,” stated Associate Professor Proctor. Navigating the Bid Process Working with BESydney , the bid committee were able to align the goals and requirements of the conference with local strengths and perspectives, positioning Sydney as the right choice for the next rotation of the international conference. Organizing a successful bid campaign can offer broader perspectives on research disciplines and academic cultures by providing access to global networks and international societies that engage in different ways of working. “Organizing a major international conference is an important service to the scientific community. It provides a forum to highlight our work, and a unique opportunity for local students and researchers to engage with the international community.” —Associate Professor Michael Proctor, Macquarie University “Although I have previously been involved in the organization of smaller scientific meetings, this is the first time I have been part of a team bidding for a major international conference,” says Associate Professor Proctor. He added that “Bidding for and organizing a global meeting is a wonderful opportunity to reconsider how we work and to learn from other perspectives and cultures. Hosting an international scientific conference provides a forum to highlight our work, and a unique opportunity for local students and researchers to engage with the international community in constructive service to our disciplines. It has been a wonderful opportunity to learn about the bidding process and to make a case for Sydney as the preferred destination for Interspeech.” Showcasing Local Excellence One of the primary opportunities associated with hosting your global meeting in Sydney is to showcase the strengths of your local research, industries and communities. The Interspeech bid team wanted to demonstrate the strength of speech research in Australasia and provide a platform for local researchers to engage with the international community. The chosen conference theme, “Diversity and Equity – Speaking Together,” highlights groundbreaking work on inclusivity and support for under-resourced languages and atypical speech. Interspeech 2026 in Sydney will provide significant opportunities for Australasian researchers – especially students and early career researchers – to engage with a large, international association. This engagement is expected to catalyze more local activity in important growth areas such as machine learning and language modeling. Interspeech 2026 will be an important milestone for ASSTA . After successfully hosting the International Congress of Phonetic Sciences (ICPhS) in Melbourne in 2019, this will be an opportunity to host another major international scientific meeting with a more technological focus, attracting an even wider range of researchers and reaching across a more diverse group of speech-related disciplines. “It will also be an important forum to showcase work done by ASSTA members on indigenous language research and sociophonetics – two areas of particular interest and expertise in the Australasian speech research community,” says Associate Professor Proctor. Looking Ahead Interspeech 2026 will be held at the International Convention Centre (ICC) Sydney in October, with an estimated attendance of over 1,200 international delegates. aspect_ratio The larger bid team included colleagues from all major universities in Australian and New Zealand with active involvement in speech science, and they received invaluable insights and support from senior colleagues at the International Speech Communication Association (ISCA). This collaborative effort ensured the development of a compelling bid which addressed all necessary aspects, from scientific content to logistical details. As preparations for Interspeech 2026 continue, the Sydney 2026 team are focused on ensuring the conference is inclusive and representative of the diversity in speech and language research. They are planning initiatives to support work on lesser-studied languages and atypical speech and hearing, to make speech and language technologies more inclusive. “In a time of increasing insularity and tribalism,” Associate Professor Proctor says, “we should embrace opportunities to bring people together from all over the world to focus on common interests and advancement of knowledge, and to turn our attention to global concerns and our shared humanity.” For more information on how to become a Global Conference Leader sign up here .",
    "published": "Fri, 23 May 2025 14:00:03 +0000",
    "author": "BESydney",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Opera Includes AI Agents in Latest Web Browser",
    "link": "https://spectrum.ieee.org/agentic-ai-opera-mini",
    "summary": "The Opera Web browser, first introduced 30 years ago , has over its long tenure helped to pioneer features that would later become commonplace among all Web browsers—including tabs, sync, and built-in search. Opera was among the first to introduce a built-in AI assistant (Aria) as well as the ability to use locally running models with its developer version. Now, Opera aims to be the first to offer a new kind of AI agent–based browsing, with a feature called Browser Operator . AI agents are an emerging trend in artificial intelligence, built around AI-powered assistants that perform extended tasks beyond a single query or command-line action. And many tech observers argue agent-based (or “agentic”) AI will be a big deal in the years ahead. At the company’s Opera Days event last month , Henrik Lexow , director of product marketing technologies, demonstrated the multifaceted versatility of agentic AI. In one demo, he booked a complicated travel itinerary; in another, he ordered flowers to be delivered to an event attendee. The Opera browser runs on a range of platforms from high-end gaming devices ( Opera GX ) to low-end phones ( Opera Mini ) . Mini is Opera’s most popular browser, with nearly 70 million monthly active users in Africa alone, and over 1 billion downloads worldwide from the Google Play Store . The Global Reach of Opera Mini Launched 20 years ago in 2005, Opera Mini gave users access to the Internet on lower-end consumer devices, especially feature phones. While the low-end phone marketplace today has expanded to include some smartphones, Internet access limitations and throttled data plans of old are still a going concern around the globe. So Opera Mini continues to combine page compression and snapshotting to reduce the requirements of today’s resource-intensive websites. Instead of loading pages directly from the source, Mini has the option of loading them from a snapshot on Opera’s servers, removing excessive JavaScript or video to render the page more manageable over low-data connections. Despite the different browser variants, each Opera version is built upon the same AI Composer Engine . For Opera Mini and its user base, this gives access to third-party AI models that typically need a powerful device to run locally, or have high costs to access as a service. With the forthcoming version 2.0, Aria reportedly will prioritize even more the system’s response speed. “Everyone gets the same experience,” says Tim Lesnik , Opera Mini’s product manager. “Where Aria is available in a particular country, there are no limitations imposed in any way, shape, or form.” However, patterns differ within user groups and within different countries, says Monika Kurczyńska , Opera’s AI R&D lead. For example, browser usage in Brazil and Nigeria from students peaks during the school year, and then drops off again during school holidays—so much so that initially the Opera team were worried that Aria had stopped working in those countries. “The first time that happened, we were like, my goodness, what’s happened here? Something must have broken,” says Lesnik. Opera’s and Aria’s Many Languages Aria supports more than 50 languages, and for each of these, it provides prompt examples to get users started. “ We’ve got a range of different prompts,” says Lesnik. “Those prompts are all the same in the different countries, but they are translated right now. What we know we need to do better is understand that users in Nigeria are using Aria in a different way from users in Indonesia.” Language support in large language models (LLMs) is inconsistent outside of globally popular languages including English, French, Chinese, and Spanish. Yet, as with prompt examples, an LLM can often translate questions and answers it doesn’t have direct responses to. Kurczyńska, who is Polish, said LLMs treat different languages—and the number of tokens (the building blocks of text that an LLM understands) each language requires—quite differently. “Different languages act and behave in different ways in LLMs,” says Kurczyńska. “For example, [using] the same sentence with a similar number of characters in Polish and English, the LLM uses more tokens in Polish.” While work remains to make all features production-ready, bringing agentic browsing to hundreds of millions of Opera users globally, especially those in parts of the world often ignored by larger technology brands, is a mammoth task. Hugging Face , a popular repository of AI models, has nearly 200,000 models that support English , but only 11,000 that support Chinese , and less than 10,000 that support Spanish . In March, in fact, researchers in Singapore introduced what they called Babel, an LLM they claim can support 90 percent of the world’s speakers in a single model. At Opera, Lesnik and Kurczyńska say they plan to tackle the many-language problem through AI feature drops every two weeks, across parallel public-developer and beta versions of the company’s browsers. This story was updated on 15 May, 2025 to change Opera’s affiliation (Norwegian, not Chinese-Norwegian as a pervious version of this story stated), as well as clarify details of Opera’s AI models concerning capability and variability among the range of Opera browsers available today. Also, a misspelling of Opera Mini product manager Tim Lesnik’s name was corrected.",
    "published": "Wed, 14 May 2025 14:00:03 +0000",
    "author": "Chris Chinchilla",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "China Makes High-Speed Laser Links in Orbit",
    "link": "https://spectrum.ieee.org/satellite-internet-china-crosslink",
    "summary": "A commercial Chinese firm has demonstrated ultrafast data laser transmission between two satellites, marking a step forward for the country’s communications megaconstellation plans. Laser Starcom , a commercial aerospace firm established in Beijing in 2020, announced in March that it had achieved a 400-gigabit-per-second communications link between satellites. Its two satellites, Guangchuan 01 and 02, launched into low Earth orbit (LEO) in November last year on a commercial Zhuque-2 rocket developed by the Beijing-based Landspace company. The pair of Guangchuan spacecraft completed their optical transmission test on 18 March, according to a Laser Starcom statement , across a separation between satellites of 640 kilometers. The company said the test established an enhanced intersatellite, or crosslink, communication system, supporting future high-bandwidth space networks. In addition to showcasing the growth of the nation’s satellite Internet development, the news further highlights the role of commercial companies in China’s growing space ambitions. Laser Starcom did not release all the details of its test. However, its statement noted that the satellites transmitted 14.4 terabytes of “business data” over a total communication time of 6 minutes and 44 seconds. (Doing the math on these numbers yields less than 300 Gb/s of bandwidth. The quoted 400 Gb/s rate, however, includes other components, including service and protocol data.) The company also reported that the satellite links remained stable over time, with tracking errors of less than five microradians (0.0002865 degrees). The tracking precision, the company says, was essential to the laser-link test. While terrestrial fiber optics require no pointing precision, laser optical communications in orbit require ultraprecise steerable telescopes to be able to send beams either to another satellite or, adding in atmospheric complications, to the ground. The fact of being in orbit brings additional challenges. Satellites in LEO —a region above Earth at an orbital altitude of between 300 and 2,000 km—travel at approximately 28,000 kilometers per hour, or 7.8 kilometers per second. “Those steerable telescope assemblies need to compensate for the high speed that satellites are traveling at in space,” Harald Hauschildt, head of the European Space Agency ’s Optical and Quantum Communication Office , told IEEE Spectrum . So fast, steerable mirrors are needed to keep crosslink tracking during signal transmissions—because even tiny misalignments of the laser traffic can break the connection. What Are the Challenges of Laser Satellite Communications? The challenges Laser Starcom and other commercial and research efforts around the world face are worthwhile, because laser links operate at far higher frequencies than radio waves. The European Space Agency is working with partners to develop its High-throughput Digital and Optical Network ( HydRON ) laser-based satellite system and aims to demonstrate optical communications networks in orbit at speeds up to 100 Gb/s and higher. This data rate, according to the ESA, is expected ultimately to be scalable to one terabit per second . And according to Hauschildt, HydRON could in that sense one day compete with terrestrial fiber-optic networks. The E.U. is in fact developing a satellite communications constellation, IRIS² , in response to SpaceX’s Starlink as well as China’s Guowang and Qianfan constellations . Jade Wang , of MIT’s Lincoln Laboratory , was part of the team that developed TBIRD , a compact payload on a small satellite that demonstrated a 200 Gb/s space-to-ground link in 2023 and tackled some of the added challenges of transmitting through the atmosphere—including atmospheric turbulence scattering or distorting the beam. The Lincoln Lab researchers achieved this high-data-rate optical link with a commercial off-the-shelf optical transceiver. “We developed an automatic repeat request protocol, and we wrapped it around the existing transceiver, and we did it in a way that was efficient and that allowed us to get error-free performance space-to-ground,” says Wang. That test followed a long line of experiments, including the 2013 NASA lunar laser communication demonstration (LLCD), that solved challenges such as achieving a high accuracy of pointing for the beams. Regarding the Laser Starcom test, Wang says it appears to have achieved a higher data rate than has been achieved globally before. “But if they’re still using commercial technologies, it’s more of an incremental change than a technological shift,” Wang says. What Are the Next Steps for Laser Satellite Communications? The already operational Starlink uses laser crosslinks of around 100 Gb/s. But the speeds achievable, Wang says, represent trade-offs among a number of parameters including terminal size, power usage, and data rate. “So if you want more data rate, you have to have more power and/or a larger class [of terminal],” Wang says. “It’s just about what the mission wants.” Laser Starcom’s website says the company’s terminals support multiple communication rates of 10 Gb/s, 100 Gb/s, and 400 Gb/s. These scalable rates could suit different mission requirements. While satellite crosslinks are now fundamental to the proliferated LEO satellite communications systems noted above, there are other uses. Space-to-ground laser communications can boost the amount of data sent back to Earth in one transmission. Remote sensing satellites collect huge amounts of Earth observation data but have only about 5 minutes of visibility to a ground station as they whizz across the sky. Laser communications, with vastly higher transmission rates compared with those of radio frequencies, can significantly increase the volume of data returned to Earth during each ground pass. The same goes for data-heavy science missions. Laser comms could also support countries’ lunar plans, which in China’s case is the International Lunar Research Station . “What I’m hoping to see is a robust industry, supporting free-space optical communications for multiple applications, and not just proliferated LEO,” says Wang. “I’m very excited with what’s happening in laser communication and for where this field is going next.”",
    "published": "Mon, 12 May 2025 15:00:03 +0000",
    "author": "Andrew Jones",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Breaking 6G Barriers: How Researchers Made Ultra-Fast Wireless Real",
    "link": "https://content.knowledgehub.wiley.com/on-demand-digital-event-innovating-for-6g-with-keysight-northeastern-university/",
    "summary": "Keysight visited 6G researchers at Northeastern University who are working to overcome the challenges of high-speed, high-bandwidth wireless communication. They shared concepts from their cutting-edge research, including overcoming increased path loss and noise at higher frequencies, potential digital threats to communication channels, and real-time upper-layer network applications. During this event, you will gain insights into the following 6G topics: Using broadband MIMO systems to increase data throughput and transmission distance. Emulating an eavesdropping attack on a 6G signal to test for vulnerabilities. Testing real-time sub-THz for network research. Register now for this free webinar!",
    "published": "Mon, 12 May 2025 12:00:03 +0000",
    "author": "Keysight",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Master 5G and 6G Basics With IEEE’s New Training Program",
    "link": "https://spectrum.ieee.org/ieee-5g-and-6g-training",
    "summary": "As 5G network capabilities expand globally, the need for trained engineers who know the protocols and procedures required to set up and manage telecommunications systems grows. The newest telecom generation has brought higher data transmission speeds, lower latency, and increased device connectivity to a wide variety of devices used for health care, entertainment, manufacturing, and transportation. Remote surgery, self-driving cars, real-time industrial monitoring, and immersive virtual reality experiences are just some of the innovations that 5G has made possible. The more recent enhancements, known as 5G-Advanced , include the integration of artificial intelligence and machine learning solutions to enable more intelligent network management. The developments are paving the way for 6G, expected to be commercially available by 2030. Key differences between 5G and 6G are expected to include enhanced scalability, increased utilization of the radio spectrum, and dynamic access to different connection types including cellular, satellite, and Wi-Fi. The improvements likely will result in more reliable connections with fewer interruptions—which is essential for supporting drones, robots, and more advanced technologies. To get engineers up to speed on the two technologies, IEEE and telecom training provider Wray Castle have launched the 5G/6G Essential Protocols and Procedures Training and Innovation Testbed . Self-paced learning, course books, and videos The training program provides a deep dive into essential 5G protocols including the network function (NF) framework, registration processes, and packet data unit (PDU) session establishment. The NF framework supports functions required for 5G networks to operate. Registration processes involve the steps needed for devices to connect to the network. PDU session establishment is the procedure for setting up data sessions between devices and the network. The comprehensive training includes 11 hours of on-demand, self-paced online learning, illustrated digital course books, and instructional videos. Learners receive three months of access to the IEEE 5G/6G Innovation Testbed , a cloud-based, private, secure, end-to-end 5G network testing platform that offers hands-on experience. The platform is for those who want to try out their 5G enhancements, run trials of future 6G functions, or test updates for converged networks. Users may test and retest as many times as they want at no additional cost. Key differences between 5G and 6G are expected to include enhanced scalability, increased utilization of the radio spectrum, and dynamic access to different connection types including cellular, satellite, and Wi-Fi. Tailored for professionals such as system engineers, integrators, and technical experts, the program provides knowledge and practical skills needed to excel in the evolving telecommunications landscape. After successful completion of the training program, learners earn an IEEE certificate bearing 11 professional development hours—which can be shared on résumés and professional networking sites such as LinkedIn . Ensure your mobile network expertise is up to speed on the latest advancements in wireless technology. Complete this form to learn more. This article appears in the September 2025 print issue.",
    "published": "Tue, 06 May 2025 18:00:03 +0000",
    "author": "Angelique Parashis",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Ready to Optimize Your Resource Intensive EM Simulations?",
    "link": "https://content.knowledgehub.wiley.com/indoor-propagation-modeling-using-wipl-d-software-part-i/",
    "summary": "This paper explores the use of WIPL-D software for simulating indoor electromagnetic (EM) propagation in both 2D and 3D, addressing the growing need for accurate modeling due to increasing electronic device usage. While 3D simulations offer detailed wave propagation analysis, they require substantial computational resources, especially at high frequencies, whereas 2D simulations - assuming an infinite structure with a constant cross-section - provide a computationally efficient alternative with minimal accuracy loss for many practical scenarios. The study examines the effects of material properties (e.g., concrete vs. metallic pillars) on signal distortion and evaluates different signal types, such as Dirac delta and Gaussian pulses, concluding that 2D modeling can often serve as a viable, resource-saving substitute for 3D simulations in telecommunication applications for smart environments. In this Whitepaper You’ll Learn: The trade-offs between 2D and 3D EM modeling for indoor scenarios. Practical strategies for reducing computational resources without losing accuracy. Why 2D EM modeling can be a game-changer for indoor propagation simulations. Download this free whitepaper now!",
    "published": "Tue, 06 May 2025 15:07:09 +0000",
    "author": "WIPL-D",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Rugged Microdata Centers Bring Rural Reliability",
    "link": "https://spectrum.ieee.org/rural-data-centers",
    "summary": "Rural connectivity is still a huge issue. As of 2022, approximately 28 percent of Americans living in rural areas did not have access to broadband Internet, which at that time was defined by 25 megabits per second for download speeds and 3 Mb/s for upload speeds by the Federal Communications Commission (FCC). As of 2024, the FCC came out with a new benchmark with higher speed requirements—increasing the number of people whose connections don’t meet the definition. One potential solution to the problem is small, rugged data centers with relatively old, redundant components, placed strategically in rural areas such that crucial data can be stored locally and network providers can route through them, providing redundancy. “We are not the big AI users,” said Doug Recker, the President and Founder of Duos Edge AI , in a talk delivered at the Data Center World conference in Washington, D.C., earlier this month. “We’re still trying to resolve the problem from 20 years ago. These aren’t high-bandwidth or high-power data centers. We don’t need them out there. We just need better connectivity. We need robust networks.” The Jacksonville, Fla.–based startup provides small data centers (about the size of a shipping container) to rural areas, mostly in the Texas panhandle. It recently added such a data center in Amarillo, working with the local school district to provide more robust connectivity to students. The school district runs its learning platform on Amazon Web Services (AWS) and can now store that platform locally in the data center. Previously, data had to travel to and from Dallas, over 500 kilometers away. Network outages were a common occurrence, impeding student learning. Recker’s company paid the upfront cost of US $1.2 million to $1.5 million to build the 15-cabinet data center, which it calls a pod. Duos is making the money back by charging a monthly usage and maintenance fee (between $1,800 and $3,000 per shelf) to the school district and other customers. The company follows a ”build what’s needed and they will come” approach. Once the data center is installed, Recker says, existing network providers colocate there, providing redundancy and reliability to the customers. The pod provides a seed around which network providers can build a hub-and-spoke-type network. Three Requirements for Edge Data Centers The trick to making these edge data centers financially profitable is minimizing their energy usage and maximizing their reliability. To minimize energy use, Duos uses relatively old, time-tested equipment. For reliability, every piece of equipment is duplicated, including uninterruptible power supply batteries, generators, and air-conditioning units. They also have to locate the pods in places where there would still be a large enough number of potential customers to justify building a 15-rack pod (the equipment is rented out per rack). The pods are unmanned, but efficient and timely maintenance is key. “Say your AC unit goes down at 2:00 in the morning,” Recker says. “It’s redundant, but you don’t want it to be down, so you have to dispatch somebody who can get into a pod at 2:00 in the morning.” Duos has a system for dispatching maintenance workers and an auditing standard that remotely keeps track of all the work that has been done or needs to be done on each piece of equipment. Each pod also has a clean room to prevent maintenance workers from tracking in dust or dirt from outside while they work on repairs. The compact data center allows the Amarillo school district to have affordable and reliable connectivity for their digital learning platform. Students will soon have access to AI-powered tools, simulations, and real-time data for their classes. “The pod enables that to happen because they can compute on site and host that environment on site where they couldn’t do it before because of the latency issues,” says Recker. Duos is also placing pods elsewhere in the Texas panhandle, as well as in Florida. And they’re getting so much demand in Amarillo that they’re planning to install a second pod. Recker says that although they initially built the pod in collaboration with the school district, other local institutions quickly became interested as well, including hospitals, utility companies, and farmers.",
    "published": "Wed, 30 Apr 2025 13:00:03 +0000",
    "author": "Dina Genkina",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "As Nigeria’s Cashless Transition Falters, POS Operators Thrive",
    "link": "https://spectrum.ieee.org/nigeria-money",
    "summary": "Cash is expensive in Nigeria. When undercover agents for the Central Bank of Nigeria tried to buy cash on the open market, they found sellers charging markups of 20 to 40 percent of face value, the bank governor, Olayemi Cardoso, said at a March event in Abuja . Since 2012, the Central Bank has promoted a series of policies to reduce the amount of cash in circulation and shift Nigerians to electronic payments, which are lower cost, more secure, and more traceable. The Central Bank releases limited cash to commercial banks, who in turn cannot match public demand. When the banks do have cash, middlemen often take it in bulk to sell onward at a higher price. In exchange, the Central Bank also built an ever-more-capable digital infrastructure for electronic payments, boosting Nigeria’s financial technology industry, and the volume of electronic payments in Nigeria grew around 16 times from 2018 to 2024 . “Once that foundation was there, the cashless economy has done well,” says electrical engineer Funke Opeke , an eminence in the Nigerian technology scene who founded and later sold a crucial telecommunications and data services company, MainOne. On the one hand, that is a victory. On the other hand, only those with reliable access to the Internet (about half of Nigeria’s population) can count on electronic payments. The rest still need cash. Nigerians Rely on Point-of-Sale Operators Agricultural consultant ‘Gboyega Osobu , in Lagos, once had a client that developed a cashless pay product but soon ran up against reality: Managers needed cold hard cash when they went out to visit farmers in the field. Enter the point-of-sale (POS) operators. In urban peripheries and rural places with at least intermittent Internet connectivity, tens of millions of entrepreneurs have begun using POS terminals and bags of cash to do the work that conventional banks do not: provide an avenue to deposit or withdraw cash and make transfers. POS operators throng commercial roads throughout Nigeria, with the acronym often hand-painted on a shack or nearby wall. Other operators walk around markets, looking like restaurant waitstaff come to collect payment, but offering their services to anyone who needs cash or to make a payment, for a 1 to 2 percent transaction fee. “The ordinary Nigerian prefers to go to the POS to get cash rather than go to the ATM,” says Zakari Isah , a POS operator and general secretary of the Association of POS Users of Nigeria (APOSUN). That may be because getting and using cash in Nigeria is a quest unsuitable for the faint of heart. The biggest available bill is 1,000 naira , equivalent to about US $0.63 (the average daily wage is around $5). There are only so many notes you can carry in even the fattest wallet, and that’s if you can find an ATM. There were only 21,500 ATMs ( as of 2024 ) serving Nigeria’s 220 million people, putting Nigeria ahead of the sub-Saharan African average, but behind middle-income countries. If you manage to find an ATM, it must have electricity , an Internet connection, and sufficient cash , none of which are guaranteed for long in Nigeria. Many banks limit daily withdrawals to 20,000 to 50,000 naira ($12.60 to $31.50) per day, and they still can’t keep up with demand. ATMs per 100,000 adults, per World Bank Nigeria (2021) 16.1ATMs United States (2009) 174 ATMs Euro area (2021) 62.8 ATMs Sub-Saharan Africa (2021) 6.9 ATMs In 2013, the Central Bank of Nigeria wrote regulations allowing retailers with POS terminals to offer “agent banking” services on behalf of banks. This included depositing and withdrawing money from an account, paying utility bills, and other simple transactions—but not more complex operations such as loans, insurance, or handling foreign currency. Isah started his POS operations in 2015, in which he retails not goods but cash and other financial services. He and other early adopters learned the hard way that it is wise to keep only small amounts of cash on hand if you operate on the street, and to stay in busy places, such as markets, ideally within the field of view of a security camera. As a particular business scales up above certain daily cash amounts, however, APOSUN recommends operators rent an office, hire security, and keep detailed customer records. Still, despite the risks and barriers, Nigeria now has somewhere between 20 million and 30 million POS operators, dwarfing the size of the conventional bank and ATM network. For an investment of as little as 30,000 naira, an entrepreneur can buy a POS terminal and start facilitating payments. If they front some of their own cash, which many buy from local businesses, they can soon begin selling cash, too. Academic researchers figure that both ATMs and POS payments on the whole have had a positive impact on the country’s economy. Yet they and Isah decry the effect of weak network connectivity on Nigerians’ financial lives. “Poor network service impedes transactions,” Isah says. There have been cases of network interruptions in the middle of a transaction that leave customers in the lurch and furious with their POS operator, he says. As Nigeria expands its broadband backbone to more and more rural areas, POS operators will surely follow, with their low-tech banking helping unbanked residents handle cash shortages until they, too, adopt mobile payments.",
    "published": "Wed, 30 Apr 2025 12:00:03 +0000",
    "author": "Lucas Laursen",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "These Companies Were the Patent Powerhouses of 2024",
    "link": "https://spectrum.ieee.org/patent-power-2025",
    "summary": "This article is best viewed on desktop. In 2006, IEEE Spectrum ranked patenting powerhouses in our first annual patent survey . The survey, conducted by the research firm 1790 Analytics , examined the number and influence of U.S. patents generated by more than 1,000 organizations. Semiconductor manufacturer Micron Technology came out on top at the time, with IBM, Hewlett-Packard, Intel, and Broadcom rounding out the top five. Nearly 20 years later, every company on the top 10 list has been usurped. Once mighty companies have fallen in the ranks, others have come and gone, and the top spots are largely filled by today’s Big Tech companies. In place of semiconductors and computer systems , the top categories in this year’s scorecard are all about Internet services —the category labeled “Telecom and Internet”—and consumer electronics . Digging into the data reveals Amazon ’s might, the hidden power of subsidiaries, and which countries are producing U.S. patents—it’s not just the United States. You can explore it all for yourself in the interactive graphic below. Simply click on a category to see which companies produced the most powerful patent portfolios in 2024. The rankings are based on Pipeline Power , a metric calculated by 1790 Analytics that combines several elements of an organization’s patent portfolio into one number. In addition to the number of patents granted in a given year, this metric takes into account four variables representing the quality and impact of those patents. (More details on the calculations are below in the Methodology section.) Amazon Tops the List At a glance, it’s clear that Amazon leads with the highest patent power. The tech giant has produced a more influential patent portfolio than entire industry categories. However, Amazon didn’t produce the largest number of patents in 2024. That achievement goes to Samsung ; with more than 9,000 patents, the electronics company was granted more than twice the number produced by the second most prolific company, Taiwan Semiconductor Manufacturing Co. (TSMC). Meanwhile, Amazon ranked 20th in terms of the raw number of patents. So how does it have the largest power? In short, because its patents tend to be cited more frequently, and by a variety of other patents. Similarly, Snap , the company that owns Snapchat and Bitmoji, ranks above more frequent filers Qualcomm and Google , despite being granted just 770 patents last year. Hidden Power in Subsidiaries Several companies have a greater patent power than is immediately visible, when you consider their subsidiaries. For example, Alphabet is categorized as a conglomerate with a patent power of about 4,056. But it has two subsidiaries: Google and Waymo, both in the Telecom and Internet category. Adding in its subsidiaries, Alphabet’s patent power would roughly double, achieving a score higher than those of all the other conglomerates combined. Defense contractor RTX (formerly Raytheon) has the most subsidiaries included in the data collection. Seven companies listed in the Aerospace category are all owned by RTX, in addition to RTX itself. Adding their combined patent power to RTX’s accounts for more than two-thirds of the combined patent power of all Aerospace companies. RTX is the only company with more than two subsidiaries included in the survey. Sony , Johnson & Johnson , and GE Vernova all have two each, and several other companies have one. In Consumer Electronics, Apple leads the pack by a large margin, with about 40 percent of the category’s patent power. Samsung has patents filed both under its primary company, Samsung Electronics, and a subsidiary that focuses on display technology. These two companies take the second and third place in the category. But even grouped together, their combined patent power is well behind Apple’s. Western Digital’s Sandisk Corp , in the Computer Peripherals category, is the subsidiary with the largest patent power for 2024. Sandisk has a score of about 5,087, more than triple that of its parent company. In fact, Sandisk’s successes in flash storage led to its launch as an independent public company in February of 2025. The AI Boom and Patents Splunk , owned by Cisco, tops the Computer Software category, beating out Microsoft , Oracle , and Intuit . This company specializes in collecting and organizing large amounts of machine-generated data. While Splunk is less of a household name than Microsoft, its research into managing data generated by AI has helped launch it to the top of its category. Overall, the number of AI patents filed has grown over the past few years, according to 1790 Analytics. Although many of these were submitted by organizations in the Computer Software category, some AI powerhouses fall under other categories, like Conglomerates. Which Countries Are Producing U.S. Patents? The companies represented here include only those with at least 25 patents granted by the U.S. Patent and Trademark Office in 2024. Out of 247 organizations that meet this requirement, 148 were from the United States. So where else are companies filing U.S. patents from? Japan comes in second place, with 24 companies. These companies span a range of categories, including Consumer Electronics, Computer Peripherals, and Semiconductors. The Japanese company with the highest patent power is Semiconductor Energy Laboratory, taking the top spot in its category. After Japan, Germany and South Korea are tied with nine companies each, and Taiwan has eight listed. The South Korean companies are nearly all in the Consumer Electronics or Automotive and Transportation categories, the Taiwanese companies are mostly in Computer Peripherals, and the German companies span a wider range of categories. Following these, France and China both have seven companies listed, and many more countries have one or two companies listed. Big Names, Small Patent Power Patent power is but one measure of a company’s impact. Some well-known companies have surprisingly small patent power, compared to their cultural or market influence. Microsoft, for example, ranks 31st in patent power, despite being one of the most valuable companies based on market capitalization . Meta also falls surprisingly low, considering the company’s hefty research and development budget. In 2024, the social media mammoth spent US $43 billion, an amount surpassed only by Alphabet in one survey. (Notably, the survey omitted Amazon because it doesn’t report R&D expenses as a separate line item. Amazon’s R&D spending likely exceeds that of Alphabet.) Looking at the strength of a company’s patent portfolio doesn’t replace these other metrics, but it does provide another view of an organization’s impact. Amazon, Apple, Snap, Samsung, and Qualcomm, in that order, are 2024’s victors. What companies will rise to the top in 2025? Methodology The scorecards are based on patents issued by the U.S. Patent and Trademark Office. Focusing on a single patent system helps 1790 Analytics more easily track which previous inventions are referenced by patents, because citation practices vary across different patent systems. Note that focusing on U.S. patents does not restrict the analysis to U.S.-based innovation; approximately half of all U.S. patents are from other countries. In constructing the scorecards, 1790 Analytics measured the following four metrics for each organization: Growth measures if an organization produced more patents in 2024 than in the previous five years. For example, if an organization averaged 100 patents per year between 2019 and 2023 and was granted 125 patents last year, its Pipeline Growth for 2024 would be 1.25. Impact measures how frequently all patents issued in 2024 cite a specific organization’s patents from the past five years. It is calculated by counting how many times that organization’s patents were cited and dividing that number by the average number of citations for all patents from the same technology. That number is then adjusted to remove high amounts of self-citations. This is done by discounting all self-citations that make up more than 30 percent of an organization’s total citations. Originality measures the variety in the sources of an organization’s 2024 patents. Patents that draw from a wide range of earlier technologies to create something new are deemed more original than patents that make incremental improvements upon the same technology. Generality measures how varied the 2024 patents are that cite earlier patents from a given organization. It is based on the idea that patents cited by later patents from many different fields tend to have more general application than patents cited only by later patents from the same field. Together, the total patents produced by an organization multiplied by its growth, impact, originality, and generality define that organization’s Pipeline Power . In the patent scorecard, organizations are ranked by their Pipeline Power, accounting for both the quality and quantity of their patents. Category descriptions Aerospace : These companies develop and manufacture commercial airplanes, spacecraft, and military aircraft. This includes companies that supply specific parts, like engines. Automotive and Other Transportation : Companies that develop cars and car parts, including electric and autonomous vehicles and their parts, are included in this category. Companies involved in making trains and motorcycles are as well. Biomedical Devices : Firms that manufacture or design systems for medical and health care applications, such as prosthetics, imaging systems, and drug-delivery systems are included in biomedical devices. Biotechnology and Pharmaceutical : Companies that use genetic engineering or chemical processes to discover and create new drugs and therapeutics are included in this category. Computer Software : These companies develop firmware and applications for personal devices and other computing systems. Computer Hardware : Companies that manufacture computer systems and storage devices fall under this category. Computer Peripherals : Companies that manufacture auxiliary devices that connect to computers, such as keyboards and mice, scanners, or printers are included in this category. Conglomerates : Organizations in this category don’t fit neatly into one category. Many companies patent in several industries, but tend to focus on one; others are trickier to label. 3M, for example, makes consumer products, medical devices, and adhesives, along with other products. Electrical Power and Energy : The patents in this category may belong to companies investing in renewables (like solar and wind power), energy infrastructure, and more. Consumer Electronics : These companies primarily make devices people can use in their everyday lives for fitness, cleaning, entertainment, and more. Government : Federal agencies, military groups, and national labs all fall into this category. While some may seem to fit well within a single type of patents—NASA in aerospace, for example—government agencies frequently patent across a range of industries. Robotics : With greater demand for automation, the robotics industry is growing. In particular, robots are being used to make manufacturing more efficient, but this category also includes home robots and surgical robots, for example. Semiconductors : This category includes patent producers that design and manufacture chips, as well as those producing the equipment needed for that manufacturing. The majority of these companies, however, focus on manufacturing chips. Telecom and Internet : Many of today’s tech giants—including Amazon, Google, and Meta—fall under Internet services. This category also includes home Internet and wireless providers. Universities : Like government entities and conglomerates, universities tend to produce patents across various industries.",
    "published": "Wed, 23 Apr 2025 19:30:03 +0000",
    "author": "Kohava Mendelsohn",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  },
  {
    "title": "Quantum Messages Cross Germany Using Conventional Fiber",
    "link": "https://spectrum.ieee.org/quantum-key-distribution-commercial-fiber",
    "summary": "Quantum networks can detect eavesdroppers and resist hacking at levels unrivaled by any classical networks today. Yet any expensive, specialized technologies that quantum networks require could still limit their appeal to telecom and other network operators today. Now researchers at Toshiba and their colleagues reveal that quantum communications are possible across more than 250 kilometers using existing commercial telecommunication infrastructure. “This work opens the door to practical quantum networks without needing exotic hardware,” says Mirko Pittaluga , who took part in the research while at Toshiba Europe in Cambridge, England. “It lowers the entry barrier for industry adoption.” Quantum networks can transmit encrypted messages that are theoretically impossible to eavesdrop on. They rely on the fact that measuring a quantum system—for instance, scanning a light pulse to see what data it might hold—disturbs the signal’s overall properties. Secret-key encryption (whether quantum or classical) relies on a sender and receiver sharing a secret key— usually a long, alphanumeric string— used to both encrypt and decrypt one’s private message. Through a method known as quantum key distribution (QKD), a sender and receiver can share random secret keys over an unsecured channel, which are usually encoded in photons . Any would-be eavesdropper seeking to intercept the keys would generate anomalies in their observation of the photons that could be detected. Toshiba has estimated the market for quantum key distribution to grow to roughly US $20 billion worldwide by 2035. Scalable Quantum Networks Without Cryogenic Coolers Scientists have often assumed that making quantum networks a reality would likely involve expensive equipment such as cryogenic coolers . Now, the new system “suggests that quantum networks could become much more scalable and practical in the near term,” says Pittaluga, who is currently quantum lead at Wave Photonics in Cambridge, England. The researchers detailed their findings online 23 April in the journal Nature . In the new study, Pittaluga and his colleagues developed a quantum communications network using 254 kilometers of existing commercial optical fiber in Germany. This system linked telecom data centers at Frankfurt and Kehl, with a relay node in Kirchfeld. Quantum communications often rely on keeping the phases of light pulses—the peaks and troughs of their waves—highly synchronized, down to tens of nanometers over distances of hundreds of kilometers. To reduce any phase noise that might come from the sources of light, these systems often rely on lasers and optical cavities that are ultrastable. Instead, the new system’s central node at Kirchfeld shone laser beams at the transmitting nodes in Frankfurt and Kehl that served as reference frequencies. This let the Kirchfeld lasers create a common benchmark, which the researchers say eliminated phase noise at the Frankfurt and Kehl stations more practically and cost-effectively than specialized, ultrastable equipment would have done. To overcome any losses of signal originating in the fiber, high-performance quantum key distribution systems often use very sensitive superconductive nanowire single-photon detectors . Instead, the new system used semiconductor-based, single photon detection via avalanche photodiodes . These are tens to hundreds of times more economical than their superconductive counterparts, and don’t require expensive cryogenic equipment. However, avalanche photodiodes are roughly one-fourth to one-sixth as efficient at detecting photons compared to their superconductive counterparts—as well as hundreds of times as likely to falsely detect signals that are not actually there. To mitigate these problems, the new system transmits a reference laser pulse alongside the beam that carries the quantum data. The system also uses two sets of avalanche photodiodes at the signal’s receiving end, rather than just one. The scientists used one set of avalanche photodiodes at Frankfurt and at Kehl to run the quantum-communications protocol, while the other avalanche photodiodes analyzed the reference beam. This latter procedure, the researchers found, allowed the system to correct for noise stemming from temperature, vibrations, and other disturbances in the optical fibers. Envisioning Gigahertz Speeds Another challenge the scientists faced was integrating all the components into a working system that remained stable over time. “There were many sleepless nights debugging in the lab,” Pittaluga says. “But close teamwork and a commitment to simplicity helped us get through it.” All in all, the new system doubled the distance achieved in previous research for practical quantum key distribution without cryogenic cooling. However, it transmitted encryption keys at a rate of only 110 bits per second. “Increasing the key distribution rate beyond 110 bits per second is a natural next step, and there are both engineering and research avenues to achieve this,” Pittaluga says. “For instance, one straightforward improvement is increasing the encoding rate of the protocol. Currently, we’re operating at 500 megahertz, but scaling up to a few gigahertz is within reach with existing technology. That alone could offer roughly a tenfold increase in key rate.” In the long term, one promising direction for quantum networks is the development of quantum repeaters , which the research community is actively exploring, Pittaluga says. “These would allow us to extend both the distance and performance of secure quantum links significantly,” he notes. Still, overall, “it’s important to highlight that a few hundred bits per second of secret key generation is not necessarily a limitation, depending on the application,” Pittaluga says. “In many practical scenarios, such as using quantum keys to periodically refresh classical symmetric encryption systems , this rate is actually quite sufficient.”",
    "published": "Wed, 23 Apr 2025 15:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "telecom",
    "collected_at": "2025-10-08T14:03:23"
  }
]