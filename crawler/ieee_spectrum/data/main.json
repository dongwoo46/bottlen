[
  {
    "title": "3D and AI: Excellent Fits for the Fashion Industry",
    "link": "https://spectrum.ieee.org/3d-and-ai-fashion-industry",
    "summary": "When you’re buying a new item of clothing, you probably don’t give much thought to the design and assembly processes the garment went through before arriving at the store. Creating a piece of apparel starts with a designer sketching out an idea. Then a pattern is made, the fabric is chosen and cut, and the garment is sewed. Finally the clothing is packaged and shipped. To expedite the process, some apparel companies now use 3D technologies including design software , body scans , visualization, and 3D printers . The tools allow designers to envision their creations in a variety of colors, fabrics, and motifs. Avatars known as digital twins are created to simulate how the clothes will look and fit on different body types. Body scans generate measurements for better-fitting clothing and improved product design. Some manufacturers incorporate artificial intelligence to streamline operations, and additional companies likely will explore it as it becomes more accurate. Not all garment makers are utilizing 3D technologies to their fullest potential, however. To advance 3D technology for designers, manufacturers, and retailers, the 3D Retail Coalition holds an annual challenge that spotlights academic institutions and startups that are leading the way. The contest is cosponsored by the IEEE Standards Association Industry Connections 3D Body Processing program, which works with the clothing industry to create standards for technology that uses 3D scans to create digital models. The winners of this year’s contest were selected in June at the PI Apparel Fashion Tech Show , held in New York City. The Fashion Institute of Technology (FIT) placed first in the academic category. The New York City school offers programs in design, fashion, art, communications, and business. PixaScale won the startup category. Based in Herzogenaurach, Germany, the consultancy assists fashion and consumer goods companies with automating content, managing 3D digital assets, and improving workflows. Custom-made clothing by 3D and AI Ill-fitting garments, shoes, and accessories are problems for clothing companies. The average return rate worldwide for clothing ordered online is more than 25 percent , according to PrimeAI . To make ready-to-wear clothing, designers use grading, a process that takes an initial sample pattern of a base size using established standards and 3D body scans, then makes smaller and larger versions to be mass-produced. But the resulting clothes do not fit everyone. Returns, which can be frustrating for shoppers, are costly for clothing companies due to reshipping and restocking expenses. Some customers can’t be bothered to send back unwanted items, and they throw them in the garbage, where they end up in landfills. “What if we could go back to the days when you would go to a shop, get measured, and someone would custom-make your garment?” posits Leigh LaVange , an assistant professor of technical design and patternmaking at FIT. That was the idea behind LaVange’s winning project, Automated Custom Sizing. Her proposal uses 3D technology and AI to produce custom-tailored clothing on demand for all body types. She outlined short- and long-term scalable solutions in her submission. “I want to fix our fit problem, but I also realize we can’t do that as an industry without changing the manufacturing process.” —Leigh LaVange “I see it [custom sizing] as a solution that can be automated and eventually rolled out across all different types of brands,” she says. The short-term proposal involves measuring a person’s base body specifications, such as bust, waist, thighs, biceps, and hips—either manually or from a 3D body scan. An avatar of the customer is then created and entered into a database preloaded with 3D representations of various sizes of the sample garment. The AI program notes the customer’s specs and the existing sizes to determine the best fit. If, for example, the person’s chest matches the medium-size dimensions but the hips are a few millimeters larger, the program still might recommend medium because it determined the material around the hips had enough excess fabric. A rendering of an avatar wearing an item is shown to customers to help them decide whether to make the purchase. LaVange says her solution will help improve customer satisfaction and minimize returns. Her long-term plan is a truly customized fit. Using 3D body scans, an AI program would determine the necessary adjustments to the pattern based on the customer’s specifications and critical fit points, like the waist, while preserving the original design. The 3D system then would make alterations, which would be rendered on the customer’s avatar for approval. The solution would eliminate excess inventory, LaVange says, because the clothing would be custom-made. Because her proposals rely on technologies not currently used by the industry and a different way of interacting with customers, a shift in production would be required, she says. “Most manufacturing systems today are set up to produce as many units as possible in a single day,” she says. “I believe there’s a way to produce garments efficiently if you set up your manufacturing facility correctly. I want to fix our fit problem, but I also realize we can’t do that as an industry without changing the manufacturing process.” A digital asset management platform The winning submission in the startup category, AI-First DAM [digital asset management] as an Intelligent Backbone for Agile Product Development , uses 3D technology and AI to combine components of clothing design into a centralized platform. Kristian Sons , chief executive of Pixascale, launched the startup in February. He left Adidas in January after nine years at the company, where he was the technical lead for digital creation. Many apparel companies, Sons says, still store their 3D files on employees’ local drives or on Microsoft’s SharePoint , a Web-based document-management system. Those methods make things difficult because not everyone has access. Sons’ cloud-based platform addresses the issue by sharing digital assets, such as images, videos, 3D models, base styles, and documents, to all parties involved in the process. That includes designers, seamstresses, and manufacturers. His system integrates with the client’s file management system, providing access to the most recent images, renderings, and other relevant data. His DAM system also includes a library of embellishments such as zippers and buttons, as well as fabric options. “Getting this information into a platform that everyone can easily access and can understand what others did really builds a foundation for collaboration.” —Kristian Sons “Getting this information into a platform that everyone can easily access and track what others did really builds a foundation for collaboration,” he says. Sons also is working on incorporating AI agents and large language models to connect with internal systems and application programming interfaces to autonomously conduct simple research requests. That might include suggesting new products or different silhouettes, or modifying the previous season’s offerings with new colors, Sons says. “These AI agents certainly will not be perfect, but they are a good starting point so designers don’t have to start from scratch,” he says. “I think using AI agents is super exciting because in the past few years in the fashion industry, we have been talking about how AI would do the creative parts, like designing a product. But now we’re talking about the AI doing the low-level tasks.” A demonstration of how Pixascale’s DAM works is on YouTube.",
    "published": "Tue, 07 Oct 2025 18:00:03 +0000",
    "author": "Kathy Pretz",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Noncontact Motion Sensor Brings Precision to Manufacturing",
    "link": "https://spectrum.ieee.org/aeva-motion-sensor-precision-manufacturing",
    "summary": "Aeva Technologies , a developer of lidar systems based in Mountain View, Calif., has unveiled the Aeva Eve 1V, a high-precision, noncontact motion sensor built on its frequency modulated continuous wave (FMCW) sensing technology. The company says that the Eve 1V measures an object’s motion with accuracy, repeatability, and reliability—all without ever making contact with the material. That last point is key for the Eve 1V’s intended environment: Industrial manufacturing . Today’s manufacturing lines are under pressure to deliver faster production, tighter tolerances, and zero defects, often while working with a wide variety of delicate materials. Traditional tactile tools such as measuring wheels and encoders can slip, wear out, and cause costly downtime. Many noncontact alternatives, while promising, are either too expensive or fall short in accuracy and reliability under real-world conditions, says Mina Rezk , cofounder and chief technology officer at Aeva. “Eve 1V was built to solve that exact gap: A compact, eye-safe, noncontact motion sensor that delivers submillimeter-per-second velocity accuracy without touching the material, so manufacturers can eliminate slippage errors, avoid material damage, and reduce maintenance-related downtime, enabling higher yield and more predictable operations,” Rezk says. Unlike traditional lidar that sends bursts of light and waits for those bursts to return to make measurements, FMCW continuously emits a low-power laser while sweeping its frequency. By comparing outgoing and returning signals, it detects frequency shifts that reveal both distance and velocity in real time. The additional measurement of an object’s velocity to its position in three-dimensional space makes FMCW a type of 4D lidar. Eve 1V is the second member of its Eve 1 family, following the launch of the Eve 1D earlier this year . The Eve 1D is a compact displacement sensor capable of detecting movement at the micrometer scale, roughly 1/100 the thickness of a human hair. “Together, Eve 1D and Eve 1V show how we can take the same FMCW perception platform and tailor it for different industrial needs: Eve 1D for distance measurement and vibration detection, and Eve 1V for precise velocity and length measurement,” Rezk says. Future applications could extend into robotics, logistics, and consumer health, where noncontact sensing may enable the detection of microvibrations on human skin for accurate pulse and blood-pressure readings. FMCW Lidar for Precision Manufacturing The company’s core FMCW architecture, originally developed for long-range 4D lidar for automobiles, can be adjusted through software and optics for highly precise motion sensing at close range in manufacturing, according to Rezk. This flexibility means the system can track extremely slow movements, down to fractions of a millimeter per second, in a factory setting, or it can monitor faster motion over longer distances in other applications. By avoiding physical contact, Eve 1V eliminates wear and tear, slippage, contamination, or the need for physical access to the part. “That delivers three practical advantages in a factory: One, maintenance-free operation with no measuring wheels to replace or recalibrate; two, material friendliness—you can measure delicate, soft, or textured surfaces without risk of damage, and three, operational robustness—no slippage errors and fewer stoppages for service,” Rezk says. Put together, that means more uptime, steady throughput, and less scrap, he adds. When measuring velocity, engineers often rely on one of three tools: encoders, laser velocimeters, or camera-based systems. Each has its strengths and its drawbacks. Traditional encoders are low-cost but can wear down over time. Laser-based velocity-measurement systems, while precise, tend to be large and expensive, making them difficult to implement widely. And camera-based approaches can work for certain inspection tasks, but they usually require markers, controlled lighting, and complex processing to measure speed accurately. Rezk says that the Eve 1V system offers a balance of these options. It provides precise and consistent velocity measurements without contacting material, making it compact, safe, and simple to install. Its outputs are comparable with existing encoder systems, and because it doesn’t rely on physical contact, it requires minimal maintenance. This approach helps cut down on wasted energy from slippage, eliminates the need for maintenance tied to parts that wear out, and ultimately lowers long-term operating costs—especially when compared with traditional contact-based systems or expensive laser options. This method avoids stitching together frame-by-frame comparisons and resists interference from sunlight, reflections, or ambient light. Built on silicon photonics, it scales from micrometer-level sensing to millimeter-level precision over longer ranges. The result is clean, repeatable data with minimal noise—outperforming legacy lidar and camera-based systems. Aeva is expecting to begin full production of the Eve 1V in early 2026. The Eve 1V reveal follows a recent partnership with LG Innotek , a components subsidiary of South Korea’s LG Group, under which Aeva will supply its Atlas Ultra 4D lidar for automobiles, with plans to expand the technology into consumer electronics, robotics, and industrial automation.",
    "published": "Tue, 07 Oct 2025 14:00:03 +0000",
    "author": "Kate Park",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Happy IEEE Day!",
    "link": "https://spectrum.ieee.org/ieee-day",
    "summary": "Happy IEEE Day! First celebrated in 2009, IEEE Day commemorates the initial gathering of IEEE members to share their technical ideas in 1884. Worldwide celebrations demonstrate the ways thousands of IEEE members in local communities join together to collaborate on ideas that leverage technology for a better tomorrow. “ IEEE Day 2025 is a celebration of innovation, collaboration, and the incredible impact our members create worldwide,” says Abdul Halik M I, this year’s IEEE Day chair. “I encourage everyone to join in, share their stories, and be part of this global movement.” Celebrate IEEE Day with colleagues from IEEE Sections, Student Branches, Affinity groups, and Society Chapters. Events happen both virtually and in person all around the world. Join the celebration around the world! Every year, IEEE members from IEEE Sections, Student Branches, Affinity groups, and Society Chapters join hands to celebrate IEEE Day. Events happen both virtually and in person. IEEE Day celebrates the first time in history when engineers worldwide gathered to share their technical ideas in 1884. View events→ Special Activities & Offers for Members Check out our special offers and activities for IEEE members and future members. And share these with your friends and colleagues. View offers→ Compete in contests and win prizes! Have some fun and compete in the photo and video contests. Get your phone and camera ready when you attend one of the events. This year we will have both Photo and Video Contests. You can submit your entries in STEM, technical, humanitarian and social categories. View contests→",
    "published": "Tue, 07 Oct 2025 13:00:00 +0000",
    "author": "IEEE",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Natron’s Failure May Not Spell Doom for Sodium-Ion Batteries",
    "link": "https://spectrum.ieee.org/natron-sodium-ion-battery-failure",
    "summary": "Natron Energy , a Santa Clara, California-based sodium-ion battery startup, ceased operation on 3 September due to funding issues. Just a year ago, the company made headlines for its plans to build a first-of-its-kind US $1.4 billion factory in North Carolina to manufacture up to 14 gigawatt-hours of sodium-ion batteries . While experts say Natron’s closure shouldn’t be taken as a harbinger for the rest of the emerging industry in the United States, they acknowledge that the West is behind China, which is leveraging its dominance in lithium-ion batteries to forge ahead on sodium-ion battery manufacturing. In the U.S., sodium-ion startups like Natron, which launched in 2012, tend to rely on goodwill from funders, says K.M. Abraham , a retired research professor at Northeastern University in Boston and CTO of lithium-ion battery consulting firm E-KEM Sciences . This can pose challenges for companies when funding timelines outpace innovations. “Companies aren’t able to make progress quickly enough to keep up with pressure exerted by the investors,” he says. Natron’s Pioneering Prussian Blue Batteries Until recently, Natron was seen as a leader of the pack in the U.S. sodium-ion market. Part of the company’s appeal was its pioneering approach to low-cost electrodes, the conductors at the battery’s positive and negative terminals, which make contact with the non-metallic part of the circuit. The company used Prussian Blue , a pigment found in paints and dyes, to make both the cathode and anode for its three battery systems . In addition to having a low material cost, Prussian Blue’s chemical structure has large pores, helping it facilitate faster ion transfer between the electrodes. Natron was the first in the world to commercialize a sodium-ion battery using Prussian Blue, a real feat considering China’s battery manufacturing might, says Tyler Evans , co-founder and CEO of Mana Battery , a Broomfield, Colorado-based sodium-ion battery cell startup that launched in 2023. “They were doing it in the West, and they were scaling a technology that was relatively low energy density for a very specific market segment,” says Evans about Natron’s products. Mana is another U.S. startup focusing on bringing sodium-ion batteries to market. Nicholas Singstock/Mana That market included grid storage, data center power backups, and electric vehicle charging stations—large-scale stationary applications where attributes like safety and cost rank higher than energy density. Natron’s success in this space, including its plans for the North Carolina factory, prompted questions about whether sodium-ion could emerge as a direct replacement for lithium-ion batteries. United Airlines and Chevron were on the list of Natron’s investors. But Evans says scaling up a low-energy density product while building out manufacturing lines is expensive. “If you think about building a manufacturing facility where you want to produce 10 gigawatt hours of batteries, if your energy density is very low, producing an equivalent number of batteries requires more manufacturing lines,” Evans says. “If you think about building a manufacturing facility where you want to produce a gigawatt-hour of battery manufacturing capacity, if your energy density per battery cell is very low, producing that capacity requires more manufacturing lines,” Evans says, meaning significantly more capital and operational expenditure in an already capital-intensive undertaking. In 2023, Natron’s systems made it to market. The company partnered with Encorp to deploy the industry’s first multi-megawatt class power platform for industrial applications. A year later, in 2024, Natron opened the U.S.’s first commercial scale manufacturing facility in Holland, Michigan to supply data centers with energy storage. The U.S. Department of Energy’s ARPA-E program provided $19.8 million to Natron as part of a $300 million facility upgrade to transition from lithium-ion battery manufacturing to sodium-ion battery manufacturing. That facility shut its doors at the same time as Natron’s California headquarters on 3 September. A request for comment from Natron resulted in an automated message to contact the company’s primary shareholder , Sherwood Partners. Sherwood Partners did not respond to a request for comment. Sodium-Ion vs. Lithium-Ion Battery Costs Adrian Yao is the founder and team lead of Stanford’s STEER initiative , a DOE-funded research program. He’s also an author of a January 2025 paper assessing how sodium-ion batteries measure up to lithium-ion batteries in terms of technology and cost. While he was impressed with Natron’s technology and product, he says that the company may have been ahead of the curve on the data center market niche it had carved out for itself. “Hyperscalers right now, their primary concern is just getting connected and building data centers,” says Yao. “I think timing on that cycle may be early, and it’s unfortunate things don’t always work out.” Natron joins Stanford spin-out Bedrock Materials as the second sodium-ion company to fold this year. Bedrock cited market and innovation challenges for its April closure. “The battery business is very difficult. There are a lot of tombstones,” says Andrew Thomas , president and cofounder of Acculon Energy , a Columbus, Ohio-based startup marketing two battery modules with sodium-ion cells for industrial energy and EVs that travel at low speeds, like golf carts. Unlike Natron, Acculon, which launched in 2022, employs more traditional layered-metal oxides and other sodium chemistries. Thomas says it’s this distinction that makes it hard to draw conclusions about the U.S. sodium-ion battery industry as a whole in light of Natron’s closure. Comparing different sodium-ion chemistries, like Prussian Blue or layered metal oxides, is like comparing apples to oranges. “I don’t think one failure is representative of a country being unable, but we’re at a significant disadvantage given the installed base in China,” Thomas says. China is the dominant player in sodium-ion battery development, with companies like CATL displaying their designs at tech expos. Yuan Zheng/VCG/AP China’s Dominance in Battery Manufacturing China has long dominated the battery industry, and sodium-ion batteries are no exception. Today, China produces more than 75 percent of batteries sold globally, according to the International Energy Agency . On the sodium-ion front, developers like CATL have moved into second-generation batteries, with the April launch of Naxtra, a brand geared toward EV applications. Yao says he’d like to see the U.S. concentrate its focus more on building up its manufacturing prowess to compete with China. “My broader critique of the Western Hemisphere in terms of our thinking and obsession with trying to innovate ourselves out of the problem, is that we focus too much on tech,” Yao says. “We have very little manufacturing experience… Our yield rates are abysmal, and our workforce is not trained.” Founders like Evans and Thomas are optimistic about their prospects as growing demand for grid storage, data centers, and low-cost mobility applications drives the need for applications they say sodium-ion batteries are uniquely equipped to support in terms of temperature range, safety, and cost metrics. When it comes to manufacturing, Mana is taking a page from China’s playbook by partnering with existing manufacturers to scale up production. Evans says there’s an appetite for this kind of partnership in the U.S. right now. “I think it’s a commercialization sweet spot that’s specific to sodium.”",
    "published": "Mon, 06 Oct 2025 17:56:02 +0000",
    "author": "Julia Tilton",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "The Future of the Grid: Simulation-Driven Optimization",
    "link": "https://spectrum.ieee.org/multiphysics-simulation-power-grid",
    "summary": "This is a sponsored article brought to you by COMSOL . Simulation software is useful in the analysis of new designs for improving power grid resilience, ensuring efficient and reliable power distribution, and developing components that integrate alternative energy sources, such as nuclear fusion and renewables. The ability to simulate multiple physical phenomena in a unified modeling environment gives engineers a deeper understanding of how different components of the grid interact with and affect each other. For example, when designing the various components of grid infrastructure, such as transformers and transmission lines, multiphysics electromagnetic field analysis is essential for ensuring the safety of the surrounding individuals and environment. Understanding thermal behavior, another phenomenon involving multiple physics, is equally necessary for the design of grid components where heat dissipation and thermal stresses can significantly affect performance and lifespan. Structural and acoustics simulation, meanwhile, is used to predict and mitigate issues like transformer vibration and noise — an important practice for ensuring the longevity and reliability of grid components. Multiphysics simulation provides critical insight into the complex interactions at play within power grid components, enabling engineers to virtually test and optimize future grid designs. Electric breakdown and corona discharge analyses are particularly vital for high-voltage transmission lines, as such phenomena can compromise the performance of their insulation systems. Simulation allows development teams to predict where such events are likely to happen, enhancing the design of insulators and other components where the goal is to minimize energy loss and material degradation. As a real-world example, one leading manufacturer uses the COMSOL Multiphysics® simulation software software to develop magnetic couplings, a noncontact alternative to mechanical transmission that enables power transfer without the inherent friction-based limitations of continual contact. While the advantage of friction-free power transmission means that magnetic couplings have found applications in a broad range of technologies, including offshore wind turbines, these systems must be developed carefully to avoid degradation. By employing highly nonlinear hysteresis curves and applying its own material temperature dependences for magnetic loading, the manufacturer’s development team has successfully used multiphysics simulation to help prevent the permanent magnets from reaching critical temperatures, which can cause irreversible demagnetization and compromise the reliability of the designs. Additionally, due to the diverse nature of use cases for magnetic couplings, the company’s design engineers must be able to interchange shapes and materials of magnets to meet customer requirements without building costly and time-consuming prototypes — rendering multiphysics simulation a powerful approach for characterizing configurations, providing virtual prototypes of their designs, and ultimately reducing the price for customers while remaining vigilant on fine details. These examples show just a few of the ways that coupling multiple interacting physics within a single model can lead to successful simulation of real-world phenomena and thereby provide insights into current and future designs. Lightning strikes a tower’s shielded wires. The induced voltage on the three-phase conductors is computed using electromagnetic field analysis. COMSOL Improving Reliability with Digital Twins & Simulation Apps Engineering teams can also use simulation technology to create more efficient, effective, and sustainable power grids by creating digital twins. A digital twin contains a high-fidelity description of a physical product, device, or process — from the microscopic to the macroscopic level — that closely mirrors its real counterpart . For every application, the digital twin is continuously receiving information, ensuring an up-to-date and accurate representation. With this technology, grid operators and their equipment suppliers can predict which components are most likely to fail, enabling them to schedule maintenance and replacement more efficiently and thereby improving grid reliability. Digital twins can be made for equipment ranging from power sources including solar cells and wind turbines to power distribution systems and battery energy storage. An offshore wind farm where lightning strikes one of the turbine blades. The electric field on the turbine towers, seawater, and seabed is shown. COMSOL The most recent modeling and simulation technology provides power and energy companies with tools for creating digital twins in the form of standalone simulation apps, which significantly increases the number of users who have access to advanced simulation technology. By including only relevant functionality in a standalone simulation app, colleagues with no modeling and simulation experience can utilize this technology without needing guidance from the modeling specialist. Furthermore, the use of data-driven surrogate models in simulation apps enables near-instantaneous evaluation of what would otherwise be time-consuming simulations — which means that simulation technology can now be used in a real-world setting. Digital twins, in the form of standalone apps, bring the power of simulation to the field, where grid operators can utilize real-time performance information to ensure grid reliability. For instance, one organization that works with local power companies to analyze equipment maintenance and failure built a custom app based on a multiphysics model it had developed to predict cable faults and improve troubleshooting efficiency. While engineers have been utilizing simulation in labs for decades, cable failure occurs in the field, and onsite troubleshooting personnel are responsible for assessing these failure conditions. With this in mind, an engineer at the organization developed the simulation app using the Application Builder in COMSOL Multiphysics ®. Temperature distribution in a battery energy storage system (BESS). COMSOL The app features relevant parameters that troubleshooting personnel with no prior simulation experience can easily modify. Field technicians enter cable data and select the type of fault, which modifies the multiphysics model in real time, allowing the app to evaluate and output the data necessary to understand the condition that led to the fault. The app then produces a reported potential and electric field, which leads the technicians to an informed decision regarding whether they need to replace or repair the cable. Following the app’s successful deployment, the engineer who developed it stated, “The simulation app plays a key role in cable maintenance. It makes the work of our field technicians more efficient by empowering them to confidently assess and repair faults.” Routine physical tests of grid equipment cannot fully reflect conditions or determine failure types in many situations, as a large number of complex factors must be considered, such as cable structure and material, impurities in the cable, voltage fluctuation, and operating conditions and environments. As a result, simulation has proven to be indispensable in many cases for collecting accurate cable health assessments — and now in the form of custom apps, it is more accessible than ever. Generating Nuclear Solutions Simulation has also been heavily integrated into the design process of various components related to the nuclear industry. For example, simulation was used to help design generator circuit breakers (GCBs) for nuclear power plants. GCBs must be reliable and able to maintain performance even after long periods of inactivity. The COMSOL Multiphysics ® software can be used to improve the current-carrying capacity of the GCBs, which can offer protection from current surges and provide dependable electricity generation. The design of nuclear fusion machines like tokamaks has also benefitted from the use of simulation. These devices must be able to withstand high heat fluxes and plasma disruptions. COMSOL Multiphysics ® has been used to help engineers predict the effects of these problems and come up with design solutions, such as adding a structural support system that can help reduce stress and survive challenging conditions. Engineering the Grid of Tomorrow The development of next-generation power grid systems is a complex and dynamic process that requires safe, reliable, and affordable testing. Multiphysics simulation technology can play a major role in future innovations for this industry, enabling engineers to anticipate and analyze the complex interactions happening inside these devices while building upon the existing infrastructure to address the demands of modern-day consumption. COMSOL Multiphysics is a registered trademark of COMSOL AB.",
    "published": "Mon, 06 Oct 2025 10:00:05 +0000",
    "author": "Bjorn Sjodin",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "5 Ways Volunteering Can Be a Springboard for Professional Growth",
    "link": "https://spectrum.ieee.org/5-ways-volunteering-professional-growth",
    "summary": "Since the early 2000s, I’ve been actively volunteering for nonprofits and local community organizations including my children’s school’s parent-teacher organization. There I worked with teachers and other parents to plan and implement annual programs that enriched the students and the school community as a whole. Through my work, I’ve realized that volunteering isn’t just about giving back; it’s also a way to learn new skills. Every hour invested adds tangible value to one’s personal and professional growth. My experiences have shown me that membership-based organizations such as IEEE that offer volunteer opportunities should position and market them as top-tier member benefits. Even if not all members volunteer their time, the availability of the opportunities and the inspiration drawn from participating can be powerful catalysts for a more vibrant and committed community. The merits of volunteering are universally known and acknowledged. Its powerful capacity to reduce stress and boost feel-good hormones such as dopamine is widely touted, as noted in a Forbes article by Garen Staglin , the cofounder of One Mind at Work. However, the impacts of volunteering extend far beyond the typical benefits of building a résumé, acquiring skills, and feeling a sense of fulfillment. The deeper value lies in its ability to enhance professional growth and personal development in less apparent ways. The pros of volunteering Here are five ways in which the “why” of purpose can fuel the “how” of growth: Opportunities for collaboration. Volunteering provides an arena for collaboration, bringing together professionals from industry and academia, for example, in tackling shared goals. The dynamic interchange is where true growth happens. It’s not just about what gets done; it’s about the invaluable experience of working with diverse minds to solve problems—an experience that accelerates development in all areas. Incubator for leadership. In an organization such as IEEE, where volunteers are shaping its structure and future through committees and governance bodies, the environment is ideal for developing high-impact professional skills. Volunteering instills leadership skills—including strategic thinking, problem-solving, and the ability to inspire others—that can be utilized in one’s professional (and personal) life. Sharpening decision-making abilities. Volunteering lets you sharpen skills that are sometimes difficult to improve in the structured confines of a typical job. Working outside the usual professional hierarchy, volunteers get a chance to sharpen their critical thinking and decision-making skills with greater autonomy and a clear, mission-driven focus. A volunteering task, such as judging submissions for awards and scholarships, can hone analytical and fair-judgment skills. Planning conferences and events can help develop organizational, project management, and problem-solving abilities. Intellectual expansion . Volunteering is a fantastic way to boost your intellect and uncover hidden potential. Through collaborative problem-solving and organic knowledge-sharing, one can tackle complex issues. As you move from one task to the next, the skills can build on each other, creating a compounding effect that rapidly enhances your abilities. Staying current, relatable, and confident. A cumulative benefit of volunteering is that it can serve as a professional refresh for one’s skills and perspective and can expose you to new technologies and trends outside of your field. You can learn to connect and communicate more effectively across generations and backgrounds by working alongside a diverse group of people. The collaborative environment also can provide an opportunity for you to demonstrate expertise in new ways and receive peer recognition, which in turn can reinforce your professional value and boost your confidence. Volunteering isn’t just for young professionals ; it can be a catalyst for people at every stage of their career. It can boost mid-career professionals’ confidence, enabling them to hone their skills and tackle new challenges. For senior-level professionals, volunteering can expand their networks, providing fresh perspectives and collaborative opportunities outside traditional silos. Any organization that provides volunteering opportunities is providing members or employees a valuable chance for an active investment in themselves.",
    "published": "Sat, 04 Oct 2025 18:00:04 +0000",
    "author": "Prachi Jain",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Video Friday: Drone Easily Lands on Speeding Vehicle",
    "link": "https://spectrum.ieee.org/video-friday-speedy-drone-landing",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! We demonstrate a new landing system that lets drones safely land on moving vehicles at speeds up to 110 kilometers per hour. By combining lightweight shock absorbers with reverse thrust, our approach drastically expands the landing envelope, making it far more robust to wind, timing, and vehicle motion. This breakthrough opens the door to reliable high-speed drone landings in real-world conditions. [ Createk Design Lab ] Thanks, Alexis! This video presents an academic parody inspired by KAIST’s humanoid robot moonwalk. While KAIST demonstrated the iconic move with robot legs, we humorously reproduced it using the Tesollo DG-5F robot hand. A playful experiment to show that not only humanoid robots but also robotic fingers can “dance.” [ Hangyang University ] Twenty years ago, Universal Robots built the first collaborative robot . You turned it into something bigger. Our cobot was never just technology. In your hands, it became something more: a teammate, a problem-solver, a spark for change. From factories to labs, from classrooms to warehouses. That’s the story of the past 20 years. That’s what we celebrate today. [ Universal Robots ] The assistive robot Maya, newly developed at DLR, is designed to enable people with severe physical disabilities to lead more independent lives. The new robotic arm is built for seamless wheelchair integration, with optimized kinematics for stowing, ground-level access, and compatibility with standing functions. [ DLR ] Contoro and HARCO Lab have launched an open-source initiative, ROS-MCP-Server, which connects AI models (for example, Claude, GPT, Gemini) with robots using a robot operating system and the Model Context Protocol. This software enables AI to communicate with multiple ROS nodes in the language of robots. We believe it will allow robots to perform tasks previously impossible due to limited intelligence, help robotics engineers program robots more efficiently, and enable nonexperts to interact with robots without deep robotics knowledge. [ GitHub ] Thanks, Mok! Here’s a quick look at the Conference on Robotic Learning (CoRL) exhibit hall, thanks to PNDbotics. [ PNDbotics ] Old and busted: sim to real. New hotness: real to sim! [ Paper ] Any humanoid video with tennis balls should be obligated to show said humanoid failing to walk over them. [ LimX ] Thanks, Jinyan! The correct answer to the question “Can you beat a robot arm at tic-tac-toe?” should be “No. No, you cannot.” And you can’t beat a human, either, if they know what they’re doing. [ AgileX ] It was an honor to host the team from Microsoft AI as part of their larger educational collaboration with the University of Texas at Austin. During their time here, they shared this wonderful video of our lab facilities. Moody lighting is second only to random primary-colored lighting when it comes to making a lab look science-y. [ The University of Texas at Austin HCRL ] Robots aren’t just sci-fi anymore. They’re evolving fast. AI is teaching them how to adapt, learn, and even respond to open-ended questions with advanced intelligence. Aaron Saunders, chief technology officer of Boston Dynamics, explains how this leap is transforming everything, from simple controls to full-motion capabilities. While there are some challenges related to safety and reliability, AI is significantly helping robots become valuable partners at home and on the job. [ IBM ]",
    "published": "Fri, 03 Oct 2025 16:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "This Mexican Student Is Engineering a Healthier Future",
    "link": "https://spectrum.ieee.org/student-engineering-a-healthier-future",
    "summary": "Most of us have heard the adage “an ounce of prevention is worth a pound of cure.” But when it comes to personal health, many people overlook preventative measures such as diet and exercise. Instead, they tend to rely on medical professionals to save the day after they’ve gotten sick. Ximena Montserrat Ramirez Aguilar is working to change that by educating her fellow Mexicans about how to manage their health so they can avoid undergoing treatment for preventable conditions such as Type 2 diabetes and its associated conditions affecting the eyes, cardiovascular system, brain, heart, kidneys, and other organs. Ximena Montserrat Ramirez Aguilar MEMBER GRADE: Student member UNIVERSITY: Universidad Autónoma de Nuevo León, in Monterrey, Mexico MAJOR: Biomedical engineering Ramirez envisions her career as advancing health through disease prevention, but she acknowledges that, as an undergraduate, she is still discovering how to turn her vision into reality. A senior studying biomedical engineering at the Universidad Autónoma de Nuevo León (UANL), in Monterrey, Mexico, she is the founding chair of her school’s IEEE Engineering in Medicine and Biology Society (EMBS) student branch. The student member’s research interests in neuroengineering and artificial intelligence are shaping her vision for the future of health care. “I’ve always been passionate about technology and health,” she says. “Biomedical engineering is giving me a way to combine these two worlds and work on solutions that make a real difference in people’s lives.” Her growing influence in IEEE coupled with her academic achievements signal a promising, influential career. From chemistry to caring Ramirez was born in Zacatecas, Mexico , known for its silver mines, agriculture, and strong cultural pride. From a young age, she loved science—particularly chemistry—and thrived in schools designated for advanced learners. Her first exposure to the health care field came during high school, when she trained as a nursing technician. Her high school curriculum was organized as a co-op program, which included traditional classes alternating with internships in nursing. Ramirez interned at the Hospital Universitario Dr. Jose Eleuterio Gonzalez in Monterrey, Mexico. Alternating between the academic and vocational tracks allowed her to graduate with a diploma and a technical degree at the same time. Speaking of her early experiences, she says, “I saw how many patients struggled, not just with their conditions but also with the logistics of seeking and coordinating treatment,” she says. “That made me want to work at the intersection of medicine and innovation.” With her father working as a materials engineer and her mother as an accountant, she grew up in a household where technical problem-solving and analytical thinking were part of daily life. That blend of influences reinforced her decision to pursue engineering as a career rather than the medical field, she says. Exploring neuroengineering and AI Since beginning her studies at UANL in 2021, Ramirez has focused on neuroengineering, one of three specializations the school offers. She has explored the role artificial intelligence plays in diagnosing and treating conditions including Alzheimer’s disease , depression , epilepsy , and schizophrenia . Through the IEEE mentoring program , she received guidance from global experts including a doctor from India who helped refine her early AI projects. Her work quickly evolved from class assignments to projects with real-world potential. “The project I’m most excited about has not been published, but it mainly consisted of using convolutional neural networks in medical image processing (MRI) and machine learning in the diagnosis of neurodegenerative diseases,” she says. This year she broadened her scope by attending the IEEE International Conference on Robotics and Automation in Atlanta, where she gained exposure to both industrial and academic applications of robotics. “In Mexico, people usually don’t think about their health until they’re already sick. I want to focus on using technology and education to keep people healthy.” Currently she is an intern at Auna , a health care network in Latin America. She contributes to improving the patient experience in hospitals across Mexico, Colombia, and Peru. “I design projects aimed at improving the quality of care and making the hospital intervention more effective for patients across different stages: prevention/wellness, diagnosis, hospitalization, rehabilitation, and post-discharge follow-up,” She declined to provide specific examples, citing medical confidentiality agreements. “My internship is about finding ways to make health care not just effective but also more humane,” she says. “It’s about improving processes so patients feel cared for—from the moment they enter the hospital until they leave.” Finding leadership and purpose in IEEE Ramirez founded the IEEE EMBS student branch in 2023. As chair, she represents the branch at IEEE Region 9 meetings, where she advocates for mentorship opportunities and collaboration with other IEEE groups. Through her involvement, she says, she has gained not only technical knowledge but also critical soft skills in leadership, time management, and teamwork. “IEEE taught me how to lead with empathy and how to work with people from different backgrounds,” she says. “It has expanded my vision beyond Mexico, showing me challenges and innovations happening all over the world.” She says she plans to pursue a master’s degree abroad—potentially in public health or AI for medical devices—and ultimately a Ph.D. Her long-term goal is to launch a business focused on developing health care innovations, specifically in disease prevention. A future built on innovation For Ramirez, improving health care means more than developing cutting-edge technology. It also involves rethinking how people understand and manage their own health. “In Mexico, people usually don’t think about their health until they’re already sick,” she says. “I want to focus on using technology and education to keep people healthy.” Her vision is as ambitious as it is personal, rooted in her own journey from Zacatecas to Monterrey and beyond. As her career advances, she says, she intends to keep IEEE at the center of her professional life. “In IEEE I’ve found a community that challenges me to grow, supports me when I fail, and celebrates when I succeed,” she says. “It’s not just about engineering; it’s about building a better future, together.”",
    "published": "Thu, 02 Oct 2025 18:00:04 +0000",
    "author": "Willie D. Jones",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Where Will Taiwan Get Energy After Its Failed Nuclear Referendum?",
    "link": "https://spectrum.ieee.org/nuclear-energy-taiwan-maanshan-plant",
    "summary": "Taiwan failed to pass an August referendum on whether or not a nuclear plant should be restarted, if it were deemed safe to operate. While the more than 4 million votes for “yes” outnumbered the more than 1.5 million “no” votes, the number of affirmative votes failed to surpass the 25 percent threshold of eligible voters also required for the referendum to pass. As a result, Taiwan remains on the nuclear-free path it has followed since the shutdown of the nuclear plant in question, Maanshan Nuclear Power Plant , in southern Taiwan on 17 May, fulfilling a 2016 government pledge made as a result of Japan’s 2011 Fukushima disaster . However, high-tech industries, including semiconductor manufacturing, AI data centers, and AI infrastructure operators, will continue fueling electricity demand. The question remains as to whether or not Taiwan can deliver reliable clean power to support the growth of these industries amid Chinese geopolitical pressure—and without nuclear energy . Taiwan’s Nuclear Energy Debate Taiwan’s energy landscape remains complex. Nuclear power, developed since the 1970s, has seen older reactors retired since 2018 . Taiwan imports 95 percent of its energy and has a growing reliance on natural gas. But it also aims to reduce carbon emissions, improve grid reliability, and expand its energy storage options. “Without energy, there’s no industrial growth…and nuclear is an excellent option,” Nvidia CEO Jensen Huang said during a prereferendum visit to Taipei on 22 August. He met with key players in high-tech supply chains, including Taiwan Semiconductor Manufacturing Co. (TSMC), the world’s largest chip foundry producing advanced chips for smartphones, high-performance computing, and AI applications. It was not Huang’s first time advocating for nuclear energy. During Computex Taipei in May he said , “We need energy from any single source: wind, solar, nuclear. Taiwan should absolutely invest in nuclear, and it shouldn’t be a stigma to have energy.” Nvidia has been expanding in Taiwan, partnering with Foxconn and the government to build a 10,000-Blackwell GPU AI training and supercomputing facility in the south, opening a larger Taipei office, and collaborating with Taiwanese companies such as TSMC to build an AI infrastructure ecosystem. Taiwan president Lai Ching-te promised to honor the referendum result while focusing on diverse energy sources. He said Taiwan might consider advanced nuclear options if technology improves, waste decreases, and public support grows. In late August, the government approved a draft piece of legislation, the AI Basic Act , designed to create a supportive environment for AI development and use. The draft emphasizes the government’s role in promoting AI research, applications, and infrastructure. Meanwhile, the newly reshuffled Cabinet is under pressure by industry and the broader public to maintain energy security. In mid-September, newly appointed Minister of Economic Affairs Ming-hsin Kung emphasized that Taiwan is a global hub for chips and technology, shaping strategies for the next 10 to 20 years. Taiwan’s Renewable Energy Goals Kung stressed that businesses require both stable power supply and green energy to meet commitments to 100 percent renewable energy from global corporate initiative RE110 . He said the new Cabinet will continue focusing on renewable energy while adjusting rollout speed. The goal is to lift renewables to 20 percent of Taiwan’s power supply by the end of 2026—a challenging target critical in keeping Taiwan competitive in global supply chains. He estimated renewable energy will account for around 15 percent of power generation by the end of 2025, up from 11.9 percent in 2024 . A wind turbine and its solar power system are part of the Taipower Exhibit Center in Pingtung, in southern Taiwan on 29 April 2025. I-Hwa Cheng/AFP/Getty Images For solar, Kung pledged to strengthen existing projects, resolve land-use conflicts with fish farms in solar-fishery initiatives , and replace older solar panels with newer ones that produce twice as much energy. Offshore wind construction will be accelerated, and a trial program for floating wind turbines will resume. Taiwan will also actively develop other green energy sources, such as geothermal and hydrogen. On nuclear, Kung reaffirmed Taiwan’s nuclear-free path but left open the possibility of adopting advanced technologies like small modular reactors . Guidelines for evaluating potential restarts of existing plants will be released by the end of October. The first step will see the Taiwan Power Co. (Taipower) conducting assessments of all three halted nuclear plants, with initial results due next year . Maanshan, which began commercial operations in 1984, is regarded as the most likely to pass the safety self-assessments, which will focus on the ability to maintain aging equipment and upgrade earthquake resilience. In a report released on 26 September, Taiwan’s Energy Administration projects electricity demand to grow 1.7 percent annually from 2025 to 2034. The forecast factors in expansions to Taiwan’s semiconductor industry, investments in AI development, and expected energy savings. To meet rising power demand, the government currently plans to boost natural-gas generation while phasing out large nuclear, coal, and oil plants. Net additions of 12.2 gigawatts in gas-fired capacity are expected by 2034 . Semiconductor Industry Concerns But high-tech industries express concern. In early September, at Semicon Taiwan, Charles Lee , the managing director of Topco Group, a major semiconductor supplier, told IEEE Spectrum that manufacturers worry about grid stability as AI and semiconductor growth accelerates. “Highly polluting coal-fired plants are no longer an option, so we will rely more on liquefied natural gas and less-stable renewables. If nuclear plants could be restarted, I would personally welcome it,” Lee says. Meanwhile, a memory manufacturing director, who spoke on condition of anonymity because he isn’t authorized by his company to speak to the media, told Spectrum that Taiwan’s economy is still manufacturing-driven. “We’re concerned about the low efficiency of green energy. We’ve also noticed a trend abroad, with countries resuming nuclear plant construction,” he says. In a televised debate ahead of the August referendum, Tzu-Hsien Tung, chairman of Pegatron Corp., voiced support for restarting nuclear power plants. He warned that if Taiwan continues to rely on carbon-heavy electricity, local firms could face steep carbon taxes overseas, undermining their global competitiveness. Visitors view AI server samples at the Zhen Ding Tech Group booth during the Semicon Taiwan exhibition in Taipei on 10 September 2025. I-Hwa Cheng/AFP/Getty Images As Taiwanese society debated whether to restart nuclear power plants, some Taiwanese energy experts, including Tze-Luen Lin, deputy executive director of the Taiwanese government’s Office of Energy and Carbon Reduction and a political science professor at National Taiwan University, have called for fresh approaches to Taiwan’s energy resilience amid ongoing Chinese threats, echoing to notions brought by nongovernmental organizations and think thanks, such as the U.S.-based Center for Climate and Security , that a clean-energy transition can strengthen national security. At the Society for Environmental Economics and Policy Studies conference in Japan on 21 September, Lin highlighted that renewable energy is central to both energy and national security. He emphasized, “Energy resilience can only be strengthened through decentralized, locally sourced renewables, combined with microgrids and energy storage,” and warned that large, centralized power plants are easier targets for attack. Commenting on Taiwan’s possible nuclear options, Jusen Asuka , a professor at Tohoku University and chair of the session in the conference, cautioned that small modular reactors remain immature and costly, and investing heavily in them could slow renewable-energy development.",
    "published": "Thu, 02 Oct 2025 17:00:05 +0000",
    "author": "Yu-Tzu Chiu",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "The Story of Engineering Is the Story of Scale",
    "link": "https://spectrum.ieee.org/engineering-scale",
    "summary": "Engineers are masters of scale . They harness energy from the sun, wind, rivers, atoms, and ores. They manipulate electrons, photons, and crystals to compute and communicate. They devise instruments that detect perturbations in the fabric of space-time . And they grapple with challenges—anticipated or not—that are presented by the scale of the problem they are trying to solve. The articles in this issue describe engineers who think about, interact with, and create things at very precise and often mind-boggling scales. They took the point-contact transistor and scaled it over the course of decades into a product manufactured in almost unimaginably large quantities ( 13 sextillion, or 13,000,000,000,000,000,000,000, between 1947 and 2018, by one estimate ) and involving one of the most complex, yet crazily efficient workflows on the planet . They’re sequencing the genomes of 1.8 million species . They’re modeling and mitigating a potential catastrophe—the Kessler syndrome—that threatens to decimate satellites in low Earth orbit [p. 58]. Everywhere you look, engineering ingenuity is pushing against the limits of scale. That ingenuity extends to creating scales for what has yet to be measured. How will we know when AI has achieved human-level general intelligence ? How do we precisely measure the absence of matter in a vacuum ? Then there are the complexities of scaling a technology for mass adoption. Why, for example, have some humanoid robot makers announced overly optimistic deployment targets and boosted production capacity well ahead of specific humanoid robot safety standards, high reliability, decent battery life, or demand for hordes of humanoids ? And how can onshore wind turbines continue to scale up unless there’s a proven way to transport them ? “Infographics let readers grasp at a glance what would take paragraphs of explanation.” —Eliza Strickland In this issue, our editors and artists flex their data-visualization powers through compelling infographics, to help readers appreciate the scale of hundreds of gigatonnes of carbon dioxide and the immense interstellar distances we could traverse with a swarm of tiny, laser-powered space kites. “While we wanted every article to include some visual element, a few topics called for special treatment. You could tell the story of carbon capture or interstellar travel in words, but the real impact comes when you see the gaps, the scales, the leaps involved,” says Senior Editor Eliza Strickland, who curated this issue. “Infographics let readers grasp at a glance what would take paragraphs of explanation, whether it’s the ballooning demand for AI or the long journey from raw quartz to finished computer chips.” Several of these infographics, as well as the cover, were created by renowned graphic designer Carl De Torres, owner of Optics Lab. We also commissioned an essay by the nature writer Paul Bogard, who approached his topic from the human scale. Who among us has not gazed at the stars and marveled at how our eyes are absorbing light that traveled thousands of years to reach us? Bogard ventured to Chile to see how light pollution is encroaching on astronomy and changing our sense of place in the universe , perhaps irrevocably. We hope this issue sparks wonder, and conveys our appreciation for the people who measure the unmeasurable, build the unbuildable, and solve the unsolvable.",
    "published": "Thu, 02 Oct 2025 16:00:03 +0000",
    "author": "Harry Goldstein",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Getting Value from Skip-Level Conversations",
    "link": "https://spectrum.ieee.org/getting-value-skip-level-meeting",
    "summary": "This article is crossposted from IEEE Spectrum ’s careers newsletter. Sign up now to get insider tips, expert advice, and practical strategies, written i n partnership with tech career development company Taro and delivered to your inbox for free! In 2015, I joined Pinterest as a software engineer after my company was acquired. Going from a five-person company to a 500-person company was daunting, especially since I had never worked in a hypergrowth organization. I talked previously about best practices for onboarding , but I want to specifically focus on an area that I sorely neglected: the skip-level one-on-one meeting. Your “skip” is the person your manager reports to, often a director or vice president. It’s what happens when you “skip” your manager up the reporting chain. Many of the ideas from a previous newsletter, Making the Most of 1:1 Meetings With Your Boss , carry over here, but skip-level 1:1 should be treated differently. The skip meeting can feel either nerve-wracking or irrelevant for many engineers, but it’s actually a unique opportunity to accelerate your career . Here’s how you can get the most value from your skip-level meetings: Understand your organization’s overall strategy . Your skip manager has a broader scope and set of responsibilities compared to most people you interact with. This means they are naturally equipped to answer questions around organizational strategy. Ask them what their priorities are, and how your work contributes to the priorities they care about. Share “on the ground” insights . No matter how hands-on they are, senior leaders are often disconnected from day-to-day work. As an individual contributor, you are well-equipped to share the experience of getting work done, both the good and the bad. This can be especially valuable if you can proactively suggest ideas to improve everyone’s productivity. Be honest with feedback . A truly effective skip-level meeting requires you to be unafraid to share real problems and provide honest feedback. This is an opportunity to discuss systemic issues that your direct manager may not be able to address on their own. At Pinterest, one of the first decisions I had to make was about which team within the company I would join. Had I properly leveraged the advice from the director, I could have received valuable feedback about high-priority areas in the company. Whether you’re new at a company or not, don’t squander the value of building a relationship with your management chain. —Rahul 4 Ways to Conquer Imposter Syndrome Nearly 70 percent of high-achieving adults have experienced the feelings of self-doubt that characterize imposter syndrome. To help engineers manage these thoughts, IEEE Women in Engineering held a webinar on building confidence and overcoming anxiety. Read the recommendations here. Read more here. Natcast to Lay Off Majority of Its Staff Natcast, the non-profit organization created to run the U.S. CHIPS & Science Act’s National Semiconductor Technology Center (NSTC), told the majority of its staff they would be laid off, IEEE Spectrum learned. NSTC was established to advance semiconductor technology in the United States and grow the domestic workforce. The news follows an announcement from Commerce Secretary Howard Lutnick that the department would not deliver the US$7.4 billion in funds under its contract with the government. Read more here. Tech Employment is as Mixed Bag, Selective Hiring Marks a Shift The tech hiring landscape in the United States is confusing these days. To better understand the job market, Computerworld looked at recent data from the U.S. Bureau of Labor Statistics and the nonprofit trade association CompTIA. Despite a slow job market, employers are selectively hiring for roles in AI, data science, and cybersecurity. Read more here.",
    "published": "Thu, 02 Oct 2025 15:27:46 +0000",
    "author": "Rahul Pandey",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "11 Oddball Technology Records You Probably Didn’t Know",
    "link": "https://spectrum.ieee.org/11-oddball-technology-records-you-probably-didnt-know",
    "summary": "This article is part of The Scale Issue . Longest Continuously Operating Electronic Computer Voyager 1 and its twin space probe , both launched by NASA in 1977, were the first human-made objects to reach interstellar space. But that’s not the only record the spacecraft hold. Voyager 2’s Computer Command System has not been turned off since it first booted up about 48 years ago, making it the longest continuously operating electronic computer. Quietest Place on Earth Can you hear your own heartbeat? For most of us, the answer is no—unless you’re standing in Orfield Laboratories’ anechoic chamber, in which case, you might be able to hear the blood rushing through your veins and the sound of your own blinking, too. The chamber in Minneapolis holds the title for quietest place on earth, with a background noise reading of –24.9 A-weighted decibels—meaning that the ambient sound is far below the threshold of human hearing. Longest-Lasting Battery An experimental electric bell at the University of Oxford, in England, has been ringing nearly continuously for 185 years. Powered by two dry piles—an early type of battery—connected in series, the bell has rung more than 10 billion times since it was set up in 1840. Its ringing, however, is now barely audible beneath the glass bell jar protecting the experiment. Fastest Typing Using Brain Signals For people with certain neurodegenerative conditions that impact muscle control, communication can be difficult. Brain–computer interfaces offer a solution by directly translating brain waves to text. But until recently, that translation has been slow. In 2022, researchers at the University of California, San Francisco, set the record for the fastest communication via brain signals: 78 words per minute. Best-Selling Consumer Electronics Certain consumer electronics, like the iPhone, seem ubiquitous. Over 18 years and about as many generations, more than 2.3 billion Apple smartphones have been sold. But when you break it down to individual models, which devices have been the biggest success? See how some particularly popular devices compare. Strongest Magnetic Field on Earth At least among magnets that don’t explode from their own field strength, the U.S. National High Magnetic Field Laboratory’s Pulsed Field Facility holds the record for strongest magnetic field on earth. The 100-Tesla field, which is about 2 million times as strong as Earth’s magnetic field, can be turned on for 15 milliseconds just once an hour. Biggest Teatime Electricity Spike Brits love their tea. That’s why the United Kingdom’s National Grid engineers have to manage surges in energy use during popular broadcast events, when many viewers put their kettles on simultaneously. The biggest spike occurred during the 1990 World Cup semifinal. Just after England lost the game-deciding penalty shootout, demand surged by 2,800 megawatts, equivalent to the electricity used by approximately 1.1 million kettles. Strongest Robotic Arm In March, Rise Robotics celebrated the Beltdraulic SuperJammer Arm ’s setting of the Guinness World Record for Strongest Robotic Arm Prototype. A collaboration between Rise and the U.S. Air Force, the arm lifted an astonishing 3,182 kilograms, about the weight of an adult female African elephant. Unlike other heavy-lifting machines, the robot uses no hydraulics, only electric power, and it improves efficiency by generating electricity when it’s lowering a load. Smallest Pacemaker Implanting most pacemakers requires invasive surgeries. But a group of researchers at Northwestern University, in Evanston, Ill., has developed a device that can be implanted through the tip of a syringe. Measuring 3.5 millimeters in its largest dimension and suited for newborns with heart defects, the pacemaker—which is designed for patients who need only temporary pacing—safely dissolves in the body after it has done its job. Fastest Data Transfer Earlier this year, a team from the National Institute of Information and Communications Technology and Sumitomo Electric, in Japan, blasted a record 1.02 million billion bits (petabits) across 1,808 kilometers in one second, or 1.86 exabits per second-kilometer. At that rate, in one second, you could send everything everyone in the world watched on Netflix in the first half of this year from Tokyo to Shanghai 4,000 times. A special 19-core optical fiber made it possible. Fastest EV Charging The Chinese automaker BYD used a new fast-charging system that peaked at 1,002 kilowatts and added 421 kilometers of range to a Han L sedan in under five minutes. That’s about 84 kilometers per minute. Among the key innovations behind the feat: 1,500-volt silicon carbide transistors and lithium iron phosphate batteries with half the internal resistance of their predecessors.",
    "published": "Thu, 02 Oct 2025 14:00:04 +0000",
    "author": "Gwendolyn Rak",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "The Quest to Sequence the Genomes of Everything",
    "link": "https://spectrum.ieee.org/whole-genome-sequencing",
    "summary": "A gibbous moon hangs over a lonely mountain trail in the Italian Alps, above the village of Malles Venosta, whose lights dot the valley below. Benjamin Wiesmair stands next to a moth trap as tall as he is, his face, bushy beard, and hair bun lit by its purple glow. He’s wearing a headlamp, a dusty and battered smartwatch, cargo shorts, and a blue zip sweater with the sleeves pulled up. Countless moths beat frenetically around the trap’s white, diaphanous panels, which are swaying with ghostly ripples in a gentle breeze. Wiesmair squints at his smartphone, which is logged on to a database of European moth species. “ Chersotis multangula ,” he says. “Yes, we need that,” comes the crisp reply from Clara Spilker , consulting a laptop. This article is part of The Scale Issue . Wiesmair, an entomologist at the Tyrolean State Museums , in Innsbruck, Austria, and Spilker, a technical assistant at the Senckenberg German Entomological Institute , in Müncheberg, are taking part in one of the most far-reaching biological initiatives ever: obtaining a genome sequence for nearly every named species of eukaryotic organism on the planet. All 1.8 million of them. The researchers are part of an expedition for Project Psyche , which is sampling European butterflies and moths and will feed its data into the global initiative, called the Earth BioGenome Project (EBP). Eukaryotes are organisms whose cells contain a nucleus. From protozoa to human beings, all have the same basic biological mechanism for building, maintaining, and propagating their form of life: a genome. It’s the sum total of the genes carried by the creature. Twenty-two years ago, researchers announced that for the first time they had mapped, or “sequenced,” nearly all of the genes in a human genome . The project cost more than US $3 billion and took 13 years, but it eventually transformed medical practice. In the new era of genomic medicine , doctors can take a patient’s specific genetic makeup into consideration during diagnosis and treatment. The EBP aims to reach its monumental goal by 2035. As of July 2024, its tally of genomes sequenced stood at about 4,200 . Success will undoubtedly depend on researchers’ ability to scale several biotech technologies. “We need to scale, from where we’re at, more than a hundredfold in terms of the number of genomes per year that we’re producing worldwide,” says Harris Lewin , who leads the EBP and is a professor and genetics researcher at Arizona State University . One of the most crucial technologies that must be scaled is a technique called long-read genome sequencing. Specialists on the front lines of the genomic revolution in biology are confident that such scaling will be possible, their conviction coming in part from past experience. “Compared to 2001,” when the Human Genome Project was nearing completion, “it is now approximately 500,000 times cheaper to sequence DNA,” says Steven Salzberg , a Bloomberg Distinguished Professor at Johns Hopkins University and director of the school’s Center for Computational Biology . “And it is also about 500,000 times faster to sequence,” he adds. “That is the scale, over the past 25 years, a scale of acceleration that has vastly outstripped any improvements in computational technology, either in memory or speed of processors.” There are many reasons to cheer on the EBP and the technological advances that will underpin it. Having established a genome for every eukaryotic creature, researchers will gain deep new insights into the connections among the threads in Earth’s web of life, and into how evolution proceeded for its myriad life forms. That knowledge will become increasingly important as climate change alters the ecosystems on which all of those creatures, including us, depend. And although the project is a scientific collaboration, it could spin off sizable financial windfalls. Many drugs, enzymes, catalysts, and other chemicals of incalculable value were first identified in natural samples . Researchers expect many more to be discovered in the process of identifying, in effect, each of the billions of eukaryotic genes on Earth, many of which encode a protein of some kind. “One idea is that by looking at plants, which have all sorts of chemicals, often which they make in order to fight off insects or pests, we might find new molecules that are going to be important drugs,” says Richard Durbin , professor of genetics at the University of Cambridge and a veteran of several genome sequencing initiatives. The immunosuppressant and cancer drug rapamycin , to cite just one of countless examples, came from a microbe genome. Your Genes Are a Big Reason Why You’re You The EBP is an umbrella organization for some 60 projects (and counting) that are sequencing species in either a region or in a particular taxonomic group. The overachiever is the Darwin Tree of Life Project , which is sequencing all species in Britain and Ireland, and has contributed about half of all of the genomes recorded by the EBP so far. Project Psyche was spun out of the Darwin Tree of Life initiative, and both have received generous support from the Wellcome Trust . To get an idea of the magnitude of the overall EBP, consider what it takes to sequence a species. First, an organism must be found or captured and sampled, of course. That’s what brought Wiesmair, Spilker, and 41 other lepidopterists to the Italian Alps for the Project Psyche expedition this past July. Over five days, they collected more than 200 new species for sequencing, which will augment the 1,000 finished Lepidoptera genome sequences already completed and the roughly 2,000 samples awaiting sequencing. There’s still plenty of work to be done; there are around 11,000 species of moths and butterflies across Europe and Britain. After sampling, genetic material—the creature’s DNA—is collected from cells and then broken up into fragments that are short enough to be read by the sequencing machines. After sequencing, the genome data is analyzed to determine where the genes are and, if possible, what they do. Over the past 25 years, the acceleration of gene-sequencing tech has vastly outstripped any improvements in computational technology, either in memory or speed of processors. DNA is a molecule whose structure is the famous double helix . It resides in the nucleus of every cell in the body of every living thing. If you think of the molecule as a twisted ladder, the rungs of the ladder are formed by pairs of chemical units called bases. There are four different bases: adenine (A), guanine (G), cytosine (C), and thymine (T). Adenine always pairs with thymine, and guanine always pairs with cytosine. So a “rung” can be any of four things: A–T, T–A, C–G, or G–C. Those four base-pair permutations are the symbols that comprise the code of life. Strings of them make up the genome as segments of various lengths called genes . Your genes at least partially control most of your physical and many of your mental traits—not only what color your eyes are and how tall you are but also what diseases you are susceptible to, how difficult it is for you to build muscle or lose weight, and even whether you’re prone to motion sickness. How Long-Read Genome Sequencing Works Long-read sequencing starts by breaking up a sample of genetic material into pieces that are often about 20,000 base pairs long. Then the sequencing technology reads the sequence of base pairs on those DNA strands to produce random segments, called “reads,” of DNA that are at least 10,000 pairs in length. Once those long reads are obtained, powerful bioinformatics software is used to build longer stretches of contiguous sequence by overlapping reads that share the same sequence of bases. To understand the process, think of a genome as a novel, and each of its separate chromosomes as a chapter in the novel. Imagine shredding the novel into pieces of paper, each about 5 square centimeters. Your job is to reassemble them into the original novel (unfortunately for you, the pages aren’t numbered). What makes this task possible is overlap—you shredded multiple copies of the novel, and the pieces overlap, making it easier to see where one leaves off and another begins. Making it much harder, however, are the many sections of the book filled with repetitive nonsense: the same word repeated hundreds or even thousands of times. At least half of a typical mammalian genome consists of these repetitive sequences, some of which have regulatory functions and others regarded as “junk” DNA that’s descended from ancient genes or viral infections and no longer functional. Long-read technology is adept at handling these repetitive sequences. Going back to the novel-shredding analogy, imagine trying to reassemble the book after it was shredded into pieces only 1 centimeter square rather than 5. That’s analogous to the challenge that researchers formerly faced trying to assemble million-base-pair DNA sequences using older, “short-read” sequencing technology . The Two Approaches to Long-Read Sequencing The long-read sequencing market has two leading companies— Oxford Nanopore Technologies (ONT) and Pacific Biosciences of California (PacBio)—which compete intensely. The two companies have developed utterly different systems. The heart of ONT’s system is a flow cell that contains 2,000 or more extremely tiny apertures called, appropriately enough, nanopores. The nanopores are anchored in an electrically resistant membrane, which is integrated onto a sensor chip. In operation, each end of a segment of DNA is attached to a molecule called an adapter that contains a helicase enzyme . A voltage is applied across the nanopore to create an electric field, and the field captures the DNA with the attached adapter. The helicase begins to unzip the double-stranded DNA, with one of the DNA strands passing through the nanopore, base by base, and the other released into the medium. OPTICAL SEQUENCING (Pacific Biosciences) A polymerase enzyme replicates the DNA strand, matching and connecting each base to a specially engineered, complementary nucleotide. That nucleotide flashes light in a characteristic color that identifies which base is being connected. Each DNA strand is immobilized at the bottom of a well. As the DNA strand is replicated, each base while being incorporated emits a tiny flash of light in a color that is characteristic of the base. The sequence of light flashes indicates the sequence of bases. What propels the strand through the nanopore is that voltage—it’s only about 0.2 volts, but the nanopore is only 5 nanometers wide, so the electric field is several hundred thousand volts per meter. “It’s like a flash of lightning going through the pore,” says David Deamer , one of the inventors of the technology. “At first, we were afraid we would fry the DNA, but it turned out that the surrounding water absorbed the heat.” That kind of field strength would ordinarily propel the DNA-based molecule through the pore at speeds far too fast for analysis. But the helicase acts like a brake, causing the molecule to go through with a ratcheting motion, one base at a time, at a still-lively rate of about 400 bases per second. Meanwhile, the electric field also propels a flow of ions across the nanopore. This current flow is decreased by the presence of a base in the nanopore—and, crucially, the amount of the decrease depends on which of the four bases, A, T, G, or C, is entering the pore. The result is an electrical signal that can be rapidly translated into a sequence of bases. NANOPORE SEQUENCING (Oxford Nanopore) The helicase enzyme unzips and unravels the double-stranded DNA, and one strand enters the nanopore. The enzyme feeds the strand through the nanopore with a ratcheting motion, base by base. The ionic current is reduced by a characteristic amount, depending on the base. The current signal indicates the sequence of bases. PacBio’s machines rely on an optical rather than an electronic means of identifying the bases. PacBio’s latest process , which it calls HiFi, begins by capping both ends of the DNA segment and untwisting it to create a single-stranded loop. Each loop is then placed in an infinitesimally tiny well in a microchip, which can have 25 million of those wells. Attached to each loop is a polymerase enzyme, which serves a critical function every time a cell divides. It attaches to single-stranded DNA and adds the complementary bases, making each rung of the ladder whole again. PacBio uses special versions of the four bases that have been engineered to fluoresce in a characteristic color when exposed to ultraviolet light. A UV laser shines through the bottom of the tiny well, and a photosensor at the top detects the faint flashes of light as the polymerase goes around the DNA sample loop, base by base. The upshot is that there is a sequence of light flashes, at a rate of about three per second, that reveals the sequence of base pairs in the DNA sample. Because the DNA sample has been converted into a loop, the whole process can be repeated, to achieve higher accuracy, by simply going around the loop another time. PacBio’s flagship Revio machine typically makes five to 10 passes, achieving median accuracy rates as high as 99.9 percent, according to Aaron Wenger , senior director of product marketing at the company. How Researchers Will Scale Up Long-Read Sequencing That kind of accuracy doesn’t come cheap. A Revio system , which has four chips, each with 25 million wells, costs around $600,000, according to Wenger. It weighs 465 kilograms and is about the size of a large household refrigerator. PacBio says a single Revio can sequence about four entire human genomes in a 24-hour period for less than $1,000 per genome. ONT claims accuracy above 99 percent for its flagship machine, called PromethION 24 . It costs around $300,000, according to Rosemary Sinclair Dokos , chief product and marketing officer at ONT. Another advantage of the ONT PromethION system is its ability to process fragments of DNA with as many as a million base pairs. ONT also offers an entry-level system, called MinION Mk1D , for just $3,000. It’s about the size of two smartphones stacked on top of each other, and it plugs into a laptop, offering researchers a setup that can easily be toted into the field. Although researchers often have strong preferences, it’s not uncommon for a state-of-the-art genetics laboratory to be equipped with machines from both companies. At Barcelona’s Centro Nacional de Análisis Genómico, for example, researchers have access to both PacBio Revio machines as well as PromethION 24 and GridION machines from ONT. Durbin, at Cambridge University, sees lots of upside in the current situation. “It’s very good to have two companies,” he declares. “They’re in competition with each other for the market.” And that competition will undoubtedly fuel the tech advances that the EBP’s backers are counting on to get the project across the finish line. PacBio’s Wenger notes that the 25-million-well chips that underpin its Revio system are still being fabricated on 200-millimeter semiconductor wafers. A move to 300-mm wafers and more advanced lithographic techniques, he says, would enable them to get many more chips per wafer and put hundreds of millions of wells on each of those chips—if the market demands it. At ONT, Dokos describes similar math. A single flow cell now consists of more than 2,000 nanopores, and a state-of-the-art PromethION 24 system can have 24 flow cells (or upward of 48,000 nanopores) running in parallel. But a future system could have hundreds of thousands of nanopores, she says—again, if the market demands it. The EBP will need all of those advances, and more. EBP director Lewin notes that after seven years, the three-phase initiative is wrapping up phase one and preparing for phase two. The goal for phase two is to sequence 150,000 genomes between 2026 and 2030. For phase two, “We’ve got to get to 37,500 genomes per year,” Lewin says. “Right now, we’re getting close to 3,000 per year.” In phase two, the cost per genome sequenced will also have to decline from roughly $26,000 per genome in phase one to $6,100, according to the EBP’s official road map . That $6,100 figure includes all costs—not just sequencing but also sampling and the other stages needed to produce a finished genome, with all of the genes identified and assigned to chromosomes. Phase three will up the ante even higher. The road map calls for more than 1.65 million genome sequences between 2030 and 2035 at a cost of $1,900 per genome. If they can pull it off, the entire project will have cost roughly $4.7 billion—considerably less in real terms than what it cost to do just the human genome 22 years ago. All of the data collected—the genome sequences for all named species on Earth—will occupy a little over 1 exabyte (1 billion gigabytes) of digital storage. It will arguably be the most valuable exabyte in all of science. “With this genomic data, we can get to one of the questions that Darwin asked a long time ago, which is, How does a species arise? What is the origin of species? That’s his famous book where he never actually answered the question,” says Mark Blaxter , who leads the Darwin Tree of Life Project at the Wellcome Sanger Institute near Cambridge and who also conceived and started Project Psyche. “We’ll get a much, much better idea about what it is that makes a species and how species are distinct from each other.” A portion of that knowledge will come from the many moths collected on those summer nights in the Italian Alps. Lepidoptera “go back around 300 million years,” says Charlotte Wright , a co-leader, along with Blaxter, of Project Psyche. Analyzing the genomes of huge numbers of species will help explain why some branches of the Lepidoptera order have evolved far more species than others, she says. And that kind of knowledge should eventually accumulate into answers to some of biology’s most profound questions about evolution and the mechanisms by which it acts. “The amazing thing is that by doing this for all of the lepidoptera of Europe, we aren’t just learning about individual cases,” says Wright. “We’ve learned across all of it.”",
    "published": "Thu, 02 Oct 2025 13:00:04 +0000",
    "author": "Glenn Zorpette",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "The NEC-Approved Solution That’s Changing How Fleets Approach EV Charging",
    "link": "https://content.knowledgehub.wiley.com/electrify-faster-spend-less-why-fleet-managers-are-turning-to-automated-load-management/",
    "summary": "Maximize existing grid capacity and avoid costly upgrades. Learn how Automated Load Management enables faster, more affordable fleet electrification . Download the free technical guide. Download this free whitepaper now!",
    "published": "Thu, 02 Oct 2025 11:00:04 +0000",
    "author": "The Mobility House",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Muscle-Bound Micromirrors Could Bring Lidar to More Cars",
    "link": "https://spectrum.ieee.org/mems-lidar",
    "summary": "Five years ago, Eric Aguilar was fed up. He had worked on lidar and other sensors for years at Tesla and Google X, but the technology always seemed too expensive and, more importantly, unreliable. He replaced the lidar sensors when they broke—which was all too often, and seemingly at random—and developed complex calibration methods and maintenance routines just to keep them functioning and the cars drivable. So, when he reached the end of his rope, he invented a more robust technology—what he calls the “most powerful micromachine ever made.” Aguilar and his team at startup Omnitron Sensors developed new microelectromechanical systems (MEMS) technology that he claims can produce more force per unit area than any other. By supplying new levels of power to micromirrors , the technology is capable of precisely steering lidar’s laser beams, even while weathering hazardous elements and the bumps and bangs of the open road. With chips under test by auto-industry customers, Omnitron is now modifying the technology to reduce the power consumed by AI data centers. Lidar, a scanning and detection system that uses lasers to determine how far away objects are, is often adopted by self-driving cars to find obstacles and navigate. Even as the market for lidar is expected to grow by 13.6 percent annually , lidar use in the automotive industry has remained relatively stagnant in recent years, Aguilar says, in part because the technology’s lifespan is so short. Vibration from bumpy roads and severe environmental conditions are the biggest reliability killers for automotive lidar, says Mo Li , who studies photonic systems at the University of Washington. The optical alignment within the lidar package atop self-driving cars is delicate—tremors from a poor paving job could physically alter where the mirrors sit in the housing, potentially misaligning the beam and causing the system to fail. Or temperature fluctuations could cause parts to expand or contract with the same unfortunate outcome, he explains. Aguilar wondered which part broke most often and found the culprit to be scanners, the parts responsible for angling small mirrors that direct the laser beam out of the system’s housing. He wanted to make scanners that could withstand the tough conditions lidar faces, and silicon flexures stood out as a solution. These structures act like springs and allow for meticulous control of the mirrors within lidar systems without wearing out, as the standard metal springs do, Aguilar claims. Designing a better chip Aguilar hoped the new material would be the answer to the problem that plagued him, but even silicon springs didn’t make lidar systems as robust as they needed to be to withstand the elements they faced. To make lidar even stronger, the team at Omnitron aimed to design a more powerful MEMS chip by increasing the amount of force the device can apply to control the mirrors in the lidar array. And they claim to have achieved it—their chip can exert 10 times as much force per unit area on an actuator that positions a micromirror or other sensor component as the current industry standard, they say. That extra force allows for extremely valuable control in fine adjustment. To reach this achievement, they had to dig deep—literally. Omnitron’s micromirrors steer lidar beams and could find use in data centers. Omnitron In this MEMS device, the mirror and its actuator are etched into a single silicon wafer. On its nonmirror end, the actuator is covered with tiny, closely spaced plates that fit between trenches in the wafer, like the interlocking teeth of two combs. To move the mirror, voltage is applied, and electrostatic forces angle the mirror into a specific position by moving the plates up and down within the trenches as the electric field pulls across the trench sidewalls . The force that can be used to move the mirror is limited by the ratio of depth to width of the trenches, called aspect ratio. Put simply, the deeper the trenches are, the more electrostatic force can be applied to an actuator, which leads to a higher range of motion for the sensor. But fabricating deep, narrow trenches is a difficult endeavor. Overcoming this limiting factor was a must for Aguilar. Aguilar says Omnitron was able to improve on the roughly 20:1 aspect ratio he notes is typical for MEMS (other experts say 30:1 or 40:1 is closer to average these days), reaching up to 100:1 through experimentation and prototyping in small university foundries across the United States “That’s really our core breakthrough,” Aguilar says. “It was through blood, sweat, tears, and frustration that we started this company.” The startup has secured over US $800 million in letters of intent from automotive partners, Aguilar says, and is two months into an 18-month plan to prove that it can produce its chips at full demand rate. Even after verifying production capabilities, the technology will have to face “very tough” safety testing for thousands of consecutive hours in realistic conditions, like vibrations, thermal cycles, and rain, before it can come to market, Li says. Saving power In the meantime, Omnitron is applying its technology to solve a different problem faced by a different industry. By 2030, AI data centers are expected to require around 945 terawatt-hours to function—more than the country of Japan consumes today. The problem is “the way data moves,” Aguilar says. When data is sent from one part of the data center to another, optical signals are converted into electrical signals, rerouted, and then turned back to optical signals to be sent on their way. This process, which takes place in systems called network switches, burns huge amounts of power. Google’s solution, called Apollo , is to keep the data packets in the form of optical signals for the duration of their travels, which yields a 40 percent power savings, the company claims . Apollo does so by using an array of mirrors to direct the data. Aguilar is planning to make the process even more efficient using dense arrays of Omnitron’s more-powerful mirrors. Doing so could quadruple the amount of data each network switch could route by increasing the number of channels in each switch from 126 to 441, Aguilar says. Omnitron is still early in its data-center implementation, so it’s not yet clear to what degree this technology can really improve on Google’s Apollo. However, following a “critical design review” in mid-September, “one of the world’s top AI hyperscalers has requested our mirrors for their next-generation switch,” Aguilar says. “This is proof that Omnitron solves a problem that even the biggest AI infrastructure companies can’t address in-house.” And there may be even more applications to come. Omnitron has received feelers from the defense industry, space companies, and groups interested in methane detection, says Aguilar. “It’s pretty cool seeing the people knock on our door for this, because I was just focusing on lidar,” he says.",
    "published": "Wed, 01 Oct 2025 15:00:03 +0000",
    "author": "Perri Thaler",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "AI Expands the Search for New Battery Materials",
    "link": "https://spectrum.ieee.org/ai-battery-material",
    "summary": "When Microsoft researchers in 2023 identified a new kind of material that could dramatically reduce the amount of lithium needed in rechargeable batteries, it felt like combing through a haystack in record time. That’s because their discovery began as 32 million possibilities and, with the help of artificial intelligence, produced a promising candidate within 80 hours. Now researchers at the Pacific Northwest National Laboratory plan to synthesize and test the novel material, Na x Li 3−x YCl 6 , in a battery setup. It’s one of several AI-generated battery chemistries making its way to the real world. Microsoft’s experiment started when the researchers wanted to demonstrate how AI could tackle the needle-in-a-haystack problem of finding useful new materials and chemicals . They decided to seek new candidates for a rechargeable battery’s electrolyte, because a better electrolyte could make batteries safer while simultaneously improving performance, says Nathan Baker , project leader at Microsoft for Azure Quantum Elements , a program to accelerate chemistry and materials research through Microsoft’s advanced computing and AI platforms. “Our goal was to take one of these AI models and show the promise of accelerating scientific discovery—sifting through 32.5 million materials candidates and showing that we could do it in a matter of hours, not years,” Baker says. Their model, called the M3GNet framework, accelerated simulations of molecular dynamics to evaluate properties of the materials such as atomic diffusivity. First, the Microsoft researchers asked the model to drop new chemical elements into known crystalline structures in nature and determine which resulting molecules would be stable, a step that cut the 32 million starting candidates down to half a million. AI then screened those materials based on the necessary chemical abilities to make a battery work, which chopped the pool to just 800. From there, traditional computing and old-fashioned human expertise identified the novel material that could function within a battery and use 70 percent less lithium than the rechargeable batteries in commercial use today. AI’s Role in Next-Gen Battery Design The Microsoft team isn’t alone. Around the world, researchers are busy trying to develop next-generation designs to replace or improve lithium-ion batteries, which use large quantities of rare, expensive, and difficult-to-acquire elements . New battery designs could use more abundant materials, reduce the fire danger from lithium-based liquid electrolytes, and pack more energy into a smaller space. The chemistries to do this are waiting out there to be discovered, and increasingly, researchers are harnessing AI and machine learning to do the work of sorting through the mountain of data. “We are teaching AI how to be a materials scientist,” says Dibakar Datta , associate professor at the New Jersey Institute of Technology, who published a study in August that used AI to identify five candidate materials for batteries that would outperform Li-ion. Datta’s team is working on the multivalent battery: one that employs multivalent ions that can carry multiple charge levels as opposed to the single charge carried by a lithium battery. This would give the battery a greater energy storage capacity, but it also means working with larger ions from elements higher on the periodic table, like magnesium and calcium. Those larger ions won’t necessarily fit into existing battery designs without cracking or breaking the elements, Datta says. His new study used what he calls a crystal diffusion variational autoencoder (CDVAE) that could propose new materials, and a large language model that could find materials that would be the most stable in the real world. From a pool of millions of possibilities, the approach found five porous materials of the right size that could do the job. Guiding an AI model on its hunt through the nearly infinite space of possible materials is the tipping point in this field. The key to using it as a research partner is to find a happy medium between a model that works fast and a model that delivers perfectly accurate results, says Austin Sendek , professor at Stanford University who has developed algorithms to help AI discover new battery materials. “You have to traverse both breadth and depth,” says Sendek. Depth, because designing these things takes a lot of deep scientific knowledge about properties, engineering, and chemistry, and breadth, because you have to apply that knowledge across an infinite chemical space, he says. “That’s where the promise of AI comes in.” AI Battery Technology Search at IBM Researchers at IBM have taken an AI-driven approach to identify new electrolyte candidates , which involved identifying chemical formulations with far higher ionic conductivity than the lithium salts used in current batteries. A typical electrolyte can contain six to eight ingredients including salts, solvents, and additives, and it’s nearly impossible to consider all the combinations without AI. To whittle down the field, the IBM team developed chemical-foundation models trained on billions of molecules. “They capture the basic language of chemistry,” says Young-Hye Na , principal research staff member at IBM Research. Her team then trains those models with battery-related data so the AI can predict important properties for battery applications on scales from individual molecules all the way up to a whole device. Na described the work in a paper published in August in NPJ Computational Materials . Because the work investigates new combinations of existing materials rather than using AI to invent exotic new materials, its potential to help build the battery of tomorrow is that much more promising, Na says. The IBM team is now collaborating with an undisclosed EV manufacturer to design high-performance electrolytes for high-voltage batteries. IBM’s use of AI for batteries isn’t limited to the hunt for promising materials. Typically, when AI reveals a promising new material, the next step is for experimentalists to synthesize the stuff, experiment with it in the lab, and one day to test it in a real device. Machine learning (ML) will aid researchers in this testing step, too. IBM is testing the real-world viability of new battery setups by building their digital twins —virtual models that allow the researchers to predict how a particular battery chemistry would degrade over a lifetime of countless power cycles. The model, developed in collaboration with battery startup Sphere Energy , can predict a battery’s long-term behavior in as few as 50 power cycles modeled on the digital twin, says Teodoro Laino , distinguished research staff member at IBM Research. Quantum-Computing Batteries The next phase of AI battery research is quantum . As Microsoft and IBM push toward the potential of quantum computers, both see its promise to model complex chemistry with no shortcuts or compromises. Na says that while current AI is a crucial tool for investigating battery chemistry, the next step—modeling whole EV battery packs, for example, and taking into consideration all the variables they encounter in the real world—would require the power of quantum computing. As Baker puts it: “We know classical computers have problems generating accurate answers for complex substances, complex molecules, complex materials. So our goal right now is actually to change the way the data is generated by bringing quantum into the loop so that we have higher accuracy data for training ML models.” This article was updated on 2 October 2025.",
    "published": "Wed, 01 Oct 2025 14:00:04 +0000",
    "author": "Andrew Moseman",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "The Hidden Behemoth Behind Every AI Answer",
    "link": "https://spectrum.ieee.org/ai-energy-use",
    "summary": "What happens when you say “Hello” to ChatGPT? Such a simple query might seem trivial, but making it possible across billions of sessions requires immense scale. While OpenAI reveals little information about its operations, we’ve used the scraps we do have to estimate the impact of ChatGPT—and of the generative AI industry in general. This article is part of The Scale Issue . OpenAI’s actions also provide hints. As part of the United States’ Stargate Project , OpenAI will collaborate with other AI titans to build the largest data centers yet. And AI companies expect to need dozens of “Stargate-class” data centers to meet user demand.",
    "published": "Wed, 01 Oct 2025 13:00:03 +0000",
    "author": "Matthew S. Smith",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "AI-Powered Tools Enhance Global Innovation Strategies",
    "link": "https://spectrum.ieee.org/ai-powered-ip-innovation",
    "summary": "This is a sponsored article brought to you by IP.com Around the world, innovation is no longer just a function of invention. It is a strategic asset, deeply linked to economic resilience—and increasingly reliant on ever-evolving technologies like artificial intelligence (AI). As intellectual property ecosystems transform under this new reality, the need for advanced, efficient, ethical AI-supported innovation infrastructure has never been more urgent. The U.S. Patent and Trademark Office (USPTO) has emerged as a global leader in this space, signaling its commitment to responsible AI leadership through the AI and Emerging Technologies (ET) Partnership, and most recently, through its landmark guidance clarifying that human contribution is essential for inventorship in AI-assisted creations and its significant investment in deploying AI to improve prior art searches and overall examination quality. As governments, enterprises, and inventors look to chart a path through this rapidly shifting IP landscape, one innovation intelligence company has been boldly helping lead the way: IP.com . For over 20 years, IP.com has worked to empower public and private innovation stakeholders alike with an ever-expanding suite of AI-powered tools that enhance ideation, evaluate novelty, clarify patent landscapes, and protect inventive work around the globe. Today, as the world rethinks how to balance open innovation with strategic security, IP.com’s solutions are not only timely—they are essential. Aligning A National Vision with Global Innovation While each country faces its own innovation challenges and opportunities, there is a growing consensus around one idea: the systems that support innovation—including those for AI and intellectual property—must be modern, data-driven, future-ready, and responsible. That vision was echoed in the U.S. government’s recent Executive Order on artificial intelligence ( EO 14179 ), and the newly published “ America’s AI Action Plan ” which emphasize the need for safe and trustworthy AI systems to drive economic growth and national security. While the Executive Order speaks primarily to U.S. priorities, it reflects a global movement toward building responsible AI systems that deliver long-term value, foster trust, and safeguard the integrity of innovation. IP.com’s innovation platforms are deeply aligned with this broader call, providing AI solutions that are secure and responsible from the ground up. From operating in a private environment to being fully ITAR-compliant, IP.com’s approach delivers IP-advancing AI solutions that are safe and accountable so inventors, R&D teams, IP professionals, and federal agencies can work smarter, faster, and more securely. Shaping the Future of Responsible, Innovation-Supportive AI IP.com’s dual-engine AI-fueled Innovation Power (IP) Suite® mirrors how high-performing teams think while aligning closely with the strategies and priorities shaping the future of intellectual property. It responsibly integrates AI into intellectual property processes to foster innovation within a secure, inclusive framework free from ideological bias or hallucinations. The IP Suite is purpose-built to deliver actionable insights grounded in proprietary data. With security integrated at every level—including ITAR compliance and private AI environments—IP.com offers best-in-class capabilities that protect sensitive data while accelerating innovation workflows. Transparent and traceable outputs reinforce trust and accountability across the innovation lifecycle while empowering forward-thinking teams and simplifying complex technology landscapes. IP.com’s dual-engine AI-fueled Innovation Power Suite is purpose-built to deliver actionable insights grounded in proprietary data. That means inventors and engineers can use the IP Suite to safely push innovation boundaries at a rapid pace. By integrating ideation, quantitative novelty analysis, prior art analysis, and invention disclosure generation into one simple, intuitive AI workflow, IP decisions can be made better and faster to maximize ROI. As global innovation accelerates, the broader collaboration IP.com enables through its AI-fueled IP Suite is essential to aligning stakeholders around shared priorities and helping to build a resilient, secure, and future-ready IP ecosystem. A Bold Force in Global IP Advancement Though best known among insiders in patent law and innovation strategy, IP.com has shaped some of the most important developments in IP modernization over the past two decades. Used by patent trademark offices around the world, its enterprise-grade, class-leading semantic AI increases examiner efficiency, powers millions of prior art searches, and accelerates the IP and innovation work of inventors, engineers, and IP professionals alike. Today, IP.com’s tools serve clients ranging from small inventors to multinational R&D operations. And as patent filings increase globally and emerging technologies blur jurisdictional boundaries, its commitment remains the same: helping innovators everywhere create confidently, compete fairly, and protect what matters—their intellectual property. Addressing IP Theft Head-On As the threat of IP theft intensifies, organizations across the public and private sectors are reevaluating how and where they deploy AI. Foreign-backed open and consumer-grade AI solutions, while powerful, often operate in opaque environments with unclear data handling practices, raising serious concerns about data leakage. For entities managing high-value innovation or sensitive research, the risk of proprietary data being exposed or exploited through unsecured AI models has become a pressing issue. IP.com’s Innovation Power (IP) Suite® is purpose-built to meet this challenge. Designed for enterprise and public-sector use, the platform operates entirely within secure, explainable, and ITAR-compliant environments—ensuring that no prompts, queries, or intellectual property are ever shared with external models or third parties. This architecture preserves data sovereignty while also upholding innovation ethics. Furthermore, the IP Suite helps US companies and government agencies protect innovations while countering the risk of IP theft. IP.com’s secure and ITAR-compliant solutions are uniquely positioned to help enable rapid, secure AI adoption in sensitive environments. As lines between economic competition and cyber-espionage continue to blur, IP.com stands apart from competitors built on open-source models vulnerable to exploitation. IP.com offers a proven, trustworthy path forward for organizations seeking to innovate responsibly while safeguarding their most valuable ideas. Beyond IP Security: The IEEE Content Advantage IP.com further enhances innovation processes by offering engineers direct access to fully searchable IEEE content—one of the most trusted and timely sources of technical knowledge in the world. Whether evaluating the novelty of a new design or researching prior art, engineers benefit from the ability to explore a vast collection of peer-reviewed journals, conference proceedings, and technical standards—all integrated within IP.com’s AI-powered IP Suite. By embedding IEEE content directly into the research workflow, IP.com empowers engineering teams to make more informed technical and strategic decisions. During concept validation or patentability assessments, having authoritative, high-quality IEEE literature at their fingertips helps engineers validate ideas, identify gaps in the landscape, and avoid costly duplication of effort. Combined with IP.com’s advanced analytics and private, secure AI tools, the integration of IEEE content ensures that engineers not only innovate efficiently but do so with the clarity and depth of insight required in today’s fast-moving R&D environments. Few platforms offer this kind of integration. Fewer still deliver it with the semantic precision and ease of use that IP.com provides. Final Thoughts: A Shared Responsibility Innovation is borderless. Its challenges—be they technical, strategic, or ethical—are shared across geographies. And so must be its solutions. IP.com is committed to supporting innovation ecosystems worldwide with tools that uphold the values of fairness, security, and excellence. Whether advancing a single patent application or shaping and managing an entire IP portfolio, our mission remains clear: to help innovators move forward—smarter, faster, and together. Discover how IP.com supports innovation ecosystems worldwide at www.ip.com/AI",
    "published": "Wed, 01 Oct 2025 12:30:03 +0000",
    "author": "Christopher Irick",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "IEEE Collabratec Reaches 100,000 Member Milestone",
    "link": "https://spectrum.ieee.org/ieee-collabratec-milestone",
    "summary": "IEEE Collabratec reached a milestone in August: more than 100,000 IEEE members (plus 250,000 nonmembers) on the online networking platform. To commemorate the achievement, IEEE released a 100,000-member badge for users. The badges recognize members for their participation in IEEE Collabratec’s communities and discussion forums. They also reward users for creating networks with other IEEE members and solving IEEE Puzzlers brainteasers. “Since 2021 IEEE Collabratec has been a game-changer in my membership journey,” IEEE Member Jaramogi Khalfani Adofo Odhiambo says. He is a member of the IEEE Uganda Section . “I connect with fellow volunteers around the world and have found mentorship and support for personal growth. “Collabratec is more than a network; it’s a vibrant community that celebrates learning, leadership, and collaboration.” The platform was launched in 2015 to help members stay connected with the organization and local sections. Rolling out new features Since its debut, IEEE Collabratec has introduced new features. Here are some recent additions: The IEEE Puzzlers community, which premiered in 2021, offers a fun, engaging experience for those who enjoy solving brainteasers such as missing numbers and logic games. There are different levels of difficulty. People who correctly solve the puzzles receive badges and recognition on the website. There are several types of badges, based on how many puzzles users solve. A badge is awarded when participants solve 7, 15, 30, 50, and 75 puzzles. Seventeen community participants from seven countries currently have the highest level badge, which is awarded for solving 555. The IEEE Mentoring Program pairs mentors with members who are seeking guidance on topics such as their career, education, leadership, volunteering, or a particular technical field. The program, created in 2023, is open to IEEE members of any grade. Anyone can be a mentor, whether they’re a student or a seasoned professional. As of July, about 3,700 members have signed up to guide others, a 25 percent increase since last year. There are about 1,300 mentor-mentee pairings now, with IEEE senior members representing half of the mentors. A new community dedicated to mentoring is scheduled to debut this year. To better reflect members’ IEEE contributions, such as leading a committee or organizing an event, digital certificates are offered for volunteering . Each individualized document includes the person’s name, the position they held, and the years served. Every position held has its own certificate. A member’s list of roles is updated annually. Users can download the certificates and add them to their LinkedIn profile or résumé. Certificates also may be printed for displaying. “IEEE Collabratec has served as a truly unifying force across our global technical community—bridging disciplines, geographies, and generations,” IEEE Life Fellow Fredrick Mintzer says. Mintzer, recipient of the 2022 IEEE Emberson Award , is a frequent Collabratec contributor. “For a decade, Collabratec has embodied the One IEEE philosophy by fostering collaboration and empowering members and nonmembers alike to connect, contribute, discuss, debate, and grow together.” To learn more about IEEE Collabratec , check out the user guide , FAQs , and users’ forum .",
    "published": "Tue, 30 Sep 2025 18:00:04 +0000",
    "author": "Joanna Goodrich",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "“Hearing Car” Detects Sounds for Safer Driving",
    "link": "https://spectrum.ieee.org/fraunhofer-hearing-car-microphone",
    "summary": "Autonomous vehicles have eyes—cameras, lidar, radar. But ears? That’s what researchers at Fraunhofer Institute for Digital Media Technology’s Oldenburg Branch for Hearing, Speech and Audio Technology in Germany are building with the Hearing Car . The idea is to outfit vehicles with external microphones and AI to detect, localize, and classify environmental sounds, with the goal of helping cars react to hazards they can’t see. For now, that means approaching emergency vehicles—and eventually pedestrians, a punctured tire, or failing brakes. “It’s about giving the car another sense, so it can understand the acoustic world around it,” says Moritz Brandes , a project manager for the Hearing Car. In March 2025, Fraunhofer IDMT researchers drove a prototype Hearing Car 1,500 kilometers from Oldenburg to a proving ground in northern Sweden. Brandes says the trip tested the system in dirt, snow, slush, road salt, and freezing temperatures. How to Build a Car That Listens The team had a few key questions to answer: What if the microphone housings get dirty or frosted over? How does that affect localization and classification? Testing showed performance degraded less than expected once modules were cleaned and dried. The team also confirmed the microphones can survive a car wash. Each external microphone module (EMM) contains three microphones in a 15-centimeter-wide package. Mounted at the rear of the car—where wind noise is lowest—they capture sound, digitize it, convert it into spectrograms, and pass it to a region-based convolutional neural network (RCNN) trained for audio event detection. If the RCNN classifies an audio signal as a siren, the result is cross-checked with the vehicle’s cameras: Is there a blue flashing light in view? Combining “senses” like this boosts the vehicle’s reliability by lowering the odds of false positives. Audio signals are localized through beamforming , though Fraunhofer declined to provide specifics on the technique. All processing happens onboard to minimize latency. That also “eliminates concerns about what would happen in an area with poor Internet connectivity or a lot of interference from [radio-frequency] noise,” Brandes says. The workload, he adds, can be handled by a modern Raspberry Pi . According to Brandes, early benchmarks for the Hearing Car system include detecting sirens up to 400 meters away in quiet, low-speed conditions. That figure, he says, shrinks to under 100 meters at highway speeds due to wind and road noise. Alerts are triggered in about 2 seconds—enough time for drivers or autonomous systems to react. This display doubles as a control panel and dashboard letting the driver activate the vehicle’s “hearing.” Fraunhofer IDMT The History of Listening Cars The Hearing Car’s roots stretch back more than a decade. “We’ve been working on making cars hear since 2014,” says Brandes. Early experiments were modest: detecting a nail in a tire by its rhythmic tapping on the pavement or opening the trunk via voice command. A few years later, support from a tier 1 supplier (a company that provides complete systems or major components such as transmissions, braking systems, batteries, or advanced driver assistance systems (ADASs) directly to automobile manufacturers) pushed the work into automotive-grade development, soon joined by a major automaker. With EV adoption rising, automakers began to see why ears mattered as much as eyes. “A human hears a siren and reacts—even before seeing where the sound is coming from. An autonomous vehicle has to do the same if it’s going to coexist with us safely.” —Eoin King, University of Galway Sound Lab Brandes recalls one telling moment: Sitting on a test track, inside an electric vehicle that was well insulated against road noise , he failed to hear an emergency siren until the vehicle was nearly upon him. “That was a big ‘ah-ha!’ moment that showed how important the Hearing Car would become as EV adoption increased,” he says. Eoin King , a mechanical engineering professor at the University of Galway in Ireland, sees the leap from physics to AI as transformative. “My team took a very physics-based approach,” he says, recalling his 2020 work in this research area at the University of Hartford in Connecticut. “We looked at direction of arrival—measuring delays between microphones to triangulate where a sound is. That demonstrated feasibility. But today, AI can take this much further. Machine listening is really the game changer.” Physics still matters, King adds: “It’s almost like physics-informed AI. The traditional approaches show what’s possible. Now, machine learning systems can generalize far better across environments.” The Future of Audio in Autonomous Vehicles Despite progress, King, who directs the Galway Sound Lab ’s research in acoustics, noise, and vibration, is cautious. “In five years, I see it being niche,” he says. “It takes time for technologies to become standard. Lane-departure warnings were niche once too—but now they’re everywhere. Hearing technology will get there, but step by step.” Near-term deployment will likely appear in premium vehicles or autonomous fleets, with mass adoption further off. King doesn’t mince words about why audio perception matters: Autonomous vehicles must coexist with humans. “A human hears a siren and reacts—even before seeing where the sound is coming from. An autonomous vehicle has to do the same if it’s going to coexist with us safely,” he says. King’s vision is vehicles with multisensory awareness—cameras and lidar for sight, microphones for hearing, perhaps even vibration sensors for road-surface monitoring . “Smell,” he jokes, “might be a step too far.” Fraunhofer’s Swedish road test showed that durability is not a big hurdle. King points to another area of concern: false alarms. “If you train a car to stop when it hears someone yelling ‘help,’ what happens when kids do it as a prank?” he asks. “We have to test these systems thoroughly before putting them on the road. This isn’t consumer electronics, where, if ChatGPT gives you the wrong answer , you can just rephrase the question—people’s lives are at stake.” Cost is less of an issue: microphones are cheap and rugged. The real challenge is ensuring algorithms can make sense of noisy city soundscapes filled with horns, garbage trucks, and construction. Fraunhofer is now refining algorithms with broader datasets, including sirens from the United States, Germany, and Denmark. Meanwhile, King’s lab is improving sound detection in indoor contexts, which could be repurposed for cars. Some scenarios—like a Hearing Car detecting a red-light-runner’s engine revving before it’s visible—may be many years away, but King insists the principle holds: “With the right data, in theory it’s possible. The challenge is getting that data and training for it.” Both Brandes and King agree no single sense is enough. Cameras, radar, lidar—and now microphones—must work together. “Autonomous vehicles that rely only on vision are limited to line of sight,” King says. “Adding acoustics adds another degree of safety.”",
    "published": "Tue, 30 Sep 2025 14:00:03 +0000",
    "author": "Willie D. Jones",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Have We Reached a Space-Junk Tipping Point?",
    "link": "https://spectrum.ieee.org/kessler-syndrome-space-debris",
    "summary": "Low Earth orbit , where most satellites operate, has become a whirlwind of metal shards and dead, tumbling debris. Anyone with hardware or human crew in orbit knows the drill. Orbital collision warnings can be unremitting. Whether the object is a defunct satellite or a stray hunk of glass from a solar panel that shattered long ago, every item circling Earth is also a potential projectile. And nearly all of this junk, traveling at least eight times as fast as a rifle bullet , can be damaging in a collision. SpaceX’s Starlink satellites maneuvered around possible debris impacts 144,404 times over the first half of 2025. That’s a collision warning every couple of minutes, night and day, for six months straight—three times the rate of the previous six months. Looming on the horizon, too, is the threat of orbital junk overwhelming satellites’ ability to dodge disaster. Each collision then creates more fragments, in a runaway cascade that turns low Earth orbit into a hazard zone. This article is part of The Scale Issue . For satellite operators, sudden silences could be the first warning signs. Ground station crews that today coordinate elegant sequences of thruster burns will face more chaotic obstacle courses and bigger debris fields blooming across their display monitors. Communication lines and data traffic may drop from time to time, too, sowing chaos on the ground and menacing flights across the globe. And as the slow catastrophe builds, fuel reserves for satellite constellations will bleed down into the red from so many extensive orbital maneuvers. Spacecraft that’ve run dry today will be the seedbed for tumbling, hypervelocity shrapnel tomorrow. This doomsday scenario is known as the Kessler syndrome, named after the American astrophysicist Donald Kessler , who in 1976 began circulating his first notices at NASA about possible runaway orbital debris. Now, as the magnitude of the space junk problem rapidly scales up, technological responses are ramping up as well. Solutions in the offing include high-resolution orbital tracking, AI-powered constellation management, and an emerging robotic tech called “ active debris removal .” This last item involves lofting a specialized spacecraft into orbit, armed with grippers or other satellite-wrangling tech that can target and grab orbiting stuff. The removal craft then guides the space junk through reentry and the ultimate splashdown of whatever survives reentry into the middle of the ocean. But tech alone may not be enough for the magnitude of the task ahead. The debris problem could simply be growing too fast. International treaties and government regulations may be needed to classify orbits as globally managed resources, like radio spectrum . Because as Kessler himself has pointed out, space is complicated —sometimes frustratingly so . What Is the Kessler Syndrome? In the early days, those frustrations were related to simply getting the space community to realize the problem that lay ahead. Back in the early 1970s, when low Earth orbit was all but pristine, Kessler was a midcareer NASA scientist, having already notched important contributions to the Apollo and Skylab programs. As his colleague, the late NASA administrator Burton Cour-Palais , noted in a 2004 oral history , Kessler “was bringing up this orbital debris thing, and the higher-ups did not want to know about it at all.” Cour-Palais also recalled being told to urge Kessler to “come up with solutions rather than problems.” Fortunately, neither took the overly cautious route. In June 1978, the Journal of Geophysical Research published a paper by Kessler and Cour-Palais in which they argued that a rapidly growing belt of defunct satellites, collision fragments, and other detritus could “be a significant problem during the next century.” It’s a prediction that has come to pass. In April of this year, Kessler and Hugh Lewis , professor of astronautics at the University of Birmingham , in England, presented their latest models , concluding that space junk orbiting between 400 and 1,000 kilometers—where most low Earth satellites operate—is already unstable. And between 520 and 1,000 km, the researchers found, debris concentrations are at or near levels that might sustain runaway growth. A recent internal report shared with IEEE Spectrum , written by analysts at the Menlo Park, Calif.–based LeoLabs , has divided the problem into what it calls “four waves of the Kessler syndrome.” The first three waves, it says, may have already begun. They are: nontrackable stuff like tiny steel fragments and glass splinters colliding with non-operational trackable objects; nontrackable stuff impacting functioning satellites and causing malfunctions; and trackable objects hitting other trackable objects and creating a clouds of fragments. The fourth wave, in which two large pieces of debris incite a chain reaction of other collisions, has yet to occur. In LeoLabs’ observations and models, satellites and operational spacecraft including the International Space Station, and China’s Tiangong space station continue to face manageable levels of collision avoidance maneuvers—for now. “It is assumed these operational satellites will avoid catastrophic collisions with trackable objects,” the report concludes. But according to Luc Piguet , CEO and cofounder of the Lausanne, Switzerland–based startup ClearSpace , challenges for operational satellites are real and mounting. “The Kessler syndrome is a slow, crawling effect—that when it starts accelerating, it’s already too late,” he says. “The Kessler syndrome is happening.” The problem can be further segmented into specific problematic orbits, according to Darren McKnight , senior technical fellow at LeoLabs , which performs high-resolution debris tracking for private clients and government agencies. “There are certain altitudes where we’ve already passed the threshold for the Kessler syndrome,” McKnight says. For instance, at 775 km altitude, as well as at 840 km and 975 km, the collision risk is scaling up rapidly. (See graph, “Low Earth Orbit’s Most High-Risk Places.”) “We will hit a point where particular popular orbits are so risky to operate in that the benefits of operating there are outweighed by the cost and risk,” says Danielle Wood , head of MIT Media Lab ’s Space Enabled Research Group . Why Is the Kessler Syndrome Complicated? According to the European Space Agency , 14.5 million kilograms of man-made stuff circles the planet today. Compare that to 11 million kg two years ago and 8.9 million kg in 2020—a 63 percent increase over the past five years. McKnight says the Kessler problem comes into sharper focus when dividing mass in any given orbit by the volume of space that orbit occupies. The mass density in orbit, also known as the mass per cubic kilometer, provides a clue not only to the chance of orbital collisions but also to those collisions’ consequences. Two small orbiting items colliding won’t create nearly as much new debris as will two big ones. The more densely packed an orbit is, in other words, the more treacherous it is to keep a satellite at that orbit. “Mass per cubic kilometer is debris-generating potential,” McKnight says, which would be a great thing to know with confidence in all the different regions of low Earth orbit. In 2002, Space Shuttle astronauts retrieved these solar panels from the Hubble Space Telescope—revealing how destructive even small projectiles are when traveling at low Earth orbit speeds. ESA However, says Katherine Courtney , chair of the Global Network on Sustainability in Space , knowing where all orbiting stuff is today has become a tall order. “A substantial portion of smaller space junk can only be extrapolated using data collected from returned spacecraft and historical records. The vast majority can’t be tracked from the ground,” Courtney adds. Moreover, says Jonathan McDowell , astrophysicist and space historian at the Harvard & Smithsonian Center for Astrophysics , in Cambridge, Mass., once stuff in orbit goes missing, further complications emerge. Collisions between the missing matter and other debris can completely knock the collisions’ by-products into different orbits. “The operating satellites are in nice circular orbits,” McDowell says, “whereas the collision debris is crossing many orbits and affecting many more.” What’s now needed as the problem grows larger is a complete rethink, says Moriba Jah , professor of aerospace engineering and engineering mechanics at the University of Texas at Austin. “I don’t subscribe to the Kessler syndrome,” Jah says. “It’s not that cascading collisions can’t happen. It’s that the framework oversimplifies the problem and doesn’t give us a way to manage or evolve the system.” Consider instead, Jah says, a parameter he calls “orbital carrying capacity.” “If we start from the end, we can say that carrying capacity is consumed when our ability to make decisions to avert harm no longer work,” he continues. “So to me, that doesn’t necessarily look like you’re bumping into stuff. It also looks like you’re spending fuel moving around stuff so much that you can’t do the things that you wanted to do to begin with.” How to Avoid Satellite Collisions As SpaceX proved 144,404 times from December 2024 through May of this year, the Starlink constellation’s capacity to maneuver its hardware around space junk is impressive. “ Starlink is a brilliant constellation,” McKnight says. “They’re like a granny driving on the highway. They pump their brakes. They avoid everything.” However, Starlink’s own public record also showcases how rapidly the collision hazards in orbit are evolving. The company’s publicly disclosed data reveals a 22-fold increase since 2020 in the amount of ducking and dodging the constellation has needed to perform to avoid collisions with other stuff in orbit. Everyone’s ducking and dodging these days, too. “Collision avoidance is a standard practice now for every operator,” says Tim Flohrer , head of the European Space Agency’s Space Debris Office . “You want to keep your operations making sense, communicating with everybody else,” says Marlon Sorge , technical fellow at the Chantilly, Va.–based Aerospace Corp. , “and not making more of the stuff that you can’t communicate with.” Yet, space junk isn’t the only class of noncommunicative stuff up there. “More than half of the unidentified objects are Chinese satellites,” says Courtney of the Global Network on Sustainability in Space. “So they’re active satellites, but they’re just not registered as identifiable objects.” The tracked debris, the untrackable tiny debris, the bigger things that are also incommunicado—all of it combines to make for an increasingly massive headache. “Every collision-avoidance maneuver is a nuisance,” Holger Krag, head of ESA’s Space Safety office , has said. “Not only because of fuel consumption but also because of the preparation that goes into it. We have to book ground-station passes, which costs money. Sometimes we even have to switch off the acquisition of scientific data. We have to have an expert team available round the clock.” So who or what, then, could possibly keep up with the rapidly scaling nature of the Kessler problem? Artificial intelligence is the almost unanimous answer. Many of the world’s major players in low Earth orbit, including small satellite startups and big national space programs , are currently testing and developing AI constellation-management systems. Machine-learning algorithms are proving increasingly adept at making more accurate collision warnings and performing automated decision-making —as well as sharpening the resolution of small object detection to find smaller orbiting stuff than what non-AI-powered tracking tech can see. Some companies and research teams are also developing AI tools to go beyond just keeping pace with the problem, using AI to optimize fuel usage and maintain ideal satellite configurations for low battery usage and simplified signal traffic as well. However, for all its smarts, AI still can’t make the most difficult orbital hazards go away. That’s why some companies are approaching the Kessler problem as one of disposing, rather than dodging. A number of startups are actively pursuing ways to extract the most dangerous orbital objects— defunct rocket stages , dead satellites, space collision fragment clouds , and space-race relics . “The technology available to remove debris today is really toward larger pieces of debris,” says Andrew Faiola , commercial director of the Tokyo-based company Astroscale . “We’re just maturing that capability to be able to effectively, safely, and securely remove large pieces of debris.” Astroscale and ClearSpace aim to launch spacecraft over the next few years that will each target an aging satellite (a Eutelsat OneWeb satellite and ESA’s PROBA-1, respectively) for a prototype removal mission. The European radar imaging satellite Sentinel-1A caught a millimeter-sized particle impacting one of its solar panels, leaving behind a 40-centimer wide zone of damage. ESA “You need to do controlled entry,” ClearSpace’s Piguet says. “This means you need to push this satellite into Point Nemo over the South Pacific, where there’s no airlines, ground traffic, and no inhabited island.” Ideally, then, between smart constellation management, active collision avoidance, and active cleanup, low Earth orbit will become something closer to a regulated and moderated space—much like airspace around major metro areas today. “It’s much the same as air-traffic control,” Faiola says. “As the technology gets better, you start to see aircraft being stacked more closely together. You have the same amount of real estate, but you can put more objects in there more safely when you have better visibility and situational awareness of where everything is. It’s the same in space.” What Are the Solutions to the Kessler Problem? Space tech and space tech alone may one day resolve the Kessler syndrome. But as a complement to the technological innovation, international space agreements and law are also being reconsidered, because much of the existing space law standards were agreed on decades ago, during an entirely different era in low Earth orbit. For instance, between the Outer Space Treaty of 1967 and the 1972 Space Liability Convention, even an untraceable fragment of metal in space is effectively owned by the nation that launched it. This arguably means that that nation may need to give permission for anyone else to remove the fragment from orbit. “There’s no national borders up there,” says Faiola. “But every object that is cataloged is also owned by someone, a state. And you’re not allowed to touch someone else’s stuff without their permission.” In August, Japan announced it would be developing its own legal frameworks for removing space junk from orbit. And this November, in Vienna, the United Nations Office for Outer Space Affairs will be hosting a space law conference to tackle these issues as well. International agreements need reconsidering in other ways, too. Some space experts Spectrum spoke with argue for additional regulations to prevent orbits from further clogging up. “There will have to be internationally coordinated agreements on who gets what orbit and how many satellites you can have in that orbit,” says Smithsonian’s McDowell. Courtney envisions something like a worldwide space command network. “We need to be designing solutions that allow the growth to continue,” she says. “What we need is a global space traffic control solution like we have for air traffic today.” Jah of the University of Texas at Austin argues for ultimately bringing orbital space closer to its original state of being, as he puts it, “a viable commons.” When a new player—whether a company or a national space agency—wants to put something into a given orbit, he says, that new orbiting asset should be added to a master spreadsheet somewhere. “If another country wants to be able to be in that orbit, there should be an equitable way to share the carrying capacity of that orbit,” he says. Rockets, satellites, and launch systems today still follow the space race–era legacy designs that treat orbital space like an infinite junkyard, he adds. “Right now, every single object that we launch into orbit is the equivalent of a single-use plastic,” Jah says. “We need to invest in reusable and recyclable satellites.” Even if the Kessler problem on the home planet can be solved, says Courtney, the same thing could happen on other planets and moons. “We’re very worried about low Earth orbit, but [there’s also] all the commercial activity and all of the great-power competition for landing things on the moon and Mars,” she says. “We have no space-traffic coordination solutions for cislunar space , and yet that’s the race that’s just starting now,” she says. “We’re expanding outward into the solar system, and we’re just taking these problems with us.”",
    "published": "Tue, 30 Sep 2025 13:00:03 +0000",
    "author": "Margo Anderson",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Why the World Needs a Flying Robot Baby",
    "link": "https://spectrum.ieee.org/ironcub-jet-powered-flying-robot",
    "summary": "One of the robotics projects that I’ve been most excited about for years now is iRonCub , from Daniele Pucci’s Artificial and Mechanical Intelligence Lab at the Italian Institute of Technology (IIT) in Genoa, Italy. Since 2017 , Pucci has been developing a jet-propulsion system that will enable an iCub robot (originally designed in 2004 to be the approximate shape and size of a 5-year-old child) to fly like Iron Man. Over the summer, after nearly 10 years of development, iRonCub3 achieved lift-off and stable flight for the first time , with its four jet engines lifting it 50 centimeters off the ground for several seconds. The long-term vision is for iRonCub (or a robot like it) to operate as a disaster response platform, Pucci tells us. In an emergency situation like a flood or a fire, iRonCub could quickly get to a location without worrying about obstacles, and then on landing, start walking for energy efficiency while using its hands and arms to move debris and open doors. “We believe in contributing to something unique in the future,” says Pucci. “We have to explore new things, and this is wild territory at the scientific level.” Obviously, this concept for iRonCub and the practical experimentation attached to it is really cool. But coolness in and of itself is usually not enough of a reason to build a robot, especially a robot that’s a (presumably rather expensive) multi-year project involving a bunch of robotics students, so let’s get into a little more detail about why a flying robot baby is actually something that the world needs. In an emergency situation like a flood or a fire, iRonCub could quickly get to a location without worrying about obstacles, and then on landing, start walking for energy efficiency while using its hands and arms to move debris and open doors. IIT Getting a humanoid robot to do this sort of thing is quite a challenge. Together, the jet turbines mounted to iRonCub’s back and arms can generate over 1000 N of thrust, but because it takes time for the engines to spool up or down, control has to come from the robot itself as it moves its arm-engines to maintain stability. “What is not visible from the video,” Pucci tells us, “is that the exhaust gas from the turbines is at 800 °C and almost supersonic speed. We have to understand how to generate trajectories in order to avoid the fact that the cones of emission gases were impacting the robot.” Even if the exhaust doesn’t end up melting the robot, there are still aerodynamic forces involved that have until this point really not been a consideration for humanoid robots at all—in June, Pucci’s group published a paper in Nature Engineering Communications , offering a “comprehensive approach to model and control aerodynamic forces [for humanoid robots] using classical and learning techniques.” “The exhaust gas from the turbines is at 800 °C and almost supersonic speed.” —Daniele Pucci, IIT Whether or not you’re on board with Pucci’s future vision for iRonCub as a disaster-response platform, derivatives of current research can be immediately applied beyond flying humanoid robots. The algorithms for thrust estimation can be used with other flying platforms that rely on directed thrust, like eVTOL aircraft. Aerodynamic compensation is relevant for humanoid robots even if they’re not airborne, if we expect them to be able to function when it’s windy outside. More surprising, Pucci describes a recent collaboration with an industrial company developing a new pneumatic gripper. “At a certain point, we had to do force estimation for controlling the gripper, and we realized that the dynamics looked really similar to those of the jet turbines, and so we were able to use the same tools for gripper control. That was an ‘ah-ha’ moment for us: first you do something crazy, but then you build the tools and methods, and then you can actually use those tools in an industrial scenario. That’s how to drive innovation.” What’s Next for iRonCub: Attracting Talent and Future Enhancements There’s one more important reason to be doing this, he says: “It’s really cool.” In practice, a really cool flagship project like iRonCub not only attracts talent to Pucci’s lab, but also keeps students and researchers passionate and engaged. I saw this firsthand when I visited IIT last year, where I got a similar vibe to watching the DARPA Robotics Challenge and DARPA SubT —when people know they’re working on something really cool , there’s this tangible, pervasive, and immersive buzzing excitement that comes through. It’s projects like iRonCub that can get students to love robotics. In the near future, a new jetpack with an added degree of freedom will make yaw control of iRonCub easier, and Pucci would also like to add wings for more efficient long-distance flight. But the logistics of testing the robot are getting more complicated—there’s only so far that the team can go with their current test stand (which is on the roof of their building), and future progress will likely require coordinating with the Genoa airport. It’s not going to be easy, but as Pucci makes clear, “This is not a joke. It’s something that we believe in. And that feeling of doing something exceptional, or possibly historical, something that’s going to be remembered—that’s something that’s kept us motivated. And we’re just getting started.”",
    "published": "Tue, 30 Sep 2025 12:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "No Vacancy",
    "link": "https://spectrum.ieee.org/steve-searcy-october-2025-poem",
    "summary": "We forge ahead with scientific quests, but even zealous efforts hit a wall trying to make what nature most detests: nothing at all. It seems like such a little thing to do— removing every molecule. We find that though we displace almost all, a few remain behind. It takes attentive planning and robust equipment in a lab to do the chore of pumping vacuum pressure down to just a millitorr. The stalwart researcher persists and loses sleep, but can’t reach perfection—I’m afraid the universe still stubbornly refuses to be unmade. Even in deepest space, a cubic meter contains some particles. We must assess there is no void, although conditions teeter on emptiness. The quantum mysteries will vex and weary the brightest mind, the sharpest physicist. True nothingness, while wonderful in theory, does not exist.",
    "published": "Mon, 29 Sep 2025 18:00:04 +0000",
    "author": "Steven Searcy",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Electric Boats Drive New England Aquaculture",
    "link": "https://spectrum.ieee.org/electric-boats-aquaculture",
    "summary": "This article was originally published by Canary Media . At a dock along the banks of the Cousins River, Chad Strater loaded up his small aluminum workboat with power tools and a winch. Strater, who owns a marine construction business, was setting out to tinker with floating equipment at a nearby oyster farm. On the quiet morning in August, with the sun already beating down hard, his vessel whirred to life, only without the usual growl of an oil-guzzling motor. The boat is all electric. Just north of where the Cousins River meets Casco Bay, Willy Leathers was powering up his own electric watercraft, which had its first outing in July. Leathers uses his 28 -foot (8.5-meter) boat for cultivating oysters at Maine Ocean Farms , where roughly 3 million of the animals grow in dozens of floating cages. Both Strater and Leathers said they switched to electric workboats for several reasons. Their new watercraft are a cleaner alternative to the smelly, polluting petroleum-powered vessels that dominate Maine’s 3 , 500 miles (5,633 kilometers) of coastline. Electric propulsion is also significantly quieter than a gas or diesel motor. For Leathers, whose 10 -acre (4-hectare) sea farm is a significant presence in the cove where he operates, the swap is about being a good neighbor to the shoreside community. “ It’s an innovation born from necessity for us,” said Strater about his electric boat, which he docks each night at the Sea Meadow Marine Foundation , the nonprofit boatyard and aquaculture innovation hub he runs with several other small business owners. “[The boat] really works well for what we do with it, and we’re letting farmers use it to see how it could work for them.” The Rise of Electric Boats Battery-powered vessels are starting to catch on in the United States and worldwide as companies and maritime authorities work to reduce emissions and improve the experience of cruising waterways. The technology ranges from small outboard motors on workboats and recreational watercraft to powerful inboard systems on ferries, tugboats , and supply vessels for offshore wind farms and oil rigs. In recent decades, Norway, with its extensive coastline and ample government funding, has spearheaded the transition globally. China, which is both the world’s largest shipbuilder and battery manufacturer, has rapidly deployed hundreds of battery-powered vessels over the last several years. Falling battery costs, better technology, and stricter environmental rules are compelling some vessel owners to install partial or fully electric systems, primarily for watercraft that operate near the shore or on fixed routes. For commercial fishing in particular, customers are helping to drive the push to clean up. “ Everyone’s more concerned now with where their food comes from, and we’ve seen that [consumers] are looking for that complete sustainable supply chain,” said Ed Schwarz, the head of marine solutions sales in North America for Siemens Energy, which has built electric propulsion systems for U.S. ferries . Maine Ocean Farms founders Eric Oransky [left] and Willy Leathers switched to an electric workboat in July 2025. Brendan Bullock Electrification has only very recently come to the U.S. aquaculture sector. In Maine, the small but fast-growing segment includes nearly 200 farms for shellfish, fin fish, and edible seaweed. Strater and Leathers are among the first in their business to trade gas motors for electric propulsion—a switch they say they’re hoping to accelerate. Oil-guzzling motors are among the largest sources of greenhouse gas emissions for the state’s multibillion-dollar seafood sector. Still, electrifying commercial watercraft can be a difficult course to navigate , given the higher up-front costs of electric motors and the lack of charging infrastructure—and grid infrastructure in general—in rural waterfront communities. Early adopters like Strater and Leathers said they hope the experiences gained from their demonstrations can help pave the way for decarbonizing Maine’s blue economy. With the help of the Island Institute , a Maine-based nonprofit that works on marine-related energy transitions, Leathers is collecting performance data from his vessel to share more broadly with the industry. “ People say it looks cool and shiny and looks like it operates great,” Lia Morris, the Island Institute’s senior community development officer, said of electric boats. “ But we really want to be able to prove out the [business] case.” Electric boats can cost between 20 percent and 30 percent more than a gas- or diesel-powered vessel of a comparable size. However, owners can save on maintenance and fuel over the long term, Strater’s business partner Nick Planson said. “ The high-level math that we’ve come up with” is a financial break-even point of “ about four to five years, and then over a 10 -year time span, you’re definitely coming out way ahead based on the vastly reduced maintenance cost, replacement cost of failed equipment, and fuel costs,” said Planson. Battery-Powered Workboats Lack Charging Infrastructure But the initial price tag presents a significant hurdle. Strater and Planson’s sleekly designed, no-frills watercraft cost US $ 100 , 000 to build and outfit with a single electric outboard motor. Leathers’s boat, called Heron, cost about four times as much. It has two electric outboards and a ramp for unloading and hauling more than 10 , 000 oysters at a time from the sea farm to distributors waiting on the dock. Its hull is also equipped with a small cabin and toilet. Both operations relied on grant funding to defray the expense of going electric. For their part, Strater and Planson used about $ 50 , 000 from a larger U.S. Department of Agriculture small business grant they got in 2024 to establish a use case for electric workboats in the aquaculture industry. Leathers’s business, Maine Ocean Farms, was included on a collaborative $ 500 , 000 U.S. Department of Energy (DOE) grant last year that earmarked about $ 289 , 000 for boat building and propulsion systems, in addition to other funds for charging infrastructure and data collection. The prospects for funding future projects are now much murkier under the Trump administration, maritime policy experts say. The DOE ’s Office of Energy Efficiency and Renewable Energy, which awarded the money to Maine Ocean Farms and its partners, is facing significant budget cuts in the next fiscal year. The GOP-backed spending law that passed in July rescinded some unobligated grant funding for cleaning up marine diesel engines. While other programs were spared, it’s unclear whether the current Congress will approve new funding for initiatives ranging from electrifying huge urban ports to deploying low-emissions ferries in rural communities. “ We can go really fast for a short distance. We can go really slow for a long distance, and it works for what we do with it,” says Strater. But federal grants aren’t the only way to address the higher cost of electric boats. Strater and Planson also worked with Coastal Enterprises, Inc., a Maine-based community development financial institution focused on climate resilience, to establish a “ marine green” loan program that can make the up-front costs of switching to electric propulsion more accessible to small businesses. “ The more electric engines that are being employed in Maine helps lift the whole tide for everyone,” said Nick Branchina, director of CEI ’s fisheries and aquaculture program. As part of its marine green lending, CEI offers loans starting at $ 25 , 000 for small businesses to make the switch to electric propulsion and comfortably afford the cost of batteries or a shoreside charging installation. Planson said that as electrification moves beyond initial grant-funded projects, the challenge is keeping systems affordable. He said he wants to see other small business owners able to “ take a reasonable swing” at electric propulsion. Buying a boat, of course, is only the first obstacle. Electric vessel owners must also learn how to use their new propulsion systems and find a place to charge them. How Do Electric Boats Perform in Cold Weather? This summer, Leathers said he’s had no trouble making the nearly 2-mile (3-kilometer) round trip from the slip where he docks Heron in South Freeport, Maine, to his farm on Casco Bay. With a full charge, he can make trips slightly farther to meet distributors closer to Portland. But as temperatures drop this winter, Leathers said he’s not sure how far the outboards’ two batteries will take him. Cold weather can reduce battery capacity and impact performance, shrinking an electric motor’s range. It’s a part of Leathers’s demonstration to find out what the impacts are in practice. Like Leathers, Strater and Planson also work year-round. They said they’re both impressed with how their boat performed last winter after launching in the fall of 2024 . For Planson, who markets battery-powered equipment to aquaculture farmers as part of his startup, Shred Electric , a boat’s ability to run through the year’s coldest months is a key selling point. “ The proof is in the pudding,” said Planson. “ When you’re working with…waterfront applications, it really needs to work every day and all year.” Strater and Planson said their boat’s range was an important consideration when they partnered with the startup Flux Marine to build the electric outboard motor. With limited shoreside charging infrastructure in place, the boat has to make it out and back on a single charge, sometimes to aquaculture operations 7 miles (11 kilometers) away. In the 10 months since the boat’s launch, Strater has learned that range correlates to speed. He can modulate the boat’s pace depending on how far he wants to go. “ We can go really fast for a short distance. We can go really slow for a long distance, and it works for what we do with it,” he said. Soon, Maine’s early adopters will have shared access to a higher-capacity Level 2 charger that will be installed at the Sea Meadow Marine Foundation and can charge batteries in little over 2 hours, or three times as fast as the current system. The startup Aqua SuperPower was awarded a portion of the DOE funding last year to install additional marine chargers there and at a wharf in Portland owned by the Gulf of Maine Research Institute. Island Institute also helped with grant funding for the charger at the Sea Meadow boatyard. Maine will need much more high-capacity charging infrastructure for the marine industry to transition to electric propulsion, said Morris, of the Island Institute. As the state’s aquaculture and fisheries industries look to grow beyond small-scale operations, other businesses will need to charge more frequently to make longer, farther trips up and down the coast. Expanding charging stations north of Casco Bay represents what Morris calls a “ chicken and egg” problem: a dynamic where chargers are either installed before demand gets high and sit unused, or electric boats hit the water and there’s not enough charging infrastructure, stalling future adoption. This challenge is compounded by both New England’s aging grid infrastructure and the remote nature of some of the region’s waterfront access points. Getting the right amount of power to a charging station on the shore can be costly, even in Yarmouth, which sits on Casco Bay. Often it’s the last mile that can be the most expensive. At Sea Meadow Marine Foundation, three-phase power, which can accommodate higher loads, is limited by the dirt road that separates the boat launch from the more heavily trafficked U.S. Route 1 . “ There are a lot of complicated questions,” Morris said. “ I don’t think it’s unique to Maine, it’s any rural area, but complicated questions and conversations with the utilities and the rural municipalities are going to have to be solved for.” Back on the water, Leathers docked his electric boat, Heron, alongside the sea farm’s barge, where thousands of oysters pass through for processing on harvest days. He switched the motor off and hopped onto the floating platform. For a moment, the bay was calm to the point of near silence. Then Leathers picked up an oyster cage with a rattle, turning it over in his hands as water splashed out. The sounds of the workday began. “ As a whole industry, I think it’s going to take proving that someone like us can do it,” Leathers said. “ And then the next person kind of snowballing after that.”",
    "published": "Mon, 29 Sep 2025 16:00:04 +0000",
    "author": "Julia Tilton",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Array-Scale MUT Simulations Powered by the Cloud",
    "link": "https://events.bizzabo.com/764786",
    "summary": "Designing and optimizing ultrasound transducers—whether PMUTs or CMUTs—requires accuracy at scale. Yet traditional simulation approaches are often constrained to individual cells or limited structures, leaving important array-level effects poorly understood until expensive and time-consuming testing begins. This gap can lead to longer development cycles and higher risk of failed devices. In this webinar, we will introduce the improved approach: full array-scale MUT simulations with fully coupled multiphysics. By leveraging Quanscient’s cloud-native platform, engineers can model entire transducer arrays with all relevant physical interactions (electrical, mechanical, acoustic, and more) capturing system-level behaviors such as beam patterns and cross-talk that single-cell simulations miss. Cloud scalability also enables extensive design exploration. Through parallelization, users can run Monte Carlo analyses, parameter sweeps, and large-scale models in a fraction of the time, enabling rapid optimization and higher throughput in the design process. This not only accelerates R&D but ensures more reliable designs before fabrication. The session will feature real-world case examples with detailed insights of the methodology and key metrics. Attendees will gain practical understanding of how array-scale simulation can greatly improve MUT design workflows reducing reliance on costly prototypes, minimizing risk, and delivering better device performance. Join us to learn how array-scale MUT simulations in the cloud can improve MUT design accuracy, efficiency, and reliability. Register now for this free webinar!",
    "published": "Mon, 29 Sep 2025 14:47:06 +0000",
    "author": "Quanscient",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "The High-Speed Plan for Interstellar Travel",
    "link": "https://spectrum.ieee.org/high-speed-interstellar-travel",
    "summary": "To the naked eye, the stars are diamond flecks scattered across the inner surface of a celestial sphere. Telescopes have brought depth to our vision, mapping the true distances to cosmic objects. But the universe they reveal appears utterly beyond the human scale of space and time. Even the closest stars seem infinitely remote, and reaching them a thing of science fiction, save for a few dead and dying probes drifting outward for eternity . This article is part of The Scale Issue . Now, though, a cadre of researchers are working to make interstellar travel a reality, at least to our nearest neighbors. They are coalescing around an approach that could lead to closeup images of a star and an exoplanet just 25 years after mission launch. Most of each 4-meter probe will be a disc of aerographene or similar material, just a few micrometers in thickness, with optical sensors and transmitters on one side and a reflective surface on the other that the launch laser will aim at. The rim of the probe will be a 2-centimeter-thick band. The trailing edge will have apertures for interprobe laser communications. Power and processing electronics will form a ring inside the rim. The swarm’s optical transmitters will pulse in unison to send data to Earth at a rate of around 1 kilobit per second. Chris Philpot The first generation of theoretical starship designs had featured massive vehicles propelled by fission or fusion drives. Top speed was estimated at about 10 percent of the speed of light, or 0.1 c . This meant that a flyby mission to the closest star system, Proxima Centauri , would take over 42 years to reach its target. In contrast, the new generation of starship designs are tiny, and they have no drives at all. The spacecraft have a mass of a few grams each. They’ll be accelerated out of our solar system by ground- or space-based lasers, traveling at an estimated 0.2 c . A 100-gigawatt laser beam made by combining many smaller lasers will propel hundreds to thousands of tiny probes. Pushing against interstellar magnetic fields, the probes will turn edge on to minimize radiation and impact damage. By adjusting the launch laser to accelerate later probes to higher speeds than earlier ones, the string of probes will coalesce into a swarm by the time of arrival. Chris Philpot One version of this small-and-fast approach calls for sending a swarm of these puny flyers to the Proxima Centauri b exoplanet . Data would be returned by having the swarm emit light pulses in synchrony, detectable by telescopes on Earth. Put forward by a team led by Thomas Marshall Eubanks at Space Initiatives , this mission was selected for a 2024 phase one study by NASA’s Innovative Advanced Concepts program. It didn’t make the list for a phase two study this year, but Eubanks plans to retry in 2026. With a swarm, “we could do gigapixel imaging of the planet,” says Eubanks. “That’s at a level where if it was a planet like Earth, we’d be able to see things like coral reefs and airports.”",
    "published": "Mon, 29 Sep 2025 13:00:03 +0000",
    "author": "Stephen Cass",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Build Your Own Commodore 64 Cartridge",
    "link": "https://spectrum.ieee.org/commodore-64-cartridge",
    "summary": "Have you ever thought “ IEEE Spectrum is terrific, but I just wish I had a way to experience even more of it, perhaps at a local science and technology museum?” Well, I am pleased to say that your very specific wish has been granted! In collaboration with the IEEE History Center and the IEEE Global Museum and the support of generous donors, Spectrum ’s Chip Hall of Fame has been adapted into a traveling exhibit that has just begun making its way around U.S. museums, and, hopefully, the world. Our Chip Hall of Fame celebrates microchips that have had a significant impact. Six of the chips from the hall were chosen to be part of the “Chips That Shook The World” exhibit, along with artifacts embodying how each was used. One of the chosen was the 8-bit 6502 processor, so naturally we thought a Commodore 64 home computer, which used a 6502 variant, should be one of the artifacts. Which led to another thought: Why not have the C64 run a program demonstrating an 8-bit CPU in action? That’s how I ended up, 35 years after I last programmed a C64, sitting at my office desk creating a brand-new plug-in cartridge. The C64 supported plug-in cartridges as a way of distributing software, and our demo program needed to be put on one. Each morning, the museum curator can just turn on the exhibit and presto! The demo program instantly begins running. The alternatives would have required the curator to type in commands to load the demo manually. But this convenience comes with two big caveats: One, the demo has to fit into just 16 kilobytes, the maximum size of a cartridge. Even by the standards of the 1980s, this is small, as some C64 titles spanned hundreds of kilobytes by loading data in chunks from disk or tape. Two, the demo would have to be written in 6502 assembly and control the C64 video hardware directly. Cartridges require only a few components: a printed circuit board [bottom right], programmable memory chip [bottom middle], and some resistors, diodes, and a capacitor [top middle]. They are mounted in a 3D-printed shell [top left and right]. To make the video output compatible with modern screens, we used a RetroTink-2X Pro adapter [bottom left]. James Provost Fortunately, from attending Vintage Computer Federation events over the years , I knew there were a lot of free or inexpensive resources that would make it easier than ever before to do this sort of thing. The first step was to figure out just how much I could do in 16 KB. The C64’s graphics hardware was groundbreaking in its day, capable of displaying images of up to 320 by 200 pixels with a palette of 16 colors. It could also display eight sprites at once; each sprite is a moveable single-color 24-by-21 pixel bitmap. The price for this power was complexity. The video chip’s control registers , screen bitmaps, text-screen data, default and custom character sets, sprites, and color information all live in different locations scattered across memory, with some data actually living in separate RAM and ROM chips. So I sat down with the detailed memory maps and video hardware programming guides available for the C64. (This abundance of information is in stark contrast to the 1980s, when documentation was scant, even from Commodore itself). I worked out that I could cram in nine screens of explanatory text, animated graphics, and sprites. Creating these screens, including the custom character sets and sprites they rely on, was greatly simplified thanks to the online C64 graphics editor at petscii.krissz.hu . The editor can output some results as stand-alone assembly programs, which I adapted as subroutines in my demo code. I had just enough space remaining for a lucky find. I wanted to display at least one full-screen bitmapped image, but a prerendered image would have required 8 KB of data, half the cartridge’s capacity. Instead I decided to use a classic hack of programmers since the days of games like Rogue and Elite : pulling free data out of the structure of mathematics by way of procedural generation . Here’s where I got lucky: I came across the work of Marcello M., who had published the source for a C64 assembly program that quickly created a multicolor fractal Mandelbrot set using just 3.3 KB of code. With Marcello’s blessing, I incorporated his code as another subroutine. Modern Tools For Writing C64 Software The coding was done using the free IDE 65xx and Kick Assembler desktop software. I was able to test the code using the popular C64 Vice emulator , which allowed me to do handy things like examining live memory contents to find runtime bugs. Regions of the Commodore 64’s RAM were mapped to things like the system ROMs, video-color memory, and the bitmaps of characters, the latter of which were actually mapped to multiple locations in RAM when accessed by the video hardware. Some of these mappings overlapped: Inserting a 16-KB cartridge automatically disabled the ROM storing the Basic interpreter. James Provost The next step was to make a physical cartridge. Again, there’s modern help, this time in the form of US $5 printed circuit boards that need just a handful of components soldered in to make a cartridge. These components include a programmable ROM chip that I picked up for $3 on eBay . I burned my code to the memory chip with my trusty TL866 programmer and mounted it to the PCB, which in turn was mounted into a 3D-printed cartridge case . Then came the moment of truth. It probably won’t come as a surprise to regular readers that I already own an original C64, which I connect to modern flat-screen displays via a RetroTink-2X Pro adapter. So I carefully pushed the cartridge into its slot and turned the machine on. Naturally, my C64 immediately froze up. I had forgotten to remove a little bit of memory-management code that made the demo work in the Vice emulator by disabling the ROM that stores the C64’s Basic interpreter. On the real hardware, this snippet ended up disabling half the cartridge’s memory. A quick edit and a trip back to the TL866 and I was ready to try again. Success! I was finally literally ready to ship some software, all the way to Upland Exhibits , the people building our traveling display. I hope you get a chance to see my little demo and our “Chips That Changed The World” exhibit in person: We’ll post current and upcoming locations on the Chip Hall of Fame page. But in the meantime, whether you used the C64 back in the day, or are just looking for a fun coding challenge, I recommend trying your hand at programming this 8-bit classic, now that so many of the original pain points have been reduced!",
    "published": "Sun, 28 Sep 2025 13:00:01 +0000",
    "author": "Stephen Cass",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Andrew Ng: Unbiggen AI",
    "link": "https://spectrum.ieee.org/andrew-ng-data-centric-ai",
    "summary": "Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University , cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu , where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A. Ng’s current efforts are focused on his company Landing AI , which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. He has also become something of an evangelist for what he calls the data-centric AI movement , which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias. Andrew Ng on... What’s next for really big models The career advice he didn’t listen to Defining the data-centric AI movement Synthetic data Why Landing AI asks its customers to do the work The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an unsustainable trajectory . Do you agree that it can’t go on that way? Andrew Ng: This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions. When you say you want a foundation model for computer vision, what do you mean by that? Ng: This is a term coined by Percy Liang and some of my friends at Stanford to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, GPT-3 is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them. What needs to happen for someone to build a foundation model for video? Ng: I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision. Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries. Back to top It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users. Ng: Over a decade ago, when I proposed starting the Google Brain project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation. “In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.” —Andrew Ng, CEO & Founder, Landing AI I remember when my students and I published the first NeurIPS workshop paper advocating using CUDA , a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince. I expect they’re both convinced now. Ng: I think so, yes. Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.” Back to top How do you define data-centric AI, and why do you consider it a movement? Ng: Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data. When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline. The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a data-centric AI workshop at NeurIPS , and I was really delighted at the number of authors and presenters that showed up. You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them? Ng: You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn. When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set? Ng: Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of RetinaNet . It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system. “Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.” —Andrew Ng For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance. Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training? Ng: Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, Olga Russakovsky gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed Mary Gray’s presentation, which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like Datasheets for Datasets also seem like an important piece of the puzzle. One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way. When you talk about engineering the data, what do you mean exactly? Ng: In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a Jupyter notebook and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity. For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow. Back to top What about using synthetic data, is that often a good solution? Ng: I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, Anima Anandkumar gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development. Do you mean that synthetic data would allow you to try the model on more data sets? Ng: Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category. “In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.” —Andrew Ng Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data. Back to top To make these issues more concrete, can you walk me through an example? When a company approaches Landing AI and says it has a problem with visual inspection, how do you onboard them and work toward deployment? Ng: When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the LandingLens platform. We often advise them on the methodology of data-centric AI and help them label the data. One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory. How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up? Ng: It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations. In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists? So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work. Ng: Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains. Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement? Ng: In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it. Back to top This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist .”",
    "published": "Wed, 09 Feb 2022 15:31:12 +0000",
    "author": "Eliza Strickland",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "How AI Will Change Chip Design",
    "link": "https://spectrum.ieee.org/ai-chip-design-matlab",
    "summary": "The end of Moore’s Law is looming. Engineers and designers can do only so much to miniaturize transistors and pack as many of them as possible into chips . So they’re turning to other approaches to chip design, incorporating technologies like AI into the process. Samsung, for instance, is adding AI to its memory chips to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has doubled its processing power compared with that of its previous version. But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with Heather Gorr , senior product manager for MathWorks ’ MATLAB platform. How is AI currently being used to design the next generation of chips? Heather Gorr: AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider. Heather Gorr MathWorks Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI. What are the benefits of using AI for chip design? Gorr: Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a reduced order model , where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your parameter sweeps , your optimizations, your Monte Carlo simulations using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design. So it’s like having a digital twin in a sense? Gorr: Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end. So, it’s going to be more efficient and, as you said, cheaper? Gorr: Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering. We’ve talked about the benefits. How about the drawbacks? Gorr: The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it’s not going to be as accurate as that precise model that we’ve developed over the years. Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It’s a case where you might have models to predict something and different parts of it, but you still need to bring it all together. One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge. How can engineers use AI to better prepare and extract insights from hardware or sensor data? Gorr: We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start. One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on GitHub or MATLAB Central , where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI. What should engineers and designers consider wh en using AI for chip design? Gorr: Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team. How do you think AI will affect chip designers’ jobs? Gorr: It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip. How do you envision the future of AI and chip design? Gorr : It’s very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.",
    "published": "Tue, 08 Feb 2022 14:00:01 +0000",
    "author": "Rina Diane Caballar",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  },
  {
    "title": "Atomically Thin Materials Significantly Shrink Qubits",
    "link": "https://spectrum.ieee.org/2d-hbn-qubit",
    "summary": "Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality. IBM has adopted the superconducting qubit road map of reaching a 1,121-qubit processor by 2023 , leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability. Now researchers at MIT have been able to both reduce the size of the qubits and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100. “We are addressing both qubit miniaturization and quality,” said William Oliver , the director for the Center for Quantum Engineering at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.” The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit. Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C). Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator. Nathan Fiske/MIT In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another. As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance. In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates. “We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author Joel Wang , a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas. While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor. “What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.” This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits. “The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang. Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.",
    "published": "Mon, 07 Feb 2022 16:12:05 +0000",
    "author": "Dexter Johnson",
    "topic": "main",
    "collected_at": "2025-10-08T14:03:12"
  }
]