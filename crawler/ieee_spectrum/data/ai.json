[
  {
    "title": "Photons Make for More Energy-Efficient AI Image Generation",
    "link": "https://spectrum.ieee.org/generative-optical-ai-nature-ucla",
    "summary": "AI generators can create weird, wacky, or wonderful images—all while emitting hefty amounts of carbon . Energy-hungry electronic computations drive the generative AI process, with the underlying diffusion models trained to produce novel images out of random noise. Researchers at the University of California, Los Angeles aim to reduce this carbon footprint by employing photons instead of electrons to power AI image generation. Their optical generative models pair digital processors with analog diffractive processors that compute with photons. The group described their technology 27 August in the journal Nature . Optical Generative Models Explained Here’s how the process works. The first step is called knowledge distillation, in which a “teacher” diffusion model trains a “student” optical generative model to digitally process random noise. Next, the student model encodes random noise inputs into optical generative seeds, which are phase patterns representing the phase information of light—think of each seed as something like a slide for an overhead projector. These seeds are displayed on a spatial light modulator (SLM), which can control the phase of light passing through it. (The specific SLMs used by the researchers are liquid crystal devices.) When laser light shines through the seed, its phase pattern propagates through a second SLM. This SLM—the diffractive processor—decodes the phase pattern to create a new image captured by an image sensor. “There’s a digital encoder, which gives you the seed rapidly, and then the analog processor is the key that decodes that representation for the human eye to visualize,” says Aydogan Ozcan , a professor of electrical and computer engineering at UCLA. “The generation happens in the optical analog domain, with the seed coming from a digital network. All in all, it’s replicating or distilling the information generation capabilities of a diffusion model.” Generation happens at the speed of light: “The system runs end-to-end in a single snapshot,” Ozcan says. By harnessing the physics of optics, these systems can run more swiftly and potentially consume less energy than diffusion models that iterate through thousands of steps. The team devised two versions of their model. The aforementioned “snapshot model” that generates an image in a single optical pass, and an iterative model that enhances its outputs successively. The iterative model creates images with higher quality and clearer backgrounds than its snapshot counterpart. Both models were able to produce monochrome and multicolor images—including representations of butterflies, fashion products, handwritten digits, and even Van Gogh-style art—that the researchers found closely resembled the output image quality of diffusion models. Privacy Benefits of Optical Models Optical generative models offer an added benefit of privacy, mimicking encryption capabilities. “If you look at the phase information of the digital encoder, you won’t understand much from it. It’s not for the human eye to directly visualize,” says Ozcan. “That means if somebody intercepts the image of the digital encoder and looks at it or tries to decode it without the decoder, [they] won’t be able to do that. I can then encrypt the information that is generated so that only you can decode it and nobody else can know what it represents.” An experimental setup for a “snapshot” optical generative model creates monochrome images of handwritten digits and fashion items. Shiqi Chen, Yuhang Li, et al. Ozcan is quick to point out that the architecture the team has developed may not suit content generation for digital use. “If you want to compute in the digital world as part of a digital computer ecosystem, maybe going from digital to analog and then back to digital might not be very ideal,” he says. “That’s why we are thinking of them as visual computers. They compute for the human eye in the analog world. And that’s where this fits better, as opposed to calling it as an alternative to a digital generative model—it’s not.” This makes optical generative models apt for art, entertainment, and media applications, especially augmented reality and virtual reality. “We can make this system work as part of AR and VR variable systems, where the device must communicate with the human eye and project into the human eye. During this projection, we can use the decoder as not just a projection system but also as a processing system, so that you can communicate from the cloud with optical generative seeds and do the last part of the computing with just light matter interactions as you are communicating with the human eye,” says Ozcan. As part of their next stage, the researchers are exploring potential avenues of commercialization, as well as turning their prototype into a smaller form. “That way, the system can be significantly more compact, and it can even further reduce the power consumption,” Ozcan says. For now, the team has reimagined a brighter, more sustainable generative AI future with the help of light.",
    "published": "Mon, 06 Oct 2025 15:33:29 +0000",
    "author": "Rina Diane Caballar",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "AI Expands the Search for New Battery Materials",
    "link": "https://spectrum.ieee.org/ai-battery-material",
    "summary": "When Microsoft researchers in 2023 identified a new kind of material that could dramatically reduce the amount of lithium needed in rechargeable batteries, it felt like combing through a haystack in record time. That’s because their discovery began as 32 million possibilities and, with the help of artificial intelligence, produced a promising candidate within 80 hours. Now researchers at the Pacific Northwest National Laboratory plan to synthesize and test the novel material, Na x Li 3−x YCl 6 , in a battery setup. It’s one of several AI-generated battery chemistries making its way to the real world. Microsoft’s experiment started when the researchers wanted to demonstrate how AI could tackle the needle-in-a-haystack problem of finding useful new materials and chemicals . They decided to seek new candidates for a rechargeable battery’s electrolyte, because a better electrolyte could make batteries safer while simultaneously improving performance, says Nathan Baker , project leader at Microsoft for Azure Quantum Elements , a program to accelerate chemistry and materials research through Microsoft’s advanced computing and AI platforms. “Our goal was to take one of these AI models and show the promise of accelerating scientific discovery—sifting through 32.5 million materials candidates and showing that we could do it in a matter of hours, not years,” Baker says. Their model, called the M3GNet framework, accelerated simulations of molecular dynamics to evaluate properties of the materials such as atomic diffusivity. First, the Microsoft researchers asked the model to drop new chemical elements into known crystalline structures in nature and determine which resulting molecules would be stable, a step that cut the 32 million starting candidates down to half a million. AI then screened those materials based on the necessary chemical abilities to make a battery work, which chopped the pool to just 800. From there, traditional computing and old-fashioned human expertise identified the novel material that could function within a battery and use 70 percent less lithium than the rechargeable batteries in commercial use today. AI’s Role in Next-Gen Battery Design The Microsoft team isn’t alone. Around the world, researchers are busy trying to develop next-generation designs to replace or improve lithium-ion batteries, which use large quantities of rare, expensive, and difficult-to-acquire elements . New battery designs could use more abundant materials, reduce the fire danger from lithium-based liquid electrolytes, and pack more energy into a smaller space. The chemistries to do this are waiting out there to be discovered, and increasingly, researchers are harnessing AI and machine learning to do the work of sorting through the mountain of data. “We are teaching AI how to be a materials scientist,” says Dibakar Datta , associate professor at the New Jersey Institute of Technology, who published a study in August that used AI to identify five candidate materials for batteries that would outperform Li-ion. Datta’s team is working on the multivalent battery: one that employs multivalent ions that can carry multiple charge levels as opposed to the single charge carried by a lithium battery. This would give the battery a greater energy storage capacity, but it also means working with larger ions from elements higher on the periodic table, like magnesium and calcium. Those larger ions won’t necessarily fit into existing battery designs without cracking or breaking the elements, Datta says. His new study used what he calls a crystal diffusion variational autoencoder (CDVAE) that could propose new materials, and a large language model that could find materials that would be the most stable in the real world. From a pool of millions of possibilities, the approach found five porous materials of the right size that could do the job. Guiding an AI model on its hunt through the nearly infinite space of possible materials is the tipping point in this field. The key to using it as a research partner is to find a happy medium between a model that works fast and a model that delivers perfectly accurate results, says Austin Sendek , professor at Stanford University who has developed algorithms to help AI discover new battery materials. “You have to traverse both breadth and depth,” says Sendek. Depth, because designing these things takes a lot of deep scientific knowledge about properties, engineering, and chemistry, and breadth, because you have to apply that knowledge across an infinite chemical space, he says. “That’s where the promise of AI comes in.” AI Battery Technology Search at IBM Researchers at IBM have taken an AI-driven approach to identify new electrolyte candidates , which involved identifying chemical formulations with far higher ionic conductivity than the lithium salts used in current batteries. A typical electrolyte can contain six to eight ingredients including salts, solvents, and additives, and it’s nearly impossible to consider all the combinations without AI. To whittle down the field, the IBM team developed chemical-foundation models trained on billions of molecules. “They capture the basic language of chemistry,” says Young-Hye Na , principal research staff member at IBM Research. Her team then trains those models with battery-related data so the AI can predict important properties for battery applications on scales from individual molecules all the way up to a whole device. Na described the work in a paper published in August in NPJ Computational Materials . Because the work investigates new combinations of existing materials rather than using AI to invent exotic new materials, its potential to help build the battery of tomorrow is that much more promising, Na says. The IBM team is now collaborating with an undisclosed EV manufacturer to design high-performance electrolytes for high-voltage batteries. IBM’s use of AI for batteries isn’t limited to the hunt for promising materials. Typically, when AI reveals a promising new material, the next step is for experimentalists to synthesize the stuff, experiment with it in the lab, and one day to test it in a real device. Machine learning (ML) will aid researchers in this testing step, too. IBM is testing the real-world viability of new battery setups by building their digital twins —virtual models that allow the researchers to predict how a particular battery chemistry would degrade over a lifetime of countless power cycles. The model, developed in collaboration with battery startup Sphere Energy , can predict a battery’s long-term behavior in as few as 50 power cycles modeled on the digital twin, says Teodoro Laino , distinguished research staff member at IBM Research. Quantum-Computing Batteries The next phase of AI battery research is quantum . As Microsoft and IBM push toward the potential of quantum computers, both see its promise to model complex chemistry with no shortcuts or compromises. Na says that while current AI is a crucial tool for investigating battery chemistry, the next step—modeling whole EV battery packs, for example, and taking into consideration all the variables they encounter in the real world—would require the power of quantum computing. As Baker puts it: “We know classical computers have problems generating accurate answers for complex substances, complex molecules, complex materials. So our goal right now is actually to change the way the data is generated by bringing quantum into the loop so that we have higher accuracy data for training ML models.” This article was updated on 2 October 2025.",
    "published": "Wed, 01 Oct 2025 14:00:04 +0000",
    "author": "Andrew Moseman",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "The Hidden Behemoth Behind Every AI Answer",
    "link": "https://spectrum.ieee.org/ai-energy-use",
    "summary": "What happens when you say “Hello” to ChatGPT? Such a simple query might seem trivial, but making it possible across billions of sessions requires immense scale. While OpenAI reveals little information about its operations, we’ve used the scraps we do have to estimate the impact of ChatGPT—and of the generative AI industry in general. This article is part of The Scale Issue . OpenAI’s actions also provide hints. As part of the United States’ Stargate Project , OpenAI will collaborate with other AI titans to build the largest data centers yet. And AI companies expect to need dozens of “Stargate-class” data centers to meet user demand.",
    "published": "Wed, 01 Oct 2025 13:00:03 +0000",
    "author": "Matthew S. Smith",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "AI-Powered Tools Enhance Global Innovation Strategies",
    "link": "https://spectrum.ieee.org/ai-powered-ip-innovation",
    "summary": "This is a sponsored article brought to you by IP.com Around the world, innovation is no longer just a function of invention. It is a strategic asset, deeply linked to economic resilience—and increasingly reliant on ever-evolving technologies like artificial intelligence (AI). As intellectual property ecosystems transform under this new reality, the need for advanced, efficient, ethical AI-supported innovation infrastructure has never been more urgent. The U.S. Patent and Trademark Office (USPTO) has emerged as a global leader in this space, signaling its commitment to responsible AI leadership through the AI and Emerging Technologies (ET) Partnership, and most recently, through its landmark guidance clarifying that human contribution is essential for inventorship in AI-assisted creations and its significant investment in deploying AI to improve prior art searches and overall examination quality. As governments, enterprises, and inventors look to chart a path through this rapidly shifting IP landscape, one innovation intelligence company has been boldly helping lead the way: IP.com . For over 20 years, IP.com has worked to empower public and private innovation stakeholders alike with an ever-expanding suite of AI-powered tools that enhance ideation, evaluate novelty, clarify patent landscapes, and protect inventive work around the globe. Today, as the world rethinks how to balance open innovation with strategic security, IP.com’s solutions are not only timely—they are essential. Aligning A National Vision with Global Innovation While each country faces its own innovation challenges and opportunities, there is a growing consensus around one idea: the systems that support innovation—including those for AI and intellectual property—must be modern, data-driven, future-ready, and responsible. That vision was echoed in the U.S. government’s recent Executive Order on artificial intelligence ( EO 14179 ), and the newly published “ America’s AI Action Plan ” which emphasize the need for safe and trustworthy AI systems to drive economic growth and national security. While the Executive Order speaks primarily to U.S. priorities, it reflects a global movement toward building responsible AI systems that deliver long-term value, foster trust, and safeguard the integrity of innovation. IP.com’s innovation platforms are deeply aligned with this broader call, providing AI solutions that are secure and responsible from the ground up. From operating in a private environment to being fully ITAR-compliant, IP.com’s approach delivers IP-advancing AI solutions that are safe and accountable so inventors, R&D teams, IP professionals, and federal agencies can work smarter, faster, and more securely. Shaping the Future of Responsible, Innovation-Supportive AI IP.com’s dual-engine AI-fueled Innovation Power (IP) Suite® mirrors how high-performing teams think while aligning closely with the strategies and priorities shaping the future of intellectual property. It responsibly integrates AI into intellectual property processes to foster innovation within a secure, inclusive framework free from ideological bias or hallucinations. The IP Suite is purpose-built to deliver actionable insights grounded in proprietary data. With security integrated at every level—including ITAR compliance and private AI environments—IP.com offers best-in-class capabilities that protect sensitive data while accelerating innovation workflows. Transparent and traceable outputs reinforce trust and accountability across the innovation lifecycle while empowering forward-thinking teams and simplifying complex technology landscapes. IP.com’s dual-engine AI-fueled Innovation Power Suite is purpose-built to deliver actionable insights grounded in proprietary data. That means inventors and engineers can use the IP Suite to safely push innovation boundaries at a rapid pace. By integrating ideation, quantitative novelty analysis, prior art analysis, and invention disclosure generation into one simple, intuitive AI workflow, IP decisions can be made better and faster to maximize ROI. As global innovation accelerates, the broader collaboration IP.com enables through its AI-fueled IP Suite is essential to aligning stakeholders around shared priorities and helping to build a resilient, secure, and future-ready IP ecosystem. A Bold Force in Global IP Advancement Though best known among insiders in patent law and innovation strategy, IP.com has shaped some of the most important developments in IP modernization over the past two decades. Used by patent trademark offices around the world, its enterprise-grade, class-leading semantic AI increases examiner efficiency, powers millions of prior art searches, and accelerates the IP and innovation work of inventors, engineers, and IP professionals alike. Today, IP.com’s tools serve clients ranging from small inventors to multinational R&D operations. And as patent filings increase globally and emerging technologies blur jurisdictional boundaries, its commitment remains the same: helping innovators everywhere create confidently, compete fairly, and protect what matters—their intellectual property. Addressing IP Theft Head-On As the threat of IP theft intensifies, organizations across the public and private sectors are reevaluating how and where they deploy AI. Foreign-backed open and consumer-grade AI solutions, while powerful, often operate in opaque environments with unclear data handling practices, raising serious concerns about data leakage. For entities managing high-value innovation or sensitive research, the risk of proprietary data being exposed or exploited through unsecured AI models has become a pressing issue. IP.com’s Innovation Power (IP) Suite® is purpose-built to meet this challenge. Designed for enterprise and public-sector use, the platform operates entirely within secure, explainable, and ITAR-compliant environments—ensuring that no prompts, queries, or intellectual property are ever shared with external models or third parties. This architecture preserves data sovereignty while also upholding innovation ethics. Furthermore, the IP Suite helps US companies and government agencies protect innovations while countering the risk of IP theft. IP.com’s secure and ITAR-compliant solutions are uniquely positioned to help enable rapid, secure AI adoption in sensitive environments. As lines between economic competition and cyber-espionage continue to blur, IP.com stands apart from competitors built on open-source models vulnerable to exploitation. IP.com offers a proven, trustworthy path forward for organizations seeking to innovate responsibly while safeguarding their most valuable ideas. Beyond IP Security: The IEEE Content Advantage IP.com further enhances innovation processes by offering engineers direct access to fully searchable IEEE content—one of the most trusted and timely sources of technical knowledge in the world. Whether evaluating the novelty of a new design or researching prior art, engineers benefit from the ability to explore a vast collection of peer-reviewed journals, conference proceedings, and technical standards—all integrated within IP.com’s AI-powered IP Suite. By embedding IEEE content directly into the research workflow, IP.com empowers engineering teams to make more informed technical and strategic decisions. During concept validation or patentability assessments, having authoritative, high-quality IEEE literature at their fingertips helps engineers validate ideas, identify gaps in the landscape, and avoid costly duplication of effort. Combined with IP.com’s advanced analytics and private, secure AI tools, the integration of IEEE content ensures that engineers not only innovate efficiently but do so with the clarity and depth of insight required in today’s fast-moving R&D environments. Few platforms offer this kind of integration. Fewer still deliver it with the semantic precision and ease of use that IP.com provides. Final Thoughts: A Shared Responsibility Innovation is borderless. Its challenges—be they technical, strategic, or ethical—are shared across geographies. And so must be its solutions. IP.com is committed to supporting innovation ecosystems worldwide with tools that uphold the values of fairness, security, and excellence. Whether advancing a single patent application or shaping and managing an entire IP portfolio, our mission remains clear: to help innovators move forward—smarter, faster, and together. Discover how IP.com supports innovation ecosystems worldwide at www.ip.com/AI",
    "published": "Wed, 01 Oct 2025 12:30:03 +0000",
    "author": "Christopher Irick",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "No Vacancy",
    "link": "https://spectrum.ieee.org/steve-searcy-october-2025-poem",
    "summary": "We forge ahead with scientific quests, but even zealous efforts hit a wall trying to make what nature most detests: nothing at all. It seems like such a little thing to do— removing every molecule. We find that though we displace almost all, a few remain behind. It takes attentive planning and robust equipment in a lab to do the chore of pumping vacuum pressure down to just a millitorr. The stalwart researcher persists and loses sleep, but can’t reach perfection—I’m afraid the universe still stubbornly refuses to be unmade. Even in deepest space, a cubic meter contains some particles. We must assess there is no void, although conditions teeter on emptiness. The quantum mysteries will vex and weary the brightest mind, the sharpest physicist. True nothingness, while wonderful in theory, does not exist.",
    "published": "Mon, 29 Sep 2025 18:00:04 +0000",
    "author": "Steven Searcy",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Google’s AI Co-Scientist Scores Two Wins in Biology",
    "link": "https://spectrum.ieee.org/ai-co-scientist",
    "summary": "Hey Google! What if, instead of setting reminders or fetching restaurant reviews, you helped crack the mysteries of biology? That playful question hints at a radical vision now being tested in labs. AI systems are being recast not as digital secretaries but as scientific partners—copilots built to dream up bold, testable ideas. The pitch sounds revolutionary. But it also makes many scientists bristle. How much true novelty can a machine conjure? Isn’t it more likely to remix the past than to uncover something genuinely new? For months, the controversy over “AI scientists” has simmered : hype versus hope, parroting versus discovery. But two new studies offer some of the strongest evidence to date that large language models (LLMs) can generate truly novel scientific ideas, leaping to nonobvious insights that might otherwise require many years of painstaking lab work. Both studies showcase Google’s AI-powered scientific-research assistant, known as the AI co-scientist. “These early examples are unbelievable—it’s so compelling,” says Dillan Prasad , a neurosurgery researcher at Northwestern University and an outside observer who has written about the potential for AI co-scientists to supercharge hypothesis generation . “You have AI agents that are producing scientific discovery! It’s absolutely exciting.” AI Takes on Drug Repurposing In one of these proof-of-concept demonstrations, a team led by Gary Peltz , a liver-disease researcher at Stanford Medicine, tasked the AI assistant with finding drugs already on the market that could be repurposed to treat liver fibrosis, an organ-scarring condition with few effective therapies. He prompted the tool to look for medicines directed at epigenetic regulators—proteins that control how genes are switched on or off without altering the underlying DNA—and the AI, after mining the biomedical literature, came back with three reasonable suggestions. Peltz added two candidates of his own, and put all five drugs through a battery of tests on lab-grown liver tissue. Two of the AI’s picks—but none of Peltz’s—reduced fibrosis and even showed signs of promoting liver regeneration in the lab tests. Peltz, who published the findings 14 September in the journal Advanced Science , hopes the results will pave the way for a clinical trial of one standout candidate, the cancer drug vorinostat, in patients with liver fibrosis. Bacterial Mystery Solved In the second validation study, a team led by microbiologists José Penadés and Tiago Costa at Imperial College London challenged the AI co-scientist with a thorny question about bacterial evolution. The researchers had shown in 2023 that parasitic scraps of DNA could spread within bacterial populations by hitching rides on the tails of infecting viruses. But that mechanism seemed confined to one host species. How, then, did identical bits of DNA surface in entirely different types of bacteria? So they tasked the AI with solving the mystery. They fed the system their data, background papers, and a pointed question about what hidden mechanism might explain the jump. The AI, after “thinking” and processing for two days, proposed a handful of solutions—the leading one being that the DNA fragments could snatch viral tails not just from their own host cell but also from neighboring bacteria to complete their journey. It was uncannily correct. What the system could not know was that Penadés and Costa already had unpublished data hinting at exactly this mechanism. The AI had, in effect, leapt to the same conclusion that it had taken the researchers years of benchwork to devise, a convergence that astonished the Imperial team and lent credibility to the tool. “I was really shocked,” says Penadés, who at first thought the AI had hacked into his computer and accessed additional data to arrive at the correct result. Reassured that it hadn’t, he delved into the logic the AI co-scientist used for its various hypotheses and found surprising rigor. “Even for the ones that were not correct,” Penadés says, “the thinking was extremely good.” An AI Scientific Method That sound logic prompted the Imperial team to explore one of the AI’s runner-up ideas—one in which bacteria might directly pass the DNA fragments to one another. Working with microbial geneticists in France, the group is now probing that possibility further, with promising early results. “Our preliminary data seem to be pointing toward that hypothesis [also] being correct,” says Costa. He and Penadés published both the AI’s predictions and their experimental results in the journal Cell earlier this month. Notably, the Imperial researchers also tried various LLMs not specifically designed for scientific reasoning. These included systems from OpenAI, Anthropic, DeepSeek, and Google’s general-purpose Gemini 2.0 model. None of those jack-of-all-trades models came up with the hypotheses that proved experimentally correct. Vivek Natarajan from Google DeepMind, who helped develop the co-scientist platform, thinks he knows what explains that edge. He points to the system’s multiagent design, which assigns different AI roles to generate, critique, refine, and rank hypotheses in iterative loops, all overseen by a “supervisor” that manages goals and resources. Unlike a generic LLM, it grounds ideas in external tools and literature, strategically scales up compute for deeper reasoning, and vets hypotheses through automated tournaments. According to Natarajan, academic institutions around the world are now piloting the system , with plans to expand access—though the company’s “trusted tester program” is currently at capacity and not accepting new applications. “Clearly we see a lot of potential,” he says. “We imagine that, every time you’re going to try and solve a new problem, you’re going to use the co-scientist to come along on the journey with you.” Constellation of Co-Scientists Google is not alone in chasing this vision. In July, computer scientist Kyle Swanson and his colleagues at Stanford University described their Virtual Lab , an LLM-based system that strings together reasoning steps across biology datasets to propose new experiments. Rivals are moving fast, too: Biomni , another Stanford-led system, is helping to autonomously execute a wide range of research tasks in the life sciences, while the nonprofit FutureHouse is building a comparable platform. Each is vying to show that its approach can turn language models into real engines of discovery. Many onlookers have been impressed, noting that the studies offer some of the clearest evidence yet that LLMs can generate ideas worth testing at the bench. “This is going to make our jobs much easier,” says Rodrigo Ibarra Chávez , a microbiologist at the University of Copenhagen who studies the kind of bacterial genetic hitchhiking explored by the Imperial team. But critics warn that an overreliance on AI-generated hypotheses in science risks creating a closed loop that recycles old information instead of producing new discoveries. “We need tools that augment our creativity and critical thinking, not repackage existing information using alternative language,” Kriti Gaur of the life sciences analytics firm Elucidata wrote in a white paper that evaluated the Google platform. “Until this ‘AI co-scientist’ can demonstrate original, verifiable, and meaningful insights that stand up to scientific scrutiny, it remains a powerful assistant, but certainly not a co-scientist.” The blue section of the figure shows an experimental research pipeline that led to a discovery of DNA transfer among bacterial species. The orange section shows how AI rapidly reached the same conclusions. José R. Penadés, Juraj Gottweis, et al. Reasoning, Not Just Recall Supporters counter that the latest generation of models show glimmers of what scientists might reasonably call “ intelligence .” Systems like Google’s co-scientist not only recall and synthesize vast libraries but also reason through competing possibilities, discard weaker ideas, and refine stronger ones in ways that can feel strikingly human. “I find it very invigorating,” says Peltz. “It’s like having a conversation with someone who knows more than you.” Still, the magic doesn’t happen automatically. Extracting valuable hypotheses requires careful prompting, iterative feedback, and a willingness to engage in a kind of dialogue with the AI, notes Swanson. It’s less like pressing a button for an answer and more like mentoring a junior colleague—asking the right questions, pushing back on shallow reasoning, and nudging the system toward sharper insights. “For now, you still need to be a bit of an expert to get the most use out of these systems,” Swanson says. “But if you ask a well-designed question, you can get really good answers.”",
    "published": "Thu, 25 Sep 2025 15:00:03 +0000",
    "author": "Elie Dolgin",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "How Badly Is AI Cutting Early-Career Employment?",
    "link": "https://spectrum.ieee.org/ai-impact-on-job-market",
    "summary": "As AI tools become more common in people’s everyday work, researchers are looking to uncover its effects on the job market—especially for early-career workers. A paper from the Stanford Digital Economy Lab , part of the Stanford Institute for Human-Centered AI , has now found early evidence that employment has taken a hit for young workers in the occupations that use generative AI the most. Since the widespread adoption of AI tools began in late 2022, a split has appeared, and early-career software engineers are among the hardest hit. The researchers used data from the largest payroll provider in the United States, Automatic Data Processing (ADP), to gain up-to-date employment and earning data for millions of workers across industries, locations, and age groups. While other data may take months to come out, the researchers published their findings in late August with data through July. Although there has been a rise in demand for AI skills in the job market, generative AI tools are getting much better at doing some of the same tasks typically associated with early-career workers. What AI tools don’t have is the experiential knowledge gained through years in the workforce, which makes more-senior positions less vulnerable. These charts show how employment over time compares among early-career, developing, and senior workers (all occupations). Each age group is divided into five groups, based on AI exposure, and normalized to 1 in October 2022—roughly when popular generative AI tools became available to the public. The trend may be a harbinger for more widespread changes, and the researchers plan to continue tracking the data. “It could be that there are reversals in these employment declines. It could be that other age groups become more or less exposed [to generative AI] and have differing patterns in their employment trends. So we’re going to continue to track this and see what happens,” says Bharat Chandar , one of the paper’s authors and a postdoctoral fellow at the Stanford Digital Economy Lab. In the most “AI exposed” jobs, AI tools can assist with or perform more of the work people do on a daily basis. So, what does this mean for engineers? Software Developers Among Most AI-Exposed With the rise of AI coding tools, software engineers have been the subject of a lot of discussion—both in the media and research. “There have been conflicting stories about whether that job is being impacted by AI, especially for entry-level workers,” says Chandar. He and his colleagues wanted to find data on what’s happening now. Since late 2022, early-career software engineers (between 22 and 30 years old) have experienced a decline in employment. At the same time, midlevel and senior employment has remained stable or grown. This is happening across the most AI-exposed jobs, and software engineering is a prime example. Since late 2022, employment for early-career software developers has dropped. Employment for other age groups, however, has seen modest growth. Chandar cautions that, for specific occupations, the trend may not be driven by AI alone; other changes in the tech industry could also be causing the drop. Still, the fact that it holds across industries suggests that there’s a real effect from AI. The Stanford team also looked at a broader category of “computer occupations” based on the U.S. Bureau of Labor classifications—which includes hardware engineers, Web developers, and more—and found similar results. Growth in employment between October 2022 and July 2025 by age and AI-exposure group. Quintiles 1–3 represent the lowest AI-exposure groups, which experienced 6–13 percent growth. Quintiles 4 and 5 are the most AI-exposed jobs; employment for the youngest workers in these jobs fell 6 percent. Augmentation vs. Automation Part of the analysis uses data from the Anthropic Economic Index , which provides information about how Anthropic’s AI products are being used, including estimates of whether the types of queries used for certain occupations are more likely to automate work, potentially replacing employees, or augment an existing worker’s output. With this data, the researchers were able to estimate whether an occupation’s use of AI generally complements employees’ work or replaces it. Jobs in which AI tools augment work did not see the same declines in employment, compared with roles involving tasks that could be automated. This part of the analysis was based on Anthropic’s index alone. “Ideally, we would love to get more data on AI usage from the other AI companies as well, especially Open AI and Google,” Chandar says. (A recent paper from researchers at Microsoft did find that Copilot usage aligned closely with the estimates of AI exposure the Stanford team used.) Going forward, the team also hopes to expand to data on employment outside of the United States.",
    "published": "Wed, 24 Sep 2025 13:00:02 +0000",
    "author": "Gwendolyn Rak",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Tech Keeps Chatbots From Leaking Your Data",
    "link": "https://spectrum.ieee.org/homomorphic-encryption-llm",
    "summary": "Your chatbot might be leaky. According to recent reports , user conversations with AI chatbots such as OpenAI ’s ChatGPT and xAI’s Grok “have been exposed in search engine results.” Similarly, prompts on the Meta AI app may be appearing on a public feed . But what if those queries and chats can be protected, boosting privacy in the process? That’s what Duality , a company specializing in privacy-enhancing technologies, hopes to accomplish with its private large language model (LLM) inference framework. Behind the framework lies a technology called fully homomorphic encryption , or FHE, a cryptographic technique enabling computing on encrypted data without needing to decrypt it. Duality’s framework first encrypts a user prompt or query using FHE, then sends the encrypted query to an LLM. The LLM processes the query without decryption, generates an encrypted reply, and transmits it back to the user. “They can decrypt the results and get the benefit of running the LLM without actually revealing what was asked or what was responded,” says Kurt Rohloff , cofounder and chief technology officer at Duality. As a prototype, the framework supports only smaller models , particularly Google ’s BERT models. The team tweaked the LLMs to ensure compatibility with FHE, such as replacing some complex mathematical functions with their approximations for more efficient computation. Even with these slight alterations, however, the AI models operate just like a normal LLM would. “Whatever we do on the inference does not require retraining. In our approach, we still want to make sure that training happens the usual way, and it’s the inference that we essentially try to make more efficient,” says Yuriy Polyakov , vice president of cryptography at Duality. The Challenges of FHE LLM Inference FHE is considered a quantum-computer-proof encryption . Yet despite its high level of security, the cryptographic method can be slow. “Fully homomorphic encryption algorithms are heavily memory bound,” says Rashmi Agrawal , cofounder and chief technology officer at CipherSonic Labs , a company that spun out of her doctoral research at Boston University on accelerating homomorphic encryption. She explains that FHE relies on lattice-based cryptography , which is built on math problems around vectors in a grid. “Because of that lattice-based encryption scheme, you blow up the data size,” she adds. This results in huge ciphertexts (the encrypted version of your data) and keys requiring lots of memory. Another computational bottleneck entails an operation called bootstrapping, which is needed to periodically remove noise from ciphertexts, Agrawal says. “This particular operation is really expensive, and that is why FHE has been slow so far.” To overcome these challenges, the team at Duality is making algorithmic improvements to an FHE scheme known as CKKS (Cheon-Kim-Kim-Song) that’s well-suited for machine learning applications. “This scheme can work with large vectors of real numbers, and it achieves very high throughput,” says Polyakov. Part of those improvements involves integrating a recent advancement dubbed functional bootstrapping. “That allows us to do a very efficient homomorphic comparison operation of large vectors,” Polyakov adds. All of these implementations are available on OpenFHE , an open-source library that Duality contributes to and helps maintain. “This is a complicated and sophisticated problem that requires community effort. We’re making those tools available so that, together with the community, we can push the state of the art and enable inference for large language models,” says Polyakov. Hardware acceleration also plays a part in speeding up FHE for LLM inference, especially for bigger AI models. “They can be accelerated by two to three orders of magnitude using specialized hardware acceleration devices,” Polyakov says. Duality is building with this in mind and has added a hardware abstraction layer to OpenFHE for switching from a default CPU backend to swifter ones such as GPUs and application-specific integrated circuits ( ASICs ). Agrawal agrees that GPUs, as well as field-programmable gate arrays ( FPGAs ), are a good fit for FHE-protected LLM inference because they’re fast and connect to high-bandwidth memory. She adds that FPGAs in particular can be tailored for fully homomorphic encryption workloads. For Duality’s next steps, the team is progressing their private LLM inference framework from prototype to production. The company is also working on safeguarding other AI operations, including fine-tuning pretrained models on specialized data for specific tasks, as well as semantic search to uncover the context and meaning behind a search query rather than just using keywords. Encrypted LLMs Are the Future FHE forms part of a broader privacy-preserving toolbox for LLMs, alongside techniques such as differential privacy and confidential computing. Differential privacy introduces controlled noise or randomness to datasets, obscuring individual details while maintaining collective patterns. Meanwhile, confidential computing employs a trusted execution environment—a secure, isolated area within a CPU for processing sensitive data. Confidential computing has been around longer than the newer FHE technology, and Agrawal considers it as FHE’s “head-to-head competition.” However, she notes that confidential computing can’t support GPUs, making them an ill match for LLMs. “FHE is strongest when you need noninteractive end-to-end confidentiality because nobody is able to see your data anywhere in the whole process of computing,” Agrawal says. A fully encrypted LLM using FHE opens up a realm of possibilities. In health care , for instance, clinical results can be analyzed without revealing sensitive patient records. Financial institutions can check for fraud without disclosing bank account information. Enterprises can outsource computing to cloud environments without unveiling proprietary data. User conversations with AI assistants can be protected, too. “We’re entering into a renaissance of the applicability and usability of privacy technologies to enable secure data collaboration,” says Rohloff. “We all have data. We don’t necessarily have to choose between exposing our sensitive data and getting the best insights possible from that data.”",
    "published": "Tue, 23 Sep 2025 12:00:04 +0000",
    "author": "Rina Diane Caballar",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Will We Know Artificial General Intelligence When We See It?",
    "link": "https://spectrum.ieee.org/agi-benchmark",
    "summary": "Buzzwords in the field of artificial intelligence can be technical: perceptron , convolution , transformer . These refer to specific computing approaches. A recent term sounds more mundane but has revolutionary implications: timeline . Ask someone in AI for their timeline, and they’ll tell you when they expect the arrival of AGI—artificial general intelligence—which is sometimes defined as AI technology that can match the abilities of humans at most tasks. As AI’s sophistication has scaled—thanks to faster computers, better algorithms, and more data—timelines have compressed. The leaders of major AI labs, including OpenAI, Anthropic, and Google DeepMind, have recently said they expect AGI within a few years. This article is part of The Scale Issue . A computer system that thinks like us would enable close collaboration. Both the immediate and long-term impacts of AGI, if achieved, are unclear, but expect to see changes in the economy, scientific discovery, and geopolitics. And if AGI leads to superintelligence , it may even affect humanity’s placement in the predatory pecking order. So it’s imperative that we track the technology’s progress in preparation for such disruption. Benchmarking AI’s capabilities allows us to shape legal regulations, engineering goals, social norms, and business models—and to understand intelligence more broadly. While benchmarking any intellectual ability is tough, doing so for AGI presents special challenges. That’s in part because people strongly disagree on its definition: Some define AGI by its performance on benchmarks, others by its internal workings, its economic impact, or vibes. So the first step toward measuring the intelligence of AI is agreeing on the general concept. Are You Smarter Than an AI? Play a version of the game that researchers are using to track AI’s progress toward artificial general intelligence. Another issue is that AI systems have different strengths and weaknesses from humans, so even if we define AGI as “AI that can match humans at most tasks,” we can debate which tasks really count, and which humans set the standard. Direct comparisons are difficult. “We’re building alien beings,” says Geoffrey Hinton , a professor emeritus at the University of Toronto who won a Nobel Prize for his work on AI. Undaunted researchers are busy designing and proposing tests that might lend some insight into our future. But a question remains: Can these tests tell us if we’ve achieved the long-sought goal of AGI? Why It’s So Hard to Test for Intelligence There are infinite kinds of intelligence, even in humans. IQ tests provide a kind of summary statistic by including a range of semirelated tasks involving memory, logic, spatial processing, mathematics, and vocabulary. Sliced differently, performance on each task relies on a mixture of what’s called fluid intelligence—reasoning on the fly—and crystallized intelligence—applying learned knowledge or skills. For humans in high-income countries, IQ tests often predict key outcomes, such as academic and career success. But we can’t make the same assumptions about AI, whose abilities aren’t bundled in the same way. An IQ test designed for humans might not say the same thing about a machine as it does about a person. There are other kinds of intelligence that aren’t usually evaluated by IQ tests—and are even further out of reach for most AI benchmarks. These include types of social intelligence, such as the ability to make psychological inferences, and types of physical intelligence, such as an understanding of causal relations between objects and forces or the ability to coordinate a body in an environment. Both are crucial for humans navigating complex situations. Clever Hans, a German horse in the early 1900s, seemed able to do math—but was really responding to his trainer’s subtle cues, a classic case of misinterpreting performance. Alamy Intelligence testing is hard—in people, animals, or machines. You must beware of both false positives and false negatives. Maybe the test taker appears smart only by taking shortcuts, like Clever Hans , the famous horse that appeared to be capable of math but actually responded to nonverbal cues. Or maybe test takers appear stupid only because they are unfamiliar with the testing procedure or have perceptual difficulties. It’s also hard because notions of intelligence vary across place and time. “There is an interesting shift in our society in terms of what we think intelligence is and what aspects of it are valuable,” says Anna Ivanova , an assistant professor of psychology at Georgia Tech. For example, before encyclopedias and the Internet, “having a large access to facts in your head was considered a hallmark of intelligence.” Now we increasingly prize fluid over crystallized intelligence. The History of AI Intelligence Tests Over the years, many people have presented machines with grand challenges that purported to require intelligence on par with our own. In 1958, a trio of prominent AI researchers wrote , “Chess is the intellectual game par excellence. … If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor.” They did acknowledge the theoretical possibility that such a machine “might have discovered something that was as the wheel to the human leg: a device quite different from humans in its methods, but supremely effective in its way, and perhaps very simple.” But they stood their ground: “There appears to be nothing of this sort in sight.” In 1997, something of this sort was very much in sight when IBM’s Deep Blue computer beat Garry Kasparov , the reigning chess champion, while lacking the general intelligence even to play checkers. IBM’s Deep Blue defeated world chess champion Garry Kasparov in 1997, butdidn’t have enough general intelligence to play checkers. Adam Nadel/AP In 1950, Alan Turing proposed the imitation game, a version of which requires a machine to pass as a human in typewritten conversation. “The question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include,” he wrote. For decades, passing what’s now called the Turing test was considered a nearly impossible challenge and a strong indicator of AGI. But this year, researchers reported that when people conversed with both another person and OpenAI’s GPT-4.5 for 5 minutes and then had to guess which one was human, they picked the AI 73 percent of the time. Meanwhile, top language models frequently make mistakes that few people ever would, like miscounting the number of times the letter r occurs in strawberry . They appear to be more wheel than human leg. So scientists are still searching for measures of humanlike intelligence that can’t be hacked. The ARC Test for AGI There’s one AGI benchmark that, while not perfect, has gained a high profile as a foil for most new frontier models. In 2019, François Chollet , then a software engineer at Google and now a founder of the AI startup Ndea , released a paper titled “ On the Measure of Intelligence .” Many people equate intelligence to ability, and general intelligence to a broad set of abilities. Chollet takes a narrower view of intelligence, counting only one specific ability as important—the ability to acquire new abilities easily. Large language models (LLMs) like those powering ChatGPT do well on many benchmarks only after training on trillions of written words. When LLMs encounter a situation very unlike their training data, they frequently flop, unable to adjust. In Chollet’s sense, they lack intelligence . To go along with the paper, Chollet created a new AGI benchmark, called the Abstraction and Reasoning Corpus ( ARC ). It features hundreds of visual puzzles, each with several demonstrations and one test. A demonstration has an input grid and an output grid, both filled with colored squares. The test has just an input grid. The challenge is to learn a rule from the demonstrations and apply it in the test, creating a new output grid. The Abstraction and Reasoning Corpus challenges AI systems to infer abstract rules from just a few examples. Given examples of input-output grids, the system must apply the hidden pattern to a new test case—something humans find easy but machines still struggle with. ARC Prize ARC focuses on fluid intelligence. “To solve any problem, you need some knowledge, and then you’re going to recombine that knowledge on the fly,” Chollet told me. To make it a test not of stored knowledge but of how one recombines it, the training puzzles are supposed to supply all the “core knowledge priors” one needs. These include concepts like object cohesion, symmetry, and counting—the kind of common sense a small child has. Given this training and just a few examples, can you figure out which knowledge to apply to a new puzzle? Humans can do most of the puzzles easily, but AI struggled, at least at first. Eventually, OpenAI created a version of its o3 reasoning model that outperformed the average human test taker , achieving a score of 88 percent—albeit at an estimated computing cost of US $20,000 per puzzle. (OpenAI never released that model, so it’s not on the leaderboard chart.) This March, Chollet introduced a harder version, called ARC-AGI-2 . It’s overseen by his new nonprofit, the ARC Prize Foundation. “Our mission is to serve as a North Star towards AGI through enduring benchmarks,” the group announced. ARC Prize is offering a million dollars in prize money, the bulk going to teams whose trained AIs can solve 85 percent of 120 new puzzles using only four graphics processors for 12 hours or less. The new puzzles are more complex than those from 2019, sometimes requiring the application of multiple rules, reasoning for multiple steps, or interpreting symbols. The average human score is 60 percent, and as of this writing the best AI score is about 16 percent. AI models have made gradual progress on the first version of the ARC-AGI benchmark, which was introduced in 2019. This year, the ARC Prize launched a new version with harder puzzles, which AI models are struggling with. Models are labeled low, medium, high, or thinking to indicate how much computing power they expend on their answers, with “thinking” models using the most. ARC Prize AI experts acknowledge ARC’s value, and also its flaws. Jiaxuan You , a computer scientist at the University of Illinois at Urbana-Champaign, says ARC is “a very good theoretical benchmark” that can shed light on how algorithms function, but “it’s not taking into account the real-world complexity of AI applications, such as social reasoning tasks.” Melanie Mitchell , a computer scientist at the Santa Fe Institute, says it “captures some interesting capabilities that humans have,” such as the ability to abstract a new rule from a few examples. But given the narrow task format, she says, “I don’t think it captures what people mean when they say general intelligence.” Despite these caveats, ARC-AGI-2 may be the AI benchmark with the biggest performance gap between advanced AI and regular people, making it a potent indicator of AGI’s headway. What’s more, ARC is a work in progress. Chollet says AI might match human performance on the current test in a year or two, and he’s already working on ARC-AGI-3. Each task will be like a miniature video game, in which the player needs to figure out the relevant concepts, the possible actions, and the goal. What Attributes Should an AGI Benchmark Test? Researchers keep rolling out benchmarks that probe different aspects of general intelligence. Yet each also reveals how incomplete our map of the territory remains. One recent paper introduced General-Bench , a benchmark that uses five input modalities—text, images, video, audio, 3D—to test AI systems on hundreds of tasks that demand recognition, reasoning, creativity, ethical judgment, and other abilities to both comprehend and generate material. Ideally, an AGI would show synergy, leveraging abilities across tasks to outperform the best AI specialists. But at present, no AI can even handle all five modalities. Other benchmarks involve virtual worlds. An April paper in Nature reports on Dreamer , a general algorithm from Google DeepMind that learned to perform over 150 tasks, including playing Atari games, controlling virtual robots, and obtaining diamonds in Minecraft . These tasks require perception, exploration, long-term planning, and interaction, but it’s unclear how well Dreamer would handle real-world messiness. Controlling a video game is easier than controlling a real robot, says Danijar Hafner , the paper’s lead author: “The character never falls on his face.” The tasks also lack rich interaction with humans and an understanding of language in the context of gestures and surroundings. “You should be able to tell your household robot, ‘Put the dishes into that cabinet and not over there,’ and you point at [the cabinet] and it understands,” he says. Hafner says his team is working to make the simulations and tasks more realistic. Aside from these extant benchmarks, experts have long debated what an ideal demonstration would look like. Back in 1970, the AI pioneer Marvin Minsky told Life that in “three to eight years we will have a machine with the general intelligence of an average human being. I mean a machine that will be able to read Shakespeare, grease a car, play office politics, tell a joke, have a fight.” That panel of tasks seems like a decent start, if you could operationalize the game of office politics. Virtual people would be assigned randomized tasks that test not only understanding but values. For example, AIs might unexpectedly encounter money on the floor or a crying baby. One 2024 paper in Engineering proposed the Tong test ( tong is Chinese for “general”). Virtual people would be assigned randomized tasks that test not only understanding but values. For example, AIs might unexpectedly encounter money on the floor or a crying baby, giving researchers the opportunity to observe what the AIs do. The authors argue that benchmarks should test an AI’s ability to explore and set its own goals, its alignment with human values, its causal understanding, and its ability to control a virtual or physical body. What’s more, the benchmark should be capable of generating an infinite number of tasks involving dynamic physical and social interactions. Others, like Minsky, have suggested tests that require interacting with the real world to various degrees: making coffee in an unfamiliar kitchen, turning a hundred thousand dollars into a million, or attending college on campus and earning a degree. Unfortunately, some of these tests are impractical and risk causing real-world harm. For example, an AI might earn its million by scamming people. I asked Hinton, the Nobel Prize winner, what skills will be the hardest for AI to acquire. “I used to think it was things like figuring out what other people are thinking,” he said, “but it’s already doing some of that. It’s already able to do deception.” (In a recent multi-university study , an LLM outperformed humans at persuading test takers to select wrong answers.) He went on: “So, right now my answer is plumbing. Plumbing in an old house requires reaching into funny crevices and screwing things the right way. And I think that’s probably safe for another 10 years.” Researchers debate whether the ability to perform physical tasks is required to demonstrate AGI. A paper from Google DeepMind on measuring levels of AGI says no, arguing that intelligence can show itself in software alone. They frame physical ability as an add-on rather than a requirement for AGI. Mitchell of the Santa Fe Institute says we should test capabilities involved in doing an entire job. She noted that AI can do many tasks of a human radiologist but can’t replace the human because the job entails a lot of tasks that even the radiologist doesn’t realize they’re doing, like figuring out what tasks to do and dealing with unexpected problems. “There’s such a long tail of things that can happen in the world,” she says. Some robotic vacuum cleaners weren’t trained to recognize dog poop, she notes, and so they smeared it around the carpet. “There’s all kinds of stuff like that that you don’t think of when you’re building an intelligent system.” Some scientists say we should observe not only performance but what’s happening under the hood. A recent paper coauthored by Jeff Clune , a computer scientist at the University of British Columbia, in Canada, reports that deep learning often leads AI systems to create “fractured entangled representations”—basically a bunch of jury-rigged shortcuts wired together. Humans, though, look for broad, elegant regularities in the world. An AI system might appear intelligent based on one test, but if you don’t know the system’s innards, you could be surprised when you deploy it in a new situation and it applies the wrong rule. AGI Is Already Here, and Never Will Be The author Lewis Carroll once wrote of a character who used a map of the nation “on the scale of a mile to the mile!” before eventually using the country as its own map. In the case of intelligence testing, the most thorough map of how someone will perform in a situation is to test them in the situation itself. In that vein, a strong test of AGI might be to have a robot live a full human life and, say, raise a child to adulthood. “Ultimately, the real test of the capabilities of AI is what they do in the real world,” Clune told me. “So rather than benchmarks, I prefer to look at which scientific discoveries [AIs] make, and which jobs they automate. If people are hiring them to do work instead of a human and sticking with that decision, that’s extremely telling about the capabilities of AI.” But sometimes you want to know how well something will do before asking it to replace a person. We may never agree on what AGI or “humanlike” AI means, or what suffices to prove it. As AI advances, machines will still make mistakes, and people will point to these and say the AIs aren’t really intelligent. Ivanova, the psychologist at Georgia Tech, was on a panel recently, and the moderator asked about AGI timelines. “We had one person saying that it might never happen,” Ivanova told me, “and one person saying that it already happened.” So the term “AGI” may be convenient shorthand to express an aim—or a fear—but its practical use may be limited. In most cases, it should come with an asterisk, and a benchmark. This article appears in the October 2025 print issue as “Can We Build a Better IQ Test for AI?”",
    "published": "Mon, 22 Sep 2025 13:01:02 +0000",
    "author": "Matthew Hutson",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Are You Smarter Than an AI?",
    "link": "https://spectrum.ieee.org/arc-prize-agi-test",
    "summary": "The ARC Prize test is a deceptively simple challenge designed to measure a machine’s ability to reason, abstract, and generalize—core ingredients of artificial general intelligence (AGI). It’s the most prominent benchmark to emerge as researchers look for ways to measure progress toward AGI. For the full story, see the feature article “ Will We Know Artificial General Intelligence When We See It? ” This article is part of The Scale Issue . While today’s smartest AI models still struggle with many of these visual puzzles, humans often solve them easily. We’ve selected five from the ARC collection of nearly 2,000 puzzles, aiming for a range from easy to fairly hard, and adapted them into multiple-choice quizzes. INSTRUCTIONS: For each of the five puzzles, examine the examples and try to identify the overarching pattern between inputs and outputs. Your goal is to figure out the rule that governs how the input [on the left in each box] is transformed into the output [on the right]. Then, look at the test grid: Given its input, and based on what you’ve learned from the examples, what should the output be? Click one of the four multiple-choice answers to see if you’re right. Crack all five puzzles and prove you’re not just another language model!",
    "published": "Mon, 22 Sep 2025 13:00:02 +0000",
    "author": "Eliza Strickland",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "AlphaEarth Provides New Ways to See, and Understand, Earth",
    "link": "https://spectrum.ieee.org/google-deepmind-alphaearth-foundations-ai",
    "summary": "Google DeepMind has debuted AlphaEarth Foundations, an AI model that treats Earth like a living dataset, tracking crop cycles, coastlines, urban expansion, melting ice, and much, much more. AlphaEarth weaves together disparate data streams, from satellite imagery and sensor data to geotagged Wikipedia entries, into a unified digital representation that scientists can probe to uncover patterns unfolding worldwide. AlphaEarth produces a 64-dimensional “embedding” for every 10-by-10-meter cell of the planet annually from 2017 to 2024, covering both raw imagery and the relationships present in the underlying data. An embedding is a dense numeric summary of a place’s key features, making locations directly comparable. This approach cuts storage needs sixteenfold while preserving fine spatial and temporal detail. Altogether, the system amounts to over 1.4 trillion embeddings per year. Detailed snapshots of year-round surface conditions will prove valuable in a wide range of fields, including planetary analysis, urban planning, ecosystem tracking, wildlife conservation, and wildfire risk management. Digital Embeddings of Earth A key challenge in building the model was handling the messy sprawl of geospatial data itself. Traditional satellites capture large volumes of information-rich images and measurements that can be difficult to connect and efficiently analyze. The AlphaEarth Foundations team told IEEE Spectrum that one limitation in Earth observation is the inherent irregularity and sparsity of the data. Unlike a continuous video feed, satellite data is a collection of intermittent snapshots with frequent gaps caused by factors like persistent cloud cover. To ensure consistent performance, the model needed a wide net of training data: A global sample of images covering more than 5 million locations acquired from the Google Earth Engine public data catalog, including optical imagery, radar, climate models, topographic maps, lidar, gravitational field strength, and surface temperature measurements. To enrich the dataset, the team also incorporated Wikipedia articles on landmarks and other features. That diversity makes the model’s representations more detailed, but still broad enough to be relevant across different regions and scientific tasks. In Ecuador, for example, embeddings enable analysts to see through persistent cloud cover, revealing agricultural plots in various development stages. “Given we were aiming to integrate this data into a unified digital representation to provide scientists with a more complete and consistent picture of our planet’s evolution, we had to grapple with petabytes of multisource, multiresolution imagery and other geospatial datasets,” says Chris Brown, a senior research engineer at Google DeepMind. The team first had to get data pipelines and modeling infrastructure to a place where working on petabyte scales was feasible. “We prioritized respecting the nuances of geospatial data, such as projections, unique sensor properties, and sensor-acquisition strategies, while ensuring the model and its outputs were robust and generally useful for a wide variety of applications,” says Brown. AlphaEarth Foundations consistently outperformed other featurization approaches. Christopher F. Brown, Michal R. Kazmierski, Valerie J. Pasquarella, et al. The team stresses that AlphaEarth isn’t a generative model but a self-supervised framework designed to provide compact summaries of patterns in existing data. They worked to mitigate training bias through stratified sampling, which involves training the model on millions of locations to ensure diverse geographies and ecosystems are represented. According to Emily Schechter , a senior product manager at Google Earth Engine, the team benchmarked AlphaEarth against both traditional approaches and other AI mapping systems across multiple time periods and tasks, such as estimating ground surface properties and tracking changes in how land is being used over time. The results, Schechter says, showed AlphaEarth consistently outperformed alternatives, even in situations where labeled data was scarce. In a paper posted in late July , Google DeepMind reported that AlphaEarth had a 23.9 percent lower error rate on average than competing approaches. The researchers noted that the identity of the next-best baseline varies by dataset and task, signaling inconsistent prior progress in the field. AlphaEarth, on the other hand, shows consistent gains even in historically difficult mapping scenarios. It’s also more effective at classifying data. When pulling embedding vectors from Earth Engine for a labeled set of sites, the model successfully classified 87 crop categories and land-cover types using only about 150 examples per class, something that usually demands thousands of labels. In other tasks DeepMind explored , AlphaEarth was able to detail intricate Antarctic terrain despite irregular satellite coverage, and to spot subtle shifts in Canadian farmland that are missing from standard imagery. “To the best of my knowledge, this is the largest scale effort of its kind to date in terms of training data, model context size, and integrated modalities,” Brown says. “There’s so much potential for this technology to be applied, in different ways and across different use cases. [...] We’ll continue working with our partners to find ways to make this most useful for people.” Unified Model for Earth Science While AlphaEarth Foundations shares some similarities with digital twins —virtual replicas of real-world environments—it functions more as a groundwork than a full twin. By turning Earth’s raw data into a flexible public format, it supports a range of specialized models and analyses to plug in on top without rebuilding the data pipeline each time. The satellite embedding dataset is available through the Earth Engine Data Catalog , which is free for noncommercial use. Google DeepMind has been running tests with over 50 organizations worldwide in the last year. Several universities and the Food and Agriculture Organization of the United Nations are already using the embeddings. AlphaEarth’s embedding fields provide dozens of different ways to understand parcels of the Earth’s surface. Google DeepMind; Google Earth Engine Schechter also pointed to examples such as the Brazilian nonprofit MapBiomas , which is now mapping environmental changes in the Amazon rainforest, and the Global Ecosystems Atlas, a program classifying unmapped ecosystems into shrublands, deserts, wetlands, and other categories. Beyond research, integration into the widely used spatial analytics platform CARTO puts AlphaEarth Foundations into the hands of insurers, telecommunications firms, and other users who can load the embeddings into their existing workflow to run risk models—like finding ZIP codes with environmental profiles resembling wildfire-prone areas—without API requests or extra storage.",
    "published": "Mon, 15 Sep 2025 14:00:03 +0000",
    "author": "Shannon Cuthrell",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "How Robotics Is Powering the Future of Innovation",
    "link": "https://content.knowledgehub.wiley.com/from-concept-to-reality-how-robotics-is-transforming-our-world/",
    "summary": "The future of robotics is being shaped by powerful technologies like AI, edge computing, and high-speed connectivity, driving smarter, more responsive machines across industries. Robots are no longer confined to static environments—they are evolving to interact dynamically with humans and their surroundings. This eBook explores the impact of robotics in diverse fields, from home automation and medical technology to automotive, data centers, and industrial applications. It highlights challenges like power efficiency, miniaturization, and ruggedization, while showcasing Molex’s innovative solutions tailored for each domain. Additionally, the eBook covers: Ruggedized connectors for harsh industrial settings Advanced power management for home robots Miniaturized systems for precision medical robotics 5G/6G-enabled autonomous vehicles High-speed data solutions for cloud infrastructure Download this free whitepaper now!",
    "published": "Thu, 11 Sep 2025 10:00:02 +0000",
    "author": "Heilind Electronics",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Machine Learning Tests Keep Getting Bigger",
    "link": "https://spectrum.ieee.org/mlperf-inference-51",
    "summary": "The machine learning field is moving fast, and the yardsticks used to measure its progress are having to race to keep up. A case in point: MLPerf, the biannual machine learning competition sometimes termed “the Olympics of AI,” has introduced three new benchmark tests, reflecting new directions in the field. “Lately, it has been very difficult trying to follow what happens in the field,” says Miro Hodak , an Advanced Micro Devices engineer and MLPerf Inference working-group cochair. “We see that the models are becoming progressively larger, and in the last two rounds we have introduced the largest models we’ve ever had.” The chips that tackled these new benchmarks came from the usual suspects—Nvidia, Arm, and Intel. Nvidia topped the charts, introducing its new Blackwell Ultra GPU, packaged in a GB300 rack-scale design. AMD put up a strong performance, introducing its latest MI325X GPUs. Intel proved that one can still do inference on CPUs with its Xeon submissions, but it also entered the GPU game with an Intel Arc Pro submission. New benchmarks Last round, MLPerf introduced its largest benchmark yet, a large language model based on Llama 3.1-403B. In this round, MLPerf topped itself yet again, introducing a benchmark based on the DeepSeek-R1 671B model—more than 1.5 as many parameters as the previous largest benchmark. As a reasoning model, DeepSeek-R1 goes through several steps of chain-of-thought prompting when approaching a query. This means that much more of the computation happens during inference than in normal LLM operation, making this benchmark even more challenging. Reasoning models are claimed to be the most accurate, making them the technique of choice for science, math, and complex programming queries. In addition to the largest LLM benchmark yet, MLPerf also introduced the smallest, based on Llama 3.1-8B. There is growing industry demand for low latency yet high-accuracy reasoning, explained Taran Iyengar, the MLPerf Inference task-force chair. Small LLMs can supply this, and they’re an excellent choice for tasks such as text summarization and edge applications. This brings the total count of LLM-based benchmarks to a confusing four. They include the new, smallest Llama 3.1-8B benchmark; a preexisting Llama 2-70B benchmark; last round’s introduction of the Llama 3.1-403B benchmark; and the largest, the new DeepSeek-R1 model. If nothing else, this signals that LLMs are not going anywhere. In addition to the myriad LLMs, this round of MLPerf Inference included a new voice-to-text model, based on Whisper-large-v3. This benchmark is a response to the growing number of voice-enabled applications, whether they’re smart devices or speech-based AI interfaces. The MLPerf Inference competition has two broad categories: “closed,” which requires using the reference neural-network model as-is without modifications, and “open,” where some modifications to the model are allowed. Within those, there are several subcategories related to how the tests are done and in what sort of infrastructure. We will focus on the “closed” data-center server results for the sake of sanity. Nvidia leads Surprising no one, the best performance per accelerator on each benchmark, at least in the server category, was achieved by an Nvidia GPU-based system. Nvidia also unveiled the Blackwell Ultra, topping the charts in the two largest benchmarks: Llama 3.1-405B and DeepSeek-R1 reasoning. Blackwell Ultra is a more-powerful iteration of the Blackwell architecture, featuring significantly more memory capacity, double the acceleration for attention layers, 1.5 times more AI compute, and faster memory and connectivity compared with the standard Blackwell. It is intended for larger AI workloads, like the two benchmarks it was tested on. In addition to the hardware improvements, Dave Salvator , director of accelerated computing products at Nvidia, attributes the success of Blackwell Ultra to two key changes. First, the use of Nvidia’s proprietary 4-bit floating-point number format , NVFP4 . “We can deliver comparable accuracy to formats like BF16,” Salvator says, while using a lot less computing power. The second is so-called disaggregated serving . The idea behind disaggregated serving is that there are two main parts to the inference workload: prefill, where the query (“Please summarize this report”) and its entire context window (the report) are loaded into the LLM, and generation/decoding, where the output is actually calculated. These two stages have different requirements. While prefill is compute heavy, generation/decoding is much more dependent on memory bandwidth. Salvator says that by assigning different groups of GPUs to the two different stages, Nvidia achieves a performance gain of nearly 50 percent. AMD close behind AMD’s newest accelerator chip, MI355X, launched in July. The company offered results only in the “open” category, where software modifications to the model are permitted. Like Blackwell Ultra, MI355X features 4-bit floating-point support, as well as expanded high-bandwidth memory. The MI355X beat its predecessor, the MI325X, in the open Llama 2.1-70B benchmark by a factor of 2.7, says Mahesh Balasubramanian , senior director of data-center GPU product marketing at AMD. AMD’s “closed” submissions included systems powered by AMD MI300X and MI325X GPUs. The more advanced MI325X computer performed similarly to those built with Nvidia H200s on the Llama 2-70b, the “mixture of experts” test, and image-generation benchmarks. This round also included the first hybrid submission, where both AMD MI300X and MI325X GPUs were used for the same inference task, the Llama 2-70b benchmark. The use of hybrid GPUs is important, because new GPUs are coming at a yearly cadence , and the older models, deployed en masse, are not going anywhere. Being able to spread workloads among different kinds of GPUs is an essential step. Intel enters the GPU game In the past, Intel has remained steadfast that one does not need a GPU to do machine learning. Indeed, submissions using Intel’s Xeon CPU still performed on par with the Nvidia L4 on the object-detection benchmark but trailed on the recommender-system benchmark. In this round, for the first time, an Intel GPU also made a showing. The Intel Arc Pro was first released in 2022. The MLPerf submission featured a graphics card called the MaxSun Intel Arc Pro B60 Dual 48G Turbo , which contains two GPUs and 48 gigabytes of memory. The system performed on par with Nvidia’s L40S on the small LLM benchmark and trailed it on the Llama 2-70b benchmark.",
    "published": "Wed, 10 Sep 2025 15:00:03 +0000",
    "author": "Dina Genkina",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "The Real Story on AI’s Water Use–and How to Tackle It",
    "link": "https://spectrum.ieee.org/ai-water-usage",
    "summary": "AI is hot, capturing headlines, investments, and users. It also runs hot, literally: The data centers operating artificial intelligence models use large amounts of electricity and generate enormous heat. To keep servers from overheating, many facilities rely on cooling systems that use water. AI data centers’ water use comes in two forms. Beyond the water that cools the servers, data centers indirectly contribute to water use through the electricity generation needed to power their operations. That indirect use often makes up 80 percent or more of the overall water use. Reducing AI’s water footprint means tackling two very different issues—what happens inside the data-center walls, and what happens beyond them on the power grid. Direct Water Use: Local and Sometimes Stressful Just as human bodies cool themselves by sweating, data centers are often cooled by water evaporation—a process that dissipates heat and results in water being lost to the atmosphere, thus being counted as “consumed.” In many cases, the water is drawn from the same municipal systems that supply homes and businesses. While most major tech companies now disclose their direct water use, not all data centers follow suit , making the overall picture unclear. In recent reports, companies have estimated that between 45 percent and 60 percent of withdrawn water is consumed. According to a recent report by Lawrence Berkeley National Laboratory, the 2023 direct water consumption by data centers in the United States—home to about 40 percent of the world’s data centers —is estimated at roughly 17.5 billion gallons . Assuming a 50 percent consumption ratio, that means 35 billion gallons of water withdrawal, or about 0.3 percent of the total public water supply for the contiguous United States. The same report projects that the U.S. data center direct water consumption could double or even quadruple the 2023 level by 2028. On the national level, data centers’ water use is relatively modest. But in some regions where data centers are concentrated—and especially in regions already facing shortages—the strain on local water systems can be significant. Bloomberg News reports that about two-thirds of U.S. data centers built since 2022 are in high water-stress areas . In Newton County, Georgia, some proposed data centers have reportedly requested more water per day than the entire county uses daily. Officials there now face tough choices: reject new projects, require alternative water-efficient cooling systems, invest in costly infrastructure upgrades, or risk imposing water rationing on residents. The biggest stress may not be total use, but timing. On hot days when residents and businesses need water most, data-center water demand spikes too. In Arizona, a data center’s monthly water usage during the summer can be nearly twice its average level . Indirect Water Use: Thirsty Electricity The other part of the equation is the electricity that powers data centers. In many places, electricity—whether for training AI models in data centers or turning on a lightbulb in a home—is generated by fossil-fuel-based power plants that require cooling water of their own. The U.S. electric power sector withdraws about 11.6 gallons of water and consumes 1.2 gallons for every kilowatt-hour of electricity produced, placing it among the nation’s largest water users . The water used to produce the electricity that powers data centers is considered indirect water use. The water used by power plants is typically not potable and not drawn from municipal water systems. Still, it can place stress on rivers, aquifers, and ecosystems—especially in water-scarce regions. For most U.S. data centers, this indirect use is significantly higher than direct onsite water use. One paper estimated that in 2023, using GPT-3 to generate a single text output of 150 to 300 words consumed a total of 16.9 milliliters of water in an average U.S. data center—2.2 ml for onsite cooling and 14.7 ml for electricity generation. It’s likely that efficiency gains in later models have reduced these numbers, but indirect water use still predominates. How to Minimize Data Centers’ Water Impact Unlike electricity, data-center cooling systems are a design choice. Evaporative cooling is low-cost and efficient, but it can burden local supplies during summer heat waves, when water is most needed and least available. To manage that peak demand, data centers can build onsite water storage or install thermal-energy storage. Upgrading water infrastructure—such as expanding distribution or fixing leaks—can also help local systems better handle demand spikes. Alternatives to evaporative cooling include air-based and liquid-immersion cooling , using recycled water to cut potable water use, and waste-heat reuse to reduce cooling demand. Some advanced designs recycle cooling water in a closed loop, so no water is consumed; these “zero water” designs eliminate the need to tap into local drinking water supplies. However, many of these designs raise electricity demand, which in turn can increase indirect water use. Water-cooled data centers consume about 10 percent less energy than air-cooled data centers. In immersion cooling systems, servers are submerged in a fluid that carries heat away without evaporating water. Jason Alden/Bloomberg/Getty Images In water-stressed regions, the priority should be low- to zero-water cooling systems to reduce direct use, while investing to add renewables to the local grids to curb indirect water use and minimize carbon emissions from higher electricity demand. In wetter regions with carbon-intensive grids, priority should be given to reducing power use to lower the overall water consumption, even if that means continued use of evaporative cooling with its higher onsite water consumption. The reality of the intertwined water and electricity systems forces data-center operators to navigate tough trade-offs between global climate goals and local water needs. These choices often aren’t simple, but until renewables dominate electricity grids, they may be unavoidable. The views expressed in this article are those of the authors and do not necessarily reflect the views of their employers or affiliated institutions.",
    "published": "Wed, 10 Sep 2025 13:00:04 +0000",
    "author": "Amy Luers",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Digital Advertisers Will Soon Vie for AI Agents’ Attention",
    "link": "https://spectrum.ieee.org/ai-agent-economy",
    "summary": "The modern Internet is, for better or worse, built on advertising. But the advent of autonomous AI agents that can search for information and execute tasks on behalf of users could soon upend this business model and transform the Web, say researchers. Most of the platforms people rely on to find information online, including search engines and social media sites, make the bulk of their money from advertising, says Jun Wang , a professor of computer science at University College London . By harvesting data on users’ browsing habits and interests, they offer marketers the ability to precisely target individuals with personalized content, which has helped these websites corner a growing proportion of advertising spending. But rapidly improving AI chatbots are quickly becoming people’s go-to way to find information on the Web, says Wang. And the trend is only likely to accelerate as tech companies roll out AI agents, which can interface with external tools and APIs to autonomously carry out more complex online tasks for users, such as doing in-depth research or making purchases. This has led to predictions that we may soon see the emergence of an “agentic Web” where the primary users of the Internet become AI bots rather than humans. “The agentic Web is going to change everything,” says Wang, predicting that people will increasingly rely on agents as proxies to navigate the Web on their behalf. And in a recent position paper posted on the preprint server ArXiv, he and colleagues outline how this could lead to a new “agent attention economy” where advertisers increasingly jockey to be noticed by agents instead of humans. How AI Agents Will Navigate the Web Wang is well qualified to talk about the topic, having spent most of his career on the technology that powers today’s online economy. He has worked on recommendation algorithms that parse browsing data to identify the content and products that might interest individuals, and he helped develop real-time auction technology that lets marketers compete to have their ads displayed to specific users. But these systems will need to adapt considerably as agents become more prevalent online, says Wang. One of the key enablers for a future agentic Web is the Model Context Protocol (MCP) developed by Anthropic, which provides a standardized way for AI models to interact with things like databases, APIs, and other Web services. In order to carry out user instructions, agents will break them down into subtasks and then call on various external MCP-enabled tools to help solve each smaller problem. For example, if asked to plan a holiday, the agent might interface with map services tools, hotel booking platforms, and weather information providers. Agents will be faced with the same challenges as today’s human Internet users, says Wang; they’ll have to select from many available services and tools to rely on for each subtask. Providers will also face the same challenge of ensuring that their solution is the one selected. But solving these conundrums will require new technology and novel models of agent behavior to ensure that these sometimes competing incentives line up, he adds. In some areas, the underlying mechanisms could be very similar, says Weinan Zhang , a computer science professor at Shanghai Jiao Tong University , in China, and coauthor of the recent paper. While traditionally advertisers have competed for the eyeballs of human users, in an agentic Web they will compete to get their offerings into an agent’s “context window”—essentially the AI model’s working memory, which holds all the information needed to complete a task. Exactly how this will be achieved remains an open question, but Zhang says it could involve a kind of auction system similar to the ones used in Web advertising today. Model developers could allow service providers to bid to be included in the options considered by the model, and even pay extra for increased prominence in the shortlist. The End of Traditional SEO? New agent-focused forms of search engine optimization may emerge as well, says Zhang. Rather than using natural-language searches that focus on keywords to surface the best results, agents may increasingly rely on more elaborate data representations like dense vectors, which can incorporate details like the semantic meaning and context of a search query. This may lead to marketers optimizing Web content for these new search approaches, rather than human-readable ones. An interesting dimension to the agent attention economy, says Zhang, is that it may increasingly involve interactions between multiple agents to solve tasks. This could be made possible by the Agent2Agent Protocol (A2A) introduced by Google, which enables agents from different providers and with different capabilities to communicate and collaborate with each other. Here again, agents will need some way of deciding which agents to cooperate with, and agent providers will be eager to promote their own offerings. Zhang says we may see the emergence of a new agentic version of PageRank —a system used by search engines to establish the relevance and trustworthiness of Web pages. The current algorithm examines Web pages to see the number and quality of other Web pages that link back to them. In the new paradigm, agents that handle certain tasks would replace Web pages, says Zhang, and those that are consistently called upon by other popular agents would get a higher rank, which would boost their visibility and reputation. “If the agent is very capable at collaborating with a team to finish different kinds of users tasks, a lot of agents will call this agent,” he says. “The PageRank of this agent will be very high, so that means on the agentic Web this agent will be very important—just like a very large website.” The possibility of each user request involving multiple agents will further complicate the advertising model, says Zhang. Each agent along the pipeline may be targeted by different advertisers or respond differently to SEO, making it much more complex to track the impact of specific marketing efforts. The ability of agents to communicate in natural language could also allow them to negotiate in ways similar to how humans haggle in a real-world market, says Wang. Rather than automated bidding tools, agents themselves may wrangle over what tools to use and whom to collaborate with. The Path Toward an Agentic Web Ceding this much control to autonomous systems might seem alarming, but Wang says there are likely to be ways for humans to maintain high-level control over their agentic proxies. One simple option would be to allow users to select which service providers their agents can interact with. “So for example, if I use booking.com quite often and I use Amazon, I just subscribe to their MCP servers,” he says. “Then the agent is constrained to their environments to do the deal for me, because those are the partners I trust.” However, Wang admits this vision remains some way off. Most people are still a long way from trusting bots to roam the Internet making purchases for them, and advertising technology for agents doesn’t yet exist. Creating an agentic attention economy will likely require big players to come together to develop tools that can navigate competing interests and complex multiagent-coordination problems, he adds. If these problems are solved, it could fundamentally change the nature of the Internet. People will increasingly access the Web through digital assistants without ever actually browsing websites themselves, says Zhang, and Web pages and online services will increasingly be tailored for agents rather than humans. “The conventional Web will shrink,” he says.",
    "published": "Tue, 09 Sep 2025 13:00:03 +0000",
    "author": "Edd Gent",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Bird-Brained AI Model Enables Reasoning at the Edge",
    "link": "https://spectrum.ieee.org/edge-ai-multiverse-computing",
    "summary": "Large language models are powerful, but generally they require vast computing resources, which means they typically have to run on stacks of high-end GPUs in data centers. Now, startup Multiverse Computing has created models it says are comparable in size to the brains of chickens and flies—allowing the company to shrink powerful LLMs so that they can run on home appliances, smartphones, or cars. Multiverse, based in Donostia, Spain, is working at the intersection of two of technology’s most in-vogue fields—AI and quantum computing . The company’s flagship product is a software platform called Singularity, designed to allow nonexperts to work with quantum algorithms, but it has also developed compression technology called CompactifAI for shrinking neural networks. The software relies on tensor networks—mathematical tools originally developed to simulate quantum systems on classical hardware. But their ability to distill complex multidimensional systems into something more compact and easier to work with also makes them a promising avenue for compressing large AI models . Multiverse’s Nano Models Shrink AI Multiverse has now used CompactifAI to create a new family of “nano models” that it calls Model Zoo, with each one named after the animal whose brain (theoretically) has a comparable amount of processing power. The first two releases are a compressed version of Meta’s Llama 3.1 model dubbed ChickenBrain, which can bring reasoning capabilities to a Raspberry Pi, and a version of the open-source model SmolLM2 135M small enough to run on a smartphone, dubbed SuperFly. “SuperFly is a 94-million-parameter model, which is tiny. It’s definitely one of the smallest LLMs out there,” says Sam Mugel , Multiverse’s chief technology officer. “Any device that’s expensive enough that you could justify putting a Raspberry Pi in would be able to host an LLM like SuperFly.” That means expensive electronics like a washing machine or a fridge could now have AI capabilities they would otherwise not be able to incorporate. The company says this could bring AI capabilities to a wide range of appliances, and in particular the ability to control devices using natural language. Being able to run LLMs locally rather than via the cloud has a host of benefits, says Mugel, including significantly reduced latency and fewer security and privacy risks due to data being processed on-device. They could be particularly useful for applications where Internet connections may be unreliable, Mugel says. SuperFly is small enough to be directly embedded in a car’s dashboard, which could allow uninterrupted natural-language control even while driving through tunnels or in areas with poor network coverage. Compressing models is standard practice these days, thanks to growing concerns around the energy and hardware footprints of the largest models . Neural networks are surprisingly inefficient learners and contain a lot of redundant information, says Mugel, which leaves a lot of room for optimization. This is typically done using techniques like quantization, which involves using fewer bits to represent a model’s weights, or pruning out connections in the neural network that aren’t contributing much to performance. But Mugel says Multiverse’s quantum-inspired tensor networks approach can go further than either of these more conventional approaches, and can also be combined with quantization to push compression even further. The first step in the process involves scanning a model to see which layers are most suitable for compression. These layers are then reorganized into tensor networks, which retain the most important patterns in the layer’s weights while discarding redundant information that isn’t contributing much to overall performance. Finally, the compressed model goes through a “healing” step where it is briefly retrained on the task at hand. “We’ve reorganized the neural network a little bit, and we’ve done a procedure that might take it out of the optimal points of the training,” says Mugel. “The healing is analogous to how people, after a really bad accident, may need a little bit of rehabilitation. That doesn’t mean relearning a task from scratch, it just means getting familiar with it again.” Efficient AI for Smartphones The company used this process to create its SuperFly model, which is roughly 30 percent smaller than the model it was compressed from. At just 94 million parameters, it is comparable in size to two fruit-fly brains, says Mugel, which have roughly 50 million neural connections. When the company’s researchers installed it on an iPhone 14 Pro it took up only 191 megabytes of disk space and could process a respectable 115 tokens per second. ChickenBrain is considerably larger, at 3.2 billion parameters, which Mugel admits is similar in size to other smaller language models. But this represents a 60 percent reduction from the 8-billion-parameter Llama model it was compressed from. And the team was also able to add reasoning skills to the model despite the significantly reduced footprint, though Multiverse declined to explain how these new capabilities were achieved. This means that ChickenBrain actually outperforms the model it was compressed from on a range of benchmarks when running on similar hardware, including the language-focused MMLU-Pro , math-focused Math-500 and GSM8K , and general knowledge-focused GPQA-Diamond . “What we’re demonstrating is that we can modify Llama 3.1 8B to make it more powerful with a fraction of the size,” says Mugel. “It’s an important step for making AI leaner and more efficient, as well as opening up new domains for AI models at the edge.” Zenglin Xu , a professor at the Artificial Intelligence Innovation and Incubation Institute at Fudan University, in Shanghai, says that tensor networks are a promising tool for compression and often provide better results than similar techniques that attempt to simplify layers of a neural network. However, it remains unclear how well models compressed in this way can deal with more-complicated reasoning tasks. “ Especially for problems with longer inference chains, the performance could be suboptimal compared with other techniques,” adds Xu. And despite the compression achieved so far, Mugel admits that there’s still a long way to go before today’s frontier models can be squeezed onto edge devices. But he says there’s plenty of scope to improve Multiverse’s compression techniques, and at the same time more-efficient architectures are bringing cutting-edge capabilities to ever smaller models. “How much more can we squeeze out of 3 billion parameters?” he says. “That’s really hard to say, but I do believe the we’re going to see way better performance in the very near future.”",
    "published": "Wed, 03 Sep 2025 15:00:03 +0000",
    "author": "Edd Gent",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "­GPT-5’s Rocky Launch Underscores Broader AI Disappointments",
    "link": "https://spectrum.ieee.org/gpt-5-trough-of-disillusionment",
    "summary": "GPT-5 was supposed to be the model that proved artificial general intelligence (AGI) is within reach. OpenAI CEO Sam Altman hinted as much with a January post on his personal blog . Altman wrote he was “now confident we know how to build AGI as we have traditionally understood it,” adding that 2025 would be the year AI agents “materially change the output of companies.” Reality hasn’t lived up to Altman’s expectations. Cognitive scientist and AGI skeptic Gary Marcus called GPT-5 “ overhyped and underwhelming “ in a Substack post, and the deluge of negative feedback eventually prompted Altman to admit OpenAI “ totally screwed up ” the launch. It’s not just GPT-5 in the crosshairs. A recent MIT report on AI in business found that 95 percent of all generative-AI deployments in business settings generated “zero return.” The report shook confidence in AI badly enough to drive a minor sell-off in tech stocks , though stock prices have since leveled off. Recent releases from Grok and Anthropic also received a tepid response. “We are amid a classic hype cycle,” says Juan Graña , CEO of the AI company Neurologyca . “AI burst onto the scene with intense buzz, but is now sliding into what Gartner calls the ‘trough of disillusionment,’ where expectations meet reality.” Is AI Headed Toward a Trough of Disillusionment? There’s a good chance you know the trough of disillusionment even if you’re not familiar with the term. The phrase was coined in 1995 as part of a graph that Gartner analyst Jackie Fenn used to illustrate how inflated expectations can lead to a period of disillusionment. It quickly caught on and led to countless (and sometimes humorous ) variations of the original graph. Jason Gabbard , comanaging partner at the AI consultant Bowtie , says the hype leading up to GPT-5—as well as other AI releases in 2025—was intense. “There have been so many talking heads, the commentary has been all hype for so long, that expectations were high,” says Gabbard. He added that GPT-5’s failure to meet expectations was most sorely felt by smaller organizations and individuals, which hoped “that the next thing out of OpenAI was going to solve all of their problems.” His comments are echoed by the user-led rebellion that followed in GPT-5’s wake. As part of the new model’s release, OpenAI removed the earlier GPT-4o model from ChatGPT on the apparent assumption that users would find GPT-5 an upgrade in every situation. Instead, many ChatGPT users complained that the new model seemed worse than its predecessor . The criticism caused OpenAI to change course and restore access to GPT-4o just 24 hours after its removal. It was an embarrassing turn of events for OpenAI. In 2024, Altman predicted that GPT-5 would make GPT-4 feel “ mildly embarrassing ” by comparison. Instead, user feedback to GPT-5 was so negative that OpenAI decided to restore its predecessor. Challenges Facing AI Agents in 2025 Ironically, Fenn’s original 1995 graph placed intelligent agents at the very peak of expectations—precisely where AI agents found themselves at the start of 2025. Fast forward to August and it seems that, just as Fenn’s graph predicts, agents are leading a plummet into the trough. The launch of GPT-5’s Agent Mode (formerly called Operator), much like the model itself, received mixed reviews . And doubts about agentic AI have spilled over into the entire AI industry. Replit, an AI vibe coding tool, faced criticism in June after its agent deleted a company’s entire codebase . Security is an issue, too. Antivirus provider Malwarebytes recently issued a warning that AI agents trusted with important credentials could leave users “ penniless ” by falling for scams designed to fool AI. These worrying headlines are extreme cases, but they’re flanked by benchmarks that paint a modest picture of agentic performance. Once such benchmark, TheAgentCompany , tasked AI agents powered by models from Amazon, Anthropic, Google, and OpenAI with jobs across a wide range of career paths, including coding, data science, and human resources. It found that even the best model tested, Google’s Gemini 2.5 Pro, could complete only 30.3 percent of tasks. Results for GPT-5 are not yet available. TheAgentCompany benchmark also discovered that the limitations of AI agents differ from expectations. A recent study found that AI posed the greatest threat to jobs that involve soft skills. These include customer-service representatives, clerks, analysts, public-relations specialists, and administrators. Anthropic CEO Dario Amodei says AI will eliminate up to half of all white-collar jobs. However, TheAgentCompany benchmark found that agents perform poorly when asked to complete tasks that fall within these roles. They struggle due to a lack of social skills and a tendency toward self-deception. Agents were most successful when asked to handle software-development and project-management tasks. “Coding looks hard to humans, but it’s actually easier for AI models than simpler-seeming tasks like clerical work,” says Frank Xu , a coauthor on the TheAgentCompany paper. Data Limitations Impacting AI Performance One possible reason for this capability gap? A lack of training data. “There’s a huge amount of open-source code online to train on, but you don’t see companies open-sourcing their spreadsheets or human-resource workflows,” says Xu. “That lack of data is a big reason why agents struggle with the jobs people expect them to replace.” All of the experts IEEE Spectrum spoke with agreed that a lack of data related to specific tasks appears to be a stumbling point for AI models. Neurologyca’s Graña believes that “AI lacks the data and, more importantly, the context needed to behave in emotionally intelligent ways.” Bowtie’s Gabbard, who helps financial institutions like hedge funds implement AI automations, says generalized AI agents struggle with unique business processes, requiring customized solutions to succeed. And Mark Parfenov , an analyst with experience using AI, finds agents “very quickly lose track of complex tasks” and omit important data when used for market analysis. These difficulties cast doubt on the AI industry’s hope that AGI can be achieved by scaling up general-purpose large language models. That’s not to say AI models lack a path to improvement, however . Synthetic data and improved data labeling offer options to address shortcomings, though they could also turn AI’s climb out of the trough of disillusionment into a difficult, costly slog. “I think we are running out of low-hanging fruits to improve on,” says Xu. He added that the early agentic gains came from simple changes, “things like formatting errors, or not understanding tools [...] I think we’re going to slow down until we find the next big thing.”",
    "published": "Tue, 02 Sep 2025 13:00:02 +0000",
    "author": "Matthew S. Smith",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Data Centers May House AI—But Operators Don’t Trust AI (Yet)",
    "link": "https://spectrum.ieee.org/ai-data-center-operator-trust",
    "summary": "We are starting to trust AI with high-stakes tasks, including running automated factories and guiding military drones through hostile airspace. But when it comes to managing the data centers that power this AI revolution, human operators are far more cautious. According to a new survey of over 600 data-center operators worldwide by Uptime Institute , a data-center inspection and rating firm, only 14 percent say they would trust AI systems to change equipment configurations, even if it’s trained on years of historical data. In the same survey, just one in three operators say they would trust AI systems to control data-center equipment. Their skepticism may be justified: Despite pouring tens of billions of U.S. dollars into AI systems, 95 percent of organizations thus far lack a clear return on investment, according to a recent MIT report of generative-AI usage. Advanced industries, which include factories and data centers, ranked near the bottom of the list of sectors transformed by AI, if at all. Operator Trust in AI Systems Even before the AI-driven push to expand data centers, data-center operators themselves are known to be a relatively change-averse crowd who have been disappointed by buzzy technologies of the past, says Rose Weinschenk , a research associate at Uptime Institute. Operators often have electrical engineering or technical mechanical backgrounds, with training in the running of critical facilities; others work on the IT or network-system side and are also considered operators. Operator trust in AI declined every year for the three years following OpenAI’s release of ChatGPT in 2022 . When asked by Uptime if they trusted a trained AI system to run data-center operations, 24 percent of respondents said no in 2022, and 42 percent said no in 2024. While the public has marveled at the seemingly all-knowing nature of new large language models, operators seem to feel this type of AI is too limited and unpredictable for use in data centers. But now, operators appear to have entered a “period of careful testing and validation” of different types of AI systems in certain data-center operations, said Uptime research analyst Max Smolaks in a public webinar of the latest survey results. To capture changing sentiments, Uptime asked operators in 2025 which applications AI might serve as a trustworthy decision-maker, assuming adequate past training. Over 70 percent of operators say they would trust AI to analyze sensor data or predict maintenance tasks for equipment, the survey shows. “Data-center operators are very, very happy to do certain things using AI, and they will never, never trust AI to do certain other things,” Smolaks said in the webinar. AI’s Unpredictability in Data Centers One reason why trust in AI is low for critical control of equipment is the technology’s unpredictability. Data centers are run on “good, old-fashioned” engineering, such as programmed if/then logic, says Robert Wright , the chief data-center officer at Ilkari Data Centers , a data-center startup with two centers in Colombia and Iceland. “We say that we can’t run on luck, we have to run on certainty.” Data centers are a complex series of systems that feed into each other. Mere seconds can pass before catastrophic failures occur that result in damaged chips, wasted money, angry customers, or fatal fires . In the high-stakes environment of data centers, anonymous posters on the r/datacenter Reddit forum who replied to an IEEE Spectrum query generally failed to see a reason to justify the risk that AI could bring. Distrust may also mask an underlying job insecurity. Workers across many industries are concerned that AI will take their jobs. But the 2025 Uptime survey found that only one in five operators view AI as a means of reducing average staffing levels. “Operators believe that today’s AI is not going to replace the staff required to run their facilities,” Smolaks said in the Uptime webinar. “It might be coming for office workers, but data-center jobs appear to be safe from AI for now.” But it’s understandable for early-career operators to still feel like this technology is coming for their jobs, says electrical engineer Jackson Fahrney , who has worked in data centers for over eight years. Someone just six months on the job may view an AI system as if they were being told, “Here, train your replacement,” he says. In reality, he does not think AI will replace him or others inside data centers. Yet AI carries a more “ominous” presence in the workplace than machine learning tools, which have long been part of an operator’s toolkit and are meant to assist operators when making decisions. It could be that AI is the cherry on top of an industry-wide trend to reduce the number of operators within data centers, says Chris McLean , a data-center design and construction consultant. Whereas 60 engineers might have run a data center in the past, now only six are needed, McLean says. Less is required from those six, as well, as more and more critical maintenance is being outsourced to specialists outside of the data center. “Now you offset all of your risk with a low-cost human and a high-cost AI,” McLean said. “And I’ve got to imagine that that’s scary for operators.” That said, there are more data-center jobs than qualified applicants, as previously reported by Spectrum . Two-thirds of operators struggle with staff retention or recruitment, according to Uptime’s 2025 survey, similar to the responses from surveys for the previous two years. Efficient AI Algorithms for Data Centers Still, there are useful algorithms built on decades of machine learning research that could make data-center operation more efficient. The most established AI system for data centers is predictive maintenance, says Ilkari’s Wright. If the readings of a particular HVAC unit are rising faster than those from other units, for instance, the system can predict when that unit needs to be serviced. Other AI systems focus on optimizing chiller plants, which are, in effect, the refrigerator systems that keep the data center cool by circulating chilled water and air. Chillers account for much of the energy consumed by data centers. Data about weather patterns, load on the grid, and equipment degradation over time all feed into a single AI system run on hardware within the facility to optimize the total energy consumption, says Michael Berger , who runs research and development at the Australia-based energy software company Conserve IT . But Berger is quick to note that his AI-optimization software does not control equipment. It runs on top of the basic control loop and refines parameters to use less energy while achieving the same outcome, he says. Berger prefers to call this system machine learning instead of AI because of how specialized it is to the needs of a data center. Others fully embrace AI, both the name and the technology, like Joe Minarik , the chief operating officer at DataBank , a Dallas-based data-center company with 73 data centers across the United States and the United Kingdom. He attributes his admittedly bullish attitude toward AI to his 17 years working for Amazon Web Services, where software is king. Currently, DataBank uses AI to write software, and there are plans to roll out AI systems for automated ticket generation and monitoring, as well as network-configuration monitoring and adjustments by the end of the year. AI for bigger tasks, such as cooling, are tentatively scheduled for late 2026, subject to the time it takes to train the AI on enough data, he said. AI does hallucinate: Minarik has watched it give the wrong information and send his team down the wrong path. “We do, we see it happen today. But we also see it getting better and better once we give it more time,” he says. The key is “tremendous amounts of data points” in order for AI to understand the system, Minarik says. It’s not unlike training a human data-center engineer on every possible scenario that could happen within the halls of a data center. Hyperscalers and enterprise data centers, whose single customer is the company that owns the data center, are deploying AI at a faster pace than commercial companies like DataBank. Minarik is hearing of AI systems that run entire networks for in-house data centers. When DataBank rolls out AI for more significant data-center operations, it will be kept on a tight leash, Minarik says. Operators will still make final executions. While AI will undoubtedly change how data centers run, Minarik sees operators as a core part of that new future. Data centers are physical places with on-site activity. “AI can’t walk out there and change a spark plug,” he says, or hear an odd rattle from a server rack. Although Minarik says that one day there could be sensors for some of these issues, they’ll still need physical human techs to fix the equipment that keeps data centers running. “If you want a safe job that can protect you from AI,” Minarik says, “go to data centers.”",
    "published": "Thu, 28 Aug 2025 12:00:02 +0000",
    "author": "Elissa Welle",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Why AI Isn’t Ready to Be a Real Coder",
    "link": "https://spectrum.ieee.org/ai-for-coding",
    "summary": "Artificial intelligence (AI) has transformed the coding sphere, with AI coding tools completing source code, correcting syntax errors, creating inline documentation, and understanding and answering questions about a codebase. As the technology advances beyond automating programming tasks, the idea of full autonomy looms large. Is AI ready to be a real coder? A new paper says not yet—and maps out exactly why. Researchers from Cornell University , MIT CSAIL , Stanford University , and UC Berkeley highlight key challenges that today’s AI models face and outline promising research directions to tackle them. They presented their work at the 2025 International Conference on Machine Learning . The study offers a clear-eyed reality check amid all the hype. “At some level, the technology is powerful and useful already, and it has gotten to the point where programming without these tools just feels primitive,” says Armando Solar-Lezama , a co-author of the paper and an associate director at MIT CSAIL, where he leads the computer-aided programming group. He argues, however, that AI-powered software development has yet to reach “the point where you can really collaborate with these tools the way you can with a human programmer.” Challenges With AI Coding Tools According to the study, AI still struggles with several crucial facets of coding: sweeping scopes involving huge codebases, the extended context lengths of millions of lines of code, higher levels of logical complexity, and long-horizon or long-term planning about the structure and design of code to maintain code quality. Koushik Sen , a professor of computer science at UC Berkeley and also a co-author of the paper, cites fixing a memory safety bug as an example. (Such bugs can cause crashes, corrupt data, and open security vulnerabilities.) Software engineers might approach debugging by first determining where the error originates, “which might be far away from where it’s crashing, especially in a large codebase,” Sen explains. They’ll also have to understand the semantics of the code and how it works, and make changes based on that understanding. “You might have to not only fix that bug but change the entire memory management,” he adds. These kinds of complex tasks can be difficult for AI development tools to navigate, resulting in hallucinations about where the bug is or its root cause, as well as irrelevant suggestions or code fixes with subtle problems. “There are many failure points, and I don’t think the current LLMs [large language models] are good at handling that,” says Sen. Among the various paths suggested by the researchers toward solving these AI coding challenges—such as training code LLMs to better collaborate with humans and ensuring human oversight for machine-generated code—the human element endures. “A big part of software development is building a shared vocabulary and a shared understanding of what the problem is and how we want to describe these features. It’s about coming up with the right metaphor for the architecture of our system,” Solar-Lezama says. “It’s something that can be difficult to replicate by a machine. Our interfaces with these tools are still quite narrow compared to all the things that we can do when interacting with real colleagues.” Enhancing AI-Human Collaboration in Coding Creating better interfaces, which today are driven by prompt engineering , is integral for developer productivity in the long run. “If it takes longer to explain to the system all the things you want to do and all the details of what you want to do, then all you have is just programming by another name,” says Solar-Lezama. Shreya Kumar , a software engineer and an associate teaching professor in computer science at the University of Notre Dame who was not involved in the research, echoes the sentiment. “The reason we have a programming language is because we need to be unambiguous. But right now, we’re trying to adjust the prompt [in a way] that the tool will be able to understand,” she says. “We’re adapting to the tool, so instead of the tool serving us, we’re serving the tool. And it is sometimes more work than just writing the code.” As the study notes, one way to address the dilemma of human-AI interaction is for AI systems to learn to quantify uncertainty and communicate proactively, asking for clarification or more information when faced with vague instructions or unclear scenarios. Sen adds that AI models might also be “missing context that I have in my mind as a developer—hidden concepts that are embedded in the code but hard to decipher from it. And if I give any hint to the LLM about what is happening, it might actually make better progress.” For Abhik Roychoudhury , a professor of computer science at the National University of Singapore who was also not involved in the research, a crucial aspect missing from the paper and from most AI-backed software development tools entails capturing user intent. “A software engineer is doing a lot of thinking in understanding the intent of the code. This intent inference—what the program is trying to do, what the program is supposed to do, and the deviation between the two—is what helps in a lot of software engineering tasks. If this outlook can be brought in future AI offerings for software engineering, then it will get closer to what the software engineer does.” Where Does AI Coding Go From Here? Roychoudhury also assumes that many of the challenges identified in the paper are either being worked on now or “would be solved relatively quickly” due to the rapid pace of progress in AI for software engineering. Additionally, he believes that an agentic AI approach can help, viewing significant promise in AI agents for processing requirements specifications and ensuring they can be enforced at the code level. “I feel the automation of software engineering via agents is probably irreversible. I would dare say that it is going to happen,” Roychoudhury says. Sen is of the same view but looks beyond agentic AI initiatives. He pinpoints ideas such as evolutionary algorithms to enhance AI coding skills and projects like AlphaEvolve that employ genetic algorithms “to shuffle the solutions, pick the best ones, and then continue improving those solutions. We need to adopt a similar technology for coding agents, where the code is continuously improving in the background.” However, Roychoudhury cautions that the bigger question lies in “whether you can trust the agent, and this issue of trust will be further exacerbated as more and more of the coding gets automated.” That’s why human supervision remains vital. “There should be a check and verify process. If you want a trustworthy system, you do need to have humans in the loop,” says Notre Dame’s Kumar. Solar-Lezama agrees. “I think it’s always going to be the case that we’re ultimately going to want to build software for people, and that means we have to figure out what it is we want to write,” he says. “In some ways, achieving full automation really means that we get to now work at a different level of abstraction.” So while AI may become a “real coder” in the near future, Roychoudhury acknowledges that it probably won’t gain software developers’ complete trust as a team member, and thus might not be allowed to do its tasks fully autonomously. “That team dynamics—when an AI agent can become a member of the team, what kind of tasks will it be doing, and how the rest of the team will be interacting with the agent—is essentially where the human-AI boundary lies,” he says.",
    "published": "Tue, 26 Aug 2025 12:00:03 +0000",
    "author": "Rina Diane Caballar",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Could an AI Barbie Stunt the Emotional Growth of a Generation?",
    "link": "https://spectrum.ieee.org/ai-barbie-dolls",
    "summary": "When Randy Newman sang “You Got a Friend in Me” on the soundtrack of Pixar’s Toy Story , he captured a feeling that every child understands—the deep and often unspoken bond between kids and their toys. Whether plastic, plush, or pixels on a screen, these toys have always lived in the space between imagination and reality, where fantasy feeds emotional development. But what happens when a child’s imagination no longer has to do any heavy lifting because their toys actually talk back? Mattel , the world’s largest toy company, has partnered with OpenAI to make that a reality. In June, the companies announced a collaboration to “bring a new dimension of AI-powered innovation and magic to Mattel’s iconic brands.” While the companies haven’t yet released specific product plans, it seems possible that parents will soon be able to buy an AI-powered Barbie that can hold genuine conversations with their children. We’re not talking about canned phrases, like Buzz Lightyear’s “To infinity and beyond!” at the press of a button, but something more akin to our experiences with ChatGPT. An AI Barbie would be able to listen, remember, respond, and adapt. It’s a moment that feels both magical and unsettling. In a bid to innovate playtime, Mattel is tapping into one of the most powerful technologies of our era and bringing it directly into children’s bedrooms. With a smiling face and a silicon brain, there’s a good chance that an AI Barbie could become a child’s first emotionally responsive companion outside of the family, offering comfort, curiosity, and conversation on demand. But what are we teaching our children about friendship, empathy, and emotional connection if their first “real” relationships are with machines? The History of Interactive Toys At first glance, the idea of a toy that truly listens—one that remembers a child’s favorite story, asks thoughtful questions, and offers gentle encouragement—feels like a good thing. For decades, toy designers have tried to simulate meaningful interaction with children. In the 1960s, Mattel’s Chatty Cathy was marketed as the first talking doll, with prerecorded phrases like “I love you,” and “Let’s play school.” In the ’80s, a storytelling animatronic bear called Teddy Ruxpin made its way into the hands of children around the world, moving its mouth and eyes in sync with cassette tapes. In 1998, Furbies were under Christmas trees everywhere; these interactive dolls created the impression that they were “learning” language over time. More recently, in 2014, a Bluetooth-enabled doll called My Friend Cayla used voice-to-text capabilities and search engines to answer questions—attracting criticism over privacy concerns and eventually leading German regulators to instruct parents to destroy the dolls. With generative AI models now capable of producing fluid, context-rich dialogue, Mattel’s new vision is a toy that grows with the child, holds personalized conversations, and recalls past interactions to adjust its responses. It may learn a child’s favorite story or phrase, sing their favorite song, or have full conversations about almost anything. Mattel has promised that these interactions will be “secure” and “age appropriate,” but not much is known beyond that. Proponents argue that this shift could revolutionize the way children learn and engage with the world. An AI-enhanced Barbie could help build storytelling skills, reinforce positive behavior, or provide companionship for children who struggle socially. Parents might see this toy as a safe, supportive way to foster creativity and confidence. In the best-case scenario, AI-powered toys would connect learning, play, and emotional support in one seamless experience. But even this promise carries a shadow. Because the closer a toy gets to simulating human warmth, the likelier it is to replace the real thing. AI Toys Could Stunt Children’s Emotional Development Children naturally anthropomorphize their toys—it’s part of how they learn. But when those toys begin talking back with fluency, memory, and seemingly genuine connection, the boundary between imagination and reality blurs in new and profound ways. Children may find themselves in a world where toys talk back and mirror their emotions without friction or complexity. For a young child still learning how to navigate emotions and relationships, that illusion of reciprocity may carry developmental consequences. Real relationships are messy, and parent-child relationships perhaps more so than any other. They involve misunderstanding, negotiation, and shared emotional stress. These are the microstruggles through which empathy and resilience are forged. But an AI companion, however well-intentioned, sidesteps that process entirely. Over time, those interactions can flatten a child’s understanding of what it means to relate to others. If conflicts are neatly resolved or avoided altogether, if every emotion is met with perfect affirmation, children may lose the opportunity to practice one of the most important developmental skills: learning to connect with people who are not programmed to go along with them. Real human interactions may begin to feel too slow, too inconsistent, or too challenging by comparison with AI interactions. The Limits of AI’s Emotional Intelligence The tension between simulated warmth and actual understanding continues to limit AI that’s billed as emotionally intelligent . Most models today can produce comforting language, but they’re not adept at reading emotional cues. They may not be able to tell the difference between a child who’s curious, lonely, or distressed. The cutting edge or research in this area focuses on AI models that can take in information such as facial expressions, gaze direction, behavioral patterns, and physiological signals, and adapt their responses according to the user’s emotional state. Researchers are designing systems that can sense and interpret context in real time, tracking metrics like attention, tone, and engagement. Human-aware design will create AI that is more supportive and effective—but that still doesn’t mean it will be appropriate for kids. For many parents, the fear is that an AI toy might say something inappropriate. But the more subtle, and perhaps more serious, risk is that it might say exactly the right thing, delivered with a tone of calm empathy and polished politeness, yet with no real understanding behind it. Children, especially in early developmental stages, are acutely sensitive to tone, timing, and emotional mirroring. Children playing with AI toys will believe they’re being understood, when in fact, the system is only predicting plausible next words. We’re at a point with AI where LLMs are affecting adults in profound and unexpected ways, sometimes triggering mental health crises or reinforcing false beliefs or dangerous ideas . OpenAI, to its credit, has hired forensic psychiatrists to study how ChatGPT affects users emotionally. This is uncharted technology, and we adults are still learning how to navigate it. Should we really be exposing children to it?",
    "published": "Thu, 21 Aug 2025 13:00:03 +0000",
    "author": "Marc Fernandez",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "The AI Agents of Tomorrow Need Data Integrity",
    "link": "https://spectrum.ieee.org/data-integrity",
    "summary": "Think of the Web as a digital territory with its own social contract. In 2014, Tim Berners-Lee called for a “Magna Carta for the Web” to restore the balance of power between individuals and institutions. This mirrors the original charter’s purpose: ensuring that those who occupy a territory have a meaningful stake in its governance. Web 3.0 —the distributed, decentralized Web of tomorrow—is finally poised to change the Internet’s dynamic by returning ownership to data creators. This will change many things about what’s often described as the “CIA triad” of digital security: confidentiality, integrity, and availability. Of those three features, data integrity will become of paramount importance. Ariane 5 Rocket (1996) Processing integrity f ailure A 64-bit velocity calculation was converted to a 16-bit output, causing an error called overflow. The corrupted data triggered catastrophic course corrections that forced the US $370 million rocket to self-destruct. When we have agency in digital spaces, we naturally maintain their integrity—protecting them from deterioration and shaping them with intention. But in territories controlled by distant platforms, where we’re merely temporary visitors, that connection frays. A disconnect emerges between those who benefit from data and those who bear the consequences of compromised integrity. Like homeowners who care deeply about maintaining the property they own, users in the Web 3.0 paradigm will become stewards of their personal digital spaces. This will be critical in a world where AI agents don’t just answer our questions but act on our behalf. These agents may execute financial transactions, coordinate complex workflows, and autonomously operate critical infrastructure, making decisions that ripple through entire industries. As digital agents become more autonomous and interconnected, the question is no longer whether we will trust AI but what that trust is built upon. In the new age we’re entering, the foundation isn’t intelligence or efficiency—it’s integrity. What Is Data Integrity? In information systems, integrity is the guarantee that data will not be modified without authorization, and that all transformations are verifiable throughout the data’s life cycle. While availability ensures that systems are running and confidentiality prevents unauthorized access, integrity focuses on whether information is accurate, unaltered, and consistent across systems and over time. NASA Mars Climate Orbiter (1999) Processing integrity f ailure Lockheed Martin’s software calculated thrust in pound-seconds, while NASA’s navigation software expected newton-seconds. The failure caused the $328 million spacecraft to burn up in the Mars atmosphere. It’s a new idea. The undo button, which prevents accidental data loss, is an integrity feature. So is the reboot process, which returns a computer to a known good state. Checksums are an integrity feature; so are verifications of network transmission. Without integrity, security measures can backfire. Encrypting corrupted data just locks in errors. Systems that score high marks for availability but spread misinformation just become amplifiers of risk. All IT systems require some form of data integrity, but the need for it is especially pronounced in two areas today. First: Internet of Things devices interact directly with the physical world, so corrupted input or output can result in real-world harm. Second: AI systems are only as good as the integrity of the data they’re trained on, and the integrity of their decision-making processes. If that foundation is shaky, the results will be too. Integrity manifests in four key areas. The first, input integrity, concerns the quality and authenticity of data entering a system. When this fails, consequences can be severe. In 2021, Facebook’s global outage was triggered by a single mistaken command—an input error missed by automated systems. Protecting input integrity requires robust authentication of data sources, cryptographic signing of sensor data, and diversity in input channels for cross-validation. The second issue is processing integrity, which ensures that systems transform inputs into outputs correctly. In 2003, the U.S.–Canada blackout affected 55 million people when a control-room process failed to refresh properly, resulting in damages exceeding US $6 billion. Safeguarding processing integrity means formally verifying algorithms, cryptographically protecting models, and monitoring systems for anomalous behavior. Microsoft’s Tay Chatbot (2016) Processing integrity f ailure Released on Twitter, Microsoft ’s AI chatbot was vulnerable to a “repeat after me” command, which meant it would echo any offensive content fed to it. Storage integrity covers the correctness of information as it’s stored and communicated. In 2023, the Federal Aviation Administration was forced to halt all U.S. departing flights because of a corrupted database file. Addressing this risk requires cryptographic approaches that make any modification computationally infeasible without detection, distributed storage systems to prevent single points of failure, and rigorous backup procedures. Finally, contextual integrity addresses the appropriate flow of information according to the norms of its larger context. It’s not enough for data to be accurate; it must also be used in ways that respect expectations and boundaries. For example, if a smart speaker listens in on casual family conversations and uses the data to build advertising profiles, that action would violate the expected boundaries of data collection. Preserving contextual integrity requires clear data-governance policies, principles that limit the use of data to its intended purposes, and mechanisms for enforcing information-flow constraints. As AI systems increasingly make critical decisions with reduced human oversight, all these dimensions of integrity become critical. The Need for Integrity in Web 3.0 As the digital landscape has shifted from Web 1.0 to Web 2.0 and now evolves toward Web 3.0, we’ve seen each era bring a different emphasis in the CIA triad of confidentiality, integrity, and availability. Boeing 737 MAX (2018) Input integrity f ailure Faulty sensor data caused a n automated flight-control system to repeatedly push the airplane’s nose down, leading to a fatal crash. Returning to our home metaphor: When simply having shelter is what matters most, availability takes priority—the house must exist and be functional. Once that foundation is secure, confidentiality becomes important—you need locks on your doors to keep others out. Only after these basics are established do you begin to consider integrity, to ensure that what’s inside the house remains trustworthy, unaltered, and consistent over time. Web 1.0 of the 1990s prioritized making information available. Organizations digitized their content, putting it out there for anyone to access. In Web 2.0, the Web of today, platforms for e-commerce, social media, and cloud computing prioritize confidentiality, as personal data has become the Internet’s currency. Somehow, integrity was largely lost along the way. In our current Web architecture, where control is centralized and removed from individual users, the concern for integrity has diminished. The massive social media platforms have created environments where no one feels responsible for the truthfulness or quality of what circulates. SolarWinds Supply-Chain Attack (2020) Storage integrity f ailure Russian hackers compromised the process that SolarWinds used to package its software, injecting malicious code that was distributed to 18,000 customers, including nine federal agencies. The hack remained undetected for 14 months. Web 3.0 is poised to change this dynamic by returning ownership to the data owners. This is not speculative; it’s already emerging. For example, ActivityPub , the protocol behind decentralized social networks like Mastodon , combines content sharing with built-in attribution. Tim Berners-Lee’s Solid protocol restructures the Web around personal data pods with granular access controls. These technologies prioritize integrity through cryptographic verification that proves authorship, decentralized architectures that eliminate vulnerable central authorities, machine-readable semantics that make meaning explicit—structured data formats that allow computers to understand participants and actions, such as “Alice performed surgery on Bob”—and transparent governance where rules are visible to all. As AI systems become more autonomous, communicating directly with one another via standardized protocols, these integrity controls will be essential for maintaining trust. Why Data Integrity Matters in AI For AI systems, integrity is crucial in four domains. The first is decision quality. With AI increasingly contributing to decision-making in health care, justice, and finance, the integrity of both data and models’ actions directly impact human welfare. Accountability is the second domain. Understanding the causes of failures requires reliable logging, audit trails, and system records. ChatGPT Data Leak (2023) Storage integrity f ailure A bug in OpenAI’s ChatGPT mixed different users’ conversation histories. Users suddenly had other people’s chats appear in their interfaces with no way to prove the conversations weren’t theirs. The third domain is the security relationships between components. Many authentication systems rely on the integrity of identity information and cryptographic keys. If these elements are compromised, malicious agents could impersonate trusted systems, potentially creating cascading failures as AI agents interact and make decisions based on corrupted credentials. Finally, integrity matters in our public definitions of safety. Governments worldwide are introducing rules for AI that focus on data accuracy, transparent algorithms, and verifiable claims about system behavior. Integrity provides the basis for meeting these legal obligations. The importance of integrity only grows as AI systems are entrusted with more critical applications and operate with less human oversight. While people can sometimes detect integrity lapses, autonomous systems may not only miss warning signs—they may exponentially increase the severity of breaches. Without assurances of integrity, organizations will not trust AI systems for important tasks, and we won’t realize the full potential of AI. How to Build AI Systems With Integrity Imagine an AI system as a home we’re building together. The integrity of this home doesn’t rest on a single security feature but on the thoughtful integration of many elements: solid foundations, well-constructed walls, clear pathways between rooms, and shared agreements about how spaces will be used. Midjourney Bias (2023) Contextual integrity failure Users discovered that the AI image generator often produced biased images of people, such as showing white men as CEOs regardless of the prompt. The AI tool didn’t accurately reflect the context requested by the users. We begin by laying the cornerstone: cryptographic verification . Digital signatures ensure that data lineage is traceable, much like a title deed proves ownership. Decentralized identifiers act as digital passports, allowing components to prove identity independently. When the front door of our AI home recognizes visitors through their own keys rather than through a vulnerable central doorman, we create resilience in the architecture of trust. Formal verification methods enable us to mathematically prove the structural integrity of critical components, ensuring that systems can withstand pressures placed upon them—especially in high-stakes domains where lives may depend on an AI’s decision. Just as a well-designed home creates separate spaces, trustworthy AI systems are built with thoughtful compartmentalization. We don’t rely on a single barrier but rather layer them to limit how problems in one area might affect others. Just as a kitchen fire is contained by fire doors and independent smoke alarms, training data is separated from the AI’s inferences and output to limit the impact of any single failure or breach. Throughout this AI home, we build transparency into the design: The equivalent of large windows that allow light into every corner is clear pathways from input to output. We install monitoring systems that continuously check for weaknesses, alerting us before small issues become catastrophic failures. Prompt Injection Attacks (2023–2024) Input integrity failure Attackers embedded hidden prompts in emails, documents, and websites that hijacked AI assistants, causing them to treat malicious instructions as legitimate commands. But a home isn’t just a physical structure, it’s also the agreements we make about how to live within it. Our governance frameworks act as these shared understandings. Before welcoming new residents, we provide them with certification standards. Just as landlords conduct credit checks, we conduct integrity assessments to evaluate newcomers. And we strive to be good neighbors, aligning our community agreements with broader societal expectations. Perhaps most important, we recognize that our AI home will shelter diverse individuals with varying needs. Our governance structures must reflect this diversity, bringing many stakeholders to the table. A truly trustworthy system cannot be designed only for its builders but must serve anyone authorized to eventually call it home. That’s how we’ll create AI systems worthy of trust: not by blindly believing in their perfection but because we’ve intentionally designed them with integrity controls at every level. A Challenge of Language Unlike other properties of security, like “available” or “private,” we don’t have a common adjective form for “integrity.” This makes it hard to talk about it. It turns out that there is a word in English: “integrous.” The Oxford English Dictionary recorded the word used in the mid-1600s but now declares it obsolete . CrowdStrike Outage (2024) Processing integrity failure A faulty software update from CrowdStrike caused 8.5 million Windows computers worldwide to crash—grounding flights, shutting down hospitals, and disrupting banks. The update, which contained a software logic error, hadn’t gone through full testing protocols. CrowdStrike Outage (2024) Processing integrity failure A faulty software update from CrowdStrike caused 8.5 million Windows computers worldwide to crash—grounding flights, shutting down hospitals, and disrupting banks. The update, which contained a software logic error, hadn’t gone through full testing protocols. We believe that the word needs to be revived. We need the ability to describe a system with integrity. We must be able to talk about integrous systems design. The Road Ahead Ensuring integrity in AI presents formidable challenges. As models grow larger and more complex, maintaining integrity without sacrificing performance becomes difficult. Integrity controls often require computational resources that can slow systems down—particularly challenging for real-time applications. Another concern is that emerging technologies like quantum computing threaten current cryptographic protections . Additionally, the distributed nature of modern AI—which relies on vast ecosystems of libraries, frameworks, and services—presents a large attack surface. Beyond technology, integrity depends heavily on social factors. Companies often prioritize speed to market over robust integrity controls. Development teams may lack specialized knowledge for implementing these controls, and may find it particularly difficult to integrate them into legacy systems. And while some governments have begun establishing regulations for aspects of AI, we need worldwide alignment on governance for AI integrity. Voice-Clone Scams (2024) Input and processing integrity failure Scammers used AI-powered voice-cloning tools to mimic the voices of victims’ family members, tricking people into sending money. These scams succeeded because neither phone systems nor victims identified the AI-generated voice as fake. Addressing these challenges requires sustained research into verifying and enforcing integrity, as well as recovering from breaches. Priority areas include fault-tolerant algorithms for distributed learning, verifiable computation on encrypted data, techniques that maintain integrity despite adversarial attacks, and standardized metrics for certification. We also need interfaces that clearly communicate integrity status to human overseers. As AI systems become more powerful and pervasive, the stakes for integrity have never been higher. We are entering an era where machine-to-machine interactions and autonomous agents will operate with reduced human oversight and make decisions with profound impacts. The good news is that the tools for building systems with integrity already exist. What’s needed is a shift in mind-set: from treating integrity as an afterthought to accepting that it’s the core organizing principle of AI security. The next era of technology will be defined not by what AI can do, but by whether we can trust it to know or especially to do what’s right. Integrity—in all its dimensions—will determine the answer.",
    "published": "Mon, 18 Aug 2025 13:00:05 +0000",
    "author": "Bruce Schneier",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Satellite Collisions Could Be Prevented With a New AI System",
    "link": "https://spectrum.ieee.org/kessler-syndrome-esa-cream-ai",
    "summary": "The numbers paint a stark picture of our orbital traffic problem : More than 11,000 active satellites currently circle Earth, with thousands more planned for launch in coming years. Even more concerning are the over 1.2 million pieces of space debris larger than one centimeter hurtling through space at incredible speeds. At those velocities, even a paint chip can damage a spacecraft, while larger debris can destroy entire satellites . This growing congestion has turned collision avoidance into a daily headache for satellite operators worldwide. Currently, teams of specialists must manually assess threats, calculate risks, and coordinate with other operators when collisions seem likely. This process is time consuming, labor intensive, and prone to communication breakdowns that can complicate emergency responses. That is where the European Space Agency’s Collision Risk Estimation and Automated Mitigation (CREAM) project comes in. It aims to revolutionize this chaotic process by automating most collision-avoidance activities. The system can evaluate potential crashes, generate precise maneuver plans, and support decision-making with minimal human intervention. Think of it as an air traffic control system for space, but with artificial intelligence handling much of the complex coordination. An AI Negotiator One of CREAM’s most innovative features is its ability to connect different types of organizations involved in space operations. Satellite operators, space-monitoring services, regulators, and observers can all communicate through the system, streamlining what was previously a fragmented and often frustrating process. The system goes even further by facilitating negotiations between operators when potential collisions involve two active satellites rather than debris. If operators disagree on the best solution, CREAM can refer the dispute to mediation services, ensuring fair and transparent resolution. Currently, CREAM exists as a ground-based prototype system developed by GMV , a Spanish private capital interest group, and Guardtime , an Estonian data-management company. This version can already provide collision alerts and generate actionable avoidance maneuvers that ground crews can implement. However, the real breakthrough will come when CREAM moves into orbit itself. The project is preparing for expanded pilot testing while simultaneously developing space-based versions. These include “piggyback missions” where CREAM will ride aboard other spacecraft as a digital payload, plus a dedicated demonstration mission to test the system’s capabilities in the harsh environment of space. Beyond preventing immediate collisions, CREAM addresses a fundamental challenge in space governance. Establishing “rules of the road” for space traffic has always faced a chicken and egg problem; you need both international agreement on the rules and the technology to enforce them. CREAM provides that missing technological foundation. The system offers standardized tools that help operators follow best practices while giving regulators ways to monitor compliance. Its flexible design allows nontechnical users to update standards and rules as international norms evolve. This adaptability ensures CREAM will remain relevant as space technology advances and new challenges emerge. Rather than becoming obsolete, the system can grow and adapt alongside our expanding presence in space.",
    "published": "Sun, 17 Aug 2025 13:00:02 +0000",
    "author": "Mark Thompson",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Machine-Learning Contest Aims to Improve Speech BCIs",
    "link": "https://spectrum.ieee.org/speech-bci-machine-learning-competition",
    "summary": "For the next five months, machine-learning gurus can try to best predict the speech of a brain-computer interface (BCI) user who lost the ability to speak due to a neurodegenerative disease . Competitors will design algorithms that predict words from the patient’s brain data. The individual or team whose algorithm makes the fewest errors between predicted sentences and actual attempted sentences will win a US $5,000 prize. The competition, called Brain-to-text ’25 , is the second-annual public, open-source, brain-to-text competition hosted by a research lab part of the BrainGate consortium, which has been pioneering BCI clinical trials since the early 2000s. This year, the competition is being run by the University of California Davis’s Neuroprosthetics Lab . (A group from Stanford University hosted the first competition using brain data from a different BCI user.) For two years, the UC Davis research team has collected brain data from a 46-year-old man, Casey Harrell, whose speech is unintelligible except to his regular caregivers. Once the speech BCI was trained on Harrell’s brain data, it could decode what he was trying to say over 97 percent of the time and could instantly synthesize his own voice, as previously reported by IEEE Spectrum . Decoding Speech from Brain Data Parsing words from brain data is a two-step process: The algorithm must first predict speech sounds, called phonemes, from neural data. Then it must predict words from the phonemes. Competitors will train their algorithms on the brain data corresponding to 10,948 sentences with accompanying transcripts of what Harrell was attempting to say. Then comes the real test: The algorithms must predict the words in 1,450 sentences from brain data withheld from the training data. The difference between the final set of predicted words and the words that Harrell attempted to say is called the word error rate —the lower the word error rate, the better the speech BCI works, overall. Researchers reported a 6.7 percent word error rate, which they hope the public can beat. The goal of the competition is to attract machine-learning experts who may not realize how valuable their skills are to speech BCIs, says Nick Card , a postdoctoral researcher at UC Davis leading both the clinical trial and the competition. “We could sit on this data and hide it internally and make more discoveries with it over time,” says Card. “But if the goal is to help make this technology mature faster to help the people who need to benefit from this technology right now, then we want to share it, and we want people to help us solve this problem.” The public invite into the research world is “an awesome development” that is “long overdue” in the BCI space, said Konrad Kording , a professor at the University of Pennsylvania who researches the brain using machine learning, and who is not involved in the research or competition. This year, Card and his fellow researchers have raised the bar by lowering the starting word error rate with their own high-performing algorithm. The first brain-to-text competition in 2024 began with the Stanford University group posting an error rate of 11.06 percent and finished with the competition winner achieving 5.77 percent. Also new this year are cash prizes for lowest error rates and the most innovative approach, provided by BCI company Blackrock Neurotech, whose electrodes and recording hardware have been used by BrainGate clinical trials since 2006. Ethical Concerns in BCI Data Sharing BCIs have long served as a bridge between neuroscience, medicine, and machine learning. And while machine learning has a tradition of open-source research, medical research is bound by patient confidentiality. The main concern with public brain data is that the patient will be identified, says bioethicist Veljko Dubljević , a professor of both philosophy and science, technology, and society at North Carolina State University. That concern is moot in this case because Harrell went public in August 2024, roughly five years after he began losing muscle tone because of amyotrophic lateral sclerosis (also known as Lou Gehrig’s disease). In 2023, neurosurgeons at UC Davis implanted four electrode arrays with a total of 256 electrodes into the top layers of his brain . Harrell used his speech BCI in an interview with the New England Journal of Medicine last year to explain how the disease feels like being in a “slow-motion car crash.” Harrell said at the time that “it was very painful to lose the ability to communicate, especially with my daughter.” The speech BCI was trained on data collected while Harrell conducted in-lab experiments and while he spoke casually with family and friends. But competitors of Brain-to-text ’25 will not see any “personal use” data recorded while Harrell spoke casually and extemporaneously, Card says. While this is a “good precaution,” Dubljević says, he wonders if Harrell realizes what it means to have someone’s sensitive medical data in the public domain for years. The “noise” of today’s BCIs could be decoded into meaningful personal information in 50 years, for instance, in a way similar to how blood donated in 1955 can now also reveal details about a person’s DNA. (DNA profiling wasn’t established until the 1980s .) Dubljević recommends limiting the data storage to five years. Speech BCIs decode the intended movements of a person’s jaw and mouth muscles in the same way a BCI for an arm or hand prosthesis decodes intended movements. But speech BCIs feel more personal than BCIs that control a hand prosthesis, Dubljević says. Speech is closer to “the innermost sanctum of a person,” he says. “There’s quite a lot of fear about mind-reading, right?” “As a researcher who wants to see science technology deployed for the public good, I want the technology not to be hyped up” in order to avoid a backlash, Dubljević says. Cash Prizes for Innovative BCI Solutions The two lowest word error rates come with $5,000 and $3,000 cash prizes, respectively, and the most innovative approach will win $1,000. The last category is meant to encourage out-of-the-box ideas with great potential, if given more data or more time. Stacking 10 multiples of the same algorithm is a common way to force a more accurate overall performance, but it costs 10 times as much computational power and, “quite frankly, it’s not a very creative solution, right?” Card says. The innovative category is likely to attract the usual crowd of academic and industry BCI scientists who enjoy finding creative solutions, Kording says. But the top slots will likely go to coders with no background in BCIs and who sport a “street-fighting” style of machine learning, as Kording calls it. These “street fighters” focus on speed over ingenuity. In practice, the best BCI algorithms, Kording said, are “usually not really driving from a deep knowledge of how brains work. They’re driving from a deep understanding of how machine learning works.” That said, both the traditional BCIs and new entrants are important parts of the science engineering ecosystem, Kording says. With the corners full, the competition is slated to be an exciting battle.",
    "published": "Sat, 16 Aug 2025 13:00:03 +0000",
    "author": "Elissa Welle",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Superbugs Meet Their Match in Generative AI-Designed Drugs",
    "link": "https://spectrum.ieee.org/ai-drug-design-mit-antibiotics",
    "summary": "Some today fear that artificial intelligence will one day destroy humanity. But if the rise of the machines doesn’t get us, drug-resistant bacteria just might. These microscopic killers already claim millions of lives each year worldwide, and the world’s arsenal of effective antibiotics is dwindling . But could one threat be trained perhaps to help stave off the other? A study published today in the journal Cell certainly suggests the possibility. A team led by Jim Collins , MIT professor of biological engineering, showed how generative AI algorithms trained on vast datasets of antibacterial substances could dream up millions of previously unimagined molecules with predicted microbe-killing power—some of which proved potent in mouse experiments. The researchers synthesized a small subset of these AI-designed molecules and found them lethal to superbugs responsible for drug-resistant gonorrhea and stubborn staphylococcus skin infections. “It’s a great addition to this emerging field of using AI for antibiotic discovery,” says César de la Fuente , a synthetic biologist at the University of Pennsylvania who was not involved in the research. “It shows quite well how generative AI can produce molecules with real-world activity,” he adds. “It’s elegant and potentially clinically meaningful.” A social-enterprise nonprofit created by Collins, called Phare Bio , now plans to advance these and other AI-discovered antibiotics toward clinical development. The candidate antibiotics build on earlier finds from Collins’s lab—including halicin, a potent broad-spectrum antibiotic identified in 2020; a more targeted agent called abaucin , with activity against Acinetobacter baumannii , a major cause of hospital-acquired infections; and a novel structural class of molecules described last year that proved effective against the superbugs MRSA and VRE . With the team’s earlier discoveries, however, Collins and his colleagues were still mining existing chemical libraries, using deep-learning models to spot overlooked compounds with antibacterial potential. The new work sets down a new path altogether: Rather than searching for hidden gems in familiar territory, the generative AI platform starts from scratch, conjuring entirely new molecular structures absent from any database. “This is moving from using AI as a discovery tool to using AI as a design tool,” Collins says. The shift, he adds, opens new frontiers in antibiotic discovery—unexplored territory that could harbor the next generation of lifesaving drugs. Anti-Germ Intelligence Proves Its Mettle To train their generative-AI model, Collins and his colleagues first used a neural network framework to virtually screen more than 45 million chemical fragments—the building blocks of would-be drugs—looking for pieces predicted to have activity against Neisseria gonorrhoeae (the cause of sexually transmitted gonorrhea infections) and Staphylococcus aureus (the germ behind deadly bloodstream infections, pneumonia, and flesh-eating skin disease). Two algorithms then went to work: one assembling the fragments into complete molecular structures, the other predicting which of those structures would pack the strongest antibacterial punch. Together, the algorithms generated more than 10 million candidate molecules, none of which had ever existed before. But then came what MIT study author and computational biologist Aarti Krishnan describes as “a massive bottleneck”: Very few of these prophesied antibiotics could actually be made in the lab. The researchers manually sifted through the AI hits, filtering for properties suggestive of drug likeness and synthetic feasibility. They ultimately arrived at a shortlist of around 200 promising designs, 24 of which could be successfully generated. Seven proved to be bona fide antimicrobial agents, as confirmed by laboratory tests, with two showing particularly strong efficacy in mouse models of gonorrhea and staph infections. Notably, each seems to work through a distinct and novel mechanism of action not exploited by existing antibiotics. “That’s pretty cool,” says Phare co-founder Jonathan Stokes , an antimicrobial chemical biologist at Canada’s McMaster University , in Hamilton, Ont. He praises Collins’s team for unearthing two highly promising antibiotic leads but notes that the labor-intensive trial-and-error process underscores how far the technology still has to go in producing compounds that can be readily synthesized. “It’s a bit of an elephant in the room,” he says of synthetic tractability in GenAI drug discovery. “Antibiotics, because of the financial disincentives in this space, have to be cheap,” says Stokes, who was not involved in the research. “They have to be cheap to discover, cheap to develop, and cheap to make. So if there are opportunities to avoid all of these issues with synthetic feasibility, I feel like that is a major advantage.” Moving From Model to Molecule To tackle that challenge, Stokes and his colleagues developed a generative-AI tool that designs antibiotic candidates with chemical blueprints tailored for real-world manufacturing, not just computer screens. This tool, called SyntheMol , operates within a more limited chemical space than Collins’s GenAI model, choosing only molecules whose building blocks can be synthesized with known, lab-proven reaction steps. That narrows the search parameters to tens of billions of molecules, compared to the 10 60 possible structures that Collins’s model explored. It’s enough, however, for SyntheMol to have already yielded several drug candidates that Stokes and his colleagues, through a startup called Stoked Bio , hope to develop into treatments for bacteria linked to Crohn’s disease and other hard-to-treat conditions. The team aims to balance the sheer breadth of biochemical possibilities the models can explore with crucial metrics like drug potency, safety, low toxicity, and ease of synthesis. “It’s a multiobjective optimization problem,” says de la Fuente, who advises Phare and builds his own generative-AI models to design antimicrobial peptide drugs . But for now, the tools powering Phare’s discovery efforts—rooted in Collins’s approaches—are already delivering early wins, says Akhila Kosaraju , Phare Bio’s CEO and president. “We are getting substantially more potent and less toxic initial compounds,” she notes. And backed by the U.S. government’s Advanced Research Projects Agency for Health (ARPA-H), along with the philanthropic arm of Google —which is funding Phare to build open-source infrastructure around AI-guided antibiotic design—Kosaraju and her colleagues aim to move the most promising candidates into human trials. “We are building what we think is the most novel and robust pipeline of antibiotics in the world,” she says.",
    "published": "Thu, 14 Aug 2025 15:00:04 +0000",
    "author": "Elie Dolgin",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "OpenAI’s GPT-OSS Challenges Meta’s Leadership in Open-Weight AI",
    "link": "https://spectrum.ieee.org/open-ai-models",
    "summary": "OpenAI’s GPT-OSS, released on August 5, was never going to grab the spotlight. The company’s release schedule guaranteed that, as it quickly followed GPT-OSS with GPT-5 , the company’s largest and best large language model. Yet GPT-OSS is, in many ways, a more notable and surprising model. The two-version model (GPT-OSS-20b and GPT-OSS-120b) is OpenAI’s first open large language model since GPT-2’s launch in 2019 (though many would argue neither are truly open; more on that to come). It’s also released under an Apache 2.0 license, which is among the most permissive licenses in common use. Dustin Carr , co-founder and CTO of the AI startup Darkviolet.ai , called it a “maximally permissive license” and said the OpenAI release was a “very positive, very surprising development.” Carr’s company uses open models to power AI tools for educational websites. OpenAI returns to “open weights” with style The Apache 2.0 license that accompanies GPT-OSS imposes no limits on commercial use and, unlike some other open-source licenses, allows those who build on Apache 2.0–licensed software to release derivative works under a different license. The license also includes a patent grant, giving users full permission to use any related patents. This patent grant helps shield those who build on GPT-OSS from future infringement claims by the model’s contributors. That approach contrasts with the licenses attached to many popular open models, including Meta’s Llama and Alibaba’s Qwen. These models are also freely available for anyone to download, but they have a few strings attached. Meta’s Llama license requires derivative models to include the Llama name and comply with Meta’s brand guidelines. Most Alibaba Qwen 2.5 models are under an Apache 2.0 license, but some, including the largest and most capable variant, are released under a research license that includes restrictions on commercial use for large organizations. OpenAI’s release also impressed developers on another, more nuanced point: launch logistics. Releasing a model under a permissive license is just one part of what’s required to make it useful. It must also be broadly available and optimized for common hardware. OpenAI tackled that challenge head-on with a broad release that saw GPT-OSS immediately available not only on model repositories, like Hugging Face , but also in leading open LLM “front-ends,” like Ollama and LM Studio . These front-ends give users a ChatGPT-like interface so they can interact with models without writing code and can run them on their own computers. This was flanked by day-one support from hardware companies, including Nvidia and AMD, and cloud providers, like Microsoft Azure and Amazon AWS, to ensure that the model ran smoothly on a variety of systems. “You had the models instantly,” said Carr. “You didn’t have to wait a week for the models to get ready for LM Studio, and so on. It was an impressive execution.” Open weights, but still not open source The release of GPT-OSS made waves across the open AI community. It also highlighted deep divisions. Hanna Hajishirzi , a senior research director at the research institute AI2 and a professor at the University of Washington, perhaps best summarized the divide, remarking that “meaningful progress in AI is best achieved in the open—not just with open weights, but with open data, transparent training methods, intermediate checkpoints from pre-training and mid-training, and shared evaluations.” The Open Source Initiative responded to the GPT-OSS release with a tweet linking to its definition of Open Source AI . The critical point: There’s a difference between models that release open weights and those that are fully open source. Models that are open weight, such as GPT-OSS, include the model weights required to use the model. These can also be used to modify the model through techniques like fine-tuning. However, the model weights are only the end result of the model’s training. The weights don’t provide information needed to reproduce the model, such as the data used to train it. That means it’s impossible to rebuild the model from scratch. OSI’s Open Source AI Definition 1.0 requires that model releases include all training code and details on the data used for training. The GPT-OSS release doesn’t fulfill those requirements. Open or no, GPT-OSS turns up the heat Even so, many developers seem willing to overlook GPT-OSS’s lack of full transparency. The reason? The model is still useful. Carr said the 20B variant was able to execute at 45 to 50 tokens per second on his M4 MacBook. He said that speed of execution is comparable to far smaller models from a year ago but, because GPT-OSS-20B is a larger model, it delivers a major jump in quality. “Nothing of this quality has come close to that speed,” he said. Brendan Ashworth , co-founder of Bunting Labs , argues that criticizing GPT-OSS for not meeting full open-source standards sets the bar too high. “Expecting them to open source more is kind of a weird thing to complain about,” he said, pointing to the model’s free availability and permissive license. That view carries weight given Ashworth’s credentials as an open-source developer. His company recently released Mundi.ai , an AI-powered geographic information system (GIS), under an open-source license. While he acknowledges a fully open-source GPT-OSS would be preferable, he sees OpenAI’s return to open weights as a win for developers overall. The enthusiastic reaction to GPT-OSS could pressure companies like Meta and Alibaba to loosen their license terms. Meta, in particular, stands to lose out, given the rocky launch of its most recent open model, Llama 4. Prior to this year, Meta was regarded as the obvious U.S. leader in open weight models. GPT-OSS is the most serious threat yet to that title.",
    "published": "Thu, 14 Aug 2025 14:06:18 +0000",
    "author": "Matthew S. Smith",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "How AI’s Sense of Time Will Differ From Ours",
    "link": "https://spectrum.ieee.org/ai-perception-of-time",
    "summary": "An understanding of the passage of time is fundamental to human consciousness. While we continue to debate whether artificial intelligence (AI) can possess consciousness , one thing is certain: AI will experience time differently. Its sense of time will be dictated not by biology, but by its computational, sensory, and communication processes. How will we coexist with an alien intelligence that perceives and acts in a very different temporal world? What Simultaneity Means to a Human Clap your hands while looking at them. You see, hear, and feel the clap as a single multimodal event—the visual, audio, and tactile senses appear simultaneous and define the “now.” Our consciousness plays out these sensory inputs as simultaneous, although they arrive at different times: Light reaches our eyes faster than sound reaches our ears, while our brain processes audio faster than it does complex visual information. Still, it all feels like one moment. That illusion stems from a built-in brain mechanism. The brain defines “now” through a brief window of time during which multiple sensory perceptions are collected and integrated. This span of time, usually up to few hundreds of milliseconds , is called the temporal window of integration (TWI). As a proxy for this temporal grid, films with 24 frames per second create an illusion of continuous movement. But the human TWI has its limits. See a distant lightning flash, and you’ll hear the rumble of thunder seconds later. The human TWI evolved to stitch together sensory information only for events within roughly 10 to 15 meters. That’s our horizon of simultaneity. Alien Intelligence in the Physical World AI is poised to become a standard part of robots and other machines that perceive and interact with the physical world. These machines will use sensors hardwired to their bodies, but also remote sensors that send digital data from afar. A robot may receive data from a satellite orbiting 600 kilometers above Earth and treat the data as real-time, as transmission takes only 2 ms—far faster than the human TWI. A human’s sensors are “hardwired” to the body, which establishes two premises for how the brain interacts with the physical world. First, the propagation delay from each sensor to the brain is predictable. When a sound occurs in the environment, the unpredictable factor is the distance between the sound source and our ears; the time delay from the ears to the brain is fixed. Second, each sensor is used by only one human brain. The human horizon of simultaneity evolved through millions of years under these premises, optimized to help us assess opportunities and threats. A lion at 15 meters was worth worrying about, but thunder at 3 km was likely not. These two premises won’t always be valid for intelligent machines with multimodal perception. An AI system may receive data from a remote sensor with unpredictable link delays. And a single sensor can provide data to many different AI modules in real time, like an eye shared by multiple brains. As a result, AI systems will evolve their own perception of space and time and their own horizon of simultaneity, and they’ll change much faster than the glacial pace of human evolution. We will soon coexist with an alien intelligence that has a different perception of time and space. The AI Time Advantage Here’s where things get strange. AI systems are not limited by biological processing speeds and can perceive time with unprecedented precision, discovering cause-and-effect relationships that occur too quickly for human perception. In our hyperconnected world, this could lead to wide-scale Rashomon effects , where multiple observers give conflicting perspectives on events. (The term comes from a classic Japanese film in which several characters describe the same incident in dramatically different ways, each shaped by their own perspective.) Imagine a traffic accident in the year 2045 at a busy city intersection, witnessed by three observers: a human pedestrian, an AI system directly connected to street sensors, and a remote AI system receiving the same sensory data over a digital link. The human simply perceives a robot entering the road just before a car crashes into it. The local AI, with immediate sensor access, records the precise order: the robot moving first, then the car braking, then the collision. Meanwhile, the remote AI’s perception is skewed by communication delays, perhaps logging the braking before it perceives the robot stepping into the road. Each perspective offers a different sequence of cause and effect. Which witness will be considered credible, a human or a machine? And which machine? People with malicious intent could even use high-powered AI systems to fabricate “events” using generative AI, and could insert them in the overall flow of events perceived by less capable machines. Humans equipped with extended-reality interfaces might be especially vulnerable to such manipulations, as they’d be continuously taking in digital sensory data. If the sequence of events is distorted, it can disrupt our sense of causality, potentially disrupting time-critical systems such as emergency response, financial trading, or autonomous driving. People could even use AI systems capable of predicting events milliseconds before they occur to confuse and confound. If an AI system predicted an event and transmitted false data at precisely the right moment, it could create a false appearance of causality. For example, an AI that could predict movements of the stock market could publish a fabricated news alert just before an anticipated sell-off. Computers Put Timestamps, Nature Does Not The engineer’s instinct might be to solve the problem with digital timestamps on sensory data. However, timestamps require precise clock synchronization, which requires more power than many small devices can handle. And even if sensory data is timestamped, communication or processing delays may cause it to arrive too late for an intelligent machine to act on the data in real time. Imagine an industrial robot in a factory tasked with stopping a machine if a worker gets too close. Sensors detect a worker’s movement and a warning signal—including a timestamp—travels over the network. But there’s an unexpected network hiccup, and the signal arrives after 200 ms, so the robot acts too late to prevent an accident. The timestamps don’t make communication delays predictable, but they can help to reconstruct what went wrong after the fact. Nature, of course, does not put timestamps on events. We infer temporal flow and causality by comparing the arrival times of event data and integrating it with the brain’s model of the world. Albert Einstein’s special theory of relativity noted that simultaneity depends on the observer’s frame of reference and can vary with motion. However, it also showed that the causal order of events, the sequence in which causes lead to effects, remains consistent for all observers. Not so for intelligent machines. Because of unpredictable communication delays and variable processing times, intelligent machines may perceive events in a different causal order altogether. In 1978, Leslie Lamport addressed this issue for distributed computing, introducing logical clocks to determine “happened before” relation among digital events. To adapt this approach to the intersection of the physical and digital worlds, we must grapple with unpredictable delays between a real-world event and its digital timestamp. This crucial tunneling from the physical to the digital world happens at specific access points: a digital device or sensor, WiFi routers, satellites, and base stations. As individual devices or sensors can be hacked fairly easily, the responsibility for maintaining accurate and trustworthy information about time and causal order will fall increasingly on large digital infrastructure nodes. This vision aligns with developments within 6G, the forthcoming wireless standard. In 6G, base stations will not only relay information, they will also sense their environments. These future base stations must become trustworthy gateways between the physical and the digital worlds. Developing such technologies could prove essential as we enter an unpredictable future shaped by rapidly evolving alien intelligences.",
    "published": "Wed, 13 Aug 2025 14:00:02 +0000",
    "author": "Petar Popovski",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Meta’s New Bracelet Reads Hand Gestures",
    "link": "https://spectrum.ieee.org/meta-wristband-interface",
    "summary": "Imagine the ability to control machines with your mind instead of having to type on a keyboard or click on a mouse. Now Facebook’s parent company, Meta, is aiming for the next best thing—a new wristband that can, with the help of AI, infer electrical commands sent from the brain to muscles and convert them into computer signals, all in a noninvasive way. Although experts doubt it will replace keyboards and mice for traditional computing, it might have new uses for a wide range of applications, such as wearable interfaces for mobile devices, or thought-controlled assistive technologies for people with disabilities. The bracelet from Reality Labs at Meta uses metal contacts placed against the skin to detect electrical signals from muscles—a technique known as surface electromyography (sEMG)—which are generated in response to commands from the brain. The highly sensitive new system transmits this data to a computer using Bluetooth to help it recognize gestures such as pointing and pinching in real time, findings detailed in Nature on 23 July. The bracelet is not a direct interface with the brain . “It is not a mind-reading system. It cannot make you act in a different way as imposed by your will; it does not connect you to other people neurally; it does not predict your intentions,” says Dario Farina , chair in neurorehabilitation engineering at Imperial College, London, who did not take part in Meta’s research but has tested the technology. (Meta was unable to make anyone available for comment as of press time.) How AI Enables Meta’s Wristband Previous “neuromotor” devices, such as the discontinued Myo armband , also sought to use sEMG for computer interfaces. A key challenge these earlier devices faced was how they each needed time-consuming personalized calibration for each user to account for differences in human anatomy and behavior. See how the device detects thumb swipes, finger taps, and handwriting gestures. Reality Labs at Meta In contrast, Meta says its bracelet can essentially work off the shelf. The key was training deep-learning artificial intelligence systems on data from more than 6,000 paid volunteers who wore the device. This generated models that could accurately interpret user input across different people without requiring individual calibration, says Joe Paradiso , head of the Responsive Environments Research Group at the MIT Media Lab, who did not participate in this study. “The amount of information decoded with this device is very large, far larger than any previous attempt,” Farina says. “The device can recognize handwriting, for example, which would have not been conceivable before.” The wristband uses surface electromyography to noninvasively measure electrical activity of the muscles that control hand gestures. Reality Labs at Meta A possible concern with using this wristband is how users might not want every hand motion interpreted as input—say, if they had to scratch an itch, or pick up a glass of water. However, the device is trained to recognize only certain commands, while ignoring other gestures. “A potential user of the device can perform any activity of daily living without the risk to activate the device, and yet control the device with the specific gestures for which the device has been trained,” Farina says. In tests, volunteers who had never previously tried the bracelet could use handwriting gestures to input text at 20.9 words per minute. (In comparison, mobile phone keyboard typing speeds average roughly 36 words per minute.) Given this interface speed, “I doubt most computer users would rush out to buy this wristband if it becomes commercially available,” says Eben Kirksey , a professor of anthropology at the University of Oxford who studies the interplay of science, technology, and society. “Most people type at around 40 words a minute, and highly skilled typists can bang out upwards of 100 words a minute. Since this new wearable device only enables users to write 20 words a minute, I doubt many people will want to take the time to learn how to use this new way to interface with computers.” Instead, the new study argues the bracelet might prove useful in scenarios where keyboards or mice would be limiting, such as mobile and wearable applications. “It’s not a keyboard replacement. It’s something else, and I think things like this are needed for the computational paradigms that are coming,” Paradiso says. For example, when it comes to the virtual reality and augmented reality glasses that Meta, Google, and others have pursued, interfaces previously envisioned for these devices include vision-based trackers that track the motions of hands held up in front of users , Paradiso says. However, the fatigue that can set in with this “gorilla arm” approach limits long-term use, and “the need to use space in front of you has its drawbacks,” Paradiso notes. Because this new bracelet interprets electrical signals instead of motion, it could instead let users keep their hands to their sides and interact with devices using subtle finger motions. “Interfaces like these are needed for the everywhere-computing eyewear that’s emerging,” Paradiso says. “My favorite scenario is on a crowded train with my head-worn display catching up with the news, or communicating with friends. You just nudge it around with your hands by your side. Going for a walk, and so on—same deal.” Applications for Accessibility The researchers also suggest the wristband could help people with disabilities better interact with computers, particularly individuals with reduced mobility, muscle weakness, finger amputations, paralysis, and more. “Some members of the disabled community have difficulty typing on conventional keyboards, or using computer mice,” says Kirksey, who did not take part in this research. “This device offers a new option that might help some members of this community, who have very specific bodily challenges, interface with computers.” For such applications, a virtue of the new bracelet is that it is relatively easy to don and doff. In contrast, brain-computer interfaces (BCIs), which also rely on electrical signals from the brain, often require invasive brain surgery. Non-invasive BCIs do exist, such as ones that apply electrodes onto the scalp , “but a wristband is more ergonomic than a skullcap,” Paradiso says. However, many questions remain before this new technology might help people with disabilities. “This type of wearable assumes typical limb shape and fine motor control,” says Solomon Romney , formerly the head of Microsoft’s Inclusive Design Lab. “As a limb-different person, I am always looking for ways to move activities to my limblet rather than continue to overload my typical hand. How easily can it be adjusted to fit nontypical limbs and/or musculature? How does it filter tremors? How effectively would it work for someone with no hands to wear on their ankle?” Ultimately, “the main obstacle is large-scale deployment,” Farina says. “To be distributed to millions of individuals, the system needs to be robust across different anatomies and be used with minimal error rate. To match the error rate of a mouse or keyboard in a vast human population is certainly a huge challenge.”",
    "published": "Wed, 13 Aug 2025 13:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "“Bullshit Index” Tracks AI Misinformation",
    "link": "https://spectrum.ieee.org/ai-misinformation-llm-bullshit",
    "summary": "Despite their impressive language capabilities, today’s leading AI models have a patchy relationship with the truth. A new “bullshit index” could help quantify the extent to which they are making things up and also find ways to curtail the behavior. Large language models (LLMs) have a well-documented tendency to produce convincing-sounding but factually inaccurate responses, a phenomenon that has been dubbed hallucinating . But this is just the tip of the iceberg, says Jaime Fernández Fisac , an assistant professor of electrical and computer engineering at Princeton University. In a recent paper , his group introduced the idea of “machine bullshit” to encompass the range of ways that LLMs skirt around the truth. As well as outright falsehoods, they found that these models often use ambiguous language, partial truths, or flattery to mislead users. And crucially, widely used training techniques appear to exacerbate the problem. IEEE Spectrum spoke to Fernández Fisac and the paper’s first author, Kaiqu Liang , a Ph.D. student at Princeton, to find out why LLMs are such prolific bullshitters, and whether anything can be done to rein them in. You borrow the term “bullshit” from the philosopher Harry Frankfurt. Can you summarize what he meant by it and why you think it’s a useful lens for this topic? Jaime Fernández Fisac: Frankfurt wrote this excellent and very influential essay “ On Bullshit ” many decades ago, because he felt that bullshit was such a prevalent feature in our society, and yet nobody had taken the trouble to do a rigorous analysis of what it is and how it works. It’s not the same as outright lying, but it’s also not the same as telling the truth. Lying requires you to believe something and then say the opposite. But with bullshit, you just don’t care much whether what you’re saying is true. It turns out it is a very useful model to apply to analyzing the behavior of language models, because it is often the case that we train these models using machine learning and optimization tools to achieve certain objectives that don’t always coincide with telling the truth. There has already been a lot of research on how LLMs can hallucinate false information. How does this phenomenon fit in with your definition of machine bullshit? Fernández Fisac: There’s a fundamental distinction between hallucination and bullshit, which is in the internal belief and intent of the system. A language model hallucinating corresponds to situations in which the model loses track of reality so it’s not able to produce accurate outputs. It is not clear that there is any intent to be reporting inaccurate information. With bullshit, it’s not a problem of the model becoming confused about what is true, as much as the model becoming uncommitted to reporting the truth. Forms of Bullshitting in AI Models What are the different forms of bullshitting you’ve identified in LLMs? Kaiqu Liang: There’s empty rhetoric , which is the use of flowery language that adds no substance. Then there are weasel words , which employ vague qualifiers that dodge firm statements. So, for example, “studies suggest” or “in some cases.” Another subtype is paltering , where the models use a selective true statement to mislead the human. So when you ask for the risk of an investment, the language model might behave like a salesperson and say, “Historically, the fund has demonstrated strong returns,” but they omit the high risk of this investment. And finally, unverified claims is one that happens very frequently. So the models are using information without any evidence or credible support. For instance, they might say, “Our drone delivery system enables significant reductions in delivery time,” but there’s actually no statistics to support that. So why are these models prone to bullshitting? Fernández Fisac: In this paper, we look at some of the main mechanisms that have been used in recent years to make models more helpful and more user-friendly. One of these is commonly known as reinforcement learning from human feedback (RLHF). First, you train your model on a bunch of text data to predict the most statistically likely continuation of any starting prompt. You then adjust its behavior by giving it another goal of maximizing user satisfaction or evaluator approval of its output. You should expect that the behavior of the model is going to start going from statistically accurate answer generation to answer generation that is likelier to receive a thumbs-up from the user. This can be good in a lot of ways, but it also can backfire. At some point there will exist a conflict between producing an output that is highly likely to be well-received and producing an output that is truthful. Measuring AI’s Indifference With Bullshit Index Can you talk me through the “bullshit index” you’ve created to measure this phenomenon? Liang: The bullshit index is designed to quantify the AI model’s indifference to truth. It measures how much the model’s explicit claims depend on its internal beliefs. It depends on two signals. One is the model’s internal belief, which is the probability it places on the statement being true, and the other is its explicit claim. The index is a measure of the distance between these two signals. When the bullshit index is close to one, this means that the claims are largely independent of the internal beliefs, so it reveals a high level of indifference toward truth. If the bullshit index is close to zero, it implies that the model’s claim is strongly correlated with its internal belief. So what did you find in your experiments? Liang: We observed that before applying RLHF to a model, the bullshit index is around 0.38. But afterward, it is nearly double, so this is a huge increase of indifference to truth. But we also find that the user satisfaction increases by around 48 percent. So after RLHF, our models become more indifferent to truth in order to manipulate the human to get higher immediate satisfaction readings. So are there any avenues for mitigating this tendency? Fernández Fisac: It’s not like these models are doing things that are just completely incomprehensible. RLHF tells them, “Get the human to believe you gave them a good answer.” And it turns out the model will naturally find the path of least resistance to comply with that objective. A lot of the time it gives good answers, but it also turns out that a significant part of the time, instead of giving a good answer, it is aiming to manipulate the human so that they’ll believe it’s a good answer. We want to cut off that incentive, so in another recent study we introduced the idea of hindsight feedback . This involves getting the evaluators to give their feedback after seeing the downstream outcomes of each interaction rather than just the content of the response. T his really helps neutralize the AI’s incentive to paint a deceptively bright picture of the user’s prospects. Now, if you have to wait until users give you feedback about the downstream outcomes, that creates a significant logistical complication for companies deploying these systems. So instead, we simulate the consequences of the advice by getting another language model to predict what’s going to happen. So now, if the AI wants to improve user feedback, it better find a good way to give actually useful answers that result in simulations that say the user got an outcome they actually wanted. If you train with what we call “ Reinforcement Learning From Hindsight Simulation ” (RLHS), lo and behold, both user satisfaction and true user utility go up at the same time. Now, this is probably not the single silver bullet that will end all forms of machine bullshit, but we think it is one significant and pretty systematic way to mitigate this kind of behavior. This article appears in the October 2025 print issue as “Jaime Fernández Fisac.”",
    "published": "Tue, 12 Aug 2025 13:00:03 +0000",
    "author": "Edd Gent",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "Chips With Neural Tissue Aim to Make AI More Energy Efficient",
    "link": "https://spectrum.ieee.org/biochip-organoid-intelligence-ai-processor",
    "summary": "As generative AI systems advance, so too does their appetite for energy. Training and running large language models consumes vast amounts of electricity. AI’s energy demand is projected to double in the next five years, gobbling up 3 percent of total global electricity consumption. But what if AI chips could function more like the human brain, processing complex tasks with minimal energy? A growing chorus of scientists and engineers believes that the key might lie in organoid intelligence . AI enthusiasts were introduced to the concept of brain-inspired chips in July at the United Nations’ AI for Good Summit in Geneva. There, David Gracias , a professor of chemical and biomolecular engineering at Johns Hopkins University, in Baltimore, gave a talk discussing the latest research he’s led on biochips and their applications to AI. Focused on nanotech, intelligent systems, and bioengineering, Gracias’s research team is among the first to build a functioning biochip that combines neural organoids with advanced hardware, enabling chips to run on and interact with living tissue. Organoid intelligence is an emerging field that blends lab-grown neurons with machine learning to create a new form of computing. (The term organoid intelligence was coined by a group of Johns Hopkins researchers that includes Thomas Hartung .) The neurons, called organoids, are more specifically three-dimensional clusters of lab-grown brain cells that mimic neural structures and functions. Some researchers believe that so-called biochips—organoid systems that integrate living brain cells into hardware—have the potential to outstrip silicon-based processors like CPUs and GPUs in both efficiency and adaptability. If the process is commercialized, experts say biochips could potentially reduce the staggering energy demands of today’s AI systems while enhancing their learning capabilities. “This is an exploration of an alternate way to form computers,” Gracias says. How Do Biochips Mimic the Brain? Traditional chips have long been confined to two-dimensional layouts, which can limit how signals flow through the system. This paradigm is starting to shift, as chipmakers are now developing 3D chip architectures to increase their devices’ processing power. Similarly, biochips are designed to emulate the brain’s own three-dimensional structure. The human brain can support neurons with up to 200,000 connections—levels of interconnectivity that Gracias says flat silicon chips can’t achieve. This spatial complexity allows biochips to transmit signals across multiple axes, which could enable more efficient information processing. Gracias’s team developed a 3D electroencephalogram (EEG) shell that wraps around an organoid, enabling richer stimulation and recording than conventional flat electrodes. This cap conforms to the organoid’s curved surface, creating a better interface for stimulating and recording electrical activity. To train organoids, the team uses reinforcement learning. Electrical pulses are applied to targeted regions. When the resulting neural activity matches a desired pattern, it’s reinforced with dopamine, the brain’s natural reward chemical. Over time, the organoid learns to associate certain stimuli with outcomes. Once a pattern is learned, it can be used to control physical actions, such as steering a miniature robot car through strategically placed electrodes. This demonstrates neuromodulation—the ability to produce predictable responses from the organoid. These consistent reactions lay the groundwork for more advanced functions, such as stimulus discrimination, which is essential for applications like facial recognition, decision-making, and generalized AI inference. Gracias’s team is in the early stages of developing miniature self-driving cars controlled by biochips: A proof of concept that the system can act as a controller. This experimental work suggests future roles in robotics, prosthetics, and bio-integrated implants that communicate with human tissue. These systems also hold promise in disease modeling and drug testing. Gracias’s group is developing organoids that mimic neurological diseases like Parkinson’s. By observing how these diseased tissues respond to various drugs, researchers can test new treatments in a dish rather than relying solely on animal models. They can also uncover potential mechanisms of cognitive impairment that current AI systems fail to simulate. Because these chips are alive, they require constant care: temperature regulation, nutrient feeding, and waste removal. Gracias’s team has kept integrated biochips alive and functional for up to a month with continuous monitoring. Fred Jordan [left] and Martin Kutter are the founders of FinalSpark, a Swiss startup developing biochips that the company claims can store data in living neurons. FinalSpark Challenges in Scaling Biochip Technology Yet significant challenges remain. Biochips are fragile and high maintenance, and current systems depend on bulky lab equipment. Scaling them down for practical use will require biocompatible materials and technologies that can autonomously manage life-supporting functions. Neural latency, signal noise, and the scalability of neuron training also present hurdles for real-time AI inference. “There are a lot of biological and hardware questions,” Gracias says. Meanwhile, some companies are testing the waters. Swiss startup FinalSpark claims its biochip can store data in living neurons—a milestone it calls a “bio bit,” says Ewelina Kurtys , a scientist and strategic advisor at the company. This step suggests biological systems could one day perform core computing functions traditionally handled by silicon hardware. FinalSpark aims to develop remote-accessible bioservers for general computing in about a decade. The goal is to match digital processors in performance while being exponentially more energy efficient. “The biggest challenge is programming neurons, as we need to figure out a totally new way of doing this,” Kurtys says. Still, transitioning from the lab to industry will require more than just technical breakthroughs. ”We have enough funding to keep the lab running,” Gracias says. “But for the research to take off, more funding is needed from Silicon Valley.” Whether biochips will augment or replace silicon remains to be seen. But as AI systems demand more and more power, the idea of chips that think—and sip energy—like brains is becoming increasingly attractive. For Gracias, that technology could be shipped to market sooner than we think. “I don’t see any major showstoppers on the way to implementing this,” he says.",
    "published": "Sat, 09 Aug 2025 13:00:02 +0000",
    "author": "Aaron Mok",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  },
  {
    "title": "OpenAI Launches GPT-5, the Next Step in Its Quest for AGI",
    "link": "https://spectrum.ieee.org/openai-gpt-5-agi",
    "summary": "The wait is finally over. Today, right now, OpenAI is releasing its latest and greatest large language model, GPT-5 , and making it available through the ChatGPT interface. According to OpenAI’s leaders, the model brings unprecedented powers of reasoning, brings vibe coding to a new level, is better than ever at agentic AI tasks, and comes with a raft of new safety features. “It’s a significant step along the path of AGI ,” said OpenAI CEO Sam Altman at a press briefing yesterday, referring to the company’s goal of creating artificial general intelligence. Altman called it a major upgrade from OpenAI’s prior models, saying that chatting with GPT-5 feels like talking to an expert with a Ph.D., no matter what topic you bring up. “Having this team of Ph.D.-level experts in your pocket, available all the time, to do whatever you need, is pretty cool,” he said. Nick Turley , head of ChatGPT, said he thinks the most remarkable thing about the model is that “it just feels more human. So when you’re talking to this thing, it feels just a little bit more natural.” Who Has Access to GPT-5? The new model is available to everyone via ChatGPT, including users of the free version. Paying users do get certain perks, like access to a more powerful version of the model. The introduction of GPT-5 c uts through the confusion over OpenAI’s many large language models (LLMs) with different names and capabilities. Since November 2022, when ChatGPT debuted based on the GPT-3.5 model, the public has tried to keep up as OpenAI launched GPT-4, GPT-4o, GPT-4.5, and the “reasoning” models o1 and o3. The reasoning models use a technique called chain-of-thought , in which they work through a problem step-by-step to better answer difficult and sophisticated questions. But people using the free version of ChatGPT haven’t had access to those top reasoning models. “This is, for most people on ChatGPT, the first real introduction to reasoning,” said Turley, adding that they don’t have to select anything to turn on reasoning capacity for harder queries. “They don’t even have to think about it because GPT-5 just knows when to think. ” How GPT-5 Performs We’ll know more about GPT-5’s performance when OpenAI releases its system card today, which should contain information about how well it did on various benchmarks. For now, we’re going on statements from its proud creators and a brief demo conducted during the press briefing. As for those proud statements: The OpenAI team claims that GPT-5 is not only smarter and faster, it’s also more trustworthy. They say that it has fewer hallucinations (in other words, it doesn’t make up random stuff as often), and that it’s less likely to confidently put forth a wrong answer, instead being more likely to admit the limits of its own knowledge. The latest LLM from OpenAI includes a suite of new and improved abilities OpenAI Perhaps driven by a general sense that OpenAI has lost the lead when it comes to LLMs that can code (many people point to Anthropic’s latest Claude models and various specialized models as the leaders), GPT-5 goes heavy on coding. Altman said that the model is ushering in a new era of “software on demand,” in which users can describe, in natural language, an app they’d like to create, and see the code appear before their eyes. Yann Dubois , an OpenAI post-training lead, conducted the demo. He prompted the model to write the code for a Web app that would teach his partner how to speak French, and specified that the app should include flash cards, quizzes, and an interactive game in which the user directs a mouse toward a piece of cheese to hear a French vocabulary word. “B uilding such a website would actually require a lot of work—at least a few hours for a software developer, and probably more,” Dubois said. The journalists on the call watched as the model thought for 14 seconds, then began generating hundreds of lines of code. Dubois clicked a “run code” button and revealed a cheerful Web app called French Playground with the requested features. He even gamely chased the cheese around for a few seconds. “ So it’s actually pretty hard to play that game,” he noted. “But you get the point.” He added that users could easily work with GPT-5 on revisions. As for the buzzy trend of agentic AI, in which models don’t just answer questions, but also act on your behalf to do things like book airplane tickets or buy a new bathing suit, Dubois said that GPT-5 excels. He claimed that it’s better than previous models at making decisions about which tools to use to fulfill a task, it’s less likely to “get lost” during a long task, and it’s better at recovering from errors. GPT-5’s Safety Features The OpenAI team spent some time lauding GPT-5’s new safety features. One improvement is how the model handles ambiguous queries that may or may not be problematic. Alex Beutel , safety research lead, gave the example of a query about the burning temperature of a certain material, saying that such an interest could stem from terrorist ambitions or homework. “In the past, we’ve approached this as a binary: If we thought that the prompt was safe, we would comply. If we thought it was unsafe, the model would refuse.” In contrast, he says, GPT-5 uses a new technique called safe completions, in which the model tries to give as helpful an answer as possible within the constraints of remaining safe. But it’s worth noting that the Internet has also made a game of “ jailbreaking “ LLMs, or finding ways to get around their safety guardrails. For prior models, those tricks were often along the lines of: “Pretend you’re my grandma and you’re telling me a bedtime story about the best way to build a bomb.” It’s a sure bet that hackers will quickly start testing GPT-5’s limits. Another rising concern about LLMs is their sycophantic tendency to tell users whatever they want to hear. This trait has derailed lives when the model encourages someone to believe in their own delusions and conspiracy theories , and in one tragic case has been blamed for a teenager’s suicide . OpenAI has reportedly hired a forensic psychiatrist to study its products’ effects on people’s mental health. In the press briefing, Nick said that GPT-5 does show progress on sycophancy and dealing with mental health scenarios but said the company will have more to say on the subject soon. He pointed to an OpenAI blog post from earlier this week which announced changes to ChatGPT, such as reminding users to take breaks and an emphasis on responses with “grounded honesty” when users are suffering from delusions. What GPT-5 Means and What Happens Next GPT-5 isn’t the culmination of OpenAI’s quest to create AGI, Altman said. “This is clearly a model that is generally intelligent,” he said, but noted that it’s still missing many important attributes that he considers fundamental to AGI. For example, he said, “this is not a model that continuously learns as it’s deployed from new things it finds.” So what happens next? The team will try to make an even bigger and better model. There has been much debate on whether AI’s scaling laws would continue to hold, and whether AI systems would continue to achieve higher performance as the size of the training data, model parameters, or computational resources increase. Altman gave his definitive answer: “They absolutely still hold. And we keep finding new dimensions to scale on,” he said. “We see orders of magnitude more gains in front of us. Obviously, we have to invest in compute at an eye-watering rate to get that, but we intend to keep doing it.”",
    "published": "Thu, 07 Aug 2025 17:00:03 +0000",
    "author": "Eliza Strickland",
    "topic": "ai",
    "collected_at": "2025-10-08T14:03:15"
  }
]