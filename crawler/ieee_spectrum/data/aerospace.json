[
  {
    "title": "11 Oddball Technology Records You Probably Didn’t Know",
    "link": "https://spectrum.ieee.org/11-oddball-technology-records-you-probably-didnt-know",
    "summary": "This article is part of The Scale Issue . Longest Continuously Operating Electronic Computer Voyager 1 and its twin space probe , both launched by NASA in 1977, were the first human-made objects to reach interstellar space. But that’s not the only record the spacecraft hold. Voyager 2’s Computer Command System has not been turned off since it first booted up about 48 years ago, making it the longest continuously operating electronic computer. Quietest Place on Earth Can you hear your own heartbeat? For most of us, the answer is no—unless you’re standing in Orfield Laboratories’ anechoic chamber, in which case, you might be able to hear the blood rushing through your veins and the sound of your own blinking, too. The chamber in Minneapolis holds the title for quietest place on earth, with a background noise reading of –24.9 A-weighted decibels—meaning that the ambient sound is far below the threshold of human hearing. Longest-Lasting Battery An experimental electric bell at the University of Oxford, in England, has been ringing nearly continuously for 185 years. Powered by two dry piles—an early type of battery—connected in series, the bell has rung more than 10 billion times since it was set up in 1840. Its ringing, however, is now barely audible beneath the glass bell jar protecting the experiment. Fastest Typing Using Brain Signals For people with certain neurodegenerative conditions that impact muscle control, communication can be difficult. Brain–computer interfaces offer a solution by directly translating brain waves to text. But until recently, that translation has been slow. In 2022, researchers at the University of California, San Francisco, set the record for the fastest communication via brain signals: 78 words per minute. Best-Selling Consumer Electronics Certain consumer electronics, like the iPhone, seem ubiquitous. Over 18 years and about as many generations, more than 2.3 billion Apple smartphones have been sold. But when you break it down to individual models, which devices have been the biggest success? See how some particularly popular devices compare. Strongest Magnetic Field on Earth At least among magnets that don’t explode from their own field strength, the U.S. National High Magnetic Field Laboratory’s Pulsed Field Facility holds the record for strongest magnetic field on earth. The 100-Tesla field, which is about 2 million times as strong as Earth’s magnetic field, can be turned on for 15 milliseconds just once an hour. Biggest Teatime Electricity Spike Brits love their tea. That’s why the United Kingdom’s National Grid engineers have to manage surges in energy use during popular broadcast events, when many viewers put their kettles on simultaneously. The biggest spike occurred during the 1990 World Cup semifinal. Just after England lost the game-deciding penalty shootout, demand surged by 2,800 megawatts, equivalent to the electricity used by approximately 1.1 million kettles. Strongest Robotic Arm In March, Rise Robotics celebrated the Beltdraulic SuperJammer Arm ’s setting of the Guinness World Record for Strongest Robotic Arm Prototype. A collaboration between Rise and the U.S. Air Force, the arm lifted an astonishing 3,182 kilograms, about the weight of an adult female African elephant. Unlike other heavy-lifting machines, the robot uses no hydraulics, only electric power, and it improves efficiency by generating electricity when it’s lowering a load. Smallest Pacemaker Implanting most pacemakers requires invasive surgeries. But a group of researchers at Northwestern University, in Evanston, Ill., has developed a device that can be implanted through the tip of a syringe. Measuring 3.5 millimeters in its largest dimension and suited for newborns with heart defects, the pacemaker—which is designed for patients who need only temporary pacing—safely dissolves in the body after it has done its job. Fastest Data Transfer Earlier this year, a team from the National Institute of Information and Communications Technology and Sumitomo Electric, in Japan, blasted a record 1.02 million billion bits (petabits) across 1,808 kilometers in one second, or 1.86 exabits per second-kilometer. At that rate, in one second, you could send everything everyone in the world watched on Netflix in the first half of this year from Tokyo to Shanghai 4,000 times. A special 19-core optical fiber made it possible. Fastest EV Charging The Chinese automaker BYD used a new fast-charging system that peaked at 1,002 kilowatts and added 421 kilometers of range to a Han L sedan in under five minutes. That’s about 84 kilometers per minute. Among the key innovations behind the feat: 1,500-volt silicon carbide transistors and lithium iron phosphate batteries with half the internal resistance of their predecessors.",
    "published": "Thu, 02 Oct 2025 14:00:04 +0000",
    "author": "Gwendolyn Rak",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Have We Reached a Space-Junk Tipping Point?",
    "link": "https://spectrum.ieee.org/kessler-syndrome-space-debris",
    "summary": "Low Earth orbit , where most satellites operate, has become a whirlwind of metal shards and dead, tumbling debris. Anyone with hardware or human crew in orbit knows the drill. Orbital collision warnings can be unremitting. Whether the object is a defunct satellite or a stray hunk of glass from a solar panel that shattered long ago, every item circling Earth is also a potential projectile. And nearly all of this junk, traveling at least eight times as fast as a rifle bullet , can be damaging in a collision. SpaceX’s Starlink satellites maneuvered around possible debris impacts 144,404 times over the first half of 2025. That’s a collision warning every couple of minutes, night and day, for six months straight—three times the rate of the previous six months. Looming on the horizon, too, is the threat of orbital junk overwhelming satellites’ ability to dodge disaster. Each collision then creates more fragments, in a runaway cascade that turns low Earth orbit into a hazard zone. This article is part of The Scale Issue . For satellite operators, sudden silences could be the first warning signs. Ground station crews that today coordinate elegant sequences of thruster burns will face more chaotic obstacle courses and bigger debris fields blooming across their display monitors. Communication lines and data traffic may drop from time to time, too, sowing chaos on the ground and menacing flights across the globe. And as the slow catastrophe builds, fuel reserves for satellite constellations will bleed down into the red from so many extensive orbital maneuvers. Spacecraft that’ve run dry today will be the seedbed for tumbling, hypervelocity shrapnel tomorrow. This doomsday scenario is known as the Kessler syndrome, named after the American astrophysicist Donald Kessler , who in 1976 began circulating his first notices at NASA about possible runaway orbital debris. Now, as the magnitude of the space junk problem rapidly scales up, technological responses are ramping up as well. Solutions in the offing include high-resolution orbital tracking, AI-powered constellation management, and an emerging robotic tech called “ active debris removal .” This last item involves lofting a specialized spacecraft into orbit, armed with grippers or other satellite-wrangling tech that can target and grab orbiting stuff. The removal craft then guides the space junk through reentry and the ultimate splashdown of whatever survives reentry into the middle of the ocean. But tech alone may not be enough for the magnitude of the task ahead. The debris problem could simply be growing too fast. International treaties and government regulations may be needed to classify orbits as globally managed resources, like radio spectrum . Because as Kessler himself has pointed out, space is complicated —sometimes frustratingly so . What Is the Kessler Syndrome? In the early days, those frustrations were related to simply getting the space community to realize the problem that lay ahead. Back in the early 1970s, when low Earth orbit was all but pristine, Kessler was a midcareer NASA scientist, having already notched important contributions to the Apollo and Skylab programs. As his colleague, the late NASA administrator Burton Cour-Palais , noted in a 2004 oral history , Kessler “was bringing up this orbital debris thing, and the higher-ups did not want to know about it at all.” Cour-Palais also recalled being told to urge Kessler to “come up with solutions rather than problems.” Fortunately, neither took the overly cautious route. In June 1978, the Journal of Geophysical Research published a paper by Kessler and Cour-Palais in which they argued that a rapidly growing belt of defunct satellites, collision fragments, and other detritus could “be a significant problem during the next century.” It’s a prediction that has come to pass. In April of this year, Kessler and Hugh Lewis , professor of astronautics at the University of Birmingham , in England, presented their latest models , concluding that space junk orbiting between 400 and 1,000 kilometers—where most low Earth satellites operate—is already unstable. And between 520 and 1,000 km, the researchers found, debris concentrations are at or near levels that might sustain runaway growth. A recent internal report shared with IEEE Spectrum , written by analysts at the Menlo Park, Calif.–based LeoLabs , has divided the problem into what it calls “four waves of the Kessler syndrome.” The first three waves, it says, may have already begun. They are: nontrackable stuff like tiny steel fragments and glass splinters colliding with non-operational trackable objects; nontrackable stuff impacting functioning satellites and causing malfunctions; and trackable objects hitting other trackable objects and creating a clouds of fragments. The fourth wave, in which two large pieces of debris incite a chain reaction of other collisions, has yet to occur. In LeoLabs’ observations and models, satellites and operational spacecraft including the International Space Station, and China’s Tiangong space station continue to face manageable levels of collision avoidance maneuvers—for now. “It is assumed these operational satellites will avoid catastrophic collisions with trackable objects,” the report concludes. But according to Luc Piguet , CEO and cofounder of the Lausanne, Switzerland–based startup ClearSpace , challenges for operational satellites are real and mounting. “The Kessler syndrome is a slow, crawling effect—that when it starts accelerating, it’s already too late,” he says. “The Kessler syndrome is happening.” The problem can be further segmented into specific problematic orbits, according to Darren McKnight , senior technical fellow at LeoLabs , which performs high-resolution debris tracking for private clients and government agencies. “There are certain altitudes where we’ve already passed the threshold for the Kessler syndrome,” McKnight says. For instance, at 775 km altitude, as well as at 840 km and 975 km, the collision risk is scaling up rapidly. (See graph, “Low Earth Orbit’s Most High-Risk Places.”) “We will hit a point where particular popular orbits are so risky to operate in that the benefits of operating there are outweighed by the cost and risk,” says Danielle Wood , head of MIT Media Lab ’s Space Enabled Research Group . Why Is the Kessler Syndrome Complicated? According to the European Space Agency , 14.5 million kilograms of man-made stuff circles the planet today. Compare that to 11 million kg two years ago and 8.9 million kg in 2020—a 63 percent increase over the past five years. McKnight says the Kessler problem comes into sharper focus when dividing mass in any given orbit by the volume of space that orbit occupies. The mass density in orbit, also known as the mass per cubic kilometer, provides a clue not only to the chance of orbital collisions but also to those collisions’ consequences. Two small orbiting items colliding won’t create nearly as much new debris as will two big ones. The more densely packed an orbit is, in other words, the more treacherous it is to keep a satellite at that orbit. “Mass per cubic kilometer is debris-generating potential,” McKnight says, which would be a great thing to know with confidence in all the different regions of low Earth orbit. In 2002, Space Shuttle astronauts retrieved these solar panels from the Hubble Space Telescope—revealing how destructive even small projectiles are when traveling at low Earth orbit speeds. ESA However, says Katherine Courtney , chair of the Global Network on Sustainability in Space , knowing where all orbiting stuff is today has become a tall order. “A substantial portion of smaller space junk can only be extrapolated using data collected from returned spacecraft and historical records. The vast majority can’t be tracked from the ground,” Courtney adds. Moreover, says Jonathan McDowell , astrophysicist and space historian at the Harvard & Smithsonian Center for Astrophysics , in Cambridge, Mass., once stuff in orbit goes missing, further complications emerge. Collisions between the missing matter and other debris can completely knock the collisions’ by-products into different orbits. “The operating satellites are in nice circular orbits,” McDowell says, “whereas the collision debris is crossing many orbits and affecting many more.” What’s now needed as the problem grows larger is a complete rethink, says Moriba Jah , professor of aerospace engineering and engineering mechanics at the University of Texas at Austin. “I don’t subscribe to the Kessler syndrome,” Jah says. “It’s not that cascading collisions can’t happen. It’s that the framework oversimplifies the problem and doesn’t give us a way to manage or evolve the system.” Consider instead, Jah says, a parameter he calls “orbital carrying capacity.” “If we start from the end, we can say that carrying capacity is consumed when our ability to make decisions to avert harm no longer work,” he continues. “So to me, that doesn’t necessarily look like you’re bumping into stuff. It also looks like you’re spending fuel moving around stuff so much that you can’t do the things that you wanted to do to begin with.” How to Avoid Satellite Collisions As SpaceX proved 144,404 times from December 2024 through May of this year, the Starlink constellation’s capacity to maneuver its hardware around space junk is impressive. “ Starlink is a brilliant constellation,” McKnight says. “They’re like a granny driving on the highway. They pump their brakes. They avoid everything.” However, Starlink’s own public record also showcases how rapidly the collision hazards in orbit are evolving. The company’s publicly disclosed data reveals a 22-fold increase since 2020 in the amount of ducking and dodging the constellation has needed to perform to avoid collisions with other stuff in orbit. Everyone’s ducking and dodging these days, too. “Collision avoidance is a standard practice now for every operator,” says Tim Flohrer , head of the European Space Agency’s Space Debris Office . “You want to keep your operations making sense, communicating with everybody else,” says Marlon Sorge , technical fellow at the Chantilly, Va.–based Aerospace Corp. , “and not making more of the stuff that you can’t communicate with.” Yet, space junk isn’t the only class of noncommunicative stuff up there. “More than half of the unidentified objects are Chinese satellites,” says Courtney of the Global Network on Sustainability in Space. “So they’re active satellites, but they’re just not registered as identifiable objects.” The tracked debris, the untrackable tiny debris, the bigger things that are also incommunicado—all of it combines to make for an increasingly massive headache. “Every collision-avoidance maneuver is a nuisance,” Holger Krag, head of ESA’s Space Safety office , has said. “Not only because of fuel consumption but also because of the preparation that goes into it. We have to book ground-station passes, which costs money. Sometimes we even have to switch off the acquisition of scientific data. We have to have an expert team available round the clock.” So who or what, then, could possibly keep up with the rapidly scaling nature of the Kessler problem? Artificial intelligence is the almost unanimous answer. Many of the world’s major players in low Earth orbit, including small satellite startups and big national space programs , are currently testing and developing AI constellation-management systems. Machine-learning algorithms are proving increasingly adept at making more accurate collision warnings and performing automated decision-making —as well as sharpening the resolution of small object detection to find smaller orbiting stuff than what non-AI-powered tracking tech can see. Some companies and research teams are also developing AI tools to go beyond just keeping pace with the problem, using AI to optimize fuel usage and maintain ideal satellite configurations for low battery usage and simplified signal traffic as well. However, for all its smarts, AI still can’t make the most difficult orbital hazards go away. That’s why some companies are approaching the Kessler problem as one of disposing, rather than dodging. A number of startups are actively pursuing ways to extract the most dangerous orbital objects— defunct rocket stages , dead satellites, space collision fragment clouds , and space-race relics . “The technology available to remove debris today is really toward larger pieces of debris,” says Andrew Faiola , commercial director of the Tokyo-based company Astroscale . “We’re just maturing that capability to be able to effectively, safely, and securely remove large pieces of debris.” Astroscale and ClearSpace aim to launch spacecraft over the next few years that will each target an aging satellite (a Eutelsat OneWeb satellite and ESA’s PROBA-1, respectively) for a prototype removal mission. The European radar imaging satellite Sentinel-1A caught a millimeter-sized particle impacting one of its solar panels, leaving behind a 40-centimer wide zone of damage. ESA “You need to do controlled entry,” ClearSpace’s Piguet says. “This means you need to push this satellite into Point Nemo over the South Pacific, where there’s no airlines, ground traffic, and no inhabited island.” Ideally, then, between smart constellation management, active collision avoidance, and active cleanup, low Earth orbit will become something closer to a regulated and moderated space—much like airspace around major metro areas today. “It’s much the same as air-traffic control,” Faiola says. “As the technology gets better, you start to see aircraft being stacked more closely together. You have the same amount of real estate, but you can put more objects in there more safely when you have better visibility and situational awareness of where everything is. It’s the same in space.” What Are the Solutions to the Kessler Problem? Space tech and space tech alone may one day resolve the Kessler syndrome. But as a complement to the technological innovation, international space agreements and law are also being reconsidered, because much of the existing space law standards were agreed on decades ago, during an entirely different era in low Earth orbit. For instance, between the Outer Space Treaty of 1967 and the 1972 Space Liability Convention, even an untraceable fragment of metal in space is effectively owned by the nation that launched it. This arguably means that that nation may need to give permission for anyone else to remove the fragment from orbit. “There’s no national borders up there,” says Faiola. “But every object that is cataloged is also owned by someone, a state. And you’re not allowed to touch someone else’s stuff without their permission.” In August, Japan announced it would be developing its own legal frameworks for removing space junk from orbit. And this November, in Vienna, the United Nations Office for Outer Space Affairs will be hosting a space law conference to tackle these issues as well. International agreements need reconsidering in other ways, too. Some space experts Spectrum spoke with argue for additional regulations to prevent orbits from further clogging up. “There will have to be internationally coordinated agreements on who gets what orbit and how many satellites you can have in that orbit,” says Smithsonian’s McDowell. Courtney envisions something like a worldwide space command network. “We need to be designing solutions that allow the growth to continue,” she says. “What we need is a global space traffic control solution like we have for air traffic today.” Jah of the University of Texas at Austin argues for ultimately bringing orbital space closer to its original state of being, as he puts it, “a viable commons.” When a new player—whether a company or a national space agency—wants to put something into a given orbit, he says, that new orbiting asset should be added to a master spreadsheet somewhere. “If another country wants to be able to be in that orbit, there should be an equitable way to share the carrying capacity of that orbit,” he says. Rockets, satellites, and launch systems today still follow the space race–era legacy designs that treat orbital space like an infinite junkyard, he adds. “Right now, every single object that we launch into orbit is the equivalent of a single-use plastic,” Jah says. “We need to invest in reusable and recyclable satellites.” Even if the Kessler problem on the home planet can be solved, says Courtney, the same thing could happen on other planets and moons. “We’re very worried about low Earth orbit, but [there’s also] all the commercial activity and all of the great-power competition for landing things on the moon and Mars,” she says. “We have no space-traffic coordination solutions for cislunar space , and yet that’s the race that’s just starting now,” she says. “We’re expanding outward into the solar system, and we’re just taking these problems with us.”",
    "published": "Tue, 30 Sep 2025 13:00:03 +0000",
    "author": "Margo Anderson",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "The High-Speed Plan for Interstellar Travel",
    "link": "https://spectrum.ieee.org/high-speed-interstellar-travel",
    "summary": "To the naked eye, the stars are diamond flecks scattered across the inner surface of a celestial sphere. Telescopes have brought depth to our vision, mapping the true distances to cosmic objects. But the universe they reveal appears utterly beyond the human scale of space and time. Even the closest stars seem infinitely remote, and reaching them a thing of science fiction, save for a few dead and dying probes drifting outward for eternity . This article is part of The Scale Issue . Now, though, a cadre of researchers are working to make interstellar travel a reality, at least to our nearest neighbors. They are coalescing around an approach that could lead to closeup images of a star and an exoplanet just 25 years after mission launch. Most of each 4-meter probe will be a disc of aerographene or similar material, just a few micrometers in thickness, with optical sensors and transmitters on one side and a reflective surface on the other that the launch laser will aim at. The rim of the probe will be a 2-centimeter-thick band. The trailing edge will have apertures for interprobe laser communications. Power and processing electronics will form a ring inside the rim. The swarm’s optical transmitters will pulse in unison to send data to Earth at a rate of around 1 kilobit per second. Chris Philpot The first generation of theoretical starship designs had featured massive vehicles propelled by fission or fusion drives. Top speed was estimated at about 10 percent of the speed of light, or 0.1 c . This meant that a flyby mission to the closest star system, Proxima Centauri , would take over 42 years to reach its target. In contrast, the new generation of starship designs are tiny, and they have no drives at all. The spacecraft have a mass of a few grams each. They’ll be accelerated out of our solar system by ground- or space-based lasers, traveling at an estimated 0.2 c . A 100-gigawatt laser beam made by combining many smaller lasers will propel hundreds to thousands of tiny probes. Pushing against interstellar magnetic fields, the probes will turn edge on to minimize radiation and impact damage. By adjusting the launch laser to accelerate later probes to higher speeds than earlier ones, the string of probes will coalesce into a swarm by the time of arrival. Chris Philpot One version of this small-and-fast approach calls for sending a swarm of these puny flyers to the Proxima Centauri b exoplanet . Data would be returned by having the swarm emit light pulses in synchrony, detectable by telescopes on Earth. Put forward by a team led by Thomas Marshall Eubanks at Space Initiatives , this mission was selected for a 2024 phase one study by NASA’s Innovative Advanced Concepts program. It didn’t make the list for a phase two study this year, but Eubanks plans to retry in 2026. With a swarm, “we could do gigapixel imaging of the planet,” says Eubanks. “That’s at a level where if it was a planet like Earth, we’d be able to see things like coral reefs and airports.”",
    "published": "Mon, 29 Sep 2025 13:00:03 +0000",
    "author": "Stephen Cass",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Molten Regolith Electrolysis Could Make Moon Dust Useful",
    "link": "https://spectrum.ieee.org/blue-origin-molten-regolith-electrolysis",
    "summary": "Blue Origin is committed to making a permanent human presence in space a reality. To this end, the company has developed the New Shepard and New Glenn rockets to send payloads to orbit, and it aims to create superheavy launch vehicles to reach the moon (the New Armstrong rocket ) and beyond. Another focus has been on developing systems that will enable in situ resource utilization (ISRU) in extraterrestrial environments, which is essential for making space sustainable. This includes the company’s Blue Alchemist ISRU system, which recently completed its critical design review . For missions operating beyond low Earth orbit (LEO), opportunities for resupply missions will be few and far between. This is especially true where Mars is concerned because it takes six to nine months to make a one-way transit using conventional propulsion. Ensuring sustainability requires that missions be as self-sufficient as possible, which means relying on local resources to provide basic necessities. This is the purpose of Blue Alchemist, which is designed to transform lunar and Martian regolith into solar power systems, breathable oxygen, propellant, metals, and construction materials. How Molten Regolith Electrolysis Reactors Work Blue Alchemist is an end-to-end scalable system that relies on a molten regolith electrolysis (MRE) reactor. This reactor uses electrical current to separate oxygen from metals (such as iron, aluminum, and silicon) without water, toxic chemicals, or carbon emissions. The silicon can then be refined to produce radiation-resistant solar cells, while the elemental oxygen can be converted into oxygen gas, fuel cells, or liquid oxygen (LOX) propellant. The metals and ceramics can also be used as building materials for habitat structures, and to create semiconductors for electric systems. This process reduces reliance on supplies launched from Earth. It also has potential applications here on Earth, where carbon-neutral manufacturing can ensure sustainable development. In a company press release , Pat Remias, the vice president of Blue Origin’s Advanced Concepts and Enterprise Engineering said, “Blue Alchemist changes everything about how we approach space. It is the foundation for a sustainable robotic and human presence across the solar system. Each kilogram of oxygen we make on the lunar surface is one less that we have to launch from Earth, making it a giant leap toward permanent settlements as well as critical resources for transportation to the Moon, Mars, and beyond.” The company also indicates that it’s on track to scale the system to make lunar landings up to 60 percent cheaper and reduce fuel-cell and battery masses by up to 70 percent by enabling lunar refueling services. The system is being developed at Blue Origin’s Space Resources Center of Excellence, a 1.2-hectare facility with 5,575 square meters of lab space that is staffed by a team of 65 interdisciplinary experts. The technology is being developed with support from a NASA Tipping Point award, which was granted through the agency’s Game Changing Development program. With the critical design review completed, Blue Alchemist will move into the next phase of development, with an autonomous demonstration in a simulated lunar environment scheduled for 2026.",
    "published": "Sun, 21 Sep 2025 13:00:02 +0000",
    "author": "Matthew Williams",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Face to Face With the Scale of the Cosmos",
    "link": "https://spectrum.ieee.org/scale-of-light-pollution",
    "summary": "For most of us, to see a truly starry night isn’t easy. I have been writing about dark skies and light pollution for almost 20 years. And I have seen some breathtaking skies—southern Morocco at the edge of the desert so plush with stars it still seems like a dream, the Racetrack in Death Valley with stars rising in the east and dropping off the edge of the world in the west. At other times when I’ve gone to see the sky, I found too much humidity in the air, or too many clouds, or the obscuring smoke from a burning world. In the Canary Islands off the coast of Africa, the famous skies were veiled by a freak sandstorm from the Sahara. But the main reason that city dwellers can no longer see a starry night is simply all the artificial light we waste into the sky. There’s even a scale for this— the Bortle scale , named after an amateur astronomer in New York state who grew tired of younger astronomers inviting him out to supposedly dark viewing sites only to find those sites not so dark. John E. Bortle’s scale goes from 1 to 9, from darkest (no artificial light on the ground or in the sky) to brightest (inner cities). This article is part of The Scale Issue . Most people will live the majority of their lives in level 5 or above, without the experience of a naturally dark sky. And ever-increasing numbers of people live in areas with levels of 7, 8, and 9. They may think of these bright levels as normal, what “darkness” is supposed to be. Over the past decade, the shift from electric light to electronic lighting—in the form of light-emitting diodes—has made the problem worse. LEDs are generally brighter than traditional electric lights, often emit more blue-white light, and are more energy efficient. But too often this efficiency means we use more of them, creating more artificial light at night. A recent study estimated the growth of light pollution worldwide from 2011 to 2022 at 10 percent per year, a doubling roughly every eight years. RELATED: Lighting Up Yosemite Despite these conditions, Bortle Scale 1 areas still exist, just not where most of us live. When I asked a friend at the U.S. National Park Service—whose Natural Sounds and Night Skies Division measures levels of darkness throughout the National Park System—where to find a dark sky, he hesitated to name anywhere in the lower 48 states. The Outback in Australia would be a good place, he said, or Chile. And so I’m here, traveling with Pedro Sanhueza , an environmentalist and longtime dark-skies advocate who has devoted the past two decades to protecting his country’s dark skies. As we drive, Pedro tells me that in the 25 years he’s lived in La Serena, the population has almost doubled, and the lights have grown, too. The sleepy city where the Milky Way used to spend the night in everyone’s backyard is getting so big that astronomers in the Elqui Valley are beginning to worry, even as billions of dollars go into building new observatories like the Vera C. Rubin Observatory at Cerro Pachón . This image illustrates the 9-point Bortle scale, which quantifies the impact of light pollution on the darkness of a night sky at a particular location. Star visibility increases dramatically from left (urban areas with heavy light pollution) to right (excellent dark-sky conditions). P. Horálek and M. Wallner/ESO Still, tonight seems promising. The sky is mostly clear, with no storms in the forecast. Northern Chile boasts more than 300 nights of clear skies a year, and I’m grateful to see only two clouds overhead. As we leave the main highway and begin to wind through the mountains, I’m hopeful we’ll find the kind of darkness that so many places have lost. For 20 minutes, we creep around curves and navigate road repairs, ours the only car. Bats flicker above the gravel, a statuelike owl perches on a bare roadside limb. The mountains disappear as we climb, as does the road behind us, and we see only the world our headlights carve from the dark. Part of me wishes we could turn the headlights off too and ride beneath a sky filling with stars. But the rest of me knows this is a terrible idea. When we finally arrive at our destination, a small private observatory with two large telescopes, I am stunned by the brightness of the sky. In a line stretched back toward La Serena, three planets—Venus, Saturn, and Jupiter—glow like passenger jets in low approach for landing. Though I’ve never seen it before, I recognize the Southern Cross immediately, its shape shining just above a mountain’s ridge to the south. The constellation Orion catches my eye, its iconic three-star belt so familiar—until I realize it’s upside down from how I see it back home. Nearby glows the red eye of Taurus the Bull, the star of Aldebaran, only 65 light-years away. If you kept to highway speeds, it would take 800 million years to get there by car. When you get to a place that is still naturally dark and see a night sky like the one our ancestors knew, what you feel is recognition…reconnecting with something you maybe didn’t even know you’d become disconnected from. Someone points out the Magellanic Clouds, galaxies visible only from the Southern Hemisphere. They’re clouds of stars. “Yes, tens of thousands of millions,” says Pedro. The Large Magellanic Cloud alone is estimated to have 30 billion stars. And distances? The Large Magellanic Cloud lies about 160,000 light-years from Earth, the Small Magellanic Cloud about 200,000 light-years away. We use light-years—the distance that light can travel in a year—to talk about distance in space. In this case, that’s 160,000 or 200,000 multiplied by 9.5 trillion kilometers. And these are among the closest galaxies to our own Milky Way. At this, my brain begins to spin—how are we to grasp such numbers? “Even astronomers can’t truly grasp these scales,” the popular astronomer Phil Plait writes . “We work with them, and we can do math and physics with them, but our ape brains still struggle to comprehend even the distance to the moon—and the universe is two million trillion times bigger than that.” When I hear such numbers, I can’t do much but nod. And that’s okay. To stare into the sky unable to grasp what you see, the numbers and distances bending your brain beyond its reach, is part of what makes this experience so valuable—a reminder that we are not in control of everything, our anxieties are small, our lives brief, and all the more reason to savor what we see, now and here. Rather than feeling overwhelmed, seeing these distant objects causes me to feel connected to something incomprehensibly larger than me. And from this a wonder at being alive, and the welcome thought that I get to exist in a universe where this exists, too. As we stand watching, I tell Pedro I’ve been thinking about the idea of “scale,” how we make sense of the distances and numbers in the night sky, and even the question of what good it does to look further and further into space. The lights of the town of Andacollo, Chile, were visibly reduced [top] during the Blackout for Our Skies event, as seen from Cerro Tololo Inter-American Observatory. After the event [above], the level of light pollution returned to near-normal levels. Photos: D. Munizaga/CTIO/NSF/AURA/NOIRLab Earlier that day, I’d mentioned the idea of scale to Manuel Paredes , my guide at the Cerro Tololo observatory . “On the hierarchy—or scale—of needs, you have to have food and water and shelter, but life isn’t only those basic needs,” he said. “There’s also a more philosophical or spiritual or soulful desire to look out there. And with looking to the night sky, trying to understand specific things like, What is a star? What’s the sun? What is a supernova? Through these small questions, we are trying to answer these big questions, trying to get some satisfaction. I think that there’s a need to understand our position in the universe very deep in our brains and in our hearts.” When you get to a place that is still naturally dark and see a night sky like the one our ancestors knew, what you feel is recognition. It’s the feeling of reconnecting with something you maybe didn’t even know you’d become disconnected from. It’s the way the voice of an old lover or friend rings through your body when you hear it again after some years. It’s related to the way that images of the night sky—as stunning as they may be—aren’t the same experience as seeing the sky for yourself. This deep connection makes me recoil from the next thing I see: a long train of satellites rising from the horizon toward orbit. Before this, we had watched the slow scrawl of individual satellites, slight incisions on the black fabric overhead. I admit that I still feel some thrill at the sight, a thrill left over from my first sight of satellites near a northern lake decades ago. But when I was a child, there were only a few hundred of these artificial lights in the sky, and their uniqueness added to the wonder of the stars. Since then, the scale of their growth has been overwhelming. The number has already swelled to around 12,000 , and predictions are for 100,000 or more in a decade . Astronomers and dark-sky advocates have been sounding the alarm, warning that our night skies, especially after dusk and before dawn, when most people look to the sky, will become crowded with as many “moving stars” as static stars. We are cutting ourselves off from so much that might inspire us, from scales of time and distance and numbers that offer a chance to feel awe and wonder at this world where we live. Pedro doesn’t hold back about the satellites. “I hate them so much,” he says. “This is industry without control.” Much of his present work centers on helping Chile’s mines comply with national laws limiting artificial light, a vital task in a country where mining produces more than half of the exports, including nearly a quarter of the world’s copper and 30 percent of its lithium. At night, the mines shine like small cities, Pedro explains, but he’s been able to make progress, a result he attributes to the large number of engineers the mines employ. “The mines have many well-prepared people,” he says, “and so if you explain in the proper way the technological solution, they will make the change.” He and the rest of the astronomical community currently face an immense challenge from a proposed new mine in the Atacama Desert, uncomfortably close to some of the most important observatories. Pedro calls the INNA project —a proposed 7,400-acre (3,000-hectare) green-hydrogen production facility—“very bad for Chile,” with the light pollution, airborne dust, and atmospheric disturbance “underestimated systematically, the projections very limited and superficial.” Nonetheless, he says, the mine may be approved by a national government hesitant to turn down the tax revenue the mine owners insist it will bring. This seems to be the perpetual dilemma when it comes to light pollution: the promise of economic gain for some at the expense of a dark-sky heritage that belongs to everyone. I think about my 6-year-old daughter, and the future sky any child anywhere will know. The inexorable spread of artificial light, the explosion in the number of satellites—on nearly any scale of time, these changes have happened in an instant. Beneath the Chilean night sky, the suddenness seems even more startling given the scale of astronomical time, where the faintest light we can see left its source about 13 billion years ago. The erasure of this experience robs us all, and especially future generations who will never see a night like this. That the costs of such loss are intangible makes them not a bit less important. We are cutting ourselves off from so much that might inspire us, from scales of time and distance and numbers that offer a chance to feel awe and wonder at this world where we live. Even here, the lights from La Serena are rising on the western horizon to erase what low stars we might have seen. “We are losing 30 degrees above the horizon,” Pedro says. “We are fighting almost every day a new offender.” Here in Chile, a long strip of country where astronomers come to find the driest, clearest darkness in the world, the risk is that such darkness—and all the celestial beauty it brings—will slip away. There’s no reason it has to, and for now there is still time. But if we do nothing or not enough, we will lose nights like these here as we have lost them in so many other places. And the evening drive from La Serena will bring us only to somewhere darker than the city we left, rather than back in time to a world every human once knew. This article appears in the October 2025 print issue.",
    "published": "Tue, 16 Sep 2025 15:04:13 +0000",
    "author": "Paul Bogard",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "A Fusion-Reactor-Inspired Thruster Could Deorbit Space Junk",
    "link": "https://spectrum.ieee.org/kessler-syndrome-plasma-thruster-deorbit",
    "summary": "There are plenty of labs working on solutions to Kessler Syndrome , where there’s so much debris in low Earth orbit that rockets are no longer capable of reaching it without being hit with hypersonic parts of defunct equipment. While we haven’t yet gotten to the point where we’ve lost access to space, there will come a day where that will happen if we don’t do something about it. A new paper from Kazunori Takahashi of Tohoku University, in Japan, looks at a novel solution that uses a type of magnetic field typically seen in fusion reactors to decelerate debris using a plasma beam, while balancing itself with an equal and opposite thrust on the other side. Researchers have been working on two main categories of systems for the type of deorbiting work that might save us from Kessler Syndrome—contact and noncontact. Contact systems physically make contact with the debris, such as by a net or a grappling hook, and slow the debris to a point where it can deorbit safely. This method faces the challenge that most debris is rotating uncontrollably, and could potentially destroy the satellite trying to make contact with it if it moves unexpectedly—adding to the problem rather than solving it. Therefore, noncontact forms are in the ascendancy, as they allow a system designed to deorbit another satellite to stay a few meters away while still affecting its speed. Typically they use systems like lasers, ion beams, or in the case of Takahashi’s invention, plasma beams, to slow their intended target to a point where it can safely deorbit. The problem with plasma-beam-based deorbiting systems is Newton’s third law —as the plasma is being directed toward the target, it is pushing the operational system away from the defunct one, essentially acting as a small plasma thruster. As the distance between the two increases, the slowing effect of the plasma decreases. To solve this problem, Takahashi and his fellow researchers presented a bidirectional thruster in a paper in 2018 that counteracted the pushing force of the plasma used to slow the target with an equal force in the opposite direction, allowing it to maintain its position. Advancements in Plasma Thruster Technology However, in that original paper, the thrust was too weak to effectively deorbit some of the larger potential targets for such a mission. So Takahashi set about improving the design by implementing a “cusp-type” magnetic field. These are typically used in fusion reactors to ensure the plasma doesn’t interact with the wall of the magnetic chamber. The cusp of a magnetic field is a point at which two opposing magnetic fields meet and cancel out, creating a quick change in direction for the forces they apply. Ideally, this results in a stronger plasma beam. That is what happened when Takahashi set up an experiment to test the new cusp system with the previous “straight-field” system that had proved too weak. He saw a 20 percent improvement in the force that the plasma thruster exerted on the target, resulting in a 17.1-millinewton push at the same power level. When he bumped up the power level to 5 kilowatts (compared to the 3 kW in the original test), it showed an improved deceleration of about 25 mN, which is approaching the level of 30 mN expected to be needed to decelerate a 1-tonne piece of debris in 100 days. It also had the added benefit of using argon as fuel, which is cheaper compared with the xenon typically used in plasma thrusters. Even with this success, there’s still a lot of work to do before this becomes a fully fleshed-out system. The experiment was run in a vacuum chamber, with the plasma thruster only 30 centimeters away from the target, compared with the distance in meters that would be required in a real orbital environment. In fact, the debris target will also move relative to the deorbiting system as it slows down, so it will have to strike a balance of maintaining distance from a slowing object as well as continuing to fire the decelerating beam at it. And finally, there is the disadvantage of it using literally twice as much fuel as other solutions that don’t require thrusters operating in opposite directions—while fuel might not be much of a concern for plasma thrusters, operating one over 100 days is sure to consume a lot of it. With all that being said, any new solution to this potentially catastrophic problem is welcome, and Takahashi will likely continue work on developing this prototype. Someday soon you might even be able to watch a dual-thrust plasma engine blasting away at a large piece of space junk.",
    "published": "Sun, 14 Sep 2025 13:00:02 +0000",
    "author": "Andy Tomaswick",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Is Pivotal Helix eVTOL a First Responder in the Sky?",
    "link": "https://spectrum.ieee.org/pivotal-helix",
    "summary": "When Sinatra sang “Come Fly with Me,” he couldn’t have imagined anything like the Pivotal Helix . The Helix is a single-seat, electric vertical take-off and landing aircraft, or eVTOL . And where dozens of eVTOL startups have failed to take flight, Pivotal says it’s on track to deliver its first customer model in the first quarter of 2026—almost certainly the first eVTOL maker to reach this milestone. Helix predecessor, Blackfly, has taken more than 1,000 piloted flights, and its maker has trained more than 50 pilots to fly this unique aircraft with eight electrified rotors whirling on a pair of detachable wings. Those trainees now include a new set of customers: Fire departments and paramedics, who see intriguing possibilities in emergency responses, including landing in places helicopters can’t reach, or in which the machines would be too disruptive. For example, the Helix recently flew a trio of safety demonstrations in California with fire departments in San Bernardino , Southern Marin, and Cosumnes. One demo flight ended with a discreet touchdown in a residential cul-de-sac in Thousand Oaks. “You couldn’t dream of doing that in a helicopter,” says Ken Karklin , chief executive of the Palo Alto, Calif.–based Pivotal, due to the disruptive noise and rotor wash of a fossil-fueled chopper. Testing by NASA engineers, Karklin says, showed the Helix sends only about 70 decibels of sound to the ground from a 150-foot flight altitude. “When the Helix passes overhead at 200 feet [61 meters], you may not hear it at all,” he says. The San Bernardino County Fire Department says the Helix highlights the potential of eVTOLs to dramatically reduce response times, and overcome barriers such as traffic congestion, long rural distances or hostile natural terrain. “This aircraft represents a powerful new tool in time-sensitive trauma care,” the department said in a media release. Two Weeks of Training and Then Up, Up, and Away Skepticism continues to swirl around eVTOLs , prompted in part by their short electric range and limited capacity for people or cargo. Many eVTOL proponents pitch a future of short-hop air taxis, essentially Ubers of the sky. Those backers tend to downplay massive hurdles in securing certification from the Federal Aviation Administration , which has struggled to come up with safety, airspace, and other guidelines for eVTOLs. Another challenge will be finding and training qualified pilots who will undoubtedly demand more pay than an Uber jockey. The Helix’s secret sauce, according to Karklin, is accessibility—in technical, regulatory and financial terms. “We’re flying prolifically today, and the [eVTOL] companies chasing FAA certification aren’t close,” he says. Built largely from carbon fiber, and weighing just under 158 kilograms, the Helix conforms with the FAA’s rules for ultralight aircraft. That means the Helix does not need formal certification, and buyers don’t need a pilot’s license to take to the skies. IEEE Spectrum’s Glenn Zorpette flew the Helix’s predecessor, the BlackFly , in California in 2022, and was blown away by the experience. (The company was then under a previous corporate identity as Opener, founded by BlackFly inventor Marcus Leng). Pivotal conducted a demonstration of its Helix eVTOL for the San Bernardino County (Calif.) Fire Protection District in June 2025. Bobby Montalvo/San Bernardino County Fire Protection District After a two-week training course that includes flight simulations, buyers graduate to unsupervised solo flights. Then they’re free, under the FAA’s ultralight rules, to fly during daylight, as long as they avoid congested areas, busy airports, or restricted airspace. The company plans to back sales with recurrent training to keep pilots up to speed. Unlike most eVTOLs , the Helix doesn’t move its rotors to adjust its attitude. Instead, the entire airframe tilts upward for vertical takeoff and landing, and shifts to horizontal to cruise up to 32 kilometers (20 miles) at a software-limited 103-kilometer-per-hour (64 mile-per-hour) top speed. The design helps make the Helix less daunting for rookie pilots; a single by-wire joystick controls every aspect of flight. Karklin says pilots can spend their time hovering, cruising, or banking, rather than constantly monitoring instruments that include battery state of charge and navigation. “They can keep eyeballs outside the canopy on a beautiful landscape,” Karklin says. Compared with the third-generation BlackFly aircraft, the Helix has about 97 percent new components , including the cylindrical cells in its 9-kilowatt-hour battery. The Helix can fly at higher altitudes and in thinner, warmer air without its motors overheating at high rpm’s: Picture Denver or Lake Tahoe, where even ground takeoffs begin at a mile or more above sea level. I mprovements include power and propulsion gains, automotive-grade batteries and electronics, and a redesigned monocoque, canopy, and flight instruments. A smartphone app records data on every flight, and manages charging and service. As with the BlackFly, the Helix’s wings can be quickly detached and stored on a car trailer, along with the pod-shaped fuselage, for convenient transport. While the Blackfly can cover 32 km in ideal conditions, Karklin’s personal best is 28 km. The owner of an earthbound EV might be tempted to push it, but the stakes are higher in the sky: Pivotal’s training instructs pilots to find a safe landing spot with no less than 20 percent of battery charge remaining. Proprietary chemistry ensures cells can deliver 100 percent of rated discharge all the way to that 20 percent barrier, ensuring no loss of thrust as the battery is depleted. Pivotal believes its Helix eVTOL is a natural transportation mode for emergency responders who need to get to residential addresses quickly and land without the noise and rotor wash of a conventional helicopter. Pivotal A Hybrid eVTOL is in the Works for Potential Military Customers Triple redundancy in the computer flight controllers helps ensure safety. If all else fails, the Helix’s pilot-operated parachute can slow its descent to about five meters per second. That’s swift enough to destroy the carbon-fiber fuselage in an emergency landing, but gentle enough to let the pilot unlatch the canopy and walk away injury-free in 97 percent of test scenarios. And t he Helix can now carry 100 kg (220 pounds) of payload between its pilot and assorted gear, a 10 percent gain over the BlackFly. “We’re now up to carrying the 80th percentile American male in weight, up from 51 percent,” Karklin says with a smile. For beefier first responders, that still doesn’t seem to leave much room for gear. Karklin notes an EMT’s “go bag” of medical supplies can weigh up to 25 kg, in part because they’ve never been created with mass in mind. Working with paramedics, the company has created an optimized medical kit that weights as little as 7 kg, including a small defibrillator. And if the Helix can’t haul as much equipment as an ambulance, it can get a first responder on the ground quicker to save or stabilize a patient, or to establish communications and survey the scene for rescuers to come. The vehicle trailer for the Pivotal Helix eVTOL is part of a package that adds $50,000 to the cost of the basic aircraft. Pivotal The company is taking deposits for a Helix with a base cost of US $190,000 but reaching roughly $260,000 with options such as a travel trailer, a Global System for Mobile Communications (GSM) radio, or an Automatic Dependent Surveillance-Broadast (ADS-B) radio system. Karklin acknowledges the limitations of today’s lithium-ion batteries to get aircraft off the ground and keep them flying over meaningful distances. “There’s nothing that beats the energy density of diesel fuel, with Jet-A right behind,” he says, referring to the kerosene-based aviation fuel. So Pivotal is also developing a new BlackFly with a gas-electric hybrid power train, strictly for demonstration purposes for now, that might range up to 322 km (200 miles), for potential defense customers . Yet Karklin is bullish on the potential for simple, plug-in rechargeable eVTOLs. The company’s new development program, he discloses, is training EMTs from two fire departments to fly its Helixes, and seeking other customers in the first-responder space. “Electric aviation has a rich future ahead of it,” he says. This article was updated on 12 September 2025 to correct a noise figure and better distinguish between Blackfly and Helix stats.",
    "published": "Thu, 11 Sep 2025 12:30:03 +0000",
    "author": "Lawrence Ulrich",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "White Hat Hackers Reveal Vulnerabilities in Software Used by NASA",
    "link": "https://spectrum.ieee.org/satellite-hacking-cybersecurity-nasa-visionspace",
    "summary": "Open-source software used by space agencies and companies to control satellites contained vulnerabilities that could have allowed hackers to hijack those satellites, according to a duo of white hat hackers. The hackers revealed the vulnerabilities, which have now been patched, at the Black Hat USA and DEF CON conferences held in August in Las Vegas. The hackers put to the test NASA’s core Flight System , deployed on onboard computers of a wide range of space missions including the US $10 billion James Webb Space Telescope and Intuitive Machine’s Odysseus lunar lander , which touched down on the moon in February. The security researchers also examined the Yamcs mission control system developed by European company Space Applications Services , which is used by satellite controllers on the ground. Space Systems Cybersecurity Challenges The security researchers— Andrzej Olchawa and Milenko Starcik , from space cybersecurity company VisionSpace—described the vulnerabilities as “trivial,” “low-hanging fruit,” and “easy to exploit” by anyone able to understand the open-source code. The vulnerabilities found in this study have since been fixed, but the researchers warn that the cybersecurity of space systems has long been overlooked and caution that closed-source software systems, which are not accessible to independent researchers to test and examine, may be equally easy to breach. Olchawa, who in the past worked at the European Space Agency’s (ESA) space operations center in Germany, and Starcik, a space systems security specialist, spoke to IEEE Spectrum about their work and the lessons the space sector can learn from it. What prompted you to probe the security of these systems? Milenko Starcik: When we started looking into the security of space systems, we realized that there had not been that much done about the security of the ground segment and the space segment. Most of the security research that has been done in the past focused on the user segment, which is fair because that’s also where we saw attacks in the past, like the ViaSat attack in 2022 , which took down ViaSat’s satellite broadband modems on the eve of Russia’s invasion of Ukraine. Because we come from the space industry, we knew that there are many potential vulnerabilities in the ground segment and space segment, which, fortunately, haven’t been exploited yet. We wanted to see how ready these systems are for a possible attack. Can you tell me what you did? Did you hack an actual satellite? Andrzej Olchawa: No, this was just a simulation. We chose open-source software that is available for free to anyone. Anyone can look at the source code, and anyone can deploy it into their own environment and use it, for example, to control a university CubeSat . So, we looked at the code with the intention of breaking it and taking over the system, and within an hour or two, we were able to find those vulnerabilities. We looked at two types of open-source software—the core Flight System by NASA, which is a system running on satellites, and the Yamcs system, which is used by ground controllers to command satellites. We found staggering vulnerabilities in both, which would allow somebody like a state-sponsored hacker to take complete control of a satellite. Starcik: We found 37 separate vulnerabilities. In the demonstration at Black Hat USA, we showed that we would be able to send a command to a spacecraft and make it fire a thruster and change its orbit. Where you surprised? Olchawa: Yes and no. I was definitely surprised that we were able to find such low-hanging fruit in space systems that come from NASA or commercial companies that collaborate with big space agencies. But I have been aware of the fact that the whole space industry has only just recently started waking up to the problem of cybersecurity. When you talk to people, many of them think that they don’t need to bother with cybersecurity, perhaps because they are researcher organizations and they don’t see why anybody would bother hacking them. We find these views quite funny, but that’s what you hear. Starcik: Part of the problem is that we see this move from private networks, leased lines, and closed systems toward cloud, VPN connections, and operators dialing in and performing their duties from home. Everything is getting more connected, and therefore more difficult to shield. These spacecraft control applications really are just Web-based interfaces that you use to control your spacecraft. And once you have that access, even if you don’t have full permissions, we found many ways to bypass authentication or take over the entire system, issue our own commands, delete files, and send commands to the spacecraft. So if I were an adversary, what would I do to exploit these vulnerabilities? Olchawa: With the NASA core Flight System, you would need to have access to a ground station and be somewhere where the frequencies used to control spacecraft are not protected. If I tried to do that in Germany, I would probably have police at my doorstep within an hour. But if you were a state-funded hacker somewhere in Russia, China, or North Korea, it would surely be possible for you to do that. Once you have that ground station access, you can uplink commands to the spacecraft and have it do whatever you want. With the Yamcs mission control system, it’s much easier, and you can do it from anywhere. You would conduct a phishing campaign, and once you succeeded in making one of the legitimate users click a link, you would upload a configuration file into the mission control system, containing a malicious code. These are called XSS vulnerabilities , and upon triggering, the adversaries can perform any action using any available functionality of the system. For example, send an arbitrary command to the spacecraft. At Black Hat, we demonstrated how that would work in practice by sending a command to a simulator to trigger an orbit transition maneuver by firing thrusters. I think this is the most realistic scenario the adversaries could use. I understand that before you published those findings, you had to alert the companies that develop those systems so they could patch the vulnerabilities before you release the information. Does it mean that it’s all good and safe now, and there is no reason to worry anymore? Starcik: We looked into open-source software because that’s accessible. We can run the source code on our computers, and we can research it. But the vast majority of software used in the space industry is closed-source. That means no one can look into that. With the open-source software, we test it, we find vulnerabilities, we report them, and hopefully, now it’s a bit more secure. The rest, we know nothing about. Olchawa: We have experience with the European space industry, and we know how insecure many of these systems are, although we cannot publish anything about them. From what I have seen, security tends to be the last bullet point in requirements for any mission. I think it is slowly changing. We see more requests for penetration testing and other kinds of security testing, but I think it’s still very early days.",
    "published": "Wed, 10 Sep 2025 15:18:19 +0000",
    "author": "Tereza Pultarova",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "If We Want Bigger Wind Turbines, We’re Gonna Need Bigger Airplanes",
    "link": "https://spectrum.ieee.org/wind-turbine-blade-transport-plane",
    "summary": "The world’s largest airplane, when it’s built, will stretch more than a football field from tip to tail. Sixty percent longer than the biggest existing aircraft, with 12 times as much cargo space as a 747, the behemoth will look like an oil tanker that’s sprouted wings—aeronautical engineering at a preposterous scale. Called WindRunner, and expected by 2030, it’ll haul just one thing: massive wind-turbine blades. In most parts of the world, onshore wind-turbine blades can be built to a length of 70 meters, max. This size constraint comes not from the limits of blade engineering or physics; it’s transportation. Any larger and the blades couldn’t be moved over land, since they wouldn’t fit through tunnels or overpasses, or be able to accommodate some of the sharper curves of roads and rails. This article is part of The Scale Issue . So the WindRunner’s developer, Radia of Boulder, Colo., has staked its business model on the idea that the only way to get extralarge blades to wind farms is to fly them there. “The companies in the industry…know how to make turbines that are the size of the Eiffel Tower with blades that are longer than a football field,” says Mark Lundstrom , Radia’s founder and CEO. “But they’re just frustrated that they can’t deploy those machines [on land].” Radia’s plane will be able to hold two 95-meter blades or one 105-meter blade, and land on makeshift dirt runways adjacent to wind farms. This may sound audacious—an act of hubris undertaken for its own sake. But Radia’s supporters argue that WindRunner is simply the right tool for the job—the only way to make onshore wind turbines bigger. Bigger turbines, after all, can generate more energy at a lower cost per megawatt. But the question is: Will supersizing airplanes be worth the trouble? Wind Turbine Blade Transportation Challenges Lundstrom, an aerospace engineer, founded Radia nine years ago after coming across a plea for help from wind-turbine manufacturers. In their plea, posted as a press release, the manufacturers said they could build bigger onshore blades if there were simply a way to move them, Lundstrom recalls. In the United States, for example, the height of interstate highway overpasses—typically 4.9 meters (16 feet)—won’t allow for bigger turbine blades to pass. The overpass limitation is true for Europe too. There’s more flexibility in the developing world, where there are fewer tunnels and overpasses generally, Lundstrom says. But many of the roads aren’t paved or hardened, which makes it much tougher to move 50-tonne objects around. Some regions in China don’t have the same road constraints, allowing extralarge onshore wind turbines to be built there. Last year, Chinese multinational Sany Renewable Energy announced that it had installed a 15-megawatt model in Tongyu , Jilin province, in northeast China, with blades that are 131 meters long. The blades were manufactured in an industrial park in Inner Mongolia, an 1,800-kilometer trek from where they were ultimately installed. The WindRunner Offshore wind farm developers suffer from the logistical and practical challenges of operating in open ocean, but finding vessels big enough to transport the blades isn’t one of those. The biggest offshore blades measure nearly 150 meters, and they’re usually transported via cargo ship. Manufacturers typically locate their facilities on the coast . Onshore, the movement of blades has met the hard limits of infrastructure. Shipping them in multiple pieces and reassembling them on-site won’t work because the joints would create weak spots. Junctions would also add too much weight compared with that of blades made from single pieces of polymer, says Doug Arent , executive director at the National Renewable Energy Laboratory Foundation and emeritus NREL researcher. “It comes down to the stress engineering of the components,” Arent says. Blades could one day be 3D-printed on-site, which could negate the need for an airplane, but that research is still in early stages, he says. (Lundstrom says 3D-printed blades will never happen, since it would require a large, sophisticated manufacturing facility to be built at every wind farm.) If moving blades in pieces is folly, then the way forward is to fly. But even the largest existing cargo planes—the C-5 and C-17 flown by the U.S. Air Force and the Russo-Ukrainian Antonov AN-124 Ruslan—can’t accommodate large turbine blades. “There really is no big cargo aircraft in production, or planned, except for ours,” Lundstrom says. How to Make the World’s Largest Aircraft Fly What you can experience of Radia’s WindRunner today fits inside a conference room in the company’s Boulder headquarters. Here, a kind of gazebo made of two-by-fours houses a flight simulator, where I’m trying to virtually fly, and land, the behemoth. There’s a couple of pilot chairs, a joystick, a throttle, a video screen with a head-up display, and a few buttons to operate the simulated landing gear and wing flaps. The grid of flight instruments that will occupy the cockpit space above the pilot’s head are not finished yet. Instead, laminated pictures of the eventual controls are Velcroed in place. It takes surprisingly few levers and controls to fly the WindRunner. “Physics is physics,” says my copilot Etan Karni , principal engineer and head of Radia’s advanced systems groups. As Karni controls the WindRunner’s airspeed, I pull up on the joystick and guide it off the runway of a virtual Denver International Airport. A few minutes later I make a planned U-turn around a nearby lake. The maneuver is wobbly; I remind myself to move the joystick gently even though this is such a big bird. The WindRunner With Karni’s aid in controlling the landing gear and flaps, we set down back in Denver. I not only keep the WindRunner in one enormous piece but also bring it to a stop at the very front of the runway, just before the visible streaks of burned rubber from other airliners. In the real world, this remarkable feat of deceleration will enable the WindRunner to stop within 10 lengths of the aircraft—about 1,080 meters. And the aircraft won’t need the perfected runways of contemporary airports. It’s designed, by necessity, to land on and take off from rugged dirt tracks—like access roads on the perimeter of a wind farm, but wider. These capabilities are enabled by the plane’s relatively light weight, its wing and body shape, and its big tires. Optimized for cargo volume rather than mass—because turbine blades are huge but not dense—WindRunner is, effectively, one giant cargo hold with the bare minimum of amenities required to make it fly. “Landing on dirt basically comes down to how many pounds per wheel you have,” Lundstrom told me. WindRunner’s four jet engines will aid with short takeoffs. “When the aircraft is empty,” Lundstrom says, “the engines are so powerful that the vehicle has a thrust-to-weight ratio similar to early fighter jets.” (Radia chose an engine already in use by modern airlines, but hasn’t disclosed which one.) To allow the plane to quickly turn skyward without scraping its underside, its back end will sweep away from the ground at a sharp angle. A single tail tall enough to stabilize the WindRunner would exceed airports’ height limit of 24 meters, so Radia designed it with two risers in the shape of the letter H . For landings, the aircraft’s broad and stubby wings use their nearly 1,000-square-meter surface to catch air and decelerate quickly. Twenty big tires borrowed from the classic design of the U.S. Air Force’s C-130 Hercules will help WindRunner slow down after it touches the ground. The plane’s mouth flips up to reveal its cavernous interior, a feature borrowed from the Antonov An-124. The cockpit, itself about as big as an entire Gulfstream private jet, looks like a pimple bulging from the WindRunner’s staggering frame. It sticks out from the fuselage to avoid interfering with cargo space and is the only part of the plane designed for human habitation. During flight, the hold is only pressurized to about the level of the peak of Mt. Everest, to save energy. Why Wind Turbines Got Bigger During my visit to Radia, a virtual-reality headset lets me behold the colossus from underneath its wing and inside its cargo bay. It feels like standing next to a warehouse that can fly. Seeing the virtual superplane towering above, and grasping the plane’s monumental scale makes me wonder if this adventure in engineering is necessary, that surely there’s another way. The largest helicopters built in the Western Hemisphere can carry up to 15 tonnes, but megablades can weigh four to five times that, Lundstrom notes. Blimps and airships can carry the weight, but they bring a laundry list of complications. They’re too slow, need an expensive hangar to shield them from bad weather, require helium—which is currently scarce—and struggle to land when it’s windy. “And by the way, wind farms tend to be windy,” he says. And, since the world’s biggest cargo planes can’t be stretched to meet the length of a 100-meter blade, nor can they land on short, rugged runways, a new design is needed. Still, the fundamental question remains: Is increasing the size of onshore wind turbines by 50 percent worth the trouble? Michael Howland , a wind-optimization expert at MIT, says there’s a huge value proposition in it. A turbine’s power-generation capacity increases by the cube of the wind speed blowing through it and the square of the diameter of the circle created by the spinning blades, he says. In other words, bigger turbines, while more expensive per individual unit, more than make up for it in generating capacity. That’s why the size of turbines has grown steadily larger over the years. “You’re able to have half as many,” Lundstrom adds. “So even though the cost of each turbine has gone up, the cost per gigawatt goes way down.” He estimates that GigaWind turbines would decrease the cost of energy by 20 to 35 percent while increasing output by 10 to 20 percent, potentially doubling wind’s profitability even with the cost of all those flights included. Having fewer total turbines means a wind farm could space them farther apart, avoiding airflow interference. The turbines would be nearly twice as tall , so they’ll reach a higher, gustier part of the atmosphere. And big turbines don’t need to spin as quickly, so they would make economic sense in places with average wind speeds around 5 meters per second compared with the roughly 7 m/s needed to sustain smaller units. “The result…is more than a doubling of the acres in the world where wind is viable,” Lundstrom says. Upon the WindRunner’s landing at a wind farm, rail equipment will roll turbine blades off the plane. Radia To kick-start this market, and to support the first WindRunners, Radia is developing a business arm that partners with wind-turbine manufacturers to develop new wind farms both domestically and internationally. WindRunners would deliver blades to those farms and those developed by other companies. The scope of Radia’s plan, and the ambition behind it, has impressed many observers, including Howland. “I was both surprised but also very impressed by the innovative spirit of the idea,” he says. “It’s great to be ambitious in terms of solving the grand challenges.” But onshore “gigawind” is full of unknowns, he notes. Less is understood about the flow physics and engineering of record-breaking turbine sizes. Plus, huge blades could create wakes so large that the turbines behind them would be noticeably affected by variations in air temperature and even the Coriolis effect caused by Earth’s rotation, and might require innovation in fundamental science, he says. Then there’s the question of the big plane’s carbon footprint. To move enough blades for a whole wind-farm operation, a WindRunner might fly back and forth from factory to farm every day for months, carrying one or two blades at a time. This may create more carbon emissions compared with trucking them. But Radia argues that the increased amount of clean energy created by advanced wind farms would be far more than enough to offset the CO 2 from the jet engines. Besides, the biggest component of a wind farm’s carbon footprint is the concrete and steel. With longer blades allowing for fewer turbines to create the same amount of energy, carbon contributions should decrease, Lundstrom argues. As Radia continues its quest, a dark cloud hangs over the endeavor. U.S. President Donald Trump and his administration have made multiple attempts to grind the American wind-energy industry to a halt by pausing approvals, permits, and government loans. But Lundstrom pushes back against the notion that the prevailing winds out of Washington will clip Radia’s wings. There’s simply too much money to be made, he says. “My belief is that [it’ll] sort itself out….We’ll be delivering [planes] at the end of this administration,” Lundstrom says. Increasing the scale at which societies can produce wind power is crucial for a future without fossil fuels. And that scale, he says, can’t be reached without a new airplane to make it possible. This article was updated on 17 September 2025. This article appears in the October 2025 print issue as “An Airplane Longer Than a Football Field.”",
    "published": "Wed, 10 Sep 2025 12:00:04 +0000",
    "author": "Andrew Moseman",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "More-Sophisticated Codes to Track Deep-Space Probes",
    "link": "https://spectrum.ieee.org/deep-space-communication-tech",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. As deep-space probes continue to explore throughout the solar system and beyond , scientists on the ground are racing to keep in touch with the many increasingly far-flung missions. To that end, researchers in China have developed deep-space-communications processing algorithms that will allow scientists to pinpoint probes with remarkable, meter-level precision—as far as 180 million kilometers from Earth. Xiaoyu Dang—a professor at Hangzhou Dianzi University ’s Space Information Research Institute (SIRI), in China—says that communicating with a spacecraft millions or billions of kilometers away is incredibly difficult. “Signals get very weak, and existing ranging codes struggle to pinpoint a spacecraft’s exact distance precisely over these enormous gaps,” he says. To maintain contact with space probes, the radio signals sent back and forth between Earth and deep space contain special ranging codes, which act as a kind of measuring tape or ruler. The code contains mathematical “notches” in it that enable mission planners to track individual signals as they travel to the probe and back. The travel time of the signal is then used to calculate the probe’s distance from Earth. Dang and his colleagues have developed a set of eight new codes (called Legendre Sequence Ranging Codes, or LS codes) which offer a much longer “measuring tape.” The team’s LS codes offer extended pattern length in deep-space signals compared with existing deep-space-communications codes. And that means the new codes can track probes increasingly farther away. Testing Deep-Space Signal Codes In simulations, the new LS codes reliably measured distances that are between 12 and 2,375 times as far as the distances that existing measuring codes can measure. As a result, the codes could enable precision communication with probes as far as 180 million km away from Earth, or 1.2 times the Earth–sun distance. On the other hand, the deepest deep-space probes travel billions of kilometers and often require specialized technologies and codes to maintain contact. For example, Voyager 1 , which launched in 1977, has traveled nearly 25 billion km from Earth. Remarkably, NASA still communicates with the probe, as recently as earlier this year . But communication with Voyager 1 is also very complex, Dang says, noting that scientists must combine Voyager’s original ranging code with additional low-frequency signals. This technique, he adds, is like using multiple rulers of different sizes and frequencies. By contrast, the new LS codes will enable precise, long-distance communication without complicating the signals and signal processing. “This is invaluable for faster, simpler navigation during critical operations,” Dang says. He also notes that the new LS codes will offer more flexibility than previous ranging codes, allowing for fine-tuning of the codes to suit specific mission needs. Scientists can also choose to modify the codes to achieve optimal long-distance precision, or other desirable goals. For instance, the code can be optimized for faster signal “locking” times when making contact with the probe, depending on the nature of the mission. Dang says the researchers aim to test their codes using hardware in laboratories. Then, if the tests are successful, the team expects to propose the code to space agencies for use in future missions. He says the team also plans to further develop and harden the codes to ensure they are robust against the harsh realities of space. The researchers’ deep-space signal-communications codes are described in a study published 11 August in IEEE Communications Letters .",
    "published": "Wed, 03 Sep 2025 13:09:00 +0000",
    "author": "Michelle Hampson",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "6G Wireless Networks to Use Satellites as Base Stations",
    "link": "https://spectrum.ieee.org/6g-wireless",
    "summary": "The future of wireless communication is today being sketched out in the skies and in space. A new generation of intelligent aerospace platforms—drones, airships, and satellites—will be part of tomorrow’s 6G networks , acting as, in effect, base stations in the sky. They’re expected to roll out in the early 2030s. Researchers at the King Abdullah University of Science and Technology (KAUST) in Thuwal, Saudi Arabia, are amid the vanguard of innovators now imagining next-gen telecom networks in the atmosphere, the stratosphere, and orbit. The sky won't be the limit for next-gen wireless platforms",
    "published": "Fri, 29 Aug 2025 13:00:02 +0000",
    "author": "Margo Anderson",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Fancy Flying Trick Could Bring Sensors to Earth’s “Ignorosphere”",
    "link": "https://spectrum.ieee.org/atmospheric-sensors-mesosphere-photophoresis",
    "summary": "Earth’s atmosphere is large, extending out to around 10,000 kilometers from the surface of the planet. It’s so large, in fact, that scientists break it into five separate sections. There’s one particular section that hasn’t received a whole lot of attention due to the difficulty in maintaining any craft there. Planes and balloons can visit the troposphere and stratosphere, the two sections closest to the ground, while satellites can sit in orbit in the thermosphere and exosphere, allowing for a platform for consistent observations. But the mesosphere, the section in the middle, is too close to have a stable orbit but too sparse in air density for traditional airplanes or balloons to work. As a result, we don’t have a lot of data on it, but it impacts climate and weather forecasting, so scientists have simply had to make a lot of assumptions about what it’s like up there. Now, a new study from researchers at Harvard and the University of Chicago might have found a way to put stable sensing platforms into the mesosphere, using a novel flight mechanism known as photophoresis . The mesosphere itself is located between 50 and 85 km up, and while it isn’t technically considered “space,” it is very different from the lower levels of the atmosphere we are more accustomed to. It’s affected both by weather from below and above, reacting to solar storms as often as hurricanes. Since it serves as that kind of interface level, it plays a critical role in how the layers both above and below it react as well. But we haven’t been able to place any stable monitoring equipment in it due to the difficulty for the two types of continual monitoring systems we have—balloons and satellites. This has led to the moniker “ ignorosphere ,” because scientists have been forced to essentially ignore the existence of this layer of atmosphere due to lack of data. Photophoresis Powers a New Atmospheric Sensor Enter the new paper , published on 13 August in Nature , about long-term sensors in the mesosphere. Photophoresis is a process where more energy is created when gas molecules bounce off the “warm” side of an object than when they bounce off the “cool” side. In this case, the warm side is the side of the object facing the sun, while the cool side is the underside facing Earth. The effect is only noticeable in low-pressure environments, which is exactly what the mesosphere is. Admittedly, the force from photophoresis is minuscule, so the researchers had to develop really tiny parts to have any chance of taking advantage of it. They recruited experts in nanofabrication techniques to make centimeter-scale structures as a proof of concept and tested them in a vacuum chamber designed to have the same pressure as the mesosphere. The prototypes reacted as expected and managed to levitate a structure with just 55 percent of sunlight at a pressure comparable to that of the mesosphere. That marks a first that anyone has ever demonstrated a functional prototype of a photophoresis-powered flight, mainly due to how light the structure itself was. Devices powered by this technique could be sent to monitor the mesosphere, but they could also be useful farther afield. Mars is an obvious candidate, because its low-pressure and sparse atmosphere are both hallmarks of the planet but also largely unexplored at different layers. Other planets and moons could be potential targets as well—anything that has an atmosphere that is spare enough to support a levitating spacecraft could be served by one of these fliers. Unfortunately, there’s still some advanced engineering left to do. The nanofabrication technique that was used to build the flight structure didn’t include any functional hardware, such as sensors or wireless communication equipment. A structure that simply floats without transmitting data isn’t scientifically useful, so in order for these devices to start making the type of scientific impact they hope to, the nanofabrication techniques will need to be improved to create a functional payload. The researchers have no doubt that is possible, though, and have already created a startup company called Rarefied Technologies , which was accepted into the Breakthrough Energy Fellows program last year. With that support, and some ongoing research in nanofabrication, hopefully it will only be a matter of time before we see centimeter-size sensors scattered throughout the “ignorosphere” and beyond.",
    "published": "Tue, 26 Aug 2025 13:00:04 +0000",
    "author": "Andy Tomaswick",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "NASA Cut 4,000 Staff. Where Will They Go?",
    "link": "https://spectrum.ieee.org/nasa-budget-cuts-trump-staff",
    "summary": "“I worked at NASA proudly for 36 years,” says Jennifer Mason . “I decided to retire after it became clear that the current administration does not value science or federal employees.” There are thousands of people like Mason right now, leaving not just the U.S. space agency but jobs in medical research, environmental protection, energy , and elsewhere across the government. Mason worked at the Johnson Space Center in Houston, most recently as a technology manager for Gateway , the first planned space station in lunar orbit and part of the Artemis moon-landing campaign. It was rewarding work, Mason says, but the White House proposed canceling Gateway and cutting about a quarter of NASA’s budget overall—and unless Congress intervenes, science programs could be cut by half. Mason says she had also been active in a NASA-supported group for gay, lesbian, and transgender employees and their loved ones, and such groups were shut down in January by White House executive order . Put all those factors together, Mason says, and it was time to wrap up a long career. In some ways, this year’s government cuts resemble a corporate downsizing, intended to eliminate layers of fat and waste. But government is not a corporation, and NASA’s purpose is sometimes as much symbolic as it is practical. So the administration’s actions have raised major issues. What shape does NASA take now? Can it continue as a driver, and expression, of American leadership? Will it be leaner and more effective, or will it be too hollowed out to pursue its many missions? In the often-sterile language of government, NASA is going through a RIF , short for “reduction in force,” and as part of it, many employees were offered a DRP , short for “deferred resignation program.” It puts most signers on paid leave until early January so they can look for new jobs. NASA says about 4,000 people, or one-fifth of its total workforce, have taken DRP offers. Mason turned the DRP down: “I did not want to be on extended administrative leave and risk my paycheck if I choose to speak out against Trump’s priorities.” Where Will NASA Employees Go? While debate goes on about American space goals, there is the more immediate issue of what will happen to those roughly 4,000 people. Some will probably find jobs in private-sector aerospace, defense, or university research; some may move into other engineering fields. Many, like Mason, may retire, years before they might otherwise have planned. Apparently, nobody is quite sure what the breakdown is likely to be, or at least willing to talk publicly about post-NASA employment trends. “Good data is hard to come by,” says a source who, like many others, did not want to be quoted by name. Data from before 2025 show high demand for people in the space sector; companies like Blue Origin, SpaceX, and newer startups have been growing while NASA shrinks. The nonprofit Space Foundation reported in April that over the last decade, employment at space companies increased 27 percent, almost double the 14 percent growth for the private sector overall. The foundation said space industry salaries were also nearly double the rest of the private sector economy: US $131,000 in 2022, compared to $72,608 nationally. A spacewalker works outside the International Space Station. NASA Engineers who leave NASA are likely to be sought out, not only because of their skills in project management, robotics, and other areas, but because so many of their aerospace peers are reaching retirement age. Deloitte, the accounting firm, wrote in October 2024 that “any amount of retirement in the aerospace and defense industry exacerbates existing talent gaps, as retirees take with them a wealth of institutional knowledge.” “But that doesn’t necessarily translate into these people finding jobs that are aligned with their values,” says Colette Delawalla , a doctoral student in psychology at Emory University who, in February, started an activist group called Stand Up for Science . “Where are the positions that allow them to serve the American public?” NASA management declined an interview request from IEEE Spectrum . “Safety remains a top priority for our agency as we balance the need to become a more streamlined and more efficient organization and work to ensure we remain fully capable of pursuing a Golden Era of exploration and innovation, including to the Moon and Mars,” said spokeswoman Cheryl Warner in a statement sent to reporters after the deadline to accept buyout offers passed on 25 July. The White House press office, in response to a request for comment, referred Spectrum back to NASA. Cultural Mismatch in Space Careers Delawalla says there’s a demographic and philosophical mismatch between private sector space companies and former NASA staff—many of the newer companies are looking for young, aggressive engineers, but many of the people leaving government jobs are the more senior ones. Taking a DRP offer might mean abandoning a space mission to which they’ve devoted years or even decades. “The ambition is not to climb a corporate ladder. The ambition is to learn more,” she says. Bethany Ehlmann , a professor of planetary science at the California Institute of Technology who is also president of the Planetary Society , says she knows some Ph.D. students who, having spent years with NASA careers as their goal, are now considering jobs in other countries. “U.S. leadership in space depends on industry, NASA, and universities that are educating the workforce, all working together,” Ehlmann says. “It’s a delicate ecosystem, and it’s just really hard when there’s a change of this magnitude. It’s not obvious that people can land in the space ecosystem.” Multiple exposures show the progression of a partial solar eclipse over the Washington Monument on 8 April 2024. Bill Ingalls/NASA/AP That system has been changing for years, starting in the Obama administration and accelerating after President Trump first took office. The plan has been for NASA to let companies like SpaceX take over most of its operational work, such as launching astronauts or satellites, sometimes with the government as a paying customer. That, ideally, would leave NASA free to concentrate on exploration, doing research that pushes back the frontiers of knowledge but doesn’t necessarily turn a profit. Will that model still be viable if the agency loses too many people with the institutional knowledge to see long-term projects through to completion? Many people in the field, at major aerospace companies and industry groups, declined requests for interviews or did not reply. Certainly, others concede, the agency has been weighed down by bureaucracy, but they say this year’s buyout offers were too broad and too scattershot to make NASA truly more efficient. “From everything I have seen so far, there’s no thought or succession planning going into this,” says one veteran of several planetary research missions who asked to remain anonymous. “I mean, people leave but there is no plan on how their roles and responsibilities will be picked up by others and replaced. It’s left gaping holes across the agency, reducing the integrity of everything NASA does.” NASA Workforce Challenges Stand Up for Science circulated a letter of dissent to NASA’s acting administrator, Sean Duffy , saying that “the culture of fear of retaliation cultivated by this administration” was enough to put space missions and even astronauts’ lives at risk. The letter was titled the Voyager Declaration , and 183 current or former NASA employees, including several former astronauts, publicly signed it. The group said another 180 signed but didn’t want their names on record. Jennifer Mason signed the letter and urged current NASA friends to join. “If you don’t feel comfortable speaking up, if it doesn’t feel like a safe place, then you might not bring up a mission safety issue because you don’t want to be the troublemaker.” But some people, she says, were afraid even to sign anonymously, worried that they could be found out and face retribution. Mason happens to be a second-generation NASA employee—both her parents worked in Houston—and says her son, now in college, would like to be third-generation. His interest is in the use of AI in control systems. But she says she’s urged him to apply to graduate school, partly because an advanced degree would be good, partly too in hope that the current tumult will pass. “He’s reading the writing on the wall,” says Mason. “And right now it’s a stressful time to be starting an aerospace career.”",
    "published": "Mon, 25 Aug 2025 14:31:58 +0000",
    "author": "Ned Potter",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "China, Russia, and U.S. Race to Develop Lunar Nuclear Reactors",
    "link": "https://spectrum.ieee.org/lunar-nuclear-reactor-nasa-moon",
    "summary": "China, Russia, and the United States are racing to put nuclear power plants on the moon. China and Russia in May agreed to work together to complete a lunar nuclear reactor by 2036 . In response, NASA’s interim chief Sean Duffy announced in August that the United States would fast track its lunar nuclear power program to have one ready by 2030. But this sudden frenzy raises a few questions—such as why do we want nuclear reactors on the moon in the first place? And how would they work? To find out, IEEE Spectrum spoke with Katy Huff , a nuclear engineer and the director of the Advanced Reactor Fuel Cycles Laboratory at the University of Illinois at Urbana-Champaign. Huff previously served as the assistant secretary for nuclear energy at the U.S. Department of Energy (DOE). Why do the world’s biggest space organizations want nuclear reactors on the moon, and what would they power? Katy Huff : There’s a growing interest in having a more sustained presence of humans on the moon for scientific discovery . Resources like helium-3, which can serve as a fusion fuel, may be part of the appeal. NASA is planning to build this kind of lunar exploration base through its Artemis program, and China and Russia are working together to build one called the International Lunar Research Station . Any such lunar base would absolutely need nuclear power. Renewables alone are too intermittent to meet the energy needs of life on the moon. Plus, the cost of getting things into space scales by mass, so the unmatched energy density of uranium fission is our greatest opportunity. Why is it suddenly a race? What’s the urgency? Huff: The momentum began with the fission surface power project at NASA, which a few years ago solicited designs for 40-kilowatt lunar microreactors. Three designs were selected and awarded US $5 million each. Since then, China and Russia have announced on at least three occasions a joint effort to design their own lunar microreactor with a launch target in the mid-2030s. In response, NASA is accelerating its timeline for the U.S. reactor to 2030 and increasing the target power capacity to 100 kW. Sean Duffy has said publicly that if China and Russia are the first to stake a claim for a lunar power plant, they could declare a de facto keep-out zone , limiting the United States’ options to site its base. So the U.S. aims to get there before China and Russia to claim a region with access to water ice, which aids life support for astronauts. Designing Lunar Nuclear Reactors What are the considerations for designing a nuclear reactor for the moon? Huff : In very low gravity, fluids won’t behave exactly as they do on Earth. So the circulation patterns for the reactor’s fluid coolants will need to be recalculated. And the moon’s large temperature swings, which vary hundreds of degrees from lunar day to night, will require the reactor to use systems that are more isolated from those swings. On Earth we eject waste heat easily because there are thermally stable heat sinks like water bodies available. What kind of reactor do you expect NASA to choose? Katy Huff previously served as the assistant secretary for nuclear energy at the U.S. Department of Energy (DOE). Katy Huff Huff : It would make sense if NASA chose one of the three designs previously selected for the fission surface power program, rather than starting from scratch. But with the over-doubling of target capacity, from 40 kW to 100 kW, there will be a bit of a redesign involved, because you don’t just turn up the knob. The three awards went to Lockheed Martin / BWXT , Westinghouse / Aerojet Rocketdyne , and X-energy / Boeing . Some of them are developing microreactors that are based around tristructural isotropic [TRISO] fuel , which is a type of highly robust uranium fuel, so I would expect the lunar reactor to be designed using that. For the coolant, I don’t expect them to choose water, because water’s thermal properties limit the range of temperatures it can cool effectively, which constrains reactor efficiency. And I don’t expect it to be liquid salt either, because it can be corrosive, and this lunar reactor needs to operate for 10 years with no intervention. So I suspect they’ll choose a gas such as helium. And then for power conversion, NASA’s directive explicitly said that a closed Brayton cycle would be a requirement. What would transport and startup look like? Huff : The reactor would be fully constructed on Earth and ready to go, with the fuel in place. My expectation is that it would be transported with the control elements fully inserted into the reactor to prevent a chain reaction from starting during transit. Once on the moon, a startup sequence would be initiated remotely or by the astronauts there. The control rods would then withdraw from the reactor, and a small neutron source like californium-252 would kick off the reaction. A deadline of 2030 feels pretty rushed considering the United States doesn’t have a final design for the reactor, nor firm plans for a lunar base. Huff : Right. That timeline does appear ambitious. We’ll have a hard enough time getting a reactor of this scale deployed as a prototype terrestrially in the next four and a half years. Getting one launch-ready and onto the moon by then is a recipe for eventually having to explain why we didn’t meet that timeline. And that could be a problem, reputationally, for nuclear energy more so than space exploration because people love NASA. Little kids and grown-ups alike wear NASA T-shirts. No one’s wearing DOE T-shirts. Risks of Lunar Reactor Launch What are the risks if something goes wrong with the launch? Huff : Beautifully enough, fresh uranium fuel doesn’t present a radiological hazard the way spent uranium would. Only after it becomes the fission products is it significantly radioactive. So as long as the reactor doesn’t operate before launch, the hazard is quite low. Even if the fuel were dispersed over the Earth, it wouldn’t pose a significant danger to the people around it. I literally have a sample of uranium sitting by my desk. On top of that, there’s a robust launch safety protocol already established for any radiological object. NASA has a lot of experience with this from sending plutonium thermoelectric generators , which are more like a nuclear battery, for previous missions. Things have gone wrong with some of the fission reactors previously launched into space; what happened to those? Huff : The biggest fission reactors anyone has launched into space were the 5 kW electric TOPAZ-I reactors that were part of the Soviet program. One of them had a serious incident and broke apart. It’s now in high orbit in pieces, including some of its sodium coolant, which is just sort of floating around up there as liquid metal spheres. But that doesn’t impact the Earth because it’s a tiny amount of radiological source material at an incredible distance from Earth. The more unfortunate incident happened with the Soviet Kosmos 954 reactor, which, after operating in orbit, experienced uncontrolled reentry and disintegrated over a 600-kilometer swath of Canadian territory. What happens if an asteroid hits the moon or directly hits the lunar nuclear reactor? Huff : A direct strike could damage the reactor and cause localized dispersion of the fuel. This might be a motivation to use TRISO fuel. It’s so robust because the fuel and fission products are housed in thousands of spherical, chia seed–size particles that are coated in silicon carbide. It can withstand incredible impacts and heat—well beyond the temperature of lava. Testing has shown that even when subjected to 1,700°C heat for 300 hours , TRISO retains its fission products with no failures. So in the unlikely event that there’s a dead-on collision with a large asteroid at the reactor site, the debris of the reactor may be distributed in the dust of the moon, but all those little TRISO particles will hopefully remain intact.",
    "published": "Fri, 22 Aug 2025 13:00:03 +0000",
    "author": "Emily Waltz",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "What I Learned From a Janky Drone",
    "link": "https://spectrum.ieee.org/stem-education-in-africa",
    "summary": "The package containing the ArduCopter 2.8 board finally arrived from China, bearing the weight of our anticipation. I remember picking it up, the cardboard box weathered slightly from its journey. As I tore through the layers of tape, it felt like unwrapping a long-awaited gift. But as I lifted the ArduCopter 2.8 board out of the box, my heart sank. The board, which was to be the cornerstone of our project, looked worn out and old, with visible scuffs and bent pins. This was just one of a cascade of setbacks my team would face. It all started when I was assigned a project in machine design at Obafemi Awolowo University (OAU), located in the heart of Ilé-Ifẹ̀, an ancient Yoruba city in Osun State, in southwest Nigeria, where I am a mechanical engineering student entering my final year of a five-year program. OAU is one of Nigeria’s oldest and most prestigious universities, known for its beautiful campus and architecture. Some people I know refer to it as the “Stanford of Nigeria” because of the significant number of brilliant startups it has spun off. Despite its reputation, though, OAU—like every other federally owned institution in Nigeria—is underfunded and plagued by faculty strikes , leading to interruptions in academics. The lack of funding means students must pay for their undergraduate projects themselves, making the success of any project heavily dependent on the students’ financial capabilities. The Student & the Professor Two perspectives on engineering education in Africa Johnson I. Ejimanya is a one-man pony express. Walking the exhaust-fogged streets of Owerri, Nigeria, Ejimanya, the engineering dean of the Federal University of Technology, Owerri, carries with him a department’s worth of communications, some handwritten, others on disk. He’s delivering them to a man with a PC and an Internet connection who converts the missives into e-mails and downloads the responses. To Ejimanya, broadband means lugging a big bundle of printed e-mails back with him to the university, which despite being one of the country’s largest and most prestigious engineering schools, has no reliable means of connecting to the Internet. I met Ejimanya when I visited Nigeria in 2003 to report on how the SAT-3/WASC, the first undersea fiber-optic cable to connect West Africa to the world, was being used. (The passage above is from my February 2004 IEEE Spectrum article “ Surf Africa .”) Beyond the lack of computers and Internet access, I saw labs filled with obsolete technology from the 1960s. If students needed a computer or to get online, they went to an Internet cafe, their out-of-pocket costs a burden on them and their families. So is the situation any better 20-plus years on? The short answer is yes. But as computer science professor Engineer Bainomugisha and IEEE student member Oluwatosin Kolade attest in the following pages, there’s still a long way to go. Both men are engineers but at different stages of their academic journey: Bainomugisha went to college in the early 2000s and is now a computer science professor at Makerere University in Kampala, Uganda. Kolade is in his final semester as a mechanical engineering student at Obafemi Awolowo University in Ilé-Ifẹ̀, Nigeria. They describe the challenges they face and what they see as the path forward for a continent brimming with aspiring engineers but woefully short on the resources necessary for a robust education. —Harry Goldstein Dr. Oluwaseun K. Ajayi , an expert in computer-aided design (CAD), machine design, and mechanisms, gave us the freedom to choose our final project. I proposed a research project based on a paper titled “ Advance Simulation Method for Wheel-Terrain Interactions of Space Rovers: A Case Study on the UAE Rashid Rover ” by Ahmad Abubakar and coauthors . But due to the computational resources required, it was rejected. Dr. Ajayi instead proposed that my fellow students and I build a surveillance drone, as it aligned with his own research. Dr. Ajayi, a passionate and driven researcher, was motivated by the potential real-world applications of our project. His constant push for progress, while sometimes overwhelming, was rooted in his desire to see us produce meaningful work. As my team finished scoping out the preliminary concepts of the drone in CAD designs, we were ready to contribute money toward implementing our idea. We conducted a cost analysis and decided to use a third-party vendor to help us order our components from China. We went this route due to shipping and customs issues we’d previously experienced. Taking the third-party route was supposed to solve the problem. Little did we suspect what was coming. By the time we finalized our cost analysis and started to gather funds, the price of the components we needed had skyrocketed due to a sudden economic crisis and depreciation of the Nigerian naira by 35 percent against the U.S. dollar at the end of January 2024. This was the genesis of our problem. Related: Learning More With Less Initially, we were a group of 12, but due to the high cost per person, Dr. Ajayi asked another group, led by Tonbra Suoware , to merge with mine. Tonbra’s team had been planning a robotic arm project until Dr. Ajayi merged our teams and instructed us to work on the drone, with the aim of exhibiting it at the National Space Research and Development Agency , in Abuja, Nigeria. The merger increased our group to 25 members, which helped with the individual financial burden but also meant that not everyone would actively participate in the project. Many just contributed their share of the money. Tonbra and I drove the project forward. Supply Chain Challenges in African Engineering Education With Dr. Ajayi’s consent, my teammates and I scrapped the “surveillance” part of the drone project and raised the money for developing just the drone, totaling approximately 350,000 naira (approximately US $249). We had to cut down costs, which meant straying away from the original specifications of some of the components, like the flight controller, battery, and power-distribution board. Otherwise, the cost would have been way more unbearable. We were set to order the components from China on 5 February 2024. Unfortunately, it was a long holiday in China, we were told, so we wouldn’t get the components until March. This led to tense discussions with Dr. Ajayi, despite having briefed him about the situation. Why the pressure? Our school semester ends in March, and having components arrive in March would mean that the project would be long overdue by the time we finished it. At the same time, we students had a compulsory academic-industrial training at the end of the semester. Oluwatosin Kolade, a mechanical engineering student at Nigeria’s Obafemi Awolowo University, says the drone project taught him the value of failure. Andrew Esiebo But what choice did we have? We couldn’t back down from the project—that would have cost us our grade. We got most of our components by mid-March, and immediately started working on the drone. We had the frame 3D-printed at a cost of 50 naira (approximately US $0.03) per gram for a 570-gram frame, for a total cost of 28,500 naira (roughly US $18). Next, we turned to building the power-distribution system for the electrical components. Initially, we’d planned to use a power-distribution board to evenly distribute power from the battery to the speed controllers and the rotors. However, the board we originally ordered was no longer available. Forced to improvise, we used a Veroboard instead. We connected the battery in a configuration parallel to the speed controllers to ensure that each rotor received equal power. This improvisation did mean additional costs, as we had to rent soldering irons, hand drills, hot glue, cables, a digital multimeter, and other tools from an electronics hub in downtown Ilé-Ifẹ̀. Everything was going smoothly until it was time to configure the flight controller—the ArduCopter 2.8 board—with the assistance of a software program called Mission Planner . We toiled daily, combing through YouTube videos, online forums, Stack Exchange, and other resources for guidance, all to no avail. We even downgraded the Mission Planner software a couple of times, only to discover that the board we’d waited for so patiently was obsolete. It was truly heartbreaking, but we couldn’t order another one because we didn’t have time to wait for it to arrive. Plus, getting another flight controller would’ve cost an additional sum—240,000 naira (about US $150) for a Pixhawk 2.4.8 flight controller —which we didn’t have. We knew our drone would be half-baked without the flight controller. Still, given our semester-ending time constraint, we decided to proceed with the configuration of the transmitter and receiver. We made the final connections and tested the components without the flight controller. To ensure that the transmitter could control all four rotors simultaneously, we tested each rotor individually with each transmitter channel. The goal was to assign a single channel on the transmitter that would activate and synchronize all four rotors, allowing them to spin in unison during flight. This was crucial, because without proper synchronization, the drone would not be able to maintain a stable flight. “This experience taught me invaluable lessons about resilience, teamwork, and the harsh realities of engineering projects done by students in Nigeria.” After the final configuration and components testing, we set out to test our drone in its final form. But a few minutes into the testing, our battery failed. This failure meant the project had failed, and we were incredibly disappointed. When we finally submitted our project to Dr. Ajayi, the deadline had passed. He told us to charge the battery so he could see the drone come alive, even though it couldn’t fly. But circumstances didn’t allow us to order a battery charger, and we were at a loss as to where to get help with the flight controller and battery. There are no tech hubs available for such things in Ilé-Ifẹ̀. We told Dr. Ajayi we couldn’t do as he’d asked and explained the situation to him. He finally allowed us to submit our work, and all team members received course credit. Resourcefulness is not a substitute for funding This experience taught me invaluable lessons about resilience, teamwork, and the harsh realities of engineering projects done by students in Nigeria. It showed me that while technical knowledge is crucial, the ability to adapt and improvise when faced with unforeseen challenges is just as important. I also learned that failure, though disheartening, is not an ending but a stepping stone toward growth and improvement. In my school, the demands on mechanical engineering students are exceptionally high. For instance, in a single semester, I was sometimes assigned up to four different major projects, each from a different professor. Alongside the drone project, I worked on two other substantial projects for other courses. The reality is that a student’s ability to score well in these projects is often heavily dependent on financial resources. We are constantly burdened with the costs of running numerous projects. The country’s ongoing economic challenges, including currency devaluation and inflation, only exacerbate this burden. In essence, when the world, including graduate-school-admission committees and industry recruiters, evaluates transcripts from Nigerian engineering graduates, it’s crucial to recognize that a grade may not fully reflect a student’s capabilities in a given course. They can also reflect financial constraints, difficulties in sourcing equipment and materials, and the broader economic environment. This understanding must inform how transcripts are interpreted, as they tell a story not just of academic performance but also of perseverance in the face of significant challenges. As I advance in my education, I plan to apply these lessons to future projects, knowing that perseverance and resourcefulness will be key to overcoming obstacles. The failed drone project has also given me a realistic glimpse into the working world, where unexpected setbacks and budget constraints are common. It has prepared me to approach my career with both a practical mindset and an understanding that success often comes from how well you manage difficulties, not just how well you execute plans.",
    "published": "Wed, 20 Aug 2025 13:00:03 +0000",
    "author": "Oluwatosin Kolade",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Satellite Collisions Could Be Prevented With a New AI System",
    "link": "https://spectrum.ieee.org/kessler-syndrome-esa-cream-ai",
    "summary": "The numbers paint a stark picture of our orbital traffic problem : More than 11,000 active satellites currently circle Earth, with thousands more planned for launch in coming years. Even more concerning are the over 1.2 million pieces of space debris larger than one centimeter hurtling through space at incredible speeds. At those velocities, even a paint chip can damage a spacecraft, while larger debris can destroy entire satellites . This growing congestion has turned collision avoidance into a daily headache for satellite operators worldwide. Currently, teams of specialists must manually assess threats, calculate risks, and coordinate with other operators when collisions seem likely. This process is time consuming, labor intensive, and prone to communication breakdowns that can complicate emergency responses. That is where the European Space Agency’s Collision Risk Estimation and Automated Mitigation (CREAM) project comes in. It aims to revolutionize this chaotic process by automating most collision-avoidance activities. The system can evaluate potential crashes, generate precise maneuver plans, and support decision-making with minimal human intervention. Think of it as an air traffic control system for space, but with artificial intelligence handling much of the complex coordination. An AI Negotiator One of CREAM’s most innovative features is its ability to connect different types of organizations involved in space operations. Satellite operators, space-monitoring services, regulators, and observers can all communicate through the system, streamlining what was previously a fragmented and often frustrating process. The system goes even further by facilitating negotiations between operators when potential collisions involve two active satellites rather than debris. If operators disagree on the best solution, CREAM can refer the dispute to mediation services, ensuring fair and transparent resolution. Currently, CREAM exists as a ground-based prototype system developed by GMV , a Spanish private capital interest group, and Guardtime , an Estonian data-management company. This version can already provide collision alerts and generate actionable avoidance maneuvers that ground crews can implement. However, the real breakthrough will come when CREAM moves into orbit itself. The project is preparing for expanded pilot testing while simultaneously developing space-based versions. These include “piggyback missions” where CREAM will ride aboard other spacecraft as a digital payload, plus a dedicated demonstration mission to test the system’s capabilities in the harsh environment of space. Beyond preventing immediate collisions, CREAM addresses a fundamental challenge in space governance. Establishing “rules of the road” for space traffic has always faced a chicken and egg problem; you need both international agreement on the rules and the technology to enforce them. CREAM provides that missing technological foundation. The system offers standardized tools that help operators follow best practices while giving regulators ways to monitor compliance. Its flexible design allows nontechnical users to update standards and rules as international norms evolve. This adaptability ensures CREAM will remain relevant as space technology advances and new challenges emerge. Rather than becoming obsolete, the system can grow and adapt alongside our expanding presence in space.",
    "published": "Sun, 17 Aug 2025 13:00:02 +0000",
    "author": "Mark Thompson",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "How to Accelerate Large Antenna Array Simulations",
    "link": "https://content.knowledgehub.wiley.com/efficient-simulation-of-radiation-pattern-diagrams-for-complex-electromagnetic-problems/",
    "summary": "In advanced electromagnetic (EM) design, speed and accuracy are critical – especially for large antenna arrays and complex scattering problems. But traditional simulation methods often require costly, repetitive computations just to evaluate radiation patterns across different scenarios. Our latest whitepaper, Efficient Simulation of Radiation Pattern Diagrams for Complex Electromagnetic Problems , introduces two breakthrough techniques that slash simulation time without sacrificing precision: “One Element at a Time” – Simulate once, generate any beam pattern instantly. Matrix-Based Acceleration – Faster far-field calculations for large datasets. Download this free whitepaper now!",
    "published": "Tue, 29 Jul 2025 14:47:07 +0000",
    "author": "WIPL-D",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "VERVE Probe Would Investigate Venus for Signs of Life",
    "link": "https://spectrum.ieee.org/life-on-venus-verve-mission",
    "summary": "Venus has always seemed like the last place you’d expect to find life. With surface temperatures hot enough to melt lead and crushing atmospheric pressure, our neighboring planet appears utterly hostile. But high in its clouds, where conditions are surprisingly Earth-like, scientists have discovered something extraordinary: mysterious gases that shouldn’t exist, unless something is alive up there…perhaps! Over the past five years, researchers have detected phosphine and ammonia in Venus’s atmosphere , two gases that on Earth are produced almost exclusively by biological processes or industrial activity. Since Venus has no factories, the discovery has sparked one of the most intriguing questions in astrobiology: Could microbial life be floating in the planet’s clouds? Now, a U.K.-backed mission plans to answer that question once and for all. Jane Greaves , a professor of astronomy at Cardiff University, and her team have unveiled VERVE ( Venus Explorer for Reduced Vapours in the Environment ), an ambitious probe that would hitch a ride to Venus with the European Space Agency’s EnVision mission , scheduled for 2031. Phosphine Discovery Sparks Debate The story began in 2020, when Greaves and colleagues first detected phosphine in Venus’s clouds using the James Clerk Maxwell Telescope in Hawaii. The announcement sent shockwaves through the scientific community. On Earth, phosphine is primarily produced by anaerobic bacteria, microbes that thrive in oxygen-free environments like swamps and in the guts of animals. Finding it on Venus suggested the tantalizing possibility of aerial life. However, the discovery proved controversial. Follow-up observations by other teams failed to replicate the findings , leading to heated scientific debates. But Greaves’s team didn’t give up. Through persistent monitoring, they discovered something crucial: The phosphine signal appeared to follow Venus’s day-night cycle, being destroyed by sunlight and varying with time and location across the planet. The plot thickened when the team announced the tentative detection of ammonia in Venus’s clouds. Like phosphine, ammonia on Earth is primarily produced by biological activity and industrial processes. But there are no known atmospheric or geological phenomena that can explain its presence on Venus. VERVE Mission Targets Venus’s Clouds The proposed VERVE mission would cost £43 million (nearly US $58 million), a fraction of typical planetary missions, and would search for and map these gases along with other hydrogen-rich compounds that shouldn’t exist on Venus. The CubeSat-sized probe would detach from EnVision upon arrival and conduct an independent survey while the main mission studies Venus’s surface and interior. The target zone for potential life lies about 50 kilometers above the Venusian surface, where temperatures range from a comfortable 30 °C to 70 °C and atmospheric pressure resembles Earth’s surface conditions. In this “ Goldilocks zone ” of the atmosphere, extremophile microbes (organisms that thrive in harsh conditions) could theoretically survive. These organisms could be remnants from Venus’s more temperate past. Billions of years ago, it might have had liquid water oceans and Earth-like conditions. As the planet’s runaway greenhouse effect took hold, any life that existed might have retreated to the more hospitable cloud layers, evolving to survive in this aerial niche. The only way to resolve the mystery once and for all is direct investigation and, if successful, the mission could mark one of the most significant discoveries in human history: proof that life exists beyond Earth.",
    "published": "Thu, 17 Jul 2025 12:00:03 +0000",
    "author": "Mark Thompson",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "First Space-Based Gravitational Wave Detector Begins Construction",
    "link": "https://spectrum.ieee.org/laser-interferometer-space-antenna",
    "summary": "Last month, work began on the world’s first space-based gravitational wave detector. The European Space Agency (ESA) and partner aerospace companies are developing orbiting detector spacecraft to observe gravitational waves from some of the universe’s most massive objects. The Laser Interferometer Space Antenna satellite constellation (LISA) will be three satellites orbiting the sun in a triangular formation, each separated from the other by 2.5 million kilometers, trailing 60 million km behind Earth. Each of LISA’s three spacecraft, expected to launch in 2035, will shoot laser beams at the other two to measure distances between the satellites down to the distance of a picometer (1 trillionth of a meter). Detecting interference patterns in the laser light, LISA is expected to discover whole new ranges of gravitational waves , the mysterious ripples in spacetime triggered by collisions of extremely massive objects such as black holes and dense stars. The nearly $2 billion mission, led by the ESA , will be a next-generation observatory, joining the ranks of the current workhorse of gravitational wave physics, the Nobel Prize –winning Laser Interferometer Gravitational-Wave Observatory (LIGO), in Livingston, La., and Hanford, Wash. Astrophysicists hope LISA will fill major gaps in their understanding of the evolution of supermassive black holes, the gargantuan monsters that reside at the center of galaxies such as our Milky Way. “Ground-based detectors can measure gravitational waves with frequencies from about 20 hertz to up to a few kilohertz,” says Guido Mueller, a professor of precision interferometry at the Max Planck Institute for Gravitational Wave Physics in Germany and a member of the LISA science consortium. “In the future, we might be able to push toward 1 hertz on the ground, but below that, it will be virtually impossible because of the sources of noise present on Earth.” An orbiting detector like LISA will be able to capture those slower oscillating gravitational waves, thus opening entirely new possibilities in black hole research. The frequency of gravitational waves is determined by the energy of the collision that gives rise to them. LIGO does a good job picking up waves generated by collisions of smaller black holes—up to a hundred times as massive as our sun. However, it can’t detect collisions of galactic supermassive black holes with the mass of millions of suns. Mergers of these monsters produce slow millihertz waves, according to Mueller. And those can only be detected from space. How LISA Will See the Universe’s Biggest Black Holes In addition to the perfect silence of space, LISA will also benefit from the vast distance between the three spacecraft of the interferometer, enabling it to detect much fainter signals than the Earth-based LIGO. “In space we can have a much larger interferometer,” said Frank Steier, lead system engineer for the LISA Spacecraft, at OHB Systems , the lead contractor building the LISA satellites. “On the ground we are limited to a distance of a few kilometers, but in space we can have the instrument operate across distances of millions of kilometers.” At the heart of each LISA spacecraft are two free-floating golden cubes, about the size of a Rubik’s cube . These cubes function as test masses as well as mirrors, reflecting the laser light shot from the other two spacecraft. When a gravitational wave passes the spacecraft, the golden cubes’ positions change by up to a nanometer. With a slight expansion or compression in spacetime, the travel time of the laser light shortens or expands. By studying tiny variations in the interference patterns from the combined laser signals , researchers can study both the gravitational waves and the nature of the supermassive bodies that created them. ESA tested the basics of the LISA system in a precursor mission called LISA Pathfinder , which orbited Earth between 2015 and 2016. Still, many technical challenges remain to be solved. NASA’s Gravity Recovery and Climate Experiment Follow-On (GRACE-FO) mission is currently using a simpler interferometry system to study Earth’s gravitational anomalies using two satellites orbiting Earth 220 km apart. But LISA’s “long-arm” interferometer, spanning 2.5 million km, is a complete novelty, said Mueller. European Space Agency director of science Carole Mundell [left] shakes hands with Chiara Pedersoli [right], CEO of Bremen, Germany’s OHB Systems—lead contractor building ESA’s LISA gravitational wave detector. M. Polo/ESA The optical bench recombining the laser beams will be much larger and more complex than what’s on LISA Pathfinder and Grace FO missions, according to Mueller. LISA’s phase measurement system (a.k.a. its phasemeter ) must be able to measure the phase evolution of the laser signals and the time variations in its travel time with unprecedented accuracy. To keep the golden cubes in perfect free fall, engineers are developing a Drag-Free Attitude Control System (DFACS), which will keep the hexagonal, 800-kilogram satellites perfectly centered around the cubes. The system will detect the most minuscule changes to the spacecraft’s attitude and nudge it back. “We have to make sure that we manage the gravity of the satellite in a way that it doesn’t disturb the measurements,” said Steier. “That’s something you only need for a mission like LISA.” Black Hole Ringdowns and Other Observational Targets In addition to capturing gravitational waves oscillating at very slow frequencies, LISA will be able to record much longer progressions of gravitational waves than LIGO. It will thus provide deeper views into the cataclysmic events that gave rise to them. From the moment two black holes enter each other’s gravitational field, the spacetime around them begins to quiver. Although the two bodies merge within a split second, the new black hole continues to emit gravitational waves as it settles down. This is called a “ ringdown .” The LISA detector will be able to detect gravitational waves produced at various stages of black hole mergers in the distant universe—including approach, merger, and ringdown. LISA is also being designed to detect gravitational waves triggered by collisions of small black holes and neutron stars in the Milky Way galaxy. “These black hole mergers take millions of years. But on Earth we can only detect a few seconds at best. Then it’s gone,” Steier said. “With LISA, we will have a much greater sensitivity and will be able to receive much longer signals. It will not be just a ping but a long continuous signal, which will allow us to observe the evolution of the merging system for months or even years.” Science and engineering teams from 10 European countries have begun developing the range of hardware elements needed to make the mission work— including the precision optics, avionics, and golden test masses, noted above. However, non-engineering challenges may also lie ahead. The LISA mission is a collaboration with NASA, but the Trump administration has expressed intentions of cutting its contribution to LISA. NASA was supposed to develop twin telescopes that will be placed on board each LISA spacecraft to transmit and receive the powerful laser beams. Mueller said he hopes Europe would be able to cover the shortfall if those cuts were to make it through the U.S. Senate. “If they drop off now, it’s much better than if they drop off five or six years from now,” Mueller said. “The earlier, the better, as we can actually start our own program developments and not be hit with a large schedule penalty.”",
    "published": "Wed, 16 Jul 2025 14:00:02 +0000",
    "author": "Tereza Pultarova",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Citizen Scientists Help Confirm Distant Exoplanet",
    "link": "https://spectrum.ieee.org/exoplanet-citizen-science",
    "summary": "Distant exoplanets can be dodgy to spot even in the best of observations. Despite the challenges, a team of astronomers just reported the discovery of a gas giant exoplanet that lies about 400 light-years from Earth. It’s called TOI-4465 b and it takes 12 hours to transit across the face of its star during its 102-day orbit. TOI-4465 b was actually first observed by the Transiting Exoplanet Survey Satellite (TESS) and reported as a transit event. A transit is when the object being observed crosses in front of its star from our point of view here on Earth. That means the planet has to be in just the right orbit to be seen. In addition, other objects can also transit stars (such as small black holes). So, just seeing it transit wasn’t enough to confirm TOI-4465 b as a planet. Astronomers needed to see it transit more than once to make sure it just wasn’t a one-time wiggle in the data. aspect_ratio Due to its lengthy orbit, astronomers only get three chances each year to observe TOI-4465 b, and those windows are challenging, according to Zahra Essack of the University of New Mexico. She led a team project focused on this gas giant planet . “The observational windows are extremely limited. Each transit lasts about 12 hours, but it is incredibly rare to get 12 full hours of dark, clear skies in one location,” she said. “The difficulty of observing the transit is compounded by weather, telescope availability, and the need for continuous coverage. Characteristics of TOI-4465 b This gas giant exoplanet is part of the population of distant worlds that lie in the size range of our own Jupiter. It’s actually about 25 percent larger in radius and has about six times Jupiter’s mass packed inside. Interestingly, this massive, dense world is pretty temperate as big Jupiters go—its temperature ranges from 375 to 478 kelvins (about 200 to 400°F). TOI-4465 b’s size and moderate temperatures put it in a rare class of Jupiter-type planets. It bridges the gap between the really hot Jupiters, most of which orbit extremely close to their stars in very short (around 10 days or less) orbits and our own cold Jupiter (which orbits the Sun in 11.8 years). The 102-day orbit would put TOI-4465 b somewhere between Mercury and Venus if this planet were in our own Solar System. This size, orbit, and lower temperature make the planet a great target for future observations, particularly by the James Webb Space Telescope, which could also study its atmosphere in some detail. “This discovery is important because long-period exoplanets (defined as having orbital periods longer than 100 days) are difficult to detect and confirm due to limited observational opportunities and resources. As a result, they are underrepresented in our current catalog of exoplanets,” explained Essack. “Studying these long-period planets gives us insights into how planetary systems form and evolve under more moderate conditions.” Citizen Science in Exoplanet Discovery So, how do scientists go about observing TOI-4465 b given the challenges it poses? Essack and her team used the power of citizen science to extend the observing time around the world. The team put out a call for people with telescopes powerful enough to observe the star and its possible planet. At least 24 citizen scientists across 10 countries used their personal telescopes to track the next transit. Given their positions around the world, these citizen scientists were able to cover the 12-hour observation time needed to confirm the transit. Their data complemented additional data from professional observatories such as Palomar in California, the Whipple Observatory in Arizona, the La Silla Observatory in Chile, and others. “The discovery and confirmation of TOI-4465 b not only expands our knowledge of planets in the far reaches of other star systems but also shows how passionate astronomy enthusiasts can play a direct role in frontier scientific research,” said Essack. “It is a great example of the power of citizen science, teamwork, and the importance of global collaboration in astronomy.” The Transiting Exoplanet Survery Satellite (TESS)—being prepared here in 2018 by NASA technicians—gathered the original data that led to TOI-4465 b’s eventual confirmation as an exoplanet. Kim L. Shiflett/NASA Several key programs enabled this global effort, beginning with the original TESS data. Members of the TESS Follow-up Observing Program Subgroup 1 (TFOP SG1), the Unistellar Citizen Science Network (a group of amateurs and professionals who use computerized Unistellar telescopes), and the TESS Single Transit Planet Candidate (TSTPC) Working Group, led by University of New Mexico professor Diana Dragomir, also contributed. “What makes this collaboration effective is the infrastructure behind it. The Unistellar network provides standardized equipment and data processing pipelines, enabling high-quality contributions from citizen scientists. TFOP SG1 offers a global coordination framework that connects professional and amateur astronomers and observational facilities. The TSTPC Working Group, led by Professor Dragomir, brings together the detection and follow-up expertise needed for these challenging observations,” said Essack. The idea of using amateur astronomers’ telescopes to participate in scientific observations is not new. For example, in the 1980s, a team of observers around the world made up part of the International Halley Watch. It was created to get images of the comet from its first appearance in 1985 to mid-1986 from as many observers in as many parts of the world as possible. Other citizen-scientist efforts cropped up, with both amateur and professional observatories working together to study such objects as variable stars, blazars, supernovas, novas, occultations, eclipses, and much more. Today, people also get involved in other branches of science, including biology, health and medical research, ecology, and other crowdsourced research. Their participation allows scientists to get more data over longer periods of time and stretches research budgets in the process.",
    "published": "Sun, 06 Jul 2025 13:00:02 +0000",
    "author": "Carolyn Collins Petersen",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Vera Rubin: This Is How Far Engineers Go to Explore the Universe",
    "link": "https://spectrum.ieee.org/vera-rubin-engineering",
    "summary": "The Vera C. Rubin Observatory , in Chile, saw its first photon a few months ago, a monumental event 25 years in the making. Senior Editor Evan Ackerman’s journey to Chile, to see the observatory and talk with the team about the multiple and massive engineering challenges they overcame, itself took more than a year to plan. The visit was initiated by Italian photographer Enrico Sacchetti , who had arranged for exclusive access to the telescope. The story we wanted Ackerman to tell required more than a quick tour. So Sacchetti and Ackerman arranged to spend three nights on the summit of Cerro Pachón, sleeping during the day and then staying up late with the engineers and scientists as they worked to get Rubin “on sky.” Ackerman and Sacchetti didn’t know exactly what would happen while they were there. In some ways, they got lucky—the few days before first photon were full of frantic activity. In other ways, they weren’t so fortunate. The dome covering the telescope wasn’t working, the moon (bane of astronomers everywhere) was near full, and Sacchetti came down with an illness that nearly required him to be evacuated down the mountain. “Spotting a viscacha near the observatory is good luck for that night’s seeing.” —Evan Ackerman “For a slightly panicked 24 hours, I enlisted the help of some amateur photographers among the Rubin staff to make sure that we had all the photos that we’d need,” Ackerman says. You can see some of their excellent work in “ How the Rubin Observatory Will Reinvent Astronomy .” And Sacchetti recovered enough to get the crucial shots he wanted. The same characteristics that make Cerro Pachón the perfect place for observatories can make it a challenging place to work. For Sacchetti and Ackerman, as well as the Rubin staff, schlepping up to the 2,600-meter summit from sea level took some adjustment. Ackerman didn’t have much of a physical reaction to the altitude. But he learned that mentally, the thin air hits everyone a little differently. “I discovered a complete inability to remember schedules,” Ackerman recalls. “William O’Mullane, data-management project manager for Rubin, told me that for him, it’s feeling that he knows the answer to a question, but not what the answer actually is.” In addition to scheduled interviews with engineers and astronomers, Ackerman wandered around the control room, joining conversations that seemed interesting. The Rubin staff isn’t superstitious, but he nonetheless heard some rumors involving the local fauna. Viscachas, which are a type of chinchilla the size of a rabbit, are a good omen for astronomers at the Rubin observatory. Evan Ackerman “Spotting a viscacha near the observatory is good luck for that night’s seeing, as it should be. It looks like an aggressively cute cross between a squirrel and a rabbit, but it’s technically a kind of large chinchilla,” he says. Less cute are the Andean condors that live on the cliffs near the Southern Astrophysical Research Telescope, which is also located on Cerro Pachón. Seeing them in the air in the evening is a bad sign, Ackerman was told, which may be somewhat grounded in reality, since the thermals that the condors ride imply turbulent air around the mountain. Even the other “unlucky” parts of the visit helped make the story better. The full moon, while overpowering much of the sky, lit up the outside of the observatory and resulted in some fantastic nighttime photos. And the temporarily nonfunctional dome led to several in-depth conversations about how difficult it is to get all of these bespoke systems to work with one another, and helped Ackerman appreciate the complex job of the commissioning engineers.",
    "published": "Wed, 02 Jul 2025 20:26:42 +0000",
    "author": "Harry Goldstein",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Superconducting Motor Could Propel Electric Aircraft",
    "link": "https://spectrum.ieee.org/electric-aircraft-motor-hinetics",
    "summary": "Of the countless technologies invented over the past half century, high-temperature superconductors are among the most promising and yet also the most frustrating. Decades of research has yielded an assortment of materials that superconduct at temperatures as high as -140 °C (133 kelvins) at ambient pressure. And yet commercial applications have been elusive. Now, though, a couple of developments could finally push high-temperature superconductors into commercial use. One is the availability, at relatively moderate cost, of copper-oxide-based superconducting tape, which is being produced by a few companies for startups working on tokamak fusion reactors . The reactors use the superconducting tape, which is typically made of yttrium barium copper oxide, in powerful electromagnets. The other development involves a different group of startups that are using the tape to build electric motors with very high power-to-weight ratios, mainly for use in electric aircraft. Among that latter group of startups is Hinetics , formed in 2017 to commercialize research led by Kiruba Haran at the University of Illinois Urbana-Champaign. This past April, the company tested a prototype motor outfitted with superconducting rotor magnets. According to Haran, the tests, which included spinning a propeller in a laboratory setup, validated key components of the company’s designs for superconducting motors that will operate at power levels of 5 and 10 megawatts. Such levels would be high enough to power a regional passenger airliner with multiple motors. The work was funded in part by a grant from the Advanced Research Projects Agency–Energy (ARPA-E). “HTS [high temperature superconductors] are having a moment, because the costs are coming down rapidly, driven by all the work on fusion,” Haran says. “A lot of people are ramping up production, and new startups, and new capabilities, are coming into the market.” Hinetics is one of perhaps a dozen companies, large and small, trying to use high-temperature superconductors to build extremely efficient motors with very high power density. These include aerospace giant Airbus, which is working on a superconducting airliner under a program called ZEROe , as well as Toshiba, Raytheon, and U.K. startup HyFlux . However, Hinetics is taking an unusual approach. Common approaches to building a superconducting machine use the superconducting material for either the rotor or stator coils, or both. Typically, the coils are cooled with a liquid or gas kept at a sufficiently low temperature by an external cryocooling system. The fluid cools the superconducting coils by convection, by physically flowing through heat exchangers in contact with the coils and carrying away heat as it does so. The system has been used successfully in some experimental motors and generators, but it suffers from several fundamental problems. A big one is the need to circulate the cooling fluid through the rotor coils, which are embedded in a rotor assembly that is spinning at perhaps thousands of revolutions per minute. Another problem is that this approach requires a complicated cryocooling system that includes pumps, seals, gaskets, pipes, insulation, a rotary coupling that transfers the cryogen into and out of the rotor, and other components that can fail and that add considerable weight. The rotor coils in an experimental Hinetics electric motor are made of a high-temperature superconductor. They are cooled by a cryocooler that runs axially down the center of the motor. The rotor assembly and the cryocooler are enclosed within a vacuum vessel. Hinetics Hinetics’s Revolutionary Idea: Spin the Cryocooler Hinetics’s system, on the other hand, uses a self-contained cryocooler that is small enough to be attached to the rotor, and which spins along with it, eliminating the need to pass fluids into and out of a spinning vessel. With this arrangement, “you don’t have to immerse the superconductor into the fluid,” notes Laurent Pilon, an associate director for technology at ARPA-E. Instead, “there’s a cryocooler, and a cold connection, and you pull out the heat from the superconducting magnetic coils to the cryocooler, performing a refrigeration cycle. The beauty here is that it simplifies everything because now you just have the cryocooler that spins with the shaft.” In this configuration, the rotor assembly, including the coils, is cooled by conduction rather than convection. The rotor is installed within a vacuum chamber. Heat from the superconducting magnet assembly is transferred through a “thermal bus,” which is basically just a disk-shaped copper structure that conducts the heat to the cryocooler, which is attached to the other side of the copper disk. One of the challenges, Haran says, was finding a cryocooler small and light enough to spin at high rates and keep functioning while doing so. For its proof-of-concept unit, the Hinetics team used an off-the-shelf Stirling-cycle cooler from Sunpower . It can remove only 10 watts of heat from the rotor assembly but, in this configuration, that’s all that’s needed to keep the rotor coils superconducting, Haran says. One potential drawback of the system is that, because of this relatively low heat-removal capacity, the cryocooler takes a few hours to cool the superconducting magnet sufficiently to start operating. Future versions will reduce the period needed, according to Haran. And on the bright side, the low heat-removal rate means high efficiency, because the cooler has just enough power to maintain the low temperatures needed during operation, and not much excess capacity. To provide electric power to the spinning cryostat and rotor magnets the prototype used a slip ring. But future versions of the motor will use a wireless system, possibly based on inductive coupling, Haran says. Tests of Hinetics’s superconducting motor this past April validated the basic design and cleared the way for construction of more powerful units. Hinetics Applications on Ships Are Also Possible He opted not to make the stators superconducting, because in a typical configuration the stator is energized by an alternating-current (AC) waveform. Superconductors are only completely lossless for direct current. So the application of AC to superconducting coils in the stator would result in power losses that would require another cooling system to remove heat from the stator. Haran figures it’s not necessary. With superconductors just in the rotor coils, the motor will achieve efficiencies in the range of 98 to 99.5 percent, which is about four or five percentage points higher than what is realistically possible with a permanent-magnet synchronous motor. Haran also insists that the superconducting design would attain this high efficiency without any reduction in power density, a combination that’s hard to achieve in a conventional motor. Four or five percentage points might not seem like a lot, but it would matter in typical aviation applications, Pilon says, especially when coupled with higher power density. On its website, Hinetics claims that its motor has a continuous specific power of 10 kilowatts per kilogram , which would put the machine among the most power-dense units available, on a continuous-power basis. According to Haran, the next generation of the superconducting motor will achieve 40 kW/kg, which would be far higher than anything commercially available. Although aviation is the initial target, Haran sees potential applications in ship propulsion, where the motor’s high volumetric power density would be a draw. “What’s really exciting is that we are seeing a transformational new technology become practical,” he says. “Once you get to megawatts and low speed, anywhere you need high torque, this could be very interesting.”",
    "published": "Thu, 26 Jun 2025 12:00:03 +0000",
    "author": "Glenn Zorpette",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Another Plan to Test Satellite Deorbiting Takes Shape",
    "link": "https://spectrum.ieee.org/electrodynamic-tether-deborbit-satellite",
    "summary": "More and more satellites are being added to low Earth orbit (LEO) every month. As that number continues to increase, so do the risks of that critical area surrounding Earth becoming impassable , trapping us on the planet for the foreseeable future. Ideas from different labs have presented potential solutions to this problem, but one of the most promising, electrodynamic tethers (EDTs), have only now begun to be tested in space. A new CubeSat called the Spacecraft for Advanced Research and Cooperative Studies (SPARCS) mission from researchers at the Sharif University of Technology in Tehran hopes to contribute to that effort by testing an EDT and intersatellite communication system as well as collecting real-time data on the radiation environment of its orbital path. aspect_ratio SPARCS actually consists of two separate CubeSats. SPARCS-A is a 1U CubeSat primarily designed as a communications platform, with the mission design requiring it to talk to SPARCS-B, which is a 2U CubeSat that, in addition to the communication system, contains a EDT. That EDT, which can measure up to 12 meters in length, is deployed via a servomotor, with a camera watching to ensure proper deployment. EDTs are essentially giant poles with electric current running through them. They use this current, and the tiny magnetic field it produces, to push off of the Earth’s natural magnetic sphere using a property called the Lorentz force. This allows the satellite to adjust its orbit without the use of fuel, simply by orienting its EDT in a specific direction (which the EDT itself can assist with) and then using the Lorentz force to either push it up into a higher orbit, or—more significant for the purposes for technology demonstration—to slow the CubeSat down to a point where it can make a controlled entry into the atmosphere. Why Are EDTs Important for Satellites? That controlled-entry feature is why EDTs have garnered so much attention. Previous missions, such as KITE from JAXA and MiTEE from the University of Michigan, have already attempted to use EDTs to change their orbits. Unfortunately neither of those missions successfully utilized their EDT, though a follow-up mission called MiTEE-2 is in the works with an even larger EDT than SPARCS. The final piece of SPARCS’ kit is its dosimeter, which is intended to monitor the radiation environment of its orbit. As anyone familiar with spacecraft design knows, radiation hardening of electronics is absolutely critical to the success of a mission, but it is also expensive and time consuming, so it is best done at a minimal required level. Understanding the radiation environment of this popular orbital path can help future engineers make better, and hopefully less expensive, design decisions tailored to operation in this specific area. Engineers have already finalized the design for the mission and have run simulations showing its expected operations. They have now moved on to building an engineering model of the two CubeSats, allowing them to validate their design and test the real-world implementation before it is ready for launch. Given the current turmoil in that region of the world, there is a chance that conflict could put a halt to development of this system. But, if successfully tested and launched, the very first demonstration of an EDT system could be deployed in the not-too-distant future.",
    "published": "Mon, 23 Jun 2025 16:38:43 +0000",
    "author": "Andy Tomaswick",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "How the Rubin Observatory Will Reinvent Astronomy",
    "link": "https://spectrum.ieee.org/vera-rubin-observatory-first-images",
    "summary": "Night is falling on Cerro Pachón. A view of NSF-DOE Vera C. Rubin Observatory beneath the Milky Way galaxy. NSF-DOE Vera C. Rubin Observatory/H. Stockebrand Stray clouds reflect the last few rays of golden light as the sun dips below the horizon. I focus my camera across the summit to the westernmost peak of the mountain. Silhouetted within a dying blaze of red and orange light looms the sphinxlike shape of the Vera C. Rubin Observatory . “Not bad,” says William O’Mullane , the observatory’s deputy project manager, amateur photographer, and master of understatement. We watch as the sky fades through reds and purples to a deep, velvety black. It’s my first night in Chile. For O’Mullane, and hundreds of other astronomers and engineers, it’s the culmination of years of work, as the Rubin Observatory is finally ready to go “on sky.” Rubin is unlike any telescope ever built. Its exceptionally wide field of view, extreme speed, and massive digital camera will soon begin the 10-year Legacy Survey of Space and Time ( LSST ) across the entire southern sky. The result will be a high-resolution movie of how our solar system, galaxy, and universe change over time, along with hundreds of petabytes of data representing billions of celestial objects that have never been seen before. Stars begin to appear overhead, and O’Mullane and I pack up our cameras. It’s astronomical twilight, and after nearly 30 years, it’s time for Rubin to get to work. On 23 June, the Vera C. Rubin Observatory released the first batch of images to the public. One of them, shown here, features a small section of the Virgo cluster of galaxies. Visible are two prominent spiral galaxies (lower right), three merging galaxies (upper right), several groups of distant galaxies, and many stars in the Milky Way galaxy. Created from over 10 hours of observing data, this image represents less than 2 percent of the field of view of a single Rubin image. NSF-DOE Rubin Observatory A second image reveals clouds of gas and dust in the Trifid and Lagoon nebulae, located several thousand light-years from Earth. It combines 678 images taken by the Rubin Observatory over just seven hours, revealing faint details—like nebular gas and dust—that would otherwise be invisible. NSF-DOE Rubin Observatory Engineering the Simonyi Survey Telescope The top of Cerro Pachón is not a big place. Spanning about 1.5 kilometers at 2,647 meters of elevation, its three peaks are home to the Southern Astrophysical Research Telescope ( SOAR ), the Gemini South Telescope , and for the last decade, the Vera Rubin Observatory construction site. An hour’s flight north of the Chilean capital of Santiago, these foothills of the Andes offer uniquely stable weather. The Humboldt Current flows just offshore, cooling the surface temperature of the Pacific Ocean enough to minimize atmospheric moisture, resulting in some of the best “seeing,” as astronomers put it, in the world. It’s a complicated but exciting time to be visiting. It’s mid-April of 2025, and I’ve arrived just a few days before “first photon,” when light from the night sky will travel through the completed telescope and into its camera for the first time. In the control room on the second floor, engineers and astronomers make plans for the evening’s tests. O’Mullane and I head up into a high bay that contains the silvering chamber for the telescope’s mirrors and a clean room for the camera and its filters. Increasingly exhausting flights of stairs lead to the massive pier on which the telescope sits, and then up again into the dome. I suddenly feel very, very small. The Simonyi Survey Telescope towers above us—350 tonnes of steel and glass, nestled within the 30-meter-wide, 650-tonne dome. One final flight of stairs and we’re standing on the telescope platform. In its parked position, the telescope is pointed at horizon, meaning that it’s looking straight at me as I step in front of it and peer inside. The telescope’s enormous 8.4-meter primary mirror is so flawlessly reflective that it’s essentially invisible. Made of a single piece of low-expansion borosilicate glass covered in a 120-nanometer-thick layer of pure silver, the huge mirror acts as two different mirrors, with a more pronounced curvature toward the center. Standing this close means that different reflections of the mirrors, the camera, and the structure of the telescope all clash with one another in a way that shifts every time I move. I feel like if I can somehow look at it in just the right way, it will all make sense. But I can’t, and it doesn’t. I’m rescued from madness by O’Mullane snapping photos next to me. “Why?” I ask him. “You see this every day, right?” “This has never been seen before,” he tells me. “It’s the first time, ever, that the lens cover has been off the camera since it’s been on the telescope.” Indeed, deep inside the nested reflections I can see a blue circle, the r-band filter within the camera itself. As of today, it’s ready to capture the universe. Rubin’s Wide View Unveils the Universe Back down in the control room, I find director of construction Željko Ivezić. He’s just come up from the summit hotel, which has several dozen rooms for lucky visitors like myself, plus a few even luckier staff members. The rest of the staff commutes daily from the coastal town of La Serena, a 4-hour round trip. To me, the summit hotel seems luxurious for lodgings at the top of a remote mountain. But Ivezić has a slightly different perspective. “The European-funded telescopes,” he grumbles, “have swimming pools at their hotels. And they serve wine with lunch! Up here, there’s no alcohol. It’s an American thing.” He’s referring to the fact that Rubin is primarily funded by the U.S. National Science Foundation and the U.S. Department of Energy’s Office of Science , which have strict safety requirements. Originally, Rubin was intended to be a dark-matter survey telescope, to search for the 85 percent of the mass of the universe that we know exists but can’t identify. In the 1970s, astronomer Vera C. Rubin pioneered a spectroscopic method to measure the speed at which stars orbit around the centers of their galaxies, revealing motion that could be explained only by the presence of a halo of invisible mass at least five times the apparent mass of the galaxies themselves. Dark matter can warp the space around it enough that galaxies act as lenses, bending light from even more distant galaxies as it passes around them. It’s this gravitational lensing that the Rubin observatory was designed to detect on a massive scale. But once astronomers considered what else might be possible with a survey telescope that combined enormous light-collecting ability with a wide field of view, Rubin’s science mission rapidly expanded beyond dark matter. Trading the ability to focus on individual objects for a wide field of view that can see tens of thousands of objects at once provides a critical perspective for understanding our universe, says Ivezić. Rubin will complement other observatories like the Hubble Space Telescope and the James Webb Space Telescope . Hubble’s Wide Field Camera 3 and Webb’s Near Infrared Camera have fields of view of less than 0.05 square degrees each, equivalent to just a few percent of the size of a full moon. The upcoming Nancy Grace Roman Space Telescope will see a bit more, with a field of view of about one full moon. Rubin, by contrast, can image 9.6 square degrees at a time—about 45 full moons’ worth of sky. RELATED: A Trillion Rogue Planets and Not One Sun to Shine on Them That ultrawide view offers essential context, Ivezić explains. “My wife is American, but I’m from Croatia,” he says. “Whenever we go to Croatia, she meets many people. I asked her, ‘Did you learn more about Croatia by meeting many people very superficially, or because you know me very well?’ And she said, ‘You need both. I learn a lot from you, but you could be a weirdo, so I need a control sample.’ ” Rubin is providing that control sample, so that astronomers know just how weird whatever they’re looking at in more detail might be. Explore Rubin Observatory’s First Images With Skyviewer Rubin Observatory’s Skyviewer app lets you explore its stunning first images by interactively navigating a vast, detailed view of the cosmos — you can zoom in and out and move around to examine the rich tapestry of stars and galaxies in extraordinary detail. The area observed includes the southern region of the Virgo Cluster — approximately 55 million light-years from Earth — as well as closer stars in the Milky Way and much more distant galaxy groups. This image, built from over 3 trillion pixels of data collected in just seven nights, contains millions of galaxies. Eventually, the full Legacy Survey of Space and Time (LSST) will catalog about 20 billion galaxies of all types, and from all times in the history of the Universe. Every night, the telescope will take a thousand images, one every 34 seconds. After three or four nights, it’ll have the entire southern sky covered, and then it’ll start all over again. After a decade, Rubin will have taken more than 2 million images, generated 500 petabytes of data, and visited every object it can see at least 825 times. In addition to identifying an estimated 6 million bodies in our solar system, 17 billion stars in our galaxy, and 20 billion galaxies in our universe, Rubin’s rapid cadence means that it will be able to delve into the time domain, tracking how the entire southern sky changes on an almost daily basis. Cutting-Edge Technology Behind Rubin’s Speed Achieving these science goals meant pushing the technical envelope on nearly every aspect of the observatory. But what drove most of the design decisions is the speed at which Rubin needs to move (3.5 degrees per second)—the phrase most commonly used by the Rubin staff is “crazy fast.” Crazy fast movement is why the telescope looks the way it does. The squat arrangement of the mirrors and camera centralizes as much mass as possible. Rubin’s oversize supporting pier is mostly steel rather than mostly concrete so that the movement of the telescope doesn’t twist the entire pier. And then there’s the megawatt of power required to drive this whole thing, which comes from huge banks of capacitors slung under the telescope to prevent a brownout on the summit every 30 seconds all night long. Rubin is also unique in that it utilizes the largest digital camera ever built. The size of a small car and weighing 2,800 kilograms, the LSST camera captures 3.2-gigapixel images through six swappable color filters ranging from near infrared to near ultraviolet. The camera’s focal plane consists of 189 4K-by-4K charge-coupled devices grouped into 21 “rafts.” Every CCD is backed by 16 amplifiers that each read 1 million pixels, bringing the readout time for the entire sensor down to 2 seconds flat. Astronomy in the Time Domain As humans with tiny eyeballs and short lifespans who are more or less stranded on Earth, we have only the faintest idea of how dynamic our universe is. To us, the night sky seems mostly static and also mostly empty. This is emphatically not the case. In 1995, the Hubble Space Telescope pointed at a small and deliberately unremarkable part of the sky for a cumulative six days. The resulting image, called the Hubble Deep Field , revealed about 3,000 distant galaxies in an area that represented just one twenty-four-millionth of the sky. To observatories like Hubble, and now Rubin, the sky is crammed full of so many objects that it becomes a problem. As O’Mullane puts it, “There’s almost nothing not touching something.” One of Rubin’s biggest challenges will be deblending—­identifying and then separating things like stars and galaxies that appear to overlap. This has to be done carefully by using images taken through different filters to estimate how much of the brightness of a given pixel comes from each object. At first, Rubin won’t have this problem. At each location, the camera will capture one 30-second exposure before moving on. As Rubin returns to each location every three or four days, subsequent exposures will be combined in a process called coadding. In a coadded image, each pixel represents all of the data collected from that location in every previous image, which results in a much longer effective exposure time. The camera may record only a few photons from a distant galaxy in each individual image, but a few photons per image added together over 825 images yields much richer data. By the end of Rubin’s 10-year survey, the coadding process will generate images with as much detail as a typical Hubble image, but over the entire southern sky. A few lucky areas called “ deep drilling fields ” will receive even more attention, with each one getting a staggering 23,000 images or more. Rubin will add every object that it detects to its catalog, and over time, the catalog will provide a baseline of the night sky, which the observatory can then use to identify changes. Some of these changes will be movement—Rubin may see an object in one place, and then spot it in a different place some time later, which is how objects like near-Earth asteroids will be detected. But the vast majority of the changes will be in brightness rather than movement. RELATED: Three Steps to Stopping Killer Asteroids Every image that Rubin collects will be compared with a baseline image, and any change will automatically generate a software alert within 60 seconds of when the image was taken. Rubin’s wide field of view means that there will be a lot of these alerts—on the order of 10,000 per image, or 10 million alerts per night. Other automated systems will manage the alerts. Called alert brokers, they ingest the alert streams and filter them for the scientific community. If you’re an astronomer interested in Type Ia supernovae, for example, you can subscribe to an alert broker and set up a filter so that you’ll get notified when Rubin spots one. Many of these alerts will be triggered by variable stars, which cyclically change in brightness. Rubin is also expected to identify somewhere between 3 million and 4 million supernovae —that works out to over a thousand new supernovae for every night of observing. And the rest of the alerts? Nobody knows for sure, and that’s why the alerts have to go out so quickly, so that other telescopes can react to make deeper observations of what Rubin finds. Managing Rubin’s Vast Data Output After the data leaves Rubin’s camera, most of the processing will take place at the SLAC National Accelerator Laboratory in Menlo Park, Calif., over 9,000 kilometers from Cerro Pachón. It takes less than 10 seconds for an image to travel from the focal plane of the camera to SLAC, thanks to a 600-gigabit fiber connection from the summit to La Serena, and from there, a dedicated 100-gigabit line and a backup 40-gigabit line that connect to the Department of Energy’s science network in the United States. The 20 terabytes of data that Rubin will produce nightly makes this bandwidth necessary. “There’s a new image every 34 seconds,” O’Mullane tells me. “If I can’t deal with it fast enough, I start to get behind. So everything has to happen on the cadence of half a minute if I want to keep up with the data flow.” At SLAC, each image will be calibrated and cleaned up, including the removal of satellite trails. Rubin will see a lot of satellites, but since the satellites are unlikely to appear in the same place in every image, the impact on the data is expected to be minimal when the images are coadded. The processed image is compared with a baseline image and any alerts are sent out, by which time processing of the next image has already begun. As Rubin’s catalog of objects grows, astronomers will be able to query it in all kinds of useful ways. Want every image of a particular patch of sky? No problem. All the galaxies of a certain shape? A little trickier, but sure. Looking for 10,000 objects that are similar in some dimension to 10,000 other objects? That might take a while, but it’s still possible. Astronomers can even run their own code on the raw data. “Pretty much everyone in the astronomy community wants something from Rubin,” O’Mullane explains, “and so they want to make sure that we’re treating the data the right way. All of our code is public. It’s on GitHub . You can see what we’re doing, and if you’ve got a better solution, we’ll take it.” One better solution may involve AI. “I think as a community we’re struggling with how we do this,” says O’Mullane. “But it’s probably something we ought to do—curating the data in such a way that it’s consumable by machine learning, providing foundation models, that sort of thing.” The data management system is arguably as much of a critical component of the Rubin observatory as the telescope itself. While most telescopes make targeted observations that get distributed to only a few astronomers at a time, Rubin will make its data available to everyone within just a few days, which is a completely different way of doing astronomy. “We’ve essentially promised that we will take every image of everything that everyone has ever wanted to see,” explains Kevin Reil , Rubin observatory scientist. “If there’s data to be collected, we will try to collect it. And if you’re an astronomer somewhere, and you want an image of something, within three or four days we’ll give you one. It’s a colossal challenge to deliver something on this scale.” The more time I spend on the summit, the more I start to think that the science that we know Rubin will accomplish may be the least interesting part of its mission. And despite their best efforts, I get the sense that everyone I talk to is wildly understating the impact it will have on astronomy. The sheer volume of objects, the time domain, the 10 years of coadded data—what new science will all of that reveal? Astronomers have no idea, because we’ve never looked at the universe in this way before. To me, that’s the most fascinating part of what’s about to happen. Reil agrees. “You’ve been here,” he says. “You’ve seen what we’re doing. It’s a paradigm shift, a whole new way of doing things. It’s still a telescope and a camera, but we’re changing the world of astronomy. I don’t know how to capture—I mean, it’s the people, the intensity, the awesomeness of it. I want the world to understand the beauty of it all.” The Intersection of Science and Engineering Because nobody has built an observatory like Rubin before, there are a lot of things that aren’t working exactly as they should, and a few things that aren’t working at all. The most obvious of these is the dome. The capacitors that drive it blew a fuse the day before I arrived, and the electricians are off the summit for the weekend. The dome shutter can’t open either. Everyone I talk to takes this sort of thing in stride—they have to, because they’ve been troubleshooting issues like these for years. I sit down with Yousuke Utsumi , a camera operations scientist who exudes the mixture of excitement and exhaustion that I’m getting used to seeing in the younger staff. “Today is amazingly quiet,” he tells me. “I’m happy about that. But I’m also really tired. I just want to sleep.” Just yesterday, Utsumi says, they managed to finally solve a problem that the camera team had been struggling with for weeks—an intermittent fault in the camera cooling system that only seemed to happen when the telescope was moving. This was potentially a very serious problem, and Utsumi’s phone would alert him every time the fault occurred, over and over again in the middle of the night. The fault was finally traced to a cable within the telescope’s structure that used pins that were slightly too small, leading to a loose connection. Utsumi’s contract started in 2017 and was supposed to last three years, but he’s still here. “I wanted to see first photon,” he says. “I’m an astronomer. I’ve been working on this camera so that it can observe the universe. And I want to see that light, from those photons from distant galaxies.” This is something I’ve also been thinking about—those lonely photons traveling through space for billions of years, and within the coming days, a lucky few of them will land on the sensors Utsumi has been tending, and we’ll get to see them. He nods, smiling. “I don’t want to lose one, you know?” Rubin’s commissioning scientists have a unique role, working at the intersection of science and engineering to turn a bunch of custom parts into a functioning science instrument. Commissioning scientist Marina Pavlovic is a postdoc from Serbia with a background in the formation of supermassive black holes created by merging galaxies. “I came here last year as a volunteer,” she tells me. “My plan was to stay for three months, and 11 months later I’m a commissioning scientist. It’s crazy!” Pavlovic’s job is to help diagnose and troubleshoot whatever isn’t working quite right. And since most things aren’t working quite right, she’s been very busy. “I love when things need to be fixed because I am learning about the system more and more every time there’s a problem—every day is a new experience here.” I ask her what she’ll do next, once Rubin is up and running. “If you love commissioning instruments, that is something that you can do for the rest of your life, because there are always going to be new instruments,” she says. Before that happens, though, Pavlovic has to survive the next few weeks of going on sky. “It’s going to be so emotional. It’s going to be the beginning of a new era in astronomy, and knowing that you did it, that you made it happen, at least a tiny percent of it, that will be a priceless moment.” “I had to learn how to calm down to do this job,” she admits, “because sometimes I get too excited about things and I cannot sleep after that. But it’s okay. I started doing yoga, and it’s working.” From First Photon to First Light My stay on the summit comes to an end on 14 April, just a day before first photon, so as soon as I get home I check in with some of the engineers and astronomers that I met to see how things went. Guillem Megias Homar manages the adaptive optics system—232 actuators that flex the surfaces of the telescope’s three mirrors a few micrometers at a time to bring the image into perfect focus. Currently working on his Ph.D., he was born in 1997, one year after the Rubin project started. First photon, for him, went like this: “I was in the control room, sitting next to the camera team. We have a microphone on the camera, so that we can hear when the shutter is moving. And we hear the first click. And then all of a sudden, the image shows up on the screens in the control room, and it was just an explosion of emotions. All that we have been fighting for is finally a reality. We are on sky!” There were toasts (with sparkling apple juice, of course), and enough speeches that Megias Homar started to get impatient: “I was like, when can we start working? But it was only an hour, and then everything became much more quiet.” Another newly released image showing a small section of the Rubin Observatory’s total view of the Virgo cluster of galaxies. Visible are bright stars in the Milky Way galaxy shining in the foreground, and many distant galaxies in the background. NSF-DOE Rubin Observatory “It was satisfying to see that everything that we’d been building was finally working,” Victor Krabbendam , project manager for Rubin construction, tells me a few weeks later. “But some of us have been at this for so long that first photon became just one of many firsts.” Krabbendam has been with the observatory full-time for the last 21 years. “And the very moment you succeed with one thing, it’s time to be doing the next thing.” Since first photon, Rubin has been undergoing calibrations, collecting data for the first images that it’s now sharing with the world, and preparing to scale up to begin its survey. Operations will soon become routine, the commissioning scientists will move on, and eventually, Rubin will largely run itself, with just a few people at the observatory most nights. But for astronomers, the next 10 years will be anything but routine. “It’s going to be wildly different,” says Krabbendam. “Rubin will feed generations of scientists with trillions of data points of billions of objects. Explore the data. Harvest it. Develop your idea, see if it’s there. It’s going to be phenomenal.” This article appears in the July 2025 print issue as “Unveiling a Dynamic Universe.” Listen to a Conversation About the Rubin Observatory As part of an experiment with AI storytelling tools, author Evan Ackerman—who visited the Vera C. Rubin Observatory in Chile for four days this past April—fed over 14 hours of raw audio from his interviews and other reporting notes into NotebookLM , an AI-powered research assistant developed by Google. The result is a podcast-style audio experience that you can listen to here. While the script and voices are AI-generated, the conversation is grounded in Ackerman’s original reporting, and includes many details that did not appear in the article above. Ackerman reviewed and edited the audio to ensure accuracy, and there are minor corrections in the transcript. Let us know what you think of this experiment in AI narration. Your browser does not support the audio tag. See transcript 0:01: Today we’re taking a deep dive into the engineering marvel that is the Vera C. Rubin Observatory. 0:06: And and it really is a marvel. 0:08: This project pushes the limits, you know, not just for the science itself, like mapping the Milky Way or exploring dark energy, which is amazing, obviously. 0:16: But it’s also pushing the limits in just building the tools, the technical ingenuity, the, the sheer human collaboration needed to make something this complex actually work. 0:28: That’s what’s really fascinating to me. 0:29: Exactly. 0:30: And our mission for this deep dive is to go beyond the headlines, isn’t it? 0:33: We want to uncover those specific Kind of hidden technical details, the stuff from the audio interviews, the internal docs that really define this observatory. 0:41: The clever engineering solutions. 0:43: Yeah, the nuts and bolts, the answers to challenges nobody’s faced before, stuff that anyone who appreciates, you know, complex systems engineering would find really interesting. 0:53: Definitely. 0:54: So let’s start right at the heart of it. 0:57: The Simonyi survey telescope itself. 1:00: It’s this 350 ton machine inside a 600 ton dome, 30 m wide, huge. [The dome is closer to 650 tons.] 1:07: But the really astonishing part is its speed, speed and precision. 1:11: How do you even engineer something that massive to move that quickly while keeping everything stable down to the submicron level? [Micron level is more accurate.] 1:18: Well, that’s, that’s the core challenge, right? 1:20: This telescope, it can hit a top speed of 3.5 degrees per second. 1:24: Wow. 1:24: Yeah, and it can, you know, move to basically any point in the sky. 1:28: In under 20 seconds, 20 seconds, which makes it by far the fastest moving large telescope ever built, and the dome has to keep up. 1:36: So it’s also the fastest moving dome. 1:38: So the whole building is essentially racing along with the telescope. 1:41: Exactly. 1:41: And achieving that meant pretty much every component had to be custom designed like the pier holding the telescope up. 1:47: It’s mostly steel, not concrete. 1:49: Oh, interesting. 1:50: Why steel? 1:51: Specifically to stop it from twisting or vibrating when the telescope makes those incredibly fast moves. 1:56: Concrete just wouldn’t handle the torque the same way. [The pier is more steel than concrete, but it's still substantially concrete.] 1:59: OK, that makes sense. 1:59: And the power needed to accelerate and decelerate, you know, 300 tons, that must be absolutely massive. 2:06: Oh. 2:06: The instantaneous draw would be enormous. 2:09: How did they manage that without like dimming the lights on the whole. 2:12: Mountaintop every 30 seconds. 2:14: Yeah, that was a real concern, constant brownouts. 2:17: The solution was actually pretty elegant, involving these onboard capacitor banks. 2:22: Yep, slung right underneath the telescope structure. 2:24: They can slowly sip power from the grid, store it up over time, and then bam, discharge it really quickly for those big acceleration surges. 2:32: like a giant camera flash, but for moving a telescope, of yeah. 2:36: It smooths out the demand, preventing those grid disruptions. 2:40: Very clever engineering. 2:41: And beyond the movement, the mirrors themselves, equally critical, equally impressive, I imagine. 2:47: How did they tackle designing and making optics that large and precise? 2:51: Right, so the main mirror, the primary mirror, M1M3. 2:55: It’s a single piece of glass, 8.4 m across, low expansion borosilicate glass. 3:01: And that 8.4 m size, was that just like the biggest they could manage? 3:05: Well, it was a really crucial early decision. 3:07: The science absolutely required something at least 7 or 8 m wide. 3:13: But going much bigger, say 10 or 12 m, the logistics became almost impossible. 3:19: The big one was transport. 3:21: There’s a tunnel on the mountain road up to the summit, and a mirror, much larger than 8.4 m, physically wouldn’t fit through it. 3:28: No way. 3:29: So the tunnel actually set an upper limit on the mirror size. 3:31: Pretty much, yeah. 3:32: Building new road or some other complex transport method. 3:36: It would have added enormous cost and complexity. 3:38: So 8.4 m was that sweet spot between scientific need. 3:42: And, well, physical reality. 3:43: Wow, a real world constraint driving fundamental design. 3:47: And the mirror itself, you said M1 M3, it’s not just one simple mirror surface. 3:52: Correct. 3:52: It’s technically two mirror surfaces ground into that single piece of glass. 3:57: The central part has a more pronounced curvature. 3:59: It’s M1 and M3 combined. 4:00: OK, so fabricating that must have been tricky, especially with what, 10 tons of glass just in the center. 4:07: Oh, absolutely novel and complicated. 4:09: And these mirrors, they don’t support their own weight rigidly. 4:12: So just handling them during manufacturing, polishing, even getting them out of the casting mold, was a huge engineering challenge. 4:18: You can’t just lift it like a dinner plate. 4:20: Not quite, and then there’s maintaining it, re-silvering. 4:24: They hope to do it every 5 years. 4:26: Well, traditionally, big mirrors like this often need it more, like every 1.5 to 2 years, and it’s a risky weeks-long job. 4:34: You have to unbolt this priceless, unique piece of equipment, move it. 4:39: It’s nerve-wracking. 4:40: I bet. 4:40: And the silver coating itself is tiny, right? 4:42: Incredibly thin, just a few nanometers of pure silver. 4:46: It takes about 24 g for the whole giant surface, bonded with the adhesive layers that are measured in Angstroms. [It's closer to 26 grams of silver.] 4:52: It’s amazing precision. 4:54: So tying this together, you have this fast moving telescope, massive mirrors. 4:59: How do they keep everything perfectly focused, especially with multiple optical elements moving relative to each other? 5:04: that’s where these things called hexapods come in. 5:08: Really crucial bits of kit. 5:09: Hexapods, like six feet? 5:12: Sort of. 5:13: They’re mechanical systems with 6 adjustable arms or struts. 5:17: A simpler telescope might just have one maybe on the camera for basic focusing, but Ruben needs more because it’s got the 3 mirrors plus the camera. 5:25: Exactly. 5:26: So there’s a hexapod mounted on the secondary mirror, M2. 5:29: Its job is to keep M2 perfectly positioned relative to M1 and M3, compensating for tiny shifts or flexures. 5:36: And then there’s another hexapod on the camera itself. 5:39: That one adjusts the position and tilt of the entire camera’s sensor plane, the focal plane. 5:43: To get that perfect focus across the whole field of view. 5:46: And these hexapods move in 6 ways. 5:48: Yep, 6 degrees of freedom. 5:50: They can adjust position along the X, Y, and Z axis, and they can adjust rotation or tilt around those 3 axes as well. 5:57: It allows for incredibly fine adjustments, microp precision stuff. 6:00: So they’re constantly making these tiny tweaks as the telescope moves. 6:04: Constantly. 6:05: The active optics system uses them. 6:07: It calculates the needed corrections based on reference stars in the images, figures out how the mirror might be slightly bending. 6:13: And then tells the hexapods how to compensate. 6:15: It’s controlling like 26 g of silver coating on the mirror surface down to micron precision, using the mirror’s own natural bending modes. 6:24: It’s pretty wild. 6:24: Incredible. 6:25: OK, let’s pivot to the camera itself. 6:28: The LSST camera. 6:29: Big digital camera ever built, right? 6:31: Size of a small car, 2800 kg, captures 3.2 gigapixel images, just staggering numbers. 6:38: They really are, and the engineering inside is just as staggering. 6:41: That Socal plane where the light actually hits. 6:43: It’s made up of 189 individual CCD sensors. 6:47: Yep, 4K by 4K CCDs grouped into 21 rafts. 6:50: They give them like tiles, and each CCD has 16 amplifiers reading it out. 6:54: Why so many amplifiers? 6:56: Speed. 6:56: Each amplifier reads out about a million pixels. 6:59: By dividing the job up like that, they can read out the entire 3.2 gigapixel sensor in just 2 seconds. 7:04: 2 seconds for that much data. 7:05: Wow. 7:06: It’s essential for the survey’s rapid cadence. 7:09: Getting all those 189 CCDs perfectly flat must have been, I mean, are they delicate? 7:15: Unbelievably delicate. 7:16: They’re silicon wafers only 100 microns thick. 7:18: How thick is that really? 7:19: about the thickness of a human hair. 7:22: You could literally break one by breathing on it wrong, apparently, seriously, yeah. 7:26: And the challenge was aligning all 189 of them across this 650 millimeter wide focal plane, so the entire surface is flat. 7:34: To within just 24 microns, peak to valley. 7:37: 24 microns. 7:39: That sounds impossibly flat. 7:40: It’s like, imagine the entire United States. 7:43: Now imagine the difference between the lowest point and the highest point across the whole country was only 100 ft. 7:49: That’s the kind of relative flatness they achieved on the camera sensor. 7:52: OK, that puts it in perspective. 7:53: And why is that level of flatness so critical? 7:56: Because the telescope focuses light. 7:58: terribly. 7:58: It’s an F1.2 system, which means it has a very shallow depth of field. 8:02: If the sensors aren’t perfectly in that focal plane, even by a few microns, parts of the image go out of focus. 8:08: Gotcha. 8:08: And the pixels themselves, the little light buckets on the CCDs, are they special? 8:14: They’re custom made, definitely. 8:16: They settled on 10 micron pixels. 8:18: They figured anything smaller wouldn’t actually give them more useful scientific information. 8:23: Because you start hitting the limits of what the atmosphere and the telescope optics themselves can resolve. 8:28: So 10 microns was the optimal size, right? 8:31: balancing sensor tech with physical limits. 8:33: Now, keeping something that sensitive cool, that sounds like a nightmare, especially with all those electronics. 8:39: Oh, it’s a huge thermal engineering challenge. 8:42: The camera actually has 3 different cooling zones, 3 distinct temperature levels inside. 8:46: 3. 8:47: OK. 8:47: First, the CCDs themselves. 8:49: They need to be incredibly cold to minimize noise. 8:51: They operate at -125 °C. 8:54: -125C, how do they manage that? 8:57: With a special evaporator plate connected to the CCD rafts by flexible copper braids, which pulls heat away very effectively. 9:04: Then you’ve got the cameras, electronics, the readout boards and stuff. 9:07: They run cooler than room temp, but not that cold, around -50 °C. 9:12: OK. 9:12: That requires a separate liquid cooling loop delivered through these special vacuum insulated tubes to prevent heat leaks. 9:18: And the third zone. 9:19: That’s for the electronics in the utility trunk at the back of the camera. 9:23: They generate a fair bit of heat, about 3000 watts, like a few hair dryers running constantly. 9:27: Exactly. 9:28: So there’s a third liquid cooling system just for them, keeping them just slightly below the ambient room temperature in the dome. 9:35: And all this cooling, it’s not just to keep the parts from overheating, right? 9:39: It affects the images, absolutely critical for image quality. 9:44: If the outer surface of the camera body itself is even slightly warmer or cooler than the air inside the dome, it creates tiny air currents, turbulence right near the light path. 9:57: And that shows up as little wavy distortions in the images, messing up the precision. 10:02: So even the outside temperature of the camera matters. 10:04: Yep, it’s not just a camera. 10:06: They even have to monitor the heat generated by the motors that move the massive dome, because that heat could potentially cause enough air turbulence inside the dome to affect the image quality too. 10:16: That’s incredible attention to detail, and the camera interior is a vacuum you mentioned. 10:21: Yes, a very strong vacuum. 10:23: They pump it down about once a year, first using turbopumps spinning at like 80,000 RPM to get it down to about 102 tor. 10:32: Then they use other methods to get it down much further. 10:34: The 107 tor, that’s an ultra high vacuum. 10:37: Why the vacuum? 10:37: Keep frost off the cold part. 10:39: Exactly. 10:40: Prevents condensation and frost on those negatives when it 25 degree CCDs and generally ensures everything works optimally. 10:47: For normal operation, day to day, they use something called an ion pump. 10:51: How does that work? 10:52: It basically uses a strong electric field to ionize any stray gas molecules, mostly hydrogen, and trap them, effectively removing them from the vacuum space, very efficient for maintaining that ultra-high vacuum. 11:04: OK, so we have this incredible camera taking these massive images every few seconds. 11:08: Once those photons hit the CCDs and become digital signals, What happens next? 11:12: How does Ruben handle this absolute flood of data? 11:15: Yeah, this is where Ruben becomes, you know, almost as much a data processing machine as a telescope. 11:20: It’s designed for the data output. 11:22: So photons hit the CCDs, get converted to electrical signals. 11:27: Then, interestingly, they get converted back into light signals, photonic signals back to light. 11:32: Why? 11:33: To send them over fiber optics. 11:34: They’re about 6 kilometers of fiber optic cable running through the observatory building. 11:39: These signals go to FPGA boards, field programmable gate arrays in the data acquisition system. 11:46: OK. 11:46: And those FPGAs are basically assembling the complete image data packages from all the different CCDs and amplifiers. 11:53: That sounds like a fire hose of data leaving the camera. 11:56: How does it get off the mountain and where does it need to go? 11:58: And what about all the like operational data, temperatures, positions? 12:02: Good question. 12:03: There are really two main data streams all that telemetry you mentioned, sensor readings, temperatures, actuator positions, command set, everything about the state of the observatory that all gets collected into something called the Engineering facility database or EFD. 12:16: They use Kafka for transmitting that data. 12:18: It’s good for high volume streams, and store it in an influx database, which is great for time series data like sensor readings. 12:26: And astronomers can access that. 12:28: Well, there’s actually a duplicate copy of the EFD down at SLAC, the research center in California. 12:34: So scientists and engineers can query that copy without bogging down the live system running on the mountain. 12:40: Smart. 12:41: How much data are we talking about there? 12:43: For the engineering data, it’s about 20 gigabytes per night, and they plan to keep about a year’s worth online. 12:49: OK. 12:49: And the image data, the actual science pixels. 12:52: That takes a different path. [All of the data from Rubin to SLAC travels over the same network.] 12:53: It travels over dedicated high-speed network links, part of ESET, the research network, all the way from Chile, usually via Boca Raton, Florida, then Atlanta, before finally landing at SLAC. 13:05: And how fast does that need to be? 13:07: The goal is super fast. 13:09: They aim to get every image from the telescope in Chile to the data center at SLAC within 7 seconds of the shutter closing. 13:15: 7 seconds for gigabytes of data. 13:18: Yeah. 13:18: Sometimes network traffic bumps it up to maybe 30 seconds or so, but the target is 7. 13:23: It’s crucial for the next step, which is making sense of it all. 13:27: How do astronomers actually use this, this torrent of images and data? 13:30: Right. 13:31: This really changes how astronomy might be done. 13:33: Because Ruben is designed to generate alerts, real-time notifications about changes in the sky. 13:39: Alerts like, hey, something just exploded over here. 13:42: Pretty much. 13:42: It takes an image compared to the previous images of the same patch of sky and identifies anything that’s changed, appeared, disappeared, moved, gotten brighter, or fainter. 13:53: It expects to generate about 10,000 such alerts per image. 13:57: 10,000 per image, and they take an image every every 20 seconds or so on average, including readouts. [Images are taken every 34 seconds: a 30 second exposure, and then about 4 seconds for the telescope to move and settle.] 14:03: So you’re talking around 10 million alerts every single night. 14:06: 10 million a night. 14:07: Yep. 14:08: And the goal is to get those alerts out to the world within 60 seconds of the image being taken. 14:13: That’s insane. 14:14: What’s in an alert? 14:15: It contains the object’s position, brightness, how it’s changed, and little cut out images, postage stamps in the last 12 months of observations, so astronomers can quickly see the history. 14:24: But surely not all 10 million are real astronomical events satellites, cosmic rays. 14:30: Exactly. 14:31: The observatory itself does a first pass filter, masking out known issues like satellite trails, cosmic ray hits, atmospheric effects, with what they call real bogus stuff. 14:41: OK. 14:42: Then, this filtered stream of potentially real alerts goes out to external alert brokers. 14:49: These are systems run by different scientific groups around the world. 14:52: Yeah, and what did the brokers do? 14:53: They ingest the huge stream from Ruben and apply their own filters, based on what their particular community is interested in. 15:00: So an astronomer studying supernovae can subscribe to a broker that filters just for likely supernova candidates. 15:06: Another might filter for near Earth asteroids or specific types of variable stars. 15:12: so it makes the fire hose manageable. 15:13: You subscribe to the trickle you care about. 15:15: Precisely. 15:16: It’s a way to distribute the discovery potential across the whole community. 15:19: So it’s not just raw images astronomers get, but these alerts and presumably processed data too. 15:25: Oh yes. 15:26: Rubin provides the raw images, but also fully processed images, corrected for instrument effects, calibrated called processed visit images. 15:34: And also template images, deep combinations of previous images used for comparison. 15:38: And managing all that data, 15 petabytes you mentioned, how do you query that effectively? 15:44: They use a system called Keyserve. [The system is \"QServ.\"] 15:46: It’s a distributed relational database, custom built basically, designed to handle these enormous astronomical catalogs. 15:53: The goal is to let astronomers run complex searches across maybe 15 petabytes of catalog data and get answers back in minutes, not days or weeks. 16:02: And how do individual astronomers actually interact with it? 16:04: Do they download petabytes? 16:06: No, definitely not. 16:07: For general access, there’s a science platform, the front end of which runs on Google Cloud. 16:11: Users interact mainly through Jupiter notebooks. 16:13: Python notebooks, familiar territory for many scientists. 16:17: Exactly. 16:18: They can write arbitrary Python code, access the catalogs directly, do analysis for really heavy duty stuff like large scale batch processing. 16:27: They can submit jobs to the big compute cluster at SLEC, which sits right next to the data storage. 16:33: That’s much more efficient. 16:34: Have they tested this? 16:35: Can it handle thousands of astronomers hitting it at once? 16:38: They’ve done extensive testing, yeah, scaled it up with hundreds of users already, and they seem confident they can handle up to maybe 3000 simultaneous users without issues. 16:49: And a key point. 16:51: After an initial proprietary period for the main survey team, all the data and importantly, all the software algorithms used to process it become public. 17:00: Open source algorithms too. 17:01: Yes, the idea is, if the community can improve on their processing pipelines, they’re encouraged to contribute those solutions back. 17:08: It’s meant to be a community resource. 17:10: That open approach is fantastic, and even the way the images are presented visually has some deep thought behind it, doesn’t it? 17:15: You mentioned Robert Leptina’s perspective. 17:17: Yes, this is fascinating. 17:19: It’s about how you assign color to astronomical images, which usually combine data from different filters, like red, green, blue. 17:28: It’s not just about making pretty pictures, though they can be beautiful. 17:31: Right, it should be scientifically meaningful. 17:34: Exactly. 17:35: Lepton’s approach tries to preserve the inherent color information in the data. 17:40: Many methods saturate bright objects, making their centers just white blobs. 17:44: Yeah, you see that a lot. 17:46: His algorithm uses a different mathematical scaling, more like a logarithmic scale, that avoids this saturation. 17:52: It actually propagates the true color information back into the centers of bright stars and galaxies. 17:57: So, a galaxy that’s genuinely redder, because it’s red shifted, will actually look redder in the image, even in its bright core. 18:04: Precisely, in a scientifically meaningful way. 18:07: Even if our eyes wouldn’t perceive it quite that way directly through a telescope, the image renders the data faithfully. 18:13: It helps astronomers visually interpret the physics. 18:15: It’s a subtle but powerful detail in making the data useful. 18:19: It really is. 18:20: Beyond just taking pictures, I heard Ruben’s wide view is useful for something else entirely gravitational waves. 18:26: That’s right. 18:26: It’s a really cool synergy. 18:28: Gravitational wave detectors like Lego and Virgo, they detect ripples in space-time, often from emerging black holes or neutron stars, but they usually only narrow down the location to a relatively large patch of sky, maybe 10 square degrees or sometimes much more. 18:41: Ruben’s camera has a field of view of about 9.6 square degrees. 18:45: That’s huge for a telescope. 18:47: It almost perfectly matches the typical LIGO alert area. 18:51: so when LIGO sends an alert, Ruben can quickly scan that whole error box, maybe taking just a few pointings, looking for any new point of light. 19:00: The optical counterpart, the Killanova explosion, or whatever light accompany the gravitational wave event. 19:05: It’s a fantastic follow-up machine. 19:08: Now, stepping back a bit, this whole thing sounds like a colossal integration challenge. 19:13: A huge system of systems, many parts custom built, pushed to their limits. 19:18: What were some of those big integration hurdles, bringing it all together? 19:22: Yeah, classic system of systems is a good description. 19:25: And because nobody’s built an observatory quite like this before, a lot of the commissioning phase, getting everything working together involves figuring out the procedures as they go. 19:34: Learning by doing on a massive scale. 19:36: Pretty much. 19:37: They’re essentially, you know, teaching the system how to walk. 19:40: And there’s this constant tension, this balancing act. 19:43: Do you push forward, maybe build up some technical debt, things you know you’ll have to fix later, or do you stop and make sure every little issue is 100% perfect before moving on, especially with a huge distributed team? 19:54: I can imagine. 19:55: And you mentioned the dome motors earlier. 19:57: That discovery about heat affecting images sounds like a perfect example of unforeseen integration issues. 20:03: Exactly. 20:03: Marina Pavvich described that. 20:05: They ran the dome motors at full speed, something maybe nobody had done for extended periods in that exact configuration before, and realized, huh. 20:13: The heat these generate might actually cause enough air turbulence to mess with our image quality. 20:19: That’s the kind of thing you only find when you push the integrated system. 20:23: Lots of unexpected learning then. 20:25: What about interacting with the outside world? 20:27: Other telescopes, the atmosphere itself? 20:30: How does Ruben handle atmospheric distortion, for instance? 20:33: that’s another interesting point. 20:35: Many modern telescopes use lasers. 20:37: They shoot a laser up into the sky to create an artificial guide star, right, to measure. 20:42: Atmospheric turbulence. 20:43: Exactly. 20:44: Then they use deformable mirrors to correct for that turbulence in real time. 20:48: But Ruben cannot use a laser like that. 20:50: Why? 20:51: Because its field of view is enormous. 20:53: It sees such a wide patch of sky at once. 20:55: A single laser beam, even a pinpoint from another nearby observatory, would contaminate a huge fraction of Ruben’s image. 21:03: It would look like a giant streak across, you know, a quarter of the sky for Ruben. 21:06: Oh, wow. 21:07: OK. 21:08: Too much interference. 21:09: So how does it correct for the atmosphere? 21:11: Software. 21:12: It uses a really clever approach called forward modeling. 21:16: It looks at the shapes of hundreds of stars across its wide field of view in each image. 21:21: It knows what those stars should look like, theoretically. 21:25: Then it builds a complex mathematical model of the atmosphere’s distorting effect across the entire field of view that would explain the observed star shapes. 21:33: It iterates this model hundreds of times per image until it finds the best fit. [The model is created by iterating on the image data, but iteration is not necessary for every image.] 21:38: Then it uses that model to correct the image, removing the atmospheric blurring. 21:43: So it calculates the distortion instead of measuring it directly with a laser. 21:46: Essentially, yes. 21:48: Now, interestingly, there is an auxiliary telescope built alongside Ruben, specifically designed to measure atmospheric properties independently. 21:55: Oh, so they could use that data. 21:57: They could, but currently, they’re finding their software modeling approach using the science images themselves, works so well that they aren’t actively incorporating the data from the auxiliary telescope for that correction right now. 22:08: The software solution is proving powerful enough on its own. 22:11: Fascinating. 22:12: And they still have to coordinate with other telescopes about their lasers, right? 22:15: Oh yeah. 22:15: They have agreements about when nearby observatories can point their lasers, and sometimes Ruben might have to switch to a specific filter like the Iband, which is less sensitive to the laser. 22:25: Light if one is active nearby while they’re trying to focus. 22:28: So many interacting systems. 22:30: What an incredible journey through the engineering of Ruben. 22:33: Just the sheer ingenuity from the custom steel pier and the capacitor banks, the hexapods, that incredibly flat camera, the data systems. 22:43: It’s truly a machine built to push boundaries. 22:45: It really is. 22:46: And it’s important to remember, this isn’t just, you know, a bigger version of existing telescopes. 22:51: It’s a fundamentally different kind of machine. 22:53: How so? 22:54: By creating this massive all-purpose data set, imaging the entire southern sky over 800 times, cataloging maybe 40 billion objects, it shifts the paradigm. 23:07: Astronomy becomes less about individual scientists applying for time to point a telescope at one specific thing and more about statistical analysis, about mining this unprecedented ocean of data that Rubin provides to everyone. 23:21: So what does this all mean for us, for science? 23:24: Well, it’s a generational investment in fundamental discovery. 23:27: They’ve optimized this whole system, the telescope, the camera, the data pipeline. 23:31: For finding, quote, exactly the stuff we don’t know we’ll find. 23:34: Optimized for the unknown, I like that. 23:36: Yeah, we’re basically generating this incredible resource that will feed generations of astronomers and astrophysicists. 23:42: They’ll explore it, they’ll harvest discoveries from it, they’ll find patterns and objects and phenomena within billions and billions of data points that we can’t even conceive of yet. 23:50: And that really is the ultimate excitement, isn’t it? 23:53: Knowing that this monumental feat of engineering isn’t just answering old questions, but it’s poised to open up entirely new questions about the universe, questions we literally don’t know how to ask today. 24:04: Exactly. 24:05: So, for you, the listener, just think about that. 24:08: Consider the immense, the completely unknown discoveries that are waiting out there just waiting to be found when an entire universe of data becomes accessible like this. 24:16: What might we find? Back to top",
    "published": "Mon, 23 Jun 2025 04:01:02 +0000",
    "author": "Evan Ackerman",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Why Pilots Will Matter in the Age of Autonomous Planes",
    "link": "https://spectrum.ieee.org/autonomous-planes-certification",
    "summary": "In August 2001, an anonymous guest posted on the forum at Airliners.net, a popular aviation website. “How Long Will Pilots Be Needed?” they wondered, observing that “20 years or so down the road” technology could be so advanced that planes would fly themselves. “So would it really be useful for a person to go to college now and be an airline pilot if a few years down the road they will be phased out by technology?” Twenty-four years later, the basic technology required to make aircraft fly themselves exists, as evidenced by the fact that most commercial flights are flown largely on autopilot. Yet, the fundamental model of flying commercial aircraft hasn’t really changed. Passengers are still flown on large jetliners by two or more highly trained human pilots functioning as a team. The main reason why airlines are still decades away from pilotless planes boils down to the strict regulatory framework for aviation. At the heart of this regulation is certification—the process by which governmental authorities determine that an aircraft design is safe for flight. Even for conventional aircraft based on proven technologies, taking a concept from design through certification can require hundreds of millions of dollars and the better part of a decade. Tack on any novel technologies, such as the autonomy necessary to remove the pilot from the cockpit, and that process just gets longer and more expensive, with no guarantee of success. Nevertheless, and despite the daunting odds against them, a new generation of startups is making a run at certifying autonomous passenger and cargo aircraft, in the process laying the groundwork for the next chapter of aviation. Instead of airliners, these companies are starting with small aircraft: electric air taxis and single-engine planes that typically seat fewer than a dozen people. Not only are the associated capital costs more manageable on a startup’s budget, there’s also a persuasive safety case to be made: Small aircraft are still prone to the types of accidents that have been largely eliminated from commercial airline operations. According to statistics compiled by the Aircraft Owners and Pilots Association, around 300 people die each year in small plane and helicopter crashes in the United States alone. “Loss of control—mishandling the plane, usually as a result of disorientation or excessive workload—and controlled flight into terrain, [those] are the leading causes of accidents in small aircraft,” says Robert Rose , cofounder and CEO of Reliable Robotics , one of a few startups now working on retrofits that could enable Cessna Caravan planes to fly autonomously. A veteran of SpaceX and Tesla, Rose is adamant that “we, as a nation, possess the technology to prevent these accidents. If we can [autonomously] land a rocket on a small barge in the middle of the ocean, clearly we can find the centerline at an airport.” The economic case for autonomy in aviation While the safety argument for making small aircraft autonomous is a compelling one, the move is fundamentally rooted in economics. California-based Reliable Robotics and Massachusetts-based Merlin Labs are developing the commercial versions of their autonomous Caravans initially for the cargo feeder industry, which uses small airplanes to move packages to and from rural markets on behalf of carriers like FedEx and UPS. (Both companies also have military funding to develop autonomous aircraft.) Pilots for these feeder networks are typically flying alone, often at night and in bad weather, and their safety record is poor . This is a comparatively low-volume segment of the aviation industry, and there’s no money for second pilots and other risk mitigations typical of airline operations. Reliable Robotics is one of a couple of companies that are outfitting Cessna Caravan airplanes with advanced software to provide a high level of autonomy, for applications that include cargo transportation. Reliable Robotics The economic argument for autonomy is even more compelling in the emerging air-taxi industry , where hundreds of hopefuls—including a dozen or so serious contenders —are racing to develop electric vertical takeoff and landing aircraft to ferry passengers around crowded urban areas. Most of these eVTOLs are the size of helicopters, with space for just four or five passengers, and their proponents envision scores or even hundreds of them in the air over major cities, collectively moving millions of passengers annually. The concept is called urban air mobility , and in the speculative math that underpins it, eliminating the expense of a pilot and freeing up another seat for a paying passenger are seen as key to maximizing profits and scale. China has already certified a pilotless air taxi: the EH216-S, a two-seat multicopter developed by Guangzhou-based EHang that in March obtained initial approval from the Civil Aviation Administration of China for limited commercial sightseeing operations. However, many Western observers doubt that EHang’s design would pass muster by the U.S. Federal Aviation Administration (FAA) or the European Union Aviation Safety Agency (EASA), both of which have an especially conservative approach to safety. For that reason, most Western eVTOL makers have opted to develop piloted aircraft first and plan to introduce autonomous versions at some later date. They figure that seeking certification of novel electric aircraft designs, even without autonomy, is already a big ask of these regulators. A notable exception to this strategy is Wisk Aero , which began as a project funded by Google cofounder Larry Page and is now a wholly owned subsidiary of Boeing. In January 2022, the company declared that it would obtain FAA certification for its self-flying air taxi by the end of the decade and be operating close to 14 million flights annually within five years after that—a staggering ambition, given that the entire U.S. air traffic system currently manages around 16 million flights per year. While overheated expectations around urban air mobility have cooled considerably in the three years since that announcement, Wisk continues to forge ahead with its autonomous Generation 6 eVTOL , the company’s sixth aircraft design and the first it plans to certify for passenger-carrying operations. A mockup of Wisk’s sixth generation of electric vertical takeoff and landing aircraft was unveiled in October 2022. Wisk Importantly, Wisk, Reliable Robotics, and Merlin Labs are not just developing aircraft that they plan to certify eventually—they have already launched formal certification programs with the FAA (Merlin as part of a joint program with New Zealand’s Civil Aviation Authority). That means they’re working closely with the agency to define the rules and standards by which autonomous aircraft will be approved for commercial operations, blazing a trail for others to follow. The task is a daunting one, but the regulators and industry are not starting from scratch. Rather, they’re building on decades of certification experience and best practices that have helped to dramatically improve the safety of the aviation industry over its history. Aviation safety starts with certification Although the fatal January 2025 midair collision of an Army Black Hawk helicopter and an American Eagle CRJ700 near Washington, D.C.’s Reagan National Airport shook public confidence in the safety of the U.S. air transport system, commercial aviation remains a remarkably safe way to get around. According to researchers at MIT, the risk of a fatality from commercial air travel was just one per 13.7 million passenger boardings worldwide between 2018 and 2022. Fifty years earlier, the risk was an order of magnitude higher: one per 350,000 boardings between 1968 and 1977. There are many reasons for this great leap in safety, and the certification process is an important one. Today, a majority of aviation accidents are attributed to human error, but that’s not because people are inherently less reliable than aircraft. It’s because a systematic approach to design and testing has over the past several decades eliminated many of the mechanical problems that used to cause accidents routinely. In this context, the argument for enhancing safety through autonomy can be thought of as transferring even more responsibilities from highly variable humans to engineered systems that can be subjected to greater scrutiny. The overarching principle of certification is that the equipment and systems on an aircraft must be designed and installed so that they perform as intended during any foreseeable circumstances that they might encounter. “Perform as intended” includes not performing any unintended functions. An example of an unintended function is pushing the nose of an aircraft down past the level that a pilot can recover—that was the fatal result of a hidden software flaw that caused two crashes of the Boeing 737 Max and led to an extended grounding of the fleet while that oversight was remedied. Another key principle of certification is that the probability of a failure condition must be inversely proportional to its consequences. In other words, the more serious the impact of a failure, the more remote its chances of occurrence need to be. Aircraft are complex machines with millions of components that can and do fail, but many of these components can fail with no serious effects. For example, it’s no big deal if a lightbulb in the cabin burns out on a regular basis. Certifying authorities like the FAA generally accept a high probability of failure conditions that have a negligible impact on safety. However, failure conditions that are potentially catastrophic are required to be “extremely improbable.” Whether a failure condition is extremely improbable is fundamentally a qualitative evaluation that relies on the best judgments of engineers about how a system is likely to fail, supported by numerical assessments of the likelihood of failure. The critical systems on large commercial airliners are held to a numerical safety level of 10 -9 , meaning that catastrophic failures are expected no more than once in a billion flight hours (the equivalent of once in about 114,000 years of continuous operation). Achieving such vanishingly low probabilities may require expensive, heavy, and redundant systems, so regulators typically relax the safety expectations for small aircraft that carry fewer people. For example, a four-seat airplane like a Cessna 172 may only be held to a numerical safety level of 10 -6 , meaning that catastrophic failures are expected no more than once in a million flight hours. That said, aircraft manufacturers are free to design to higher standards, and Wisk is targeting the highest numerical safety level, 10 -9 , for its Gen 6 eVTOL. These basic principles of certification apply regardless of whether or not there’s a human pilot sitting in the cockpit, which is why developers of autonomous aircraft are confident they don’t need to completely reinvent the certification framework. “Everybody thinks that you need to think about the autonomy a different way than you would think about a piloted aircraft,” says Cindy Comer , Wisk’s vice president of certification, safety management systems and quality. “But really we just don’t get to pass off these failure conditions to a pilot. We still do our safety assessment the same way. We still may design our aircraft in a very similar way, but it may be to higher levels, it may be with more redundancy, or maybe we add equipment, because we no longer have that person that can sit there and see the things, grab the things, to pull the breakers. “So it drives our safety assessments to say, ‘Okay, we can’t put this on the pilot now. So what do we put it on?’” Key Aircraft Autonomy Projects Making autonomy certifiable presents unique challenges Answering that question—What do we put it on?—for every foreseeable failure condition is where the real work of certifying an autonomous aircraft comes in. Conventionally piloted aircraft may use the same overarching framework for certification, but they have the advantage of decades of certification history and precedent to fill in all of the details, down to requirements for such things as the actuation of the landing gear and the markings of instruments and placards. For the new systems on autonomous aircraft, many of those details must be negotiated with the FAA or some other certifying authority, which must be convinced in each instance that the proposed solution is at least as safe as the approach used on conventional aircraft. In the United States, applicants for type certificates have considerable flexibility in proposing how to meet the FAA’s safety goals. For each project incorporating novel technologies, the applicant and the agency agree on a set of requirements and standards, which becomes the “regulatory basis” for that aircraft. Theoretically, each autonomous-aircraft developer could have a very different regulatory basis, although in practice, the FAA looks for common ground. Nevertheless, the flexibility in this approach allows industry to explore a variety of possible ways to comply with a certification requirement before a solution is codified in regulation. Merlin Labs launched the flight test campaign for its certification-ready autonomy system in June 2024. The Merlin Pilot system is integrated directly onto the aircraft and is intended in the near term to reduce crew workload rather than fully replace pilots. Merlin Labs “Once you have the regulatory basis in place, then you need to come to agreement on how you’re going to demonstrate compliance to all of those regulations,” says Rose. “You can pull from existing standards, you can modify existing standards, or you can, in some cases, even just propose your own standards.” After agreeing upon the means of compliance, the applicant and regulator develop a detailed project plan that outlines the tests that will be performed and the reports—known as artifacts—that will be submitted to the regulator to support certification. For conventional piloted aircraft with a history of real-world operations, much of how those aircraft will function in the national airspace system is assumed. “Large commercial airplanes operate from airports around the world with relatively known and static equipment that helps them navigate and approach and land,” says Brian Yutko , until recently Wisk’s CEO (he now heads commercial airplane product development at Boeing). This infrastructure, he adds, has been established over decades and is reflected in the design of aircraft in ways that are often taken for granted. The existing system relies heavily on human pilots communicating with air traffic controllers over radio. Autonomous aircraft will require new concepts of operations, or “ConOps,” for how they will function, which could include using ground supervisors to handle radio calls, for example. In turn, the specifics of each ConOps will influence aircraft design requirements. According to Comer, crystallizing the ConOps at the beginning of the certification process “helps drive a common understanding of what you’re actually doing, and that may be different for every applicant with the FAA.” Basically, Wisk intends for its autonomous air taxi, which Yutko has likened to “a tram in the sky,” to fly along very specific and limited routes with predetermined emergency landing locations. Such a narrow set of tasks is an easier thing to automate than the varied and flexible operations performed by most small piloted aircraft today (or, for that matter, most self-driving cars). Meanwhile, human supervisors on the ground will monitor flights and communicate with air traffic control as required. Reliable Robotics’ automated Cessna Caravans will also have remote operators to handle communications with air traffic control, but they will fly over a much larger and more variable operating area. Because of this added complexity, Reliable has opted to split up the work of certifying its autonomous aircraft into chunks, beginning with certification of an advanced, always-on autopilot. This will assist but not replace the onboard pilot during all phases of flight, including landing as well as taxi and takeoff—which traditional autopilots are not capable of. Taking the pilot out of the cockpit will come as a follow-on certification project. Autonomous aircraft will do what autopilots can’t Proponents of autonomy like to point out that most commercial airline flights today are flown on autopilot from shortly after takeoff until touchdown or just before. It may therefore seem surprising that Europe’s aviation regulator, EASA, does not expect to see fully autonomous airliners until after 2050, while other regulators haven’t even speculated on a timeline for the shift. There are several reasons why “solving” autonomy in aircraft is not just a matter of expanding the functionality of existing autopilots. Basic flight control—moving flight-control surfaces and power inputs to make an aircraft fly how and where you want—is a relatively simple thing to automate, and most of the time, when everything goes as expected, autopilot works just fine. However, most existing autopilot systems assume there’s a human pilot, and for that reason they aren’t reliable enough to enable full autonomy. “There are autopilot actuators that go into aircraft today,” notes Reliable cofounder Rose. “But there’s a person sitting there monitoring them, and if [the actuators] do anything funny, then you click the off switch or actually, in many cases, you can just physically overpower the actuator. That’s not the case with ours—our actuators need to work all the time.” More challenging is solving for situations when everything does not go as expected, such as when another aircraft conflicts with the programmed flight path or a stray vehicle blocks the assigned runway. Autonomous-aircraft developers can’t count on a remote operator to manage these types of urgent, sudden conflicts, because the command-and-control (C2) link between the ground and the aircraft could also fail. “The aircraft, without having a [pilot] on board, needs to know where it is, and how to get where it’s going and how to avoid things along the way, over the length of its concept of operations,” says Yutko. Wisk’s Gen 6 flier will have the ability to safely complete a flight even if it loses both its C2 link and GPS signal immediately after takeoff, he says. “It turns out that if you don’t do that, then you start to impose really difficult technical requirements on the C2 link, or on your ability to maintain GPS.” In the speculative math that underpins urban air mobility, eliminating the expense of a pilot and freeing up another seat for a paying passenger are seen as key to maximizing profits and scale. Neither Wisk nor Reliable Robotics is using machine learning algorithms in its technical solutions, in large part because there’s no consensus on how to assure, to aviation’s exacting standards, the safety of such algorithms. These algorithms are frequently characterized as “nondeterministic,” meaning that their outputs can’t be reliably predicted from their inputs. Some autonomous-aircraft developers are incorporating artificial intelligence into their designs. Merlin Labs, for example, is developing natural-language-processing algorithms to communicate with air traffic control. For the most part, however, autonomous-aircraft developers aren’t counting on technology alone to solve the innumerable contingencies that can arise in flight—that’s where the ground operators come in. “We basically have taken everything that can be [automated] deterministically, and we’re making it deterministic,” Rose explains. “And all of the things that are…very hard to automate, that a human can do easily, then let the human do it.” Which raises the question: If humans are required to supervise autonomous aircraft, does the business case for them still hold up? Their developers say it does, but in ways that aren’t as simple as just striking “pilots” from the balance sheet. For example, those remote supervisors will need training, but that’s likely to be far less extensive and costly than the training required to competently fly an aircraft. For Reliable Robotics and other companies targeting cargo delivery, autonomy also promises to improve the efficiency of the existing cargo feeder network. “The reality is, in cargo aircraft, especially small cargo aircraft, pilots are super underutilized,” says Rose. Pilots at the feeder airlines may spend most of their day hanging out in a hotel room between their morning and evening flights. If people were instead managing autonomous cargo aircraft remotely, they could conceivably oversee additional flights across multiple time zones. “Our analysis has shown you can easily double the productivity of a pilot by putting them into our control center, potentially triple or quadruple the productivity [depending] on the mission set,” Rose says. Autonomous tech might eventually trickle up Even if companies like Wisk and Reliable Robotics succeed in certifying and commercializing their autonomous aircraft, human pilots still won’t face imminent extinction. Solving autonomy for one aircraft type and concept of operations doesn’t mean it’s solved for all types and concepts of operations. The technical, regulatory, and social barriers standing in the way of autonomous passenger jets are formidable. “I think for as long as we’re all alive, there will be piloted large commercial aircraft,” Yutko says. “If you solve Gen 6, you don’t get uncrewed large airplanes. You just don’t, and I’m not certain that we will in our lifetimes.” However, he does think it likely that some of the technologies now being developed at Wisk—such as navigating in the absence of GPS or techniques for automating emergency checklists—will find their way into conventionally piloted aircraft in ways that enhance safety. “I think those will be the types of things that we see in our lifetime benefiting big commercial transport applications, and I think it’s phenomenal,” adds Comer. As for whether it makes sense for anyone to embark upon a career as an airline pilot under the looming shadow of autonomy, it probably still does, at least for now. But check back in another 20 years. This story was updated on 25 June to reflect the fact that Merlin Labs has a joint certification program with New Zealand’s Civil Aviation Authority.",
    "published": "Tue, 17 Jun 2025 14:00:03 +0000",
    "author": "Elan Head",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Europe’s Plan for Faster Space Travel",
    "link": "https://spectrum.ieee.org/esa-nuclear-rocket",
    "summary": "Getting to Mars takes a really long time, about nine months using today’s rocket technology. This is because regular rocket engines burn fuel and oxygen together (like a car engine), but they’re not very efficient. The fundamental problem is that spacecraft must carry both fuel and oxidizer since there’s no air in space to support combustion. This creates a vicious circle: The more fuel you carry to go faster, the heavier your spacecraft becomes, requiring even more fuel to accelerate that extra weight. To go faster, you’d need massive amounts of fuel, making the rockets incredibly expensive and heavy. Current chemical propulsion systems are just about at their theoretical limits, with little room for improvement in efficiency. RELATED: Rockets for the Red Planet aspect_ratio Whilst NASA funding has been slashed by the Trump administration with no allocation for nuclear thermal propulsion and/or nuclear electric propulsion, scientists at the European Space Agency (ESA) have been studying nuclear propulsion. Here’s how it works: Instead of burning fuel with oxygen, a nuclear reactor heats up a propellant like hydrogen. The super-heated propellant then shoots out of the rocket nozzle, pushing the spacecraft forward. This method is much more efficient than chemical rockets. Revisiting Nuclear Rockets for Mars Nuclear rockets offer several key advantages, such as cutting Mars trip times in half—from nine months to about four to five months. The efficiency gains come from the fact that nuclear reactors produce far more energy per unit of fuel than chemical reactions. Surprisingly, astronauts would actually receive less harmful radiation on shorter trips, even though the engine itself produces radiation. This happens because space travelers are constantly bombarded by cosmic radiation during their journey, and cutting travel time in half significantly reduces their total exposure. These engines work best for big spacecraft that need to speed up and slow down dramatically, perfect for moon and Mars missions where rapid velocity changes of at least 25,000 km/h are required. The study, called “Alumni,” prioritized safety through careful design. The nuclear reactor only turns on when the spacecraft is far from Earth in a safe orbit. Before activation, the uranium fuel has very low radioactivity and isn’t toxic. Multiple radiation shields protect the crew during the short engine burns that last less than two hours. The reactor is designed never to return to Earth’s atmosphere. The research team spent over a year analyzing this technology and concluded it’s feasible for long-term development. However, there’s still significant work ahead, including laboratory testing of the new ceramic-metal reactor design, building safe testing facilities, and solving technical challenges like fuel sourcing and reactor restart systems. Nuclear thermal propulsion could revolutionize space travel, making missions to Mars and the moon faster and more practical. While the technology is promising and appears safe, it will take many years of development before we see nuclear-powered spacecraft heading to the Red Planet. It’s great to see Europe demonstrating that it has the expertise to develop this technology, potentially ushering in a new era of space exploration where distant worlds become more accessible than ever before.",
    "published": "Sat, 14 Jun 2025 13:00:03 +0000",
    "author": "Mark Thompson",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Three Steps to Stopping Killer Asteroids",
    "link": "https://spectrum.ieee.org/planetary-defense-killer-asteroids",
    "summary": "Impact was imminent. Occasional gasps arose as the asteroid took shape and a jagged, rocky surface filled the view. Then the images abruptly stopped. The mission control room at Johns Hopkins University Applied Physics Lab in Laurel, Md., erupted in cheers. “We have impact!” said the lead engineer, who gave a two-handed high five to a nearby colleague. Others waved their hands in the air in victory and slapped each other on the back. This had been a test, and humanity had passed it, taking one crucial step closer to protecting Earth from an asteroid impact. The test was the culmination of NASA’s Double Asteroid Redirection Test (DART) mission, for which I was the coordination lead. On 26 September 2022, the DART spacecraft had successfully crashed into Dimorphos, a roughly 150-meter-diameter asteroid that was 11 million kilometers from Earth. The collision nudged the asteroid and modified its trajectory. In 2022, NASA’s Double Asteroid Redirection Test slammed a golf-cart-size spacecraft, DART, into the near-Earth asteroid Dimorphos (1). DART first deployed a small observer craft, LICIACube (2), to observe the collision. DART’s impact nudged Dimorphos (3) enough to alter its future course (4). GyGinfographics; Source: NASA The celebrations in the control room were the culmination of years of effort to prove that the momentum from a golf-cart-size spacecraft can alter an asteroid’s future path. And DART’s collision with asteroid Dimorphos kicked off a new era in space exploration, in which technologies for planetary defense are now taking shape. If one day an asteroid like Dimorphos is discovered to be headed toward Earth, an interceptor craft like DART could collide with the asteroid years in advance to avert disaster. Here’s how that might work. Step 1: Find and Track Near-Earth Asteroids The first step in averting an asteroid impact with Earth is just to know what near-Earth objects (NEOs) are out there. The University of Hawaii’s Asteroid Terrestrial-impact Last Alert System (ATLAS) station, in Chile plays a critical role in these observations of NEOs, which are asteroids orbiting near Earth’s orbit. In late December, it detected a previously unknown NEO during a routine sweep of the skies. The asteroid was given the name 2024 YR4, following the standard astronomical convention for new objects. “2024 Y” represents the 24th-half-month of the year 2024 —that is, 16 to 31 December. The “R4” encodes the sequence of discovery —in this case, that it was the 117th object found during the year’s final couple of weeks. Hera This European Space Agency mission will rendezvous with the Didymos–Dimorphos asteroid system and study the aftereffects of NASA’s DART impact close up. Launch: 2024 Rendezvous: 2026 Until that point in the year, more than 3,000 NEOs had already been discovered. Nothing about 2024 YR4 initially stood out as concerning. It was a seemingly run-of-the-mill asteroid. However, further observations soon suggested it wasn’t ordinary at all. Throughout the first weeks of 2025, the probability of a 2024 YR4 collision with Earth kept growing. On 29 January, astronomers calculated its odds of eventual impact to be 1.3 percent. And in crossing the 1 percent threshold, 2024 YR4 triggered an alert from the International Asteroid Warning Network to the United Nations’ Office for Outer Space Affairs about the potential impact. Such alerts are posted publicly on the IAWN’s website . The 29 January notice assessed the regions of the planet at highest risk from 2024 YR4 (also known as its risk corridor), as well as the expected damage if the asteroid did crash into Earth. On average, an object of 2024 YR4’s size—estimated at 60 meters across—slams into our planet once every thousand years. It’s considered a “city-killer” asteroid—not big enough to trigger a mass extinction, like the estimated 10-km one that likely killed the dinosaurs, but still big enough to be deadly up to roughly 50 km from the impact location. Fortunately, by 24 February, further observations by telescopes across the globe had refined the asteroid’s trajectory enough to rule out near-term Earth impact. Yet when it comes to asteroids and Earth, there won’t always be such an uncomplicated, happy ending. Another asteroid that size or even larger will eventually be on a collision course to with the planet [see chart below]. The world’s space agencies track an estimated 95 percent of NEOs greater than 1 km in diameter. The International Asteroid Warning Network and a related Space Mission Planning Advisory Group (SMPAG) are global coordinating bodies that monitor these efforts. And thankfully, none of the giant NEOs tracked by the above pose an impact risk to Earth for at least the next hundred years. (Meanwhile, comet impacts with Earth are even rarer than those of asteroids.) But you can only track the NEOs that are known. And plenty of city-killer asteroids remain lurking and undiscovered, potentially still posing a real risk to life on the planet. In the 50-meter range, a meager 7 percent of NEOs have been found. That’s not for lack of trying. It’s just more difficult to find small asteroids because smaller asteroids appear dimmer than larger ones. NASA’s & FEMA’s 2024 Planetary-Defense Exercise Last year, NASA and the Federal Emergency Management Agency sponsored an Interagency Tabletop Exercise around a hypothetical asteroid impact threat. In the fictional scenario, telescope observations detected an NEO, yielding a 72 percent chance of the object colliding with Earth in 2038. I served as a facilitator for this tabletop exercise, which aimed to further discussion and opportunities to stress-test new approaches to a realistic “killer asteroid” scenario. One complicating factor we introduced was that the NEO’s size remained alarmingly difficult to pin down: Was it a 60-meter city killer? Or was it an 800-meter object that could devastate a country? If the latter, it would have risked the lives and livelihoods of more than 10 million people. To keep the exercise focused, we centered it around the hypothetical NEO’s detection—and the decisions and next actions that would follow. We tracked the unfolding discussions and decisions in the aftermath of detection. Several U.S. agencies and organizations participated, as did the U.N. Office of Outer Space Affairs and international partners. One of the key gaps identified was the limited readiness to quickly deploy space missions for reconnaissance of the asteroid threat and for preventing Earth impact. The scenario’s large uncertainties underscored the need for capabilities to rapidly obtain better information about the asteroid. “I know what I would prefer [to do],” said one anonymous participant quoted in the exercise’s quick-look report . “But Congress will tell us to wait.” — N.L.C. New hardware is clearly needed. Sometime soon, the Vera C. Rubin Observatory , in Chile, is expected to see first light. The observatory will survey the entire visible sky every few nights, through a 3,200-megapixel camera on an 8.4-meter telescope. No Earth-based telescope in the history of the NEO hunt can match its capabilities. Adding to our NEO search will be NASA’s NEO Surveyor , an infrared space telescope scheduled to launch as soon as 2027. Together, the two new facilities are expected to discover thousands of new-to-us near-Earth asteroids. For objects 140 meters and larger, the two telescopes will locate an anticipated 90 percent of the entire population. Once an NEO has been discovered, astronomers routinely track its orbit and extrapolate its trajectory over the coming century. So any NEO already on the books (for example, in the NASA or ESA database ) is quite likely to come with decades of warning. Ideally, that should leave ample time to develop and deploy a spacecraft to learn more about it and redirect the wayward space rock if necessary. Step 2: Send an NEO Reconnaissance Mission Imagine that the probability of 2024 YR4 colliding with Earth rose instead of fell, with the estimated impact to take place sometime in 2032. Here’s why that would have been especially worrying. Asteroid 2024 YR4’s elongated orbit made it unobservable from Earth after mid-May of this year. So we wouldn’t have been able to see it with even the most sensitive telescopes until its next swing through our region of the solar system—around June 2028. In that alternate universe, we would’ve had to wait three years to launch a reconnaissance mission to study the object up close. Only then would we have known the next steps to take to redirect the asteroid away from Earth before its fated visit four years later. As it happens, SMPAG held preliminary discussions about 2024 YR4 in late January and early February. However, because the asteroid’s risk of collision with Earth soon dwindled to zero, the group didn’t develop specific recommendations. Hayabusa2# The Japan Aerospace Exploration Agency has extended a previous mission (Hayabusa2) to encounter two more near-Earth asteroids over the next six years. Flyby: 2026 Rendezvous 2031 DART would have provided a foundation for a 2028 reconnaissance mission, as would NASA’s Lucy mission , which flew past the asteroid Dinkinesh in 2023. Reconnaissance flybys provide as little as a few precious seconds to capture the needed data about the target asteroid. Of course, inserting a reconnaissance craft into orbit around the asteroid would allow more detailed measurements. However, few NEO trajectories offer the opportunity for any maneuver other than a flyby—especially when time is of the essence. Whatever the trajectory, the most important question for a reconnaissance mission would be whether the asteroid was in fact on a course to collide with Earth in 2032. If so, where on the planet would it hit? That future impact location could potentially be narrowed down to within a hundred kilometers . The mission might also uncover some complications. For starters, we might discover that the asteroid is actually plural. Some 15 percent of NEOs are believed to have secondary objects orbiting them—they’re asteroids with moons. And some asteroids are essentially a flying jumble of rocks. Another wrinkle comes in determining the asteroid’s mass. We need to know the mass to calculate the damage it could cause on impact, as well as the oomph required to divert it. Unfortunately, the technology to measure the mass of a city-killer asteroid doesn’t exist. The mass of a larger, kilometer-size asteroid is measured by determining the gravitational pull on the reconnaissance spacecraft, but that trick doesn’t work for smaller asteroids. Right now, the best we can do is estimate the mass by measuring the asteroid’s physical size from closeup imaging during a flyby and then inferring the composition. These challenges will need to be mastered in time for the reconnaissance mission, as the spacecraft—traveling at up to 90,000 kilometers per hour—flies past the potentially irregularly shaped object or objects half-shrouded in darkness. So it probably makes sense to tackle those challenges now rather than waiting until an actual threat emerges. Step 3: Change NEO’s Course With Interceptor If the reconnaissance mission does conclude that a killer asteroid is on the way and narrows down the date of impact, then what? Returning to 2024 YR4, that might make 22 December 2032 a very bad day for one city-size region of the planet. Even if it fell in the ocean, we’d need to look at geological and oceanic computer models to forecast the tsunami risk. If that risk is small, then world leaders and NEO advisors might opt to let the asteroid proceed. On the other hand, if the asteroid is on course to strike a highly populated area, then launching a spacecraft to deflect the asteroid and prevent impact might be warranted. NEO Surveyor NASA’s infrared space telescope has been designed to detect and track near-Earth object (NEO) asteroids that are potentially hazardous to Earth. Launch: as early as 2027 Here, lessons from DART are instructive. For one thing, a spacecraft impact can pack only so much punch. It’s unclear whether a deflection spacecraft the size of DART would be able to nudge a 2024 YR4–like asteroid with enough force to avoid Earth. It’s also possible the impactor’s nudge could inadvertently cause it to land in an even worse spot, inflicting more damage. And if the asteroid is only weakly held together, a DART-like collision might break it into multiple, smaller rubble piles—one or more of which could still reach Earth. So any kind of deflection mission has to be carefully considered. Other asteroid defense technologies are also worth considering. These other options are still untested, but we might as well get started, when nothing’s yet at stake. If you have decades of lead time, for instance, a rendezvous spacecraft could be dispatched to orbit the killer asteroid and slowly and continually act on it. Researchers have suggested using such a spacecraft’s gravity to tug the asteroid off its path or ion-beam engines to gradually push it. The spacecraft could use one or both techniques over the span of years or decades to cause a large enough change in the asteroid’s trajectory to prevent Earth impact. But if time is short, there are far fewer options. If the situation is dire enough, with a monster asteroid likely heading for a populated area, then using a nuclear explosive to break up or divert the asteroid could be on the table. That’s the premise of the 1998 blockbuster Armageddon (as well as the 2021 Netflix satire Don’t Look Up ). Absurd, yes, but worth considering if you’re otherwise out of options. Of course, the whole idea of planetary defense is to have options and to do as much advance preparation as possible. A number of countries have planetary-defense missions currently in space or planned in the next few years. The ESA’s Hera mission launched last year and is on its way to rendezvous late next year with the asteroid system that DART struck, to investigate the aftermath of DART’s 2022 deflection test. The Japanese Aerospace Exploration Agency’s Hayabusa2 is set to fly by an NEO in 2026 and rendezvous with a different asteroid in 2031. It’s the next chapter to JAXA’s original Hayabusa2 mission, which brought back samples of the asteroid Ryugu in 2020. China plans to perform a kinetic impactor demonstration similar to DART, with an observer spacecraft to watch, scheduled to launch in 2027. And in 2029, a 340-meter asteroid called Apophis —after the Egyptian god of chaos and darkness—will pass within 32,000 km of Earth, which is closer than some geosynchronous satellites. This will happen on 13 April 2029—Friday the 13th, that is. Apophis won’t hit Earth, but its close pass has prompted the U.N. to designate 2029 the International Year of Asteroid Awareness and Planetary Defense . The asteroid will be bright enough to be seen by the naked eye across parts of Europe, Asia, and Africa. And NASA has redirected its OSIRIS-REx spacecraft (which returned samples of the asteroid Bennu to Earth in 2023) to rendezvous with Apophis. The renamed OSIRIS-APEX mission will give astronomers an important opportunity to further refine how we measure and characterize NEO asteroids. While NEO researchers will continue to collect new data and develop new insights and perspectives, leading toward, we hope, better and stronger planetary defense, one perennial will hold as true in the future as it does today: In this very high-stakes game, you never get to pick the asteroid. The asteroid always picks you. This article appears in the August 2025 print issue.",
    "published": "Wed, 11 Jun 2025 12:00:03 +0000",
    "author": "Nancy L. Chabot",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "Look for These 7 New Technologies at the Airport",
    "link": "https://spectrum.ieee.org/7-new-airport-tech",
    "summary": "Take a look around the airport during your travels this summer and you might spot a string of new technologies at every touchpoint: from pre-arrival, bag drop, and security to the moment you board the plane. In this new world, your face is your boarding pass, your electronic luggage tag transforms itself for each new flight, and gate scanners catch line cutters trying to sneak onto the plane early. It isn’t the future—it’s now. Each of the technologies to follow is in use at airports around the world today, transforming your journey-before-the-journey. Virtual queuing speeds up airport security As you pack the night before your trip, you ponder the age-old travel question: What time should I get to the airport? The right answer requires predicting the length of the security line. But at some airports, you no longer have to guess; in fact, you don’t have to wait in line at all. Instead, you can book ahead and choose a specific time for your security screening—so you can arrive right before your reserved slot, confident that you’ll be whisked to the front of the line, thanks to Copenhagen Optimization ’s Virtual Queuing system. Copenhagen Optimization’s machine learning models use linear regression, heuristic models, and other techniques to forecast the volume of passenger arrivals based on historical data. The system is integrated with airport programs to access flight schedules and passenger-flow data from boarding-pass scans, and it also takes in data from lidar sensors and cameras at security checkpoints, X-ray luggage scanners, and other areas. If a given day’s passenger volume ends up differing from historical projections, the platform can use real-time data from these inputs to adjust the Virtual Queuing time slots—and recommend that the airport make changes to security staffing and the number of open lanes. The Virtual Queuing system is constantly adjusting to flatten the passenger arrival curve, tactically redistributing demand across time slots to optimize resources and reduce congestion. While this system is doing the most, you as a passenger can do the least. Just book a time slot on your airport’s website or app, and get some extra sleep knowing you’ll waltz right up to the security check tomorrow morning. Electronic bag tags MCKIBILLO Checking a bag? Here’s another step you can take care of before you arrive: Skip the old-school paper tags and generate your own electronic Bagtag . This e-ink device (costing about US $80, or €70) looks like a traditional luggage-tag holder, but it can generate a new, paperless tag for each one of your flights. You provide your booking details through your airline’s app or the Bagtag app, and the Bagtag system then uses application programming interfaces and secure data protocols to retrieve the necessary information from the airline’s system: your name, flight details, the baggage you’re allowed, and the unique barcode that identifies your bag. The app uses this data to generate a digital tag. Hold your phone near your Bagtag, and it will transmit the encrypted tag data via Bluetooth or NFC. Simultaneously, your phone’s NFC antenna powers the battery-free Bagtag device. On the Bagtag itself, a low-power microcontroller decrypts the tag data and displays the digital tag on the e-ink screen. Once you’re at the airport, the tag can be scanned at the airline’s self-service bag drop or desk, just like a traditional paper tag. The device also contains an RFID chip that’s compatible with the luggage-tracking systems that some airlines are using, allowing your bag to be identified and tracked—even if it takes a different journey than you do. When you arrive at the airport, just drop that checked bag and make your way to the security area. Biometric boarding passes MCKIBILLO Over at security, you’ll need your boarding pass and ID. Compared with the old days of printing a physical slip from a kiosk, digital QR code boarding passes are quite handy—but what if you didn’t need anything besides your face? That’s the premise of Idemia Public Security ’s biometric boarding-pass technology. Instead of waiting in a queue for a security agent, you’ll approach a self-service kiosk or check-in point and insert your government-issued identification document, such as a driver’s license or passport. The system uses visible light, infrared, and ultraviolet imaging to analyze the document’s embedded security features and verify its authenticity. Then, computer-vision algorithms locate and extract the image of your face on the ID for identity verification. Next, it’s time for your close-up. High-resolution cameras within the system capture a live image of your face using 3D and infrared imaging. The system’s antispoofing technology prevents people from trying to trick the system with items like photos, videos, or masks. The technology compares your live image to the one extracted from your ID using facial-recognition algorithms. Each image is then converted into a compact biometric template—a mathematical representation of your facial features—and a similarity score is generated to confirm a match. Finally, the system checks your travel information against secure flight databases to make sure the ticket is valid and that you’re authorized to fly that day. Assuming all checks out, you’re cleared to head to the body scanners—with no biometric data retained by Idemia Public Security’s system. X-rays that can tell ecstasy from eczema meds MCKIBILLO While you pass through your security screening, that luggage you checked is undergoing its own screening—with a major new upgrade that can tell exactly what’s inside. Traditional scanners use one or a few X-ray sources and work by transmission, measuring the attenuation of the beam as it passes through the bag. These systems create a 2D “shadow” image based on differences in the amount and type of the materials inside. More recently, these systems have begun using computed tomography to scan the bag from all directions and to reconstruct 3D images of the objects inside. But even with CT, harmless objects may look similar to dangerous materials—which can lead to false positives and also require security staff to visually inspect the X-ray images or even bust open your luggage. By contrast, Smiths Detection ’s new X-ray diffraction machines measure the molecular structure of the items inside your bag to identify the exact materials—no human review required. The machine uses a multifocus X-ray tube to quickly scan a bag from various angles, measuring the way the radiation diffracts while switching the position of the focal spots every few microseconds. Then, it analyzes the diffraction patterns to determine the crystal structure and molecular composition of the objects inside the bag—building a “fingerprint” of each material that can much more finely differentiate threats, like explosives and drugs, from benign items. The system’s algorithms process this diffraction data and build a 3D spatial image, which allows real-time automated screening without the need for manual visual inspection by a human. After your bag passes through the X-ray diffraction machine without incident, it’s loaded into the cargo hold. Meanwhile, you’ve passed through your own scan at security and are ready to head toward your gate. Airport shops with no cashiers or checkout lanes MCKIBILLO While meandering over to your gate from security, you decide you could use a little pick-me-up. Just down the corridor is a convenience store with snacks, drinks, and other treats—but no cashiers. It’s a contactless shop that uses Just Walk Out technology by Amazon. As you enter the store with the tap of a credit card or mobile wallet, a scanner reads the card and assigns you a unique session identifier that will let the Just Walk Out system link your actions in the store to your payment. Overhead cameras track you by the top of your head, not your face, as you move through the store. The Just Walk Out system uses a deep-learning model to follow your movements and detect when you interact with items. In most cases, computer vision can identify a product you pick up simply based on the video feed, but sometimes weight sensors embedded in the shelves provide additional data to determine what you removed. The video and weight data are encoded as tokens, and a neural network processes those tokens in a way similar to how large language models encode text—determining the result of your actions to create a “virtual cart.” While you shop, the system continuously updates this cart: adding a can of soda when you pick it up, swapping one brand of gum for another if you change your mind, or removing that bag of chips if you put it back on the shelf. Once your shopping is complete, you can indeed just walk out with your soda and gum. The items you take will make up your finalized virtual cart, and the credit card you entered the store with will be charged as usual. (You can look up a receipt, if you want.) With provisions procured, it’s onward to the gate. Airport-cleaning robots MCKIBILLO As you amble toward the gate with your luggage and snacks, you promptly spill that soda you just bought. Cleanup in Terminal C! Along comes Avidbots’ Neo , a fully autonomous floor-scrubbing robot designed to clean commercial spaces like airports with minimal human intervention. When a Neo is first delivered to the airport, the robot performs a comprehensive scan of the various areas it will be cleaning using lidar and 3D depth cameras. Avidbots software processes the data to create a detailed map of the environment, including walls and other obstacles, and this serves as the foundation for Neo’s cleaning plans and navigation. Neo’s human overlords can use a touchscreen on the robot to direct it to the area that needs cleaning—either as part of scheduled upkeep, or when someone (ahem) spills their soda. The robot springs into action, and as it moves, it continuously locates itself within its map and plans its movements using data from wheel encoders, inertial measurement units, and a gyroscope. Neo also updates its map and adjusts its path in real time by using the lidar and depth cameras to detect any changes from its initial mapping, such as a translocated trash can or perambulating passengers. Then comes the scrubbing. Neo’s software plans the optimal path for cleaning a given area at this moment in time, adjusting the robot’s speed and steering as it moves along. A water-delivery system pumps and controls the flow of cleaning solution to the motorized brushes, whose speed and pressure can also be adjusted based on the surface the robot is cleaning. A powerful vacuum system collects the dirty water, and a flexible squeegee prevents slippery floors from being left behind. While the robot’s various sensors and planning algorithms continuously detect and avoid obstacles, any physical contact with the robot’s bumpers triggers an emergency stop. And if Neo finds itself in a situation it’s just not sure how to handle, the robot will stop and call for assistance from a human operator, who can review sensor data and camera feeds remotely to help it along. “Wrong group” plane-boarding alarm MCKIBILLO Your airport journey is coming to an end, and your real journey is about to begin. As you wait at the gate, you notice a fair number of your fellow passengers hovering to board even before the agent has made any announcements. And when boarding does begin, a surprising number of people hop in line. Could all these people really be in boarding groups 1 and 2? you wonder. If they’re not…they’ll get called out. American Airlines’ new boarding technology stops those pesky passengers who try to join the wrong boarding group and sneak onto the plane early. If one such passenger approaches the gate before their assigned group has been called, scanning their boarding pass will trigger an audible alert—notifying the airline crew, and everyone else for that matter. The passengers will be politely asked to wait to board. As they slink back into line, try not to look too smug. After all, it’s been a remarkably easy, tech-assisted journey through the airport today. This article appears in the July 2025 print issue as “A Walk Through 7 New Technologies at the Airport.”",
    "published": "Wed, 04 Jun 2025 16:00:04 +0000",
    "author": "Julianne Pepitone",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "How Ukraine’s Killer Drones Are Beating Russian Jamming",
    "link": "https://spectrum.ieee.org/ukraine-killer-drones",
    "summary": "Ukraine’s 1 June attack on multiple Russian military bases destroyed or damaged as many as 41 Russian aircraft, including some of the country’s most advanced bombers. Estimates of the sum total of the damage range from US $2 billion to $7 billion. Supposedly planned for a year and a half , the Ukrainian operation was exceptional in its sophistication: Ukrainian agents reportedly smuggled dozens of first-person-view attack drones into Russia on trucks, situating them close to the air bases where the target aircraft were vulnerable on tarmacs. The bases included one in Irkutsk, 4,300 kilometers from Ukraine, and another in south Murmansk, 1,800 km away. Remote pilots in Ukraine then launched the killer drones simultaneously. The far-reaching operation was being hailed as the most inventive and bold of the war so far. Indeed, IEEE Spectrum has been regularly covering the ascent of Ukraine’s military drone programs, both offensive and defensive , and for air , marine , and land missions. In this article, originally posted on April 6, we described another bold Ukrainian drone initiative, which was applying artificial intelligence-based navigational software to enable killer drones to navigate to targets even in the presence of heavy jamming. After the Estonian startup KrattWorks dispatched the first batch of its Ghost Dragon ISR quadcopters to Ukraine in mid-2022, the company’s officers thought they might have six months or so before they’d need to reconceive the drones in response to new battlefield realities. The 46-centimeter-wide flier was far more robust than the hobbyist-grade UAVs that came to define the early days of the drone war against Russia. But within a scant three months, the Estonian team realized their painstakingly fine-tuned device had already become obsolete. Related: Ukraine Tech Turns Combat into Real-Life “Game” Rapid advances in jamming and spoofing —the only efficient defense against drone attacks—set the team on an unceasing marathon of innovation. Its latest technology is a neural-network-driven optical navigation system, which allows the drone to continue its mission even when all radio and satellite-navigation links are jammed. It began tests in Ukraine in December, part of a trend toward jam-resistant, autonomous UAVs (uncrewed aerial vehicles). The new fliers herald yet another phase in the unending struggle that pits drones against the jamming and spoofing of electronic warfare, which aims to sever links between drones and their operators. There are now tens of thousands of jammers straddling the front lines of the war, defending against drones that are not just killing soldiers but also destroying armored vehicles, other drones, industrial infrastructure , and even tanks. During tests near Kyiv, Ukraine, in 2024, a technician prepared to release a drone outfitted with software by Auterion. Justyna Mielnikiewicz “The situation with electronic warfare is moving extremely fast,” says Martin Karmin, KrattWorks’ cofounder and chief operations officer. “We have to constantly iterate. It’s like a cat-and-mouse game.” I met Karmin at the company’s headquarters in the outskirts of Estonia’s capital, Tallinn. Just a couple of hundred kilometers to the east is the tiny nation’s border with Russia, its former oppressor. At 38, Karmin is barely old enough to remember what life was like under Russian rule, but he’s heard plenty. He and his colleagues, most of them volunteer members of the Estonian Defense League , have “no illusions” about Russia , he says with a shrug. His company is as much about arming Estonia as it is about helping Ukraine, he acknowledges. Estonia is not officially at war with Russia, of course, but regions around the border between the two countries have for years been subjected to persistent jamming of satellite-based navigation systems, such as the European Union’s Galileo satellites , forcing occasional flight cancellations at Tartu airport. In November, satellite imagery revealed that Russia is expanding its military bases along the Baltic states’ borders. “We are a small country,” Karmin says. “Innovation is our only chance.” Navigating by Neural Network In KrattWorks’ spacious, white-walled workshop, a handful of engineers are testing software. On the large ocher desk that dominates the room, a selection of KrattWorks’ devices is on display, including a couple of fixed-wing, smoke-colored UAVs designed to serve as aerial decoys, and the Ghost Dragon ISR quadcopter , the company’s flagship product. Now in its third generation, the Ghost Dragon has come a long way since 2022. Its original command-and-control-band radio was quickly replaced with a smart frequency-hopping system that constantly scans the available spectrum, looking for bands that aren’t jammed. It allows operators to switch among six radio-frequency bands to maintain control and also send back video even in the face of hostile jamming. The Ghost Dragon reconnaissance drone from Krattworks can navigate autonomously, by detecting landmarks as it flies over them. KrattWorks The drone’s dual-band satellite-navigation receiver can switch among the four main satellite positioning services: GPS, Galileo , China’s BeiDou, and Russia’s GLONASS. It’s been augmented with a spoof-proof algorithm that compares the satellite-navigation input with data from onboard sensors. The system provides protection against sophisticated spoofing attacks that attempt to trick drones into self-destruction by persuading them they’re flying at a much higher altitude than they actually are. At the heart of the quadcopter’s matte grey body is a machine-vision-enabled computer running a 1-gigahertz Arm processor that provides the Ghost Dragon with its latest superpower: the ability to navigate autonomously, without access to any global navigation satellite system (GNSS). To do that, the computer runs a neural network that, like an old-fashioned traveler, compares views of landmarks with positions on a map to determine its position. More precisely, the drone uses real-time views from a downward-facing optical camera, comparing them against stored satellite images, to determine its position. A promotional video from Krattworks depicts scenarios in which the company’s drones augment soldiers on offensive maneuvers. KrattWorks “Even if it gets lost, it can recognize some patterns, like crossroads, and update its position,” Karmin says. “It can make its own decisions, somewhat, either to return home or to fly through the jamming bubble until it can reestablish the GNSS link again.” Designing Drones for High Lethality per Cost Just as machine guns and tanks defined the First World War, drones have become emblematic of Ukraine’s struggle against Russia. It was the besieged Ukraine that first turned the concept of a military drone on its head. Instead of Predators and Reapers worth tens of millions of dollars each, Ukraine began purchasing huge numbers of off-the-shelf fliers worth a few hundred dollars apiece—the kind used by filmmakers and enthusiasts—and turned them into highly lethal weapons. A recent New York Times investigation found that drones account for 70 percent of deaths and injuries in the ongoing conflict. “We have much less artillery than Russia, so we had to compensate with drones,” says Serhii Skoryk , commercial director at Kvertus , a Kyiv-based electronic-warfare company. “A missile is worth perhaps a million dollars and can kill maybe 12 or 20 people. But for one million dollars, you can buy 10,000 drones, put four grenades on each, and they will kill 1,000 or even 2,000 people or destroy 200 tanks.” Near the Russian border in Kharkiv Oblast, a Ukrainian soldier prepared first-person-view drones for an attack on 16 January 2025. Jose Colon/Anadolu/Getty Images Electronic warfare techniques such as jamming and spoofing aim to neutralize the drone threat. A drone that gets jammed and loses contact with its pilot and also loses its spatial bearings will either crash or fly off randomly until its battery dies. According to the Royal United Services Institute , a U.K. defense think tank, Ukraine may be losing about 10,000 drones per month, mostly due to jamming. That number includes explosives-laden kamikaze drones that don’t reach their targets, as well as surveillance and reconnaissance drones like KrattWorks’ Ghost Dragon, meant for longer service. “Drones have become a consumable item,” says Karmin. “You will get maybe 10 or 15 missions out of a reconnaissance drone, and then it has to be already paid off because you will lose it sooner or later.” Russia took an unexpected step in the summer of 2024, ditching sophisticated wireless control in favor of hard-wired drones fitted with spools of optical fiber. Tech minds on both sides of the conflict have therefore been working hard to circumvent electronic defenses. Russia took an unexpected step starting in early 2024, deploying hard-wired drones fitted with spools of optical fiber. Like a twisted variation on a child’s kite, the lethal UAVs can venture 20 or more kilometers away from the controller, the hair-thin fiber floating behind them, providing an unjammable connection. “Right now, there is no protection against fiber-optic drones,” Vadym Burukin , cofounder of the Ukrainian drone startup Huless , tells IEEE Spectrum . “The Russians scaled this solution pretty fast, and now they are saturating the battle front with these drones. It’s a huge problem for Ukraine.” One way that drone operators can defeat electronic jamming is by communicating with their drone via a fiber optic line that pays out of a spool as the drone flies. This is a tactic favored by Russian units, although this particular first-person-view drone is Ukrainian. It was demonstrated near Kyiv on 29 January 2025. Efrem Lukatsky/AP Ukraine, too, has experimented with optical fiber, but the technology didn’t take off, as it were. “The optical fiber costs upwards from $500, which is, in many cases, more than the drone itself,” Burukin says. “If you use it in a drone that carries explosives, you lose some of that capacity because you have the weight of the cable.” The extra weight also means less capacity for better-quality cameras, sensors, and computers in reconnaissance drones. Small Drones May Soon Be Making Kill-or-No-Kill Decisions Instead, Ukraine sees the future in autonomous navigation. This past July, kamikaze drones equipped with an autonomous navigation system from U.S. supplier Auterion destroyed a column of Russian tanks fitted with jamming devices. “It was really hard to strike these tanks because they were jamming everything,” says Burukin. “The drones with the autopilot were the only equipment that could stop them.” Auterion’s “terminal guidance” system uses known landmarks to orient a drone as it seeks out a target. Auterion The technology used to hit those tanks is called terminal guidance and is the first step toward smart, fully autonomous drones, according to Auterion’s CEO, Lorenz Meier. The system allows the drone to directly overcome the jamming whether the protected target is a tank, a trench, or a military airfield. “If you lock on the target from, let’s say, a kilometer away and you get jammed as you approach the target, it doesn’t matter,” Meier says in an interview. “You’re not losing the target as a manual operator would.” The visual navigation technology trialed by KrattWorks is the next step and an innovation that has only reached the battlefield this year. Meier expects that by the end of 2025, firms including his own will introduce fully autonomous solutions encompassing visual navigation to overcome GPS jamming, as well as terminal guidance and smart target recognition. “The operator would only decide the area where to strike, but the decision about the target is made by the drone,” Meier explains. “It’s already done with guided shells, but with drones you can do that at mass scale and over much greater distances.” Auterion, founded in 2017 to produce drone software for civilian applications such as grocery delivery, threw itself into the war effort in early 2024, motivated by a desire to equip democratic countries with technologies to help them defend themselves against authoritarian regimes. Since then, the company has made rapid strides, working closely with Ukrainian drone makers and troops. “A missile worth perhaps a million dollars can kill maybe 12 or 20 people. But for one million dollars, you can buy 10,000 drones, put four grenades on each, and they will kill 1,000 or even 2,000 people or destroy 200 tanks.” —Serhii Skoryk, Kvertus But purchasing Western equipment is, in the long term, not affordable for Ukraine, a country with a per capita GDP of US $5,760 —much lower than the European average of $38,270 . Fortunately, Ukraine can tap its engineering workforce, which is among the largest in Europe. Before the war, Ukraine was a go-to place for Western companies looking to set up IT- and software-development centers. Many of these workers have since joined Ukraine’s DIY military-technician (“miltech”) development movement. An engineer and founder at a Ukrainian startup that produces long-range kamikaze drones, who didn’t want to be named because of security concerns, told Spectrum that the company began developing its own computers and autonomous navigation software for target tracking “just to keep the price down.” The engineer said Ukrainian startups offer advanced military-drone technology at a price that is a small fraction of what established competitors in the West are charging. Within three years of the February 2022 Russian invasion, Ukraine produced a world-class defense-tech ecosystem that is not only attracting Western innovators into its fold, but also regularly surpassing them. The keys to Ukraine’s success are rapid iterations and close cooperation with frontline troops. It’s a formula that’s working for Auterion as well. “If you want to build a leading product, you need to be where the product is needed the most,” says Meier. “That’s why we’re in Ukraine.” Burukin, from Ukrainian startup Huless, believes that autonomy will play a bigger role in the future of drone warfare than Russia’s optical fibers will. Autonomous drones not only evade jamming, but their range is limited only by their battery storage. They also can carry more explosives or better cameras and sensors than the wired drones can. On top of that, they don’t place high demands on their operators. “In the perfect world, the drone should take off, fly, find the target, strike it, and report back on the task,” Burukin says. “That’s where the development is heading.” The cat-and-mouse game is nowhere near over. Companies including KrattWorks are already thinking about the next innovation that would make drone warfare cheaper and more lethal. By creating a drone mesh network, for example, they could send a sophisticated intelligence, surveillance, and reconnaissance drone followed by a swarm of simpler kamikaze drones to find and attack a target using visual navigation. “You can send, like, 10 drones, but because they can fly themselves, you don’t need a superskilled operator controlling every single one of these,” notes KrattWorks’ Karmin, who keeps tabs on tech developments in Ukraine with a mixture of professional interest, personal empathy, and foreboding. Rarely does a day go by that he does not think about the expanding Russian military presence near Estonia’s eastern borders. “We don’t have a lot of people in Estonia,” he says. “We will never have enough skilled drone pilots. We must find another way.”",
    "published": "Mon, 02 Jun 2025 14:00:00 +0000",
    "author": "Tereza Pultarova",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  },
  {
    "title": "This Little Mars Rover Stayed Home",
    "link": "https://spectrum.ieee.org/mars-pathfinder-rover",
    "summary": "As a mere earthling, I remember watching in fascination as Sojourner sent back photos of the Martian surface during the summer of 1997. I was not alone. The servers at NASA’s Jet Propulsion Lab slowed to a crawl when they got more than 47 million hits (a record number!) from people attempting to download those early images of the Red Planet. To be fair, it was the late 1990s, the Internet was still young, and most people were using dial-up modems. By the end of the 83-day mission, Sojourner had sent back 550 photos and performed more than 15 chemical analyses of Martian rocks and soil. Sojourner , of course, remains on Mars. Pictured here is Marie Curie, its twin. Functionally identical, either one of the rovers could have made the voyage to Mars, but one of them was bound to become the famous face of the mission, while the other was destined to be left behind in obscurity. Did I write this piece because I feel a little bad for Marie Curie ? Maybe. But it also gave me a chance to revisit this pioneering Mars mission, which established that robots could effectively explore the surface of planets and captivate the public imagination. Sojourner ’s sojourn on Mars On 4 July 1997, the Mars Pathfinder parachuted through the Martian atmosphere and bounced about 15 times on glorified airbags before finally coming to a rest. The lander, renamed the Carl Sagan Memorial Station , carried precious cargo stowed inside. The next day, after the airbags retracted, the solar-powered Sojourner eased its way down the ramp, the first human-made vehicle to roll around on the surface of another planet. (It wasn’t the first extraterrestrial body, though. The Soviet Lunokhod rovers conducted two successful missions on the moon in 1970 and 1973. The Soviets had also landed a rover on Mars back in 1971, but communication was lost before the PROP-M ever deployed.) This giant sandbox at JPL provided Marie Curie with an approximation of Martian terrain. Mike Nelson/AFP/Getty Images The six-wheeled, 10.6-kilogram, microwave-oven-size Sojourner was equipped with three low-resolution cameras (two on the front for black-and-white images and a color camera on the rear), a laser hazard–avoidance system, an alpha-proton X-ray spectrometer, experiments for testing wheel abrasion and material adherence, and several accelerometers. The robot also demonstrated the value of the six-wheeled “rocker-bogie” suspension system that became NASA’s go-to design for all later Mars rovers. Sojourner never roamed more than about 12 meters from the lander due to the limited range of its radio . Pathfinder had landed in Ares Vallis , an assumed ancient floodplain chosen because of the wide variety of rocks present. Scientists hoped to confirm the past existence of water on the surface of Mars. Sojourner did discover rounded pebbles that suggested running water, and later missions confirmed it. A highlight of Sojourner ’s 83-day mission on Mars was its encounter with a rock nicknamed Barnacle Bill [to the rover’s left]. JPL/NASA As its first act of exploration, Sojourner rolled forward 36 centimeters and encountered a rock, dubbed Barnacle Bill due to its rough surface. The rover spent about 10 hours analyzing the rock, using its spectrometer to determine the elemental composition. Over the next few weeks, while the lander collected atmospheric information and took photos, the rover studied rocks in detail and tested the Martian soil. Marie Curie ’s sojourn…in a JPL sandbox Meanwhile back on Earth, engineers at JPL used Marie Curie to mimic Sojourner’s movements in a Mars-like setting . During the original design and testing of the rovers, the team had set up giant sandboxes, each holding thousands of kilograms of playground sand, in the Space Flight Operations Facility at JPL. They exhaustively practiced the remote operation of Sojourner , including an 11-minute delay in communications between Mars and Earth. (The actual delay can vary from 7 to 20 minutes.) Even after Sojourner landed, Marie Curie continued to help them strategize. Initially, Sojourner was remotely operated from Earth, which was tricky given the lengthy communication delay. Mike Nelson/AFP/Getty Images During its first few days on Mars, Sojourner was maneuvered by an Earth-based operator wearing 3D goggles and using a funky input device called a Spaceball 2003 . Images pieced together from both the lander and the rover guided the operator. It was like a very, very slow video game—the rover sometimes moved only a few centimeters a day. NASA then turned on Sojourner’s hazard-avoidance system, which allowed the rover some autonomy to explore its world. A human would suggest a path for that day’s exploration, and then the rover had to autonomously avoid any obstacles in its way, such as a big rock, a cliff, or a steep slope. JPL designed Sojourner to operate for a week. But the little rover that could kept chugging along for 83 Martian days before NASA finally lost contact, on 7 October 1997. The lander had conked out on 27 September. In all, the mission collected 1.2 gigabytes of data (which at the time was a lot ) and sent back 10,000 images of the planet’s surface. NASA held on to Marie Curie with the hopes of sending it on another mission to Mars. For a while, it was slated to be part of the Mars 2001 set of missions, but that didn’t happen. In 2015, JPL transferred the rover to the Smithsonian’s National Air and Space Museum . When NASA Embraced Faster, Better, Cheaper The Pathfinder mission was the second one in NASA administrator Daniel S. Goldin ’s Discovery Program, which embodied his “faster, better, cheaper” philosophy of making NASA more nimble and efficient. (The first Discovery mission was to the asteroid Eros.) In the financial climate of the early 1990s, the space agency couldn’t risk a billion-dollar loss if a major mission failed. Goldin opted for smaller projects; the Pathfinder mission’s overall budget, including flight and operations, was capped at US $300 million. RELATED: How NASA Built Its Mars Rovers In his 2014 book Curiosity: An Inside Look at the Mars Rover Mission and the People Who Made It Happen (Prometheus) , science writer Rod Pyle interviews Rob Manning , chief engineer for the Pathfinder mission and subsequent Mars rovers. Manning recalled that one of the best things about the mission was its relatively minimal requirements. The team was responsible for landing on Mars, delivering the rover, and transmitting images—technically challenging, to be sure, but beyond that the team had no constraints. Sojourner was succeeded by the rovers Spirit , Opportunity , and Curiosity . Shown here are four mission spares, including Marie Curie [foreground]. JPL-Caltech/NASA The real mission was to prove to Congress and the American public that NASA could do groundbreaking work more efficiently. Behind the scenes, there was a little bit of accounting magic happening, with the “faster, better, cheaper” missions often being silently underwritten by larger, older projects. For example, the radioisotope heater units that kept Sojourner ’s electronics warm enough to operate were leftover spares from the Galileo mission to Jupiter, so they were “free.” Not only was the Pathfinder mission successful but it captured the hearts of Americans and reinvigorated an interest in exploring Mars. In the process, it set the foundation for the future missions that allowed the rovers Spirit , Opportunity , and Curiosity (which, incredibly, is still operating nearly 13 years after it landed) to explore even more of the Red Planet. How the rovers Sojourner and Marie Curie got their names To name its first Mars rovers, NASA launched a student contest in March 1994, with the specific guidance of choosing a “heroine.” Entry essays were judged on their quality and creativity, the appropriateness of the name for a rover, and the student’s knowledge of the woman to be honored as well as the mission’s goals. Students from all over the world entered. Twelve-year-old Valerie Ambroise of Bridgeport, Conn., won for her essay on Sojourner Truth , while 18-year-old Deepti Rohatgi of Rockville, Md., came in second for hers on Marie Curie . Truth was a Black woman born into slavery at the end of the 18th century. She escaped with her infant daughter and two years later won freedom for her son through legal action. She became a vocal advocate for civil rights, women’s rights, and alcohol temperance. Curie was a Polish-French physicist and chemist famous for her studies of radioactivity, a term she coined. She was the first woman to win a Nobel Prize, as well as the first person to win a second Nobel. NASA subsequently recognized several other women with named structures. One of the last women to be so honored was Nancy Grace Roman , the space agency’s first chief of astronomy. In May 2020, NASA announced it would name the Wide Field Infrared Survey Telescope after Roman; the space telescope is set to launch as early as October 2026, although the Trump administration has repeatedly said it wants to cancel the project . Related: A Trillion Rogue Planets and Not One Sun to Shine on Them These days, NASA tries to avoid naming its major projects after people. It quietly changed its naming policy in December 2022 after allegations came to light that James Webb, for whom the James Webb Space Telescope is named, had fired LGBTQ+ employees at NASA and, before that, the State Department. A NASA investigation couldn’t substantiate the allegations, and so the telescope retained Webb’s name. But the bar is now much higher for NASA projects to memorialize anyone, deserving or otherwise. (The agency did allow the hopping lunar robot IM-2 Micro Nova Hopper , built by Intuitive Machines, to be named for computer-software pioneer Grace Hopper .) And so Marie Curie and Sojourner will remain part of a rarefied clique. Sojourner , inducted into the Robot Hall of Fame in 2003, will always be the celebrity of the pair. And Marie Curie will always remain on the sidelines. But think about it this way: Marie Curie is now on exhibit at one of the most popular museums in the world, where millions of visitors can see the rover up close. That’s not too shabby a legacy either. Part of a continuing series looking at historical artifacts that embrace the boundless potential of technology. An abridged version of this article appears in the June 2025 print issue. References Curator Matthew Shindell of the National Air and Space Museum first suggested I feature Marie Curie . I found additional information from the museum’s collections website , an article by David Kindy in Smithsonian magazine , and the book After Sputnik: 50 Years of the Space Age (Smithsonian Books/HarperCollins, 2007) by Smithsonian curator Martin Collins. NASA has numerous resources documenting the Mars Pathfinder mission, such as the mission website , fact sheet , and many lovely photos (including some of Barnacle Bill and a composite of Marie Curie during a prelaunch test). Curiosity: An Inside Look at the Mars Rover Mission and the People Who Made It Happen (Prometheus, 2014) by Rod Pyle and Roving Mars: Spirit, Opportunity, and the Exploration of the Red Planet (Hyperion, 2005) by planetary scientist Steve Squyres are both about later Mars missions and their rovers, but they include foundational information about Sojourner .",
    "published": "Sat, 31 May 2025 14:00:03 +0000",
    "author": "Allison Marsh",
    "topic": "aerospace",
    "collected_at": "2025-10-08T14:03:24"
  }
]