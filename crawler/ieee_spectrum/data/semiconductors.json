[
  {
    "title": "Where Will Taiwan Get Energy After Its Failed Nuclear Referendum?",
    "link": "https://spectrum.ieee.org/nuclear-energy-taiwan-maanshan-plant",
    "summary": "Taiwan failed to pass an August referendum on whether or not a nuclear plant should be restarted, if it were deemed safe to operate. While the more than 4 million votes for “yes” outnumbered the more than 1.5 million “no” votes, the number of affirmative votes failed to surpass the 25 percent threshold of eligible voters also required for the referendum to pass. As a result, Taiwan remains on the nuclear-free path it has followed since the shutdown of the nuclear plant in question, Maanshan Nuclear Power Plant , in southern Taiwan on 17 May, fulfilling a 2016 government pledge made as a result of Japan’s 2011 Fukushima disaster . However, high-tech industries, including semiconductor manufacturing, AI data centers, and AI infrastructure operators, will continue fueling electricity demand. The question remains as to whether or not Taiwan can deliver reliable clean power to support the growth of these industries amid Chinese geopolitical pressure—and without nuclear energy . Taiwan’s Nuclear Energy Debate Taiwan’s energy landscape remains complex. Nuclear power, developed since the 1970s, has seen older reactors retired since 2018 . Taiwan imports 95 percent of its energy and has a growing reliance on natural gas. But it also aims to reduce carbon emissions, improve grid reliability, and expand its energy storage options. “Without energy, there’s no industrial growth…and nuclear is an excellent option,” Nvidia CEO Jensen Huang said during a prereferendum visit to Taipei on 22 August. He met with key players in high-tech supply chains, including Taiwan Semiconductor Manufacturing Co. (TSMC), the world’s largest chip foundry producing advanced chips for smartphones, high-performance computing, and AI applications. It was not Huang’s first time advocating for nuclear energy. During Computex Taipei in May he said , “We need energy from any single source: wind, solar, nuclear. Taiwan should absolutely invest in nuclear, and it shouldn’t be a stigma to have energy.” Nvidia has been expanding in Taiwan, partnering with Foxconn and the government to build a 10,000-Blackwell GPU AI training and supercomputing facility in the south, opening a larger Taipei office, and collaborating with Taiwanese companies such as TSMC to build an AI infrastructure ecosystem. Taiwan president Lai Ching-te promised to honor the referendum result while focusing on diverse energy sources. He said Taiwan might consider advanced nuclear options if technology improves, waste decreases, and public support grows. In late August, the government approved a draft piece of legislation, the AI Basic Act , designed to create a supportive environment for AI development and use. The draft emphasizes the government’s role in promoting AI research, applications, and infrastructure. Meanwhile, the newly reshuffled Cabinet is under pressure by industry and the broader public to maintain energy security. In mid-September, newly appointed Minister of Economic Affairs Ming-hsin Kung emphasized that Taiwan is a global hub for chips and technology, shaping strategies for the next 10 to 20 years. Taiwan’s Renewable Energy Goals Kung stressed that businesses require both stable power supply and green energy to meet commitments to 100 percent renewable energy from global corporate initiative RE110 . He said the new Cabinet will continue focusing on renewable energy while adjusting rollout speed. The goal is to lift renewables to 20 percent of Taiwan’s power supply by the end of 2026—a challenging target critical in keeping Taiwan competitive in global supply chains. He estimated renewable energy will account for around 15 percent of power generation by the end of 2025, up from 11.9 percent in 2024 . A wind turbine and its solar power system are part of the Taipower Exhibit Center in Pingtung, in southern Taiwan on 29 April 2025. I-Hwa Cheng/AFP/Getty Images For solar, Kung pledged to strengthen existing projects, resolve land-use conflicts with fish farms in solar-fishery initiatives , and replace older solar panels with newer ones that produce twice as much energy. Offshore wind construction will be accelerated, and a trial program for floating wind turbines will resume. Taiwan will also actively develop other green energy sources, such as geothermal and hydrogen. On nuclear, Kung reaffirmed Taiwan’s nuclear-free path but left open the possibility of adopting advanced technologies like small modular reactors . Guidelines for evaluating potential restarts of existing plants will be released by the end of October. The first step will see the Taiwan Power Co. (Taipower) conducting assessments of all three halted nuclear plants, with initial results due next year . Maanshan, which began commercial operations in 1984, is regarded as the most likely to pass the safety self-assessments, which will focus on the ability to maintain aging equipment and upgrade earthquake resilience. In a report released on 26 September, Taiwan’s Energy Administration projects electricity demand to grow 1.7 percent annually from 2025 to 2034. The forecast factors in expansions to Taiwan’s semiconductor industry, investments in AI development, and expected energy savings. To meet rising power demand, the government currently plans to boost natural-gas generation while phasing out large nuclear, coal, and oil plants. Net additions of 12.2 gigawatts in gas-fired capacity are expected by 2034 . Semiconductor Industry Concerns But high-tech industries express concern. In early September, at Semicon Taiwan, Charles Lee , the managing director of Topco Group, a major semiconductor supplier, told IEEE Spectrum that manufacturers worry about grid stability as AI and semiconductor growth accelerates. “Highly polluting coal-fired plants are no longer an option, so we will rely more on liquefied natural gas and less-stable renewables. If nuclear plants could be restarted, I would personally welcome it,” Lee says. Meanwhile, a memory manufacturing director, who spoke on condition of anonymity because he isn’t authorized by his company to speak to the media, told Spectrum that Taiwan’s economy is still manufacturing-driven. “We’re concerned about the low efficiency of green energy. We’ve also noticed a trend abroad, with countries resuming nuclear plant construction,” he says. In a televised debate ahead of the August referendum, Tzu-Hsien Tung, chairman of Pegatron Corp., voiced support for restarting nuclear power plants. He warned that if Taiwan continues to rely on carbon-heavy electricity, local firms could face steep carbon taxes overseas, undermining their global competitiveness. Visitors view AI server samples at the Zhen Ding Tech Group booth during the Semicon Taiwan exhibition in Taipei on 10 September 2025. I-Hwa Cheng/AFP/Getty Images As Taiwanese society debated whether to restart nuclear power plants, some Taiwanese energy experts, including Tze-Luen Lin, deputy executive director of the Taiwanese government’s Office of Energy and Carbon Reduction and a political science professor at National Taiwan University, have called for fresh approaches to Taiwan’s energy resilience amid ongoing Chinese threats, echoing to notions brought by nongovernmental organizations and think thanks, such as the U.S.-based Center for Climate and Security , that a clean-energy transition can strengthen national security. At the Society for Environmental Economics and Policy Studies conference in Japan on 21 September, Lin highlighted that renewable energy is central to both energy and national security. He emphasized, “Energy resilience can only be strengthened through decentralized, locally sourced renewables, combined with microgrids and energy storage,” and warned that large, centralized power plants are easier targets for attack. Commenting on Taiwan’s possible nuclear options, Jusen Asuka , a professor at Tohoku University and chair of the session in the conference, cautioned that small modular reactors remain immature and costly, and investing heavily in them could slow renewable-energy development.",
    "published": "Thu, 02 Oct 2025 17:00:05 +0000",
    "author": "Yu-Tzu Chiu",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "The Story of Engineering Is the Story of Scale",
    "link": "https://spectrum.ieee.org/engineering-scale",
    "summary": "Engineers are masters of scale . They harness energy from the sun, wind, rivers, atoms, and ores. They manipulate electrons, photons, and crystals to compute and communicate. They devise instruments that detect perturbations in the fabric of space-time . And they grapple with challenges—anticipated or not—that are presented by the scale of the problem they are trying to solve. The articles in this issue describe engineers who think about, interact with, and create things at very precise and often mind-boggling scales. They took the point-contact transistor and scaled it over the course of decades into a product manufactured in almost unimaginably large quantities ( 13 sextillion, or 13,000,000,000,000,000,000,000, between 1947 and 2018, by one estimate ) and involving one of the most complex, yet crazily efficient workflows on the planet . They’re sequencing the genomes of 1.8 million species . They’re modeling and mitigating a potential catastrophe—the Kessler syndrome—that threatens to decimate satellites in low Earth orbit [p. 58]. Everywhere you look, engineering ingenuity is pushing against the limits of scale. That ingenuity extends to creating scales for what has yet to be measured. How will we know when AI has achieved human-level general intelligence ? How do we precisely measure the absence of matter in a vacuum ? Then there are the complexities of scaling a technology for mass adoption. Why, for example, have some humanoid robot makers announced overly optimistic deployment targets and boosted production capacity well ahead of specific humanoid robot safety standards, high reliability, decent battery life, or demand for hordes of humanoids ? And how can onshore wind turbines continue to scale up unless there’s a proven way to transport them ? “Infographics let readers grasp at a glance what would take paragraphs of explanation.” —Eliza Strickland In this issue, our editors and artists flex their data-visualization powers through compelling infographics, to help readers appreciate the scale of hundreds of gigatonnes of carbon dioxide and the immense interstellar distances we could traverse with a swarm of tiny, laser-powered space kites. “While we wanted every article to include some visual element, a few topics called for special treatment. You could tell the story of carbon capture or interstellar travel in words, but the real impact comes when you see the gaps, the scales, the leaps involved,” says Senior Editor Eliza Strickland, who curated this issue. “Infographics let readers grasp at a glance what would take paragraphs of explanation, whether it’s the ballooning demand for AI or the long journey from raw quartz to finished computer chips.” Several of these infographics, as well as the cover, were created by renowned graphic designer Carl De Torres, owner of Optics Lab. We also commissioned an essay by the nature writer Paul Bogard, who approached his topic from the human scale. Who among us has not gazed at the stars and marveled at how our eyes are absorbing light that traveled thousands of years to reach us? Bogard ventured to Chile to see how light pollution is encroaching on astronomy and changing our sense of place in the universe , perhaps irrevocably. We hope this issue sparks wonder, and conveys our appreciation for the people who measure the unmeasurable, build the unbuildable, and solve the unsolvable.",
    "published": "Thu, 02 Oct 2025 16:00:03 +0000",
    "author": "Harry Goldstein",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Muscle-Bound Micromirrors Could Bring Lidar to More Cars",
    "link": "https://spectrum.ieee.org/mems-lidar",
    "summary": "Five years ago, Eric Aguilar was fed up. He had worked on lidar and other sensors for years at Tesla and Google X, but the technology always seemed too expensive and, more importantly, unreliable. He replaced the lidar sensors when they broke—which was all too often, and seemingly at random—and developed complex calibration methods and maintenance routines just to keep them functioning and the cars drivable. So, when he reached the end of his rope, he invented a more robust technology—what he calls the “most powerful micromachine ever made.” Aguilar and his team at startup Omnitron Sensors developed new microelectromechanical systems (MEMS) technology that he claims can produce more force per unit area than any other. By supplying new levels of power to micromirrors , the technology is capable of precisely steering lidar’s laser beams, even while weathering hazardous elements and the bumps and bangs of the open road. With chips under test by auto-industry customers, Omnitron is now modifying the technology to reduce the power consumed by AI data centers. Lidar, a scanning and detection system that uses lasers to determine how far away objects are, is often adopted by self-driving cars to find obstacles and navigate. Even as the market for lidar is expected to grow by 13.6 percent annually , lidar use in the automotive industry has remained relatively stagnant in recent years, Aguilar says, in part because the technology’s lifespan is so short. Vibration from bumpy roads and severe environmental conditions are the biggest reliability killers for automotive lidar, says Mo Li , who studies photonic systems at the University of Washington. The optical alignment within the lidar package atop self-driving cars is delicate—tremors from a poor paving job could physically alter where the mirrors sit in the housing, potentially misaligning the beam and causing the system to fail. Or temperature fluctuations could cause parts to expand or contract with the same unfortunate outcome, he explains. Aguilar wondered which part broke most often and found the culprit to be scanners, the parts responsible for angling small mirrors that direct the laser beam out of the system’s housing. He wanted to make scanners that could withstand the tough conditions lidar faces, and silicon flexures stood out as a solution. These structures act like springs and allow for meticulous control of the mirrors within lidar systems without wearing out, as the standard metal springs do, Aguilar claims. Designing a better chip Aguilar hoped the new material would be the answer to the problem that plagued him, but even silicon springs didn’t make lidar systems as robust as they needed to be to withstand the elements they faced. To make lidar even stronger, the team at Omnitron aimed to design a more powerful MEMS chip by increasing the amount of force the device can apply to control the mirrors in the lidar array. And they claim to have achieved it—their chip can exert 10 times as much force per unit area on an actuator that positions a micromirror or other sensor component as the current industry standard, they say. That extra force allows for extremely valuable control in fine adjustment. To reach this achievement, they had to dig deep—literally. Omnitron’s micromirrors steer lidar beams and could find use in data centers. Omnitron In this MEMS device, the mirror and its actuator are etched into a single silicon wafer. On its nonmirror end, the actuator is covered with tiny, closely spaced plates that fit between trenches in the wafer, like the interlocking teeth of two combs. To move the mirror, voltage is applied, and electrostatic forces angle the mirror into a specific position by moving the plates up and down within the trenches as the electric field pulls across the trench sidewalls . The force that can be used to move the mirror is limited by the ratio of depth to width of the trenches, called aspect ratio. Put simply, the deeper the trenches are, the more electrostatic force can be applied to an actuator, which leads to a higher range of motion for the sensor. But fabricating deep, narrow trenches is a difficult endeavor. Overcoming this limiting factor was a must for Aguilar. Aguilar says Omnitron was able to improve on the roughly 20:1 aspect ratio he notes is typical for MEMS (other experts say 30:1 or 40:1 is closer to average these days), reaching up to 100:1 through experimentation and prototyping in small university foundries across the United States “That’s really our core breakthrough,” Aguilar says. “It was through blood, sweat, tears, and frustration that we started this company.” The startup has secured over US $800 million in letters of intent from automotive partners, Aguilar says, and is two months into an 18-month plan to prove that it can produce its chips at full demand rate. Even after verifying production capabilities, the technology will have to face “very tough” safety testing for thousands of consecutive hours in realistic conditions, like vibrations, thermal cycles, and rain, before it can come to market, Li says. Saving power In the meantime, Omnitron is applying its technology to solve a different problem faced by a different industry. By 2030, AI data centers are expected to require around 945 terawatt-hours to function—more than the country of Japan consumes today. The problem is “the way data moves,” Aguilar says. When data is sent from one part of the data center to another, optical signals are converted into electrical signals, rerouted, and then turned back to optical signals to be sent on their way. This process, which takes place in systems called network switches, burns huge amounts of power. Google’s solution, called Apollo , is to keep the data packets in the form of optical signals for the duration of their travels, which yields a 40 percent power savings, the company claims . Apollo does so by using an array of mirrors to direct the data. Aguilar is planning to make the process even more efficient using dense arrays of Omnitron’s more-powerful mirrors. Doing so could quadruple the amount of data each network switch could route by increasing the number of channels in each switch from 126 to 441, Aguilar says. Omnitron is still early in its data-center implementation, so it’s not yet clear to what degree this technology can really improve on Google’s Apollo. However, following a “critical design review” in mid-September, “one of the world’s top AI hyperscalers has requested our mirrors for their next-generation switch,” Aguilar says. “This is proof that Omnitron solves a problem that even the biggest AI infrastructure companies can’t address in-house.” And there may be even more applications to come. Omnitron has received feelers from the defense industry, space companies, and groups interested in methane detection, says Aguilar. “It’s pretty cool seeing the people knock on our door for this, because I was just focusing on lidar,” he says.",
    "published": "Wed, 01 Oct 2025 15:00:03 +0000",
    "author": "Perri Thaler",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Configuring and Controlling Complex Test Equipment Setups for Silicon Device Test and Characterization",
    "link": "https://events.bizzabo.com/762929",
    "summary": "In this webinar, we will explore efficient, accurate, and scalable techniques for analog and mixed-signal device testing using reconfigurable test setups. As semiconductor devices grow more complex, engineers face the challenge of validating performance and catching edge cases under tight schedules. Test setups often include oscilloscopes, waveform generators, network analyzers, and more, potentially from different vendors with unique automation and configuration considerations. In order to keep pace with semiconductor validation requirements, multi-channel test setups designed for flexibility and performance can help engineers scale effectively. Register now for this free webinar!",
    "published": "Thu, 25 Sep 2025 14:16:46 +0000",
    "author": "Liquid Instruments",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Microcredentials Chip Away at Semiconductor Workforce Gap",
    "link": "https://spectrum.ieee.org/microcredentials-semiconductor-workforce-development",
    "summary": "In 2017, Demis John noticed a staffing problem among the semiconductor companies in Santa Barbara. The area had about 28 small semiconductor companies at the time, many launched from the nanofabrication facility housed at University of California, Santa Barbara, where John works. But as these companies expand, “they are all headhunting the same 10 people, basically,” John says. “It really was hindering their ability to scale. When you start up a company, you might have five or six highly educated people,” he says. “As [companies] get bigger and they go beyond the research devices, they really need technicians to start making more chips.… That’s where they often had these problems.” This article is part of The Scale Issue . Now, following the CHIPS and Science Act of 2022 and increasing investment from companies like Intel and Taiwan Semiconductor Manufacturing Co. , the United States is expecting a shortage of workers who can staff new facilities. In the next few years, tens of thousands of additional skilled workers will be needed across the semiconductor industry; in 2024, McKinsey & Co. estimated a talent gap between 59,000 and 146,000 engineers and technicians before the end of the decade. As the United States invests in reshoring chip manufacturing, the industry faces a dilemma: How can the semiconductor workforce scale to meet the coming demand? Efforts to develop a strong workforce have grown, for example with government-funded initiatives from the Microelectronics Commons, a U.S. Department of Defense program that established eight hubs across the country to bridge research and manufacturing. (The National Semiconductor Technology Center was also established by the CHIPS Act in part for workforce development. However, in late August, the Commerce Department revoked funding from the nonprofit that was set up to administer the program.) Through a combination of federal programs, state funding, and private-sector partnerships, U.S. colleges and universities are working to increase talent . To fill the gap, some universities—including UC Santa Barbara—are also offering microcredential programs separate from traditional degree programs . In these bite-size courses, which can be as short as a week or two, future engineers and technicians can gain critical hands-on experience in clean-room fundamentals or an introduction to topics like lithography or etching. Deploying short, standardized, and skill-based courses across the country could be an essential part of building a sustainable U.S. semiconductor workforce. Developing Microcredentials UC Santa Barbara launched its clean-room training in 2021, opening the university’s clean room to enrolled students as well as those from outside the university, including community college students and people looking to make a career change. Many universities already have clean rooms where they teach undergraduates about semiconductor fabrication, but students outside of a four-year degree program typically can’t access these facilities to gain the necessary training. “There’s a big mismatch in culture between companies and city colleges and universities. They all want to solve the same problem, but they don’t actually understand each other’s needs that well,” John says. To him, the importance of these courses is in aligning the needs of the industry, students, and educational institutions. While developing the UC Santa Barbara course, however, John was surprised to find there was no established educational standard for those wishing to enter the semiconductor workforce outside of a bachelor’s degree. A student at UC Santa Barbara loads wafers into a machine used for plasma etching. Ben Werner Since then, he has collaborated with several other institutions and organizations working to implement a microcredential program developed by IEEE in partnership with the University of Southern California (USC) as part of California DREAMS (Defense Ready Electronics and Microdevices Superhub) , funded by the DOD. Other programs also offer short training courses, but the standardization IEEE aims to provide is important for ensuring participants’ skills are widely recognized by employers across the country. Initially, John aimed to address the shortage of technicians to help companies scale up production. But as the courses have expanded elsewhere, it has become clear that the same hands-on experience can be used for engineering students as well. Students who take these introductory courses may go on to join the workforce or continue in their education to a bachelor’s or advanced degree. “The entire ladder of different workforce exits into the semiconductor industry is really important,” says John. The industry needs operators and technicians, who may seek employment right after high school, as well as Ph.D.-level engineers. “These microcredentials get somebody into the start of that workforce ladder.” What the Semiconductor Industry Needs Microcredentials assure employers that applicants have the skills needed to work in their fabs. A common misconception is that companies need students who have already been taught how to build their particular technology. But “it doesn’t matter exactly which specific device you made. What matters is that this person has had the experience of making some real chip,” John says. He compares it to carpentry: Someone who has spent time in a woodshop making furniture may not know how to frame a house, but “all the tools are basically the same. I know they can figure it out.” So, in addition to specific skills, the course demonstrates a student’s ability to learn the processes—and tolerate the environment. With its loud machines, safety procedures, and protective bunny suits, the clean room isn’t a typical workplace. Having students experience that environment lowers the risk of employers hiring someone who dislikes it. “It doesn’t matter exactly which specific device you made. What matters is that this person has had the experience of making some real chip.” —Demis John The course has students spend several days in a clean room, which is more likely than a single clean-room day to filter out participants who wouldn’t last. That’s important for companies that invest a lot of resources in hiring and training new people, notes the University of Washington’s Darick Baker , who serves as acting director of the Washington Nanofabrication Facility, in Seattle. Can Hands-On Courses Scale Up? The hands-on experience is a critical part of semiconductor microcredential programs, because companies want employees who are excited about building things. But it also inherently limits how many students can enroll at once. “If I can handle 12 students at a time, maybe there’s the pathway to 100 students a year. But that’s not the numbers we need,” says Baker. Instead, scalability will likely come from offering courses more frequently, and at more universities. Many universities already have a clean room and courses for university students, John says, so the goal was to make it easy for universities to adapt programs already in place to fit with the microcredential program. This also requires training of the instructors. USC, for example, offers a microcredential for instructors themselves in a “train the trainer” model. For 10 years, Baker has run clean-room training courses during which students make a diode. He became excited about the possibility of awarding students IEEE’s professional microcredentials as a way to give students an advantage in the job market. Baker visited USC and UC Santa Barbara to observe their programs and realized they were already quite similar to his. With a few small changes, he could make his program meet the requirements for IEEE microcredentials. His hope is that “somebody can look at that credential and say, maybe this person doesn’t know everything about working at a fab, but they spent one week gowned-up in a bunny suit. They’re not going to quit in that first month because they can’t handle being in the lab.” Currently, these programs may have significance mostly to local employers. But “nationally, it starts to take meaning when you have a critical mass of universities that are offering these credentials,” says Baker. “The more universities we can get on board with this, the more meaning that credential has.”",
    "published": "Thu, 25 Sep 2025 13:00:03 +0000",
    "author": "Gwendolyn Rak",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Qualcomm’s New Chip Promises AI Agents in Your PC",
    "link": "https://spectrum.ieee.org/qualcomm-snapdragon-x2",
    "summary": "Qualcomm’s Snapdragon Summit, its annual conference held in Maui , Hawaii, took the wraps off the new generation of processors for PCs, Snapdragon X2. First introduced last year as part of Microsoft’s high-profile Copilot AI PC push, the processor lineup was notable because its CPU cores use the Arm instruction set instead of x 86. That initial Snapdragon X rollout was targeted at smaller and more portable Windows PCs. But f or its next move, Qualcomm is looking to expand its reach with a third-generation CPU and an upgraded neural processing unit (NPU), a one-two punch meant to accelerate AI workloads in mini PCs, all-in-one desktops, and high-performance laptops. “We’re going to bring AI everywhere,” says Qualcomm CEO Cristiano Amon. “Everything we’ve been talking about is starting to happen, and this dream of what’s going to happen with AI, it’s getting to the point where we see what’s going to happen at scale.” Keeping up with the Apples Oryon, Qualcomm’s CPU core in Snapdragon X, repeated a trick Apple pulled off with its introduction of Arm-based silicon in 2020. (The team that designed Oryon included a number of CPU architects who worked on Apple’s chips.) The first generation of the architecture placed an emphasis on efficiency and multicore performance. It also integrated an NPU, which is why it led Microsoft’s AI-focused Copilot Plus PCs . The third-gen Oryon architecture inside the Qualcomm Snapdragon X2 upped the core count from 12 to 18 for the top-end version, the Snapdragon X2 Elite Extreme. (The second generation was released only for smartphones.) But in a move that mirrors Intel and Apple’s architectures, it now combines two different kinds of CPU cores . (Qualcomm’s mobile CPUs also have two types of cores.) Unlike Apple and Intel, none of the cores in Snapdragon X2 are designed for low-performance workloads, Qualcomm stressed. Snapdragon X2 will pair what it calls “Performance” cores having multicore clock speeds up to 3.6 gigahertz (similar to those of the prior Snapdragon X) with new “Prime” cores that can reach multicore clock speeds up to 4.4 GHz, or up to 5 GHz in dual-core workloads. Why? Once again, it comes down to Apple. The world’s other major player in Arm chips for personal computing outpaced Qualcomm with the Apple M4, which achieves clock speeds up to 4.5 GHz and, in its 16-core top-tier M4 Max configuration . That left Snapdragon X a step behind in both single-core and multicore performance. Qualcomm believes that Snapdragon X2’s high clock speeds will give it the lead, though whether that’s the case remains to be seen. The first PCs with Snapdragon X2 aren’t expected until the first half of 2026, setting the stage for a showdown with Apple’s M5, which is expected to arrive in late 2025 or early 2026. More memory for more AI While Qualcomm and Apple butt heads on CPU performance, there’s another aspect of chip architecture where Qualcomm is ahead—the NPU. Competitors also have NPUs, which accelerate AI workloads , in their chips. But Qualcomm has staked the most on NPU performance. The first-gen Snapdragon X was quoted a NPU performance at 45 trillion operations per second (TOPS), and it claimed that for all chips in the product stack, not just the most expensive silicon. Snapdragon X2 will boost that metric to 80 TOPS. Snapdragon X2 also provides memory upgrades to improve AI workloads. The line of processors can support up to 128 gigabytes of advanced, low-power DRAM memory, LPDDR5x RAM. This is an upgrade from Snapdragon X, which topped out at 64 GB, and similar to AI-focused chips like AMD’s Ryzen AI Max. Memory bandwidth is also boosted from 135 GB/s to a maximum of 228 GB/s in the Snapdragon X2 Elite Extreme. Both metrics are important to generative-AI workloads. More memory makes it possible to load larger and more intelligent models, while added memory bandwidth improves the speed at which models can generate a response to a user’s prompt. A lot to offer, a lot to prove Qualcomm’s CEO speculates that personal computers will see a grand shift toward AI workloads. “As the AI can understand what we say, what we see, what we write…that becomes the new [user interface] of computers. The UI is human-centric and it gets processed where you are,” says Amon. Qualcomm’s NPU theoretically makes Snapdragon X2 a great choice for AI software. But it still has a lot to prove. While Amon’s keynote address in Maui imagined a future with numerous AI agents working on-device, the current reality is more modest. Qualcomm has Microsoft’s support and has a few software wins in specific features, like the AI-powered “magic mask” that removes unwanted objects from video in DaVinci Resolve Pro , a popular professional video editor. But such features remain exceptions to the rule, and Amon’s examples of how agentic AI might work were mostly hypothetical. An agentic AI assistant running on Qualcomm’s Snapdragon might one day be able to handle your calendar or pay your bills—but such an on-device agent doesn’t exist yet. The idea, then, appears to be “If you build it, they will come.” Qualcomm’s Oryon third-gen architecture and Snapdragon X2 chip imagines a future of personal computing with a focus on CPU and AI performance. But the company is still waiting to see if developers want to play ball.",
    "published": "Wed, 24 Sep 2025 23:41:54 +0000",
    "author": "Matthew S. Smith",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "How to Measure Nothing Better",
    "link": "https://spectrum.ieee.org/vacuum-measurement",
    "summary": "There’s no such thing as a complete vacuum. Even in the cosmic void between galaxies, there’s an estimated density of about one hydrogen or helium atom per cubic meter. But these estimates are largely theoretical—no one has yet launched a sensor into intergalactic space and beamed back the result. On top of that, we have no means of measuring vacuums that low. At least, not yet. Researchers are now developing a new vacuum-measurement tool that may be able to detect lower densities than any existing techniques can. This new quantum sensor uses individual atoms, cooled to just shy of absolute zero, to serve as targets for stray particles to hit. These atom-based vacuum measurers can detect lower atomic concentrations than ever before, and they don’t require calibration, making them a good candidate to serve as a standard. This article is part of The Scale Issue . “The atom was already our standard for time and frequency,” says Kirk Madison , professor of physics at the University of British Columbia (UBC), in Vancouver, and one of the pioneers of cold-atom-based vacuum-measurement technology. “Wouldn’t it be cool if we could make an atom the standard for vacuum measurement as well?” This quantum-sensor technology promises a dual achievement in scale: Not only does it extend our ability to measure incredibly rarefied conditions with unprecedented sensitivity, it also establishes the fundamental reference point that defines the scale itself. By eliminating the need for calibration and serving as a primary standard, this atom-based approach doesn’t just measure the farthest edges of the density spectrum—it could become the very ruler by which all other vacuum measurements are compared. Vacuum measurement on Earth While humans haven’t yet succeeded in making vacuum as pure as it is in deep space, many earthly applications still require some level of emptiness. Semiconductor manufacturing, large physics experiments in particle and wave detection , some quantum-computing platforms , and surface-analysis tools, including X-ray photoelectron spectroscopy , all require so-called ultrahigh vacuum. At these low levels of particles per unit volume, vacuum is parameterized by pressure, measured in pascals. Regular atmospheric pressure is 10 5 Pa. Ultrahigh vacuum is considered to be anything less than about 10 -7 Pa. Some applications require as low as 10 -9 Pa. The deepest depths of space still hold the nothingness record, reaching below 10 -20 Pa. The method of choice for measuring pressure in the ultrahigh vacuum regime is the ionization gauge. “They work by a fairly straightforward mechanism that dates back to vacuum tubes,” says Stephen Eckel , a member of the cold-atom vacuum-measurement team at the National Institute of Standards and Technology (NIST). A portable cold-atom vacuum-measurement tool [top] detects the fluorescence of roughly 1 million lithium atoms [bottom], and infers the vacuum pressure based on how quickly the fluorescence decays. Photos: Jayme Thornton Indeed, an ionization gauge has the same basic components as a vacuum tube. The gauge contains a heated filament that emits electrons into the chamber. The electrons are accelerated toward a positively charged grid. En route to the grid, the electrons occasionally collide with atoms and molecules flying around in the vacuum, knocking off their electrons and creating positively charged ions. These ions are then collected by a negatively charged electrode. The current generated by these positive ions is proportional to the number of atoms floating about in the vacuum, giving a pressure reading. Ion gauges are relatively cheap (under US $1,000) and commonplace. However, they come with a few difficulties. First, although the current in the ion gauge is proportional to the pressure in the chamber, that proportionality constant depends on a lot of fine details, such as the precise geometry of the filament and the grid. The current cannot be easily calculated from the electrical and physical characteristics of the setup—ion gauges require thorough calibrations. “A full calibration run on the ion gauges is like a full month of somebody’s time,” says Daniel Barker , a physicist at NIST who’s also working on the cold-atom vacuum-measurement project. Second, the calibration services provided by NIST (among others) calibrate down to only 10 -7 Pa. Performance below that pressure is questionable, even for a well-calibrated gauge. What’s more, at lower pressures, the heat from the ion gauge becomes a problem: Hotter surfaces emit atoms in a process called outgassing, which pollutes the vacuum. “If you’re shooting for a vacuum chamber with really low pressures,” Madison says, “these ionization gauges actually work against you, and many people turn them off.” Third, the reading on the ion gauge depends very strongly on the types of atoms or molecules present in the vacuum. Different types of atoms could produce readings that vary by up to a factor of four. This variance is fine if you know exactly what’s inside your vacuum chamber, or if you don’t need that precise a measurement. But for certain applications, especially in research settings, these concerns are significant. How a cold-atom vacuum standard works The idea of a cold-atom vacuum-measurement tool developed as a surprising side effect of the study of cold atoms. Scientists first started cooling atoms down in an effort to make better atomic clocks back in the 1970s. Since then, cooling atoms and trapping them has become a cottage industry, giving rise to optical atomic clocks , atomic navigation systems , and neutral-atom quantum computers. These experiments have to be done in a vacuum, to prevent the surrounding environment from heating the atoms. For decades, the vacuum was thought of as merely a finicky factor to be implemented as well as possible. “Vacuum limitations on atom traps have been known since the dawn of atom traps,” Eckel says. Atoms flying around the vacuum chamber would collide with the cooled atoms and knock them out of their trap, leading to loss. The better the vacuum, the slower that process would go. The most common vacuum-measurement tool in the high-vacuum range today is the ion gauge, basically a vacuum tube in reverse: A hot filament emits electrons that fly toward a positively charged grid, ionizing background atoms and molecules along the way. Jayme Thornton UBC’s Kirk Madison and his collaborator James Booth (then at the British Columbia Institute of Technology, in Burnaby), were among the first to turn that thinking on its head back in the 2000s. Instead of battling the vacuum to preserve the trapped atoms, they thought, why not use the trapped atoms as a sensor to measure how empty the vacuum is? To understand how they did that, consider a typical cold-atom vacuum-measurement device. Its main component is a vacuum chamber filled with a vapor of a particular atomic species. Some experiments use rubidium, while others use lithium. Let’s call it lithium between friends. A tiny amount of lithium gas is introduced into the vacuum, and some of it is captured in a magneto-optical trap. The trap consists of a magnetic field with zero intensity at the center of the trap, increasing gradually away from the center. Six laser beams point toward the center from above, below, the left, the right, the front, and the back. The magnetic and laser forces are arranged so that any lithium atom that might otherwise fly away from the center is most likely to absorb a photon from the lasers, getting a momentum kick back into the trap. The trap is quite shallow, meaning that hot atoms—above 1 kelvin or so—will not be captured. So the result is a small, confined cloud of really cold atoms, at the center of the trap. Because the atoms absorb laser light occasionally to keep them in the trap, they also reemit light, creating fluorescence. Measuring this fluorescence allows scientists to calculate how many atoms are in the trap. To use this setup to measure vacuum, you load the atoms into the magneto-optical trap and measure the fluorescence. Then, you turn off the light and hold the atoms in just the magnetic field. During this time, background atoms in the vacuum will chance upon the trapped atoms, knocking them out. After a little while, you turn the light back on and check how much the fluorescence has decreased. This measures how many atoms got knocked out, and therefore how many collisions occurred. The reason you need the trap to be so shallow and the atoms to be so cold is that these collisions are very weak. “A few collisions are quite energetic, but most of the background gas particles fly by and, like, whisper to the trapped atom, and it just gently moves away,” Madison says. This method has several advantages over the traditional ion-gauge measurement. The atomic method does not need calibration; the rate at which fluorescence dims depending on the vacuum pressure can be calculated accurately. These calculations are involved, but in a paper published in 2023 the NIST team demonstrated that the latest method of calculation shows excellent agreement with their experiment. Because this technique does not require calibration, it can serve as a primary standard for vacuum pressure, and even potentially be used to calibrate ion gauges. The cold-atom measurement is also much less finicky when it comes to the actual contents of the vacuum. Whether the vacuum is contaminated with helium or plutonium, the measured pressure will vary by perhaps only a few percent, while the ion gauge sensitivity and reading for these particles might differ by an order of magnitude, Eckel says. Cold atoms could also potentially measure much lower vacuum pressures than ion gauges can. The current lowest pressure they’ve reliably measured is around 10 -9 Pa, and NIST scientists are working on figuring out what the lower boundary might be. “We honestly don’t know what the lower limit is, and we’re still exploring that question,” Eckel says. No vacuum is completely empty. The degree to which vacuum pressure approaches pure nothingness is measured in pascals, with Earth’s atmosphere clocking in at 10 5 Pa and intergalactic space at a measly 10 -20 . In between, the new cold-atom vacuum gauges can measure further along the emptiness scale than the well-established ionization gauges can. Sources: S. Eckel (cold-atom gauge, ionization gauge); K. Zou (molecular-beam epitaxy, chemical vapor deposition); L. Monteiro, “ 1976 Standard Atmosphere Properties ” (Earth’s atmosphere); E.J. Öpik, Planetary and Space Science (1962) (Mars, moon atmosphere); A. Chambers, ‘Modern Vacuum Physics” (2004) (interplanetary and intergalactic space) Of course, the cold-atom approach also has drawbacks. It struggles at higher pressure, above 10 -7 Pa, so its applications are confined to the ultrahigh vacuum range. And, although there are no commercial atomic vacuum sensors available yet, they are likely to be much more expensive than ion gauges, at least to start. That said, there are many applications where these devices could unlock new possibilities. At large science experiments, including LIGO (the Laser Interferometer Gravitational-Wave Observatory) and ones at CERN (the European Organization for Nuclear Research), well-placed cold-atom vacuum sensors could measure the vacuum pressure and also help determine where a potential leak might be coming from. In semiconductor development, a particularly promising application is molecular-beam epitaxy (MBE). MBE is used to produce the few, highly pure semiconductor layers used in laser diodes and devices for high-frequency electronics and quantum technologies. The technique functions in ultrahigh vacuum, with pure elements in separate containers heated on one end of the vacuum. The elements travel across the vacuum until they hit the target surface, where they grow one layer at a time. Precisely controlling the proportion of the ingredient elements is essential to the success of MBE. Normally, this requires a lot of trial and error, building up thin films and checking whether the proportions are correct, then adjusting as needed. With a cold-atom vacuum sensor, the quantity of each element emitted into the vacuum can be detected on the fly, greatly speeding up the process. “If this technique could be used in molecular-beam epitaxy or other ultrahigh vacuum environments, I think it will really benefit materials development,” says Ke Zou , an assistant professor of physics at UBC who studies molecular-beam epitaxy. In these high-tech industries, researchers may find that the ability to measure nothing is everything. This article appears in the October 2025 print issue.",
    "published": "Wed, 24 Sep 2025 11:02:08 +0000",
    "author": "Dina Genkina",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "The Long Strange Trip from Silica to Smartphone",
    "link": "https://spectrum.ieee.org/the-long-strange-trip-from-silica-to-smartphone",
    "summary": "This article is part of our special report The Scale Issue . If you want to get a sense of the truly global scale of the electronics industry, look no further than your smartphone. The processor that powers it started as a humble rock, and by the time it found its way into your device, it had probably seen more of the world than you have. Along the way it was subjected to some of the most technologically sophisticated and closely guarded processes on the planet. Come along as we retrace that incredible 30,000-kilometer ride. 1. Quartz Your smartphone processor began its journey in the northwest corner of Spain, at Mina Serrabal , a quartz mine near the city of Santiago de Compostela. Quartz—or more technically, silicon dioxide or silica—is the main component of sand. But at Serrabal it can come in huge pieces twice the width of a smartphone. Mine operator Ferroglobe runs an automated system to sort the silica by size. After the pieces are washed and treated, the big ones head to the Atlantic coast for the next step in the journey. Fact: According to consultant Thunder Said Energy , 350 million tonnes of silica was mined in 2024. 2. Silicon Metal After an hour by truck , the quartz mini-boulders arrive at Sabón , Ferroglobe’s 125,000-square-meter factory in the coastal province of A Coruña. Here the quartz will be mixed with dehydrated wood chips and heated to 1,500 to 2,000 °C in a trio of electric-arc furnaces that use massive electrodes invented at this plant in the 1990s . Inside the furnace, a reaction takes place that rips the oxygen from the silica and sticks it to the carbon from the wood. The result is silicon metal and carbon monoxide. Fact: 3.8 million tonnes of silicon metal was produced in 2023, according to the U.S. Geological Survey . 3. Purified Polysilicon The resulting silicon metal is about 98 percent pure, and that’s not good enough. It will need to be at least 99.9999999 percent pure to become a microprocessor, which will require some pretty powerful chemistry. So it’s off to Wacker Chemie , in Burghausen, Germany. Here, the metal undergoes what’s called the Siemens process: It’s bathed in hydrochloric acid and reacts to form hydrogen gas and a liquid called trichlorosilane. Any impurities will be in the liquid, which is then run through a multistep distillation process that separates the pure trichlorosilane from anything unwanted. Once the needed purity is reached, the reaction is reversed: At 1,150 °C, the trichlorosilane is reacted with hydrogen to deposit multiple crystals of silicon, called polysilicon, and the resulting hydrochloric acid gas is sucked away. The polysilicon forms thick rods around heating elements. Once it’s cooled and removed from the reaction chamber, the polysilicon is smashed up for shipping. Fact: According to consultant Thunder Said Energy , 1.7 million tonnes of polysilicon was produced in 2024, most of that for solar-cell production. 4. Silicon Wafers The ultrapure silicon is made up of many crystals at different orientations. But microprocessors must be made from a single crystal. So the material might migrate to Sherman, Texas, where GlobalWafers recently opened a US $3.5 billion silicon-wafer plant. Here the polysilicon is put through what’s called the Czochralski (Cz) method . In a high-purity quartz crucible, the polysilicon is heated to about 1,425 °C and melts. Then a seed crystal with a precise crystal orientation is dipped into the melt, slowly drawn upwards, and rotated. Do all that exactly right, and you will pull up an ingot of pure, crystalline silicon that’s 300 millimeters across and several meters tall. Specialized saws then slice this pillar of semiconducting purity into wafers less than 1 millimeter thick. The wafers are cleaned, polished, and sometimes further processed, before heading to wafer fabs. Fact: According to industry association SEMI, manufacturers have shipped nearly 43 million square meters of silicon wafers in the last five years. That’s enough to cover two-thirds of the island of Manhattan. 5. Processed Wafers Now it’s off to Tainan, in southern Taiwan, where TSMC’s Fab 18 will turn these wafers into the latest smartphone processors. It’s an exceedingly intricate process, involving some of the most complex and expensive equipment on the planet, including EUV lithography systems that can cost upward of $300 million each. In Fab 18, each wafer will go through months of exquisitely precise torture to produce the transistors and wiring that make up the processors. Extreme ultraviolet radiation will print patterns onto it, hot ions will ram into its surface, precision chemical reactions will build up some parts one atomic layer at a time, acids will etch away nanometer-scale structures, and metals will electrochemically plate parts and be polished away in others. The result: a wafer full of identical processors. Fact: The maximum size of a silicon die is 858 mm 2 . Within a chip, there are more than 160 kilometers of wiring. Apple’s M3 Max processor contains 92 billion transistors. 6. Packaged Chips As amazing as these processors are, you can’t use them in this form. They first need to be packaged. For our silicon, that’s going to happen at ASE’s facility in Penang , Malaysia. A package provides the chip with mechanical protection, a way for heat to be removed, and a way of connecting the chip’s micrometer-scale parts to a circuit board’s millimeter-scale ones. To do this, the wafers are first diced into chips. Then tiny balls of solder, some only tens of micrometers across, are attached to the chips. The solder bumps are aligned to corresponding parts of the package, and the two parts are melted together. It’s becoming more common for multiple pieces of silicon to be integrated within the same package, either stacked on top of each other or positioned next to each other on a separate piece of silicon called an interposer. Other steps to the process follow, and the packaged part is now ready for its next step. Fact: Stacking multiple chips within a package could lead to GPUs with 1 trillion transistors by 2033 7. Smartphones Our packaged chip arrives next in southern India, at Foxconn’s new $2.56 billion assembly plant on the outskirts of Bengaluru. The 1.2-square-kilometer site includes dormitories to house 30,000 workers, who will turn the chip, printed circuit board, touchscreen, battery, and a multitude of other components into an iPhone—one of some 25 million the company expects to produce per year at this and three other plants . Fact: Processors and other logic chips made up 12 percent of the $463 billion cost of smartphones and other mobile devices in 2022, according to the Yole Group Global Trade Acknowledgment: This journey was inspired by a chapter in Ed Conway’s Material World: The Six Raw Materials That Shape Modern Civilization (Alfred A. Knopf, 2023). This article appears in the October 2025 print issue as “From Silica to Smartphone.”",
    "published": "Tue, 23 Sep 2025 13:00:04 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "This Microscope for Chipmaking Started as a Home Video Wannabe",
    "link": "https://spectrum.ieee.org/rca-videodisc",
    "summary": "I love a good comeback story of technological innovation, struggle, failure, and redemption. The invention of the scanning capacitance microscope has all of that. In 1981, RCA filed a patent for the SCM on behalf of company researcher James R. Matey. The microscope was an unintentional by-product of the VideoDisc technology the company had been struggling to bring to market since the mid-1960s. RCA expected the VideoDisc to capture half of the home video market, but instead it lost out in a big way to VHS. RCA’s James. R. Matey invented the scanning capacitance microscope, which used sensors cannibalized from the company’s VideoDisc players. Hagley Museum and Library Despite the VideoDisc’s struggles, the underlying technology held a gem: The exquisitely sensitive capacitance sensors used in the VideoDisc players were capable of measuring capacitance differences on the scale of attofarads (1 × 10 -18 farad). But before engineers and scientists could trust Matey’s idea, they wanted an independent evaluation to confirm the accuracy of the new microscope. Researchers at the National Institute of Standards and Technology obliged. Starting in the early 1990s, they too cannibalized capacitance sensors from old VideoDisc players and custom-built a series of SCMs, such as the one pictured at top. After NIST’s validation, microscope manufacturers commercialized the SCM, chipmakers adopted them to study integrated circuits, thus opening the door to the next generation of semiconductors. Why the RCA VideoDisc Failed But no story about the scanning capacitance microscope’s triumph would be complete without some discussion of the VideoDisc’s failure. In theory, it should have thrived: It was a thoroughly researched product that anticipated an important consumer market. Its playback fidelity was superior to over-the-air programming and to magnetic tape. And yet it bombed. Why? The VideoDisc effort had begun in the early 1960s, when RCA asked itself, “What comes after color TV? What will be the next major consumer electronics system?” The company decided that the answer was some type of system to play prerecorded movies and TV shows through your television. RCA was far from alone in pursuing this idea. All of the home video systems under development included a storage medium—film, magnetic tape, nonmagnetic tape, and vinyl discs of various size and composition—and a device to play back the audio and video in high resolution. In addition to magnetic methods, information could be stored using electromechanical, photographic, electron-beam, or optical technologies. RCA VideoDiscs were easily damaged by dust and fingerprints, so they were loaded into the SelectaVision player inside plastic sleeves. Hagley Museum and Library By 1964, RCA had settled on VideoDiscs. Like a record album (which the company had pioneered), a VideoDisc was a grooved vinyl platter that uses a stylus for playback. Unlike a record, the VideoDisc carried both audio and video, at a much higher density, and the stylus was electrical instead of mechanical. (The VideoDisc is sometimes confused with the LaserDisc , a home video technology of that era that used an optical laser.) RCA called its discs Capacitance Electronic Discs. The VideoDisc player spun the 30-centimeter disc at a constant 450 rpm. A metallic stylus traced the depressions and bumps in the disc’s groove by registering differences in capacitance, similar to the way that bringing your finger into contact with a touchscreen causes a detectable change in the screen’s capacitance at that point. Solid-state circuitry unscrambled the frequency-modulated video signal encoded in the capacitance differences. These differences were on the order of femtofarads, and the video signal ran at about 910 megahertz. To get a clear picture, the VideoDisc system required very sensitive capacitance sensors to detect these tiny differences at high frequency. Unfortunately, commercialization took much longer than expected. In 1972, RCA announced that its VideoDisc would debut the following year, but it didn’t materialize. An article in Popular Science in February 1977 anticipated regional sales by the end of that year. But it wasn’t until March 1981 that the RCA SelectaVision system finally hit the market. Despite heavy promotion, it sold poorly and was pulled from the shelves in 1984. In the end, RCA sank about US $500 million over 20 years to develop the VideoDisc, and it was a total flop. How Videotape Vanquished the VideoDisc What went wrong? In a word: videotape . Magnetic tape, which RCA had rejected, turned out to have greater consumer appeal. Introduced in 1976, VHS tapes were cheaper, had more titles available for purchase or rent, and, importantly, allowed owners to record their own programs. Perhaps if the VideoDisc had launched in 1973, it might have had a chance. But the technology had other problems. Fingerprints, dust, and scratches torpedoed early designs that envisioned users removing the discs from sleeves as casually as record albums; instead, the final version kept the discs encased in a plastic shell that was then inserted into the player. RCA spent two decades developing its home video system, but in the end the SelectaVision lost out to VHS and VCRs. Hagley Museum and Library Another problem was running time. In 1977, VideoDiscs could hold only about 30 minutes of material per side. That rose to an hour per side by the time of product launch, but that still meant that any movie over 120 minutes would have to be spread over multiple discs. The first VHS tapes could hold 120 minutes of video (double that of its main tape competitor, Betamax). And VHS kept extending that lead: By the 1980s, VHS had long play (four hours) and extended play (six hours) versions, albeit with noticeable drops in resolution quality. RCA forecasters also badly misread the economics of VideoDisc players. Their 1977 price estimate for a VideoDisc player was $500 (about $2,800 in today’s dollars). The first VHS players were much more expensive, ranging from $1,000 to $1,400, but by the mid-1980s, their price had dropped to $200 to $400. VHS tapes of major Hollywood films cost about $80—much more than VideoDiscs’ $10 to $18 price tag—but only diehard fans actually paid the modern equivalent of about $440 to buy a movie on videotape. For everyone else, the Hollywood studios licensed titles to third-party rental companies. Seemingly overnight, independent video shops, supermarkets, and national chains like Blockbuster were renting movies for a small fee. For a brief period, RCA VideoDiscs shared the shelves with videotapes, but usually only at independent shops and never with as many titles available. Meanwhile, RCA struggled to sell its VideoDisc players. The company had forecast eventual annual sales of five to six million players; its first-year goal was a more modest 200,000, and yet it sold only half that number. By 1984, RCA realized the VideoDisc would never come close to 50 percent market penetration, let alone profitability, and pulled the plug. Birth of the Scanning Capacitance Microscope Normally that would be the end of the story, another failed venture in consumer electronics. But back when RCA scientists first began researching the VideoDisc, there were no microscopes capable of identifying the tiny variations in the disc that encoded the audio/video signal. The bumps and depressions were less than a tenth the size of the groove itself; even the most advanced microscopes of the day couldn’t detect features that small. A factory worker inspects an RCA VideoDisc, which encoded the audio and video signals in the disc’s groove. Hagley Museum and Library And so RCA’s James Matey developed and patented the scanning capacitance microscope (which he abbreviated SCaM, but others wisely shortened to SCM) as a quality-control tool for manufacturing the VideoDiscs. Four years after the first patent, RCA filed a reissue patent with some corrections and improvements. In a very readable paper in the March 1985 issue of the Journal of Applied Physics, Matey and fellow RCA researcher Joseph Blanc explained the new technology. The SCM could detect variations in surface topography on the order of 0.3 nanometers over areas on the order of 0.5 square micrometers. RCA delayed publication of this paper until it had shuttered the VideoDisc operation, and so Matey and Blanc concluded their paper, “We are currently in the process of adapting [the SCM] for similar uses on other samples.” The new use turned out to be in the manufacturing of the next generation of semiconductors. Semiconductor performance depends on the distribution of intentionally introduced impurities, called dopants, which change the ability of the material to conduct electricity. In the early days of semiconductor production, manufacturers used ion mass spectroscopy and a technique called spreading resistance to measure the dopant distribution in one dimension. Related: The Secondhand Origins of Silicon Valley’s Ingot Industry By the late 1980s, integrated circuits had become so small that the industry needed a way to measure the dopants in two dimensions. The SCM, used in conjunction with an atomic force microscope, fit the bill. When the conductive tip of the atomic force microscope made contact with a semiconductor surface, it created a small capacitance, on the order of attofarads to femtofarads, depending on the dopant concentration. The SCM measured the changes of the local capacitance and mapped the dopant distributions. But the technology was still novel and not yet commercially available, so researchers at NIST took up the task of testing it. In the early 1990s, Joseph Kopanski, Jay Marchiando, and David Berning began building a series of custom SCMs at the NIST Semiconductor Electronics Division. They did more than just reproduce Matey and Blanc’s results. They also provided the industry with models and software for extracting two-dimensional dopant distribution from the capacitance measurements. NIST’s validation of the SCM led to the commercial production of the instruments, which in turn led to the development of more-advanced semiconductors—an industry that is orders of magnitude more important to the global economy than a consumer product like the VideoDisc would ever have been. It’s a classic tale of redemption in the history of technology: At the start of any new tech project, no one really knows what the outcome will be. Sometimes, you just have to keep going, even through abject failure, and trust that something good will emerge on the other side. Part of a continuing series looking at historical artifacts that embrace the boundless potential of technology. An abridged version of this article appears in the October 2025 print issue as “RCA’s VideoDisc Gamble Paid Off in Chips.” References My friends at NIST first suggested the scanning capacitance microscope in their collection and pointed me to a great description of it. The Sarnoff Collection at the College of New Jersey has an equally helpful description of the rise and fall of the RCA VideoDisc . I then went to the primary sources, starting with James R. Matey’s patents . His article with Joseph Blanc, “ Scanning capacitance microscopy ” ( Journal of Applied Physics , 1 March 1985), offers a clearly written account of the limitations of previous microscopes and the possibilities of the SCM. Because sources often lead to sources, footnote 1 in Matey and Blanc’s article led me to the richly informative March 1978 issue of RCA Review , which consists of 10 articles over 228 pages devoted to the VideoDisc. Tom Howe has collected a tremendous amount of data on RCA’s Capacitance Electronic Discs on his retro website CED Magic . It is there that I learned of the February 1977 Popular Science article “Here at Last: Video-Disc Players.” Finally, business historian Margaret B.W. Graham had RCA-sanctioned access while chronicling the making of the VideoDisc as part of a Harvard Business School study. Her book RCA and the VideoDisc: The Business of Research (Cambridge University Press, 1986) remains one of the best analyses of the failure of the project.",
    "published": "Wed, 17 Sep 2025 14:00:04 +0000",
    "author": "Allison Marsh",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Chip Hall of Fame: Intel PIIX3",
    "link": "https://spectrum.ieee.org/chip-hall-of-fame-intel-piix3-usb-interface",
    "summary": "Intel PIIX3 Manufacturer: Intel Category: Interfacing Year: 1996 Until the mid-1990s, using peripheral equipment with a computer often meant plugging one of a bewildering variety of computer connector cables, installing device drivers from 3.5” floppy discs or CD-ROMS, and slotting specialized expansion cards into motherboards. This all added up to a compatibility nightmare, and consumers had to check carefully to make sure their desired peripheral could work with their make and model of computer. Then, in 1996, Intel released the PIIX3, part of the first chipset for computer motherboards that implemented the Universal Serial Bus (USB) interfacing standard. Initially used for relatively slow-speed communications with mice and keyboards, USB has since developed into the primary way computers communicate at high speeds with cameras, monitors, and other peripherals. It also created a beachhead for solid-state storage technology in personal computers, with the introduction of USB-based thumb drives in 2000. It even contributed to Apple’s revival, when, in 1998, the company opted for USB over “legacy” ports in the G3 iMac and eliminated the floppy disc drive. Its distinctive Apple “Hockey-Puck” Mouse divided opinion, but the G3 iMac became Apple’s fastest-selling computer. Having curbed the need for boxes of data cables, the current iteration of USB is now putting an end to the need to keep a collection of chargers for mobile devices. A mandate from the European Union went into force in December 2024 requiring all new small portable electronics to support charging via a USB-C cable , and all new laptops must support USB-C charging by April 2026, reducing electronic waste.",
    "published": "Mon, 15 Sep 2025 14:35:00 +0000",
    "author": "Stephen Cass",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Natcast to Lay Off Majority of Its Staff",
    "link": "https://spectrum.ieee.org/natcast-layoffs",
    "summary": "Natcast, the non-profit organization created to run the U.S. CHIPS & Science Act’s National Semiconductor Technology Center (NSTC) told the majority of its staff they would be laid off this week, IEEE Spectrum has learned. A small core team will continue on to shut Natcast down over the next few weeks, a person familiar with the matter said. The organization was founded in 2023 to run NSTC, which the law established to conduct “research and prototyping of advanced semiconductor technology and grow the domestic semiconductor workforce to strengthen the economic competitiveness and security of the domestic supply chain.” However, on 25 August Commerce Secretary Howard Lutnick stated that the department would not deliver the US $7.4 billion in funds under its contract with the government. In an accusatory letter to Natcast CEO Deirdre Hanford and a press release , Lutnick claimed the nonprofit was not created legally, and said that the National Institute of Standards and Technology would be taking over NSTC operations. The Commerce Department has been suppressing Natcast’s operations for many months, IEEE Spectrum has learned. According to multiple sources, the organization’s research agenda was completed in April and should have been accepted and processed by Commerce in a matter of weeks. But it never happened. A memo selecting awardees for research into reducing the impact of perfluoroalkyl substances (PFAS) from semiconductor manufacturing was submitted to Commerce but never approved. Three groups won parts of a $30 million grant to transform the design of RF chips using machine learning , but at least one awardee has gotten no word on when or whether the funding will flow. “This is a sad end,” said one person who requested anonymity and was involved in early debate about the NSTC. Lutnick’s characterization of Natcast’s creation as cronyism “is pretty frustrating,” this person continued. The idea that NSTC would be operated by an independent public-private partnership, which is written into the law, came from deep consultation with industry and was intended to insulate its work from politics. The organization needed such protection to make good technical decisions, because in selecting who gets funds “there are always more losers than winners,” this person said. Over the past week, Natcast has been defending itself and its staff against Lutnick’s accusations. On 4 September, in a letter to its 200 members , Hanford countered Lutnick’s assertions that the organization was packed with Biden administration cronies and that it was run as a “slush fund.” Hanford pointed to the multiple layers of government and industry oversight Natcast operates under and the experienced semiconductor industry staff it has brought on. She added that the nonprofit had stuck to its agreements with the government, submitting 119 “milestones and deliverables” to Commerce. What’s Lost Without Natcast On 8 September, Natcast published a detailed overview of what it has done so far . Accomplishments included creating an industry-driven R&D agenda, the development of multiple workforce programs, and the signing on of some 200 members. These members included cutting-edge logic and memory companies (such as Intel , Nvidia , SK Hynix , and TSMC ), small companies like Atomera , EnCharge AI , and Polar Semiconductor , and multiple universities and makers of semiconductor tools and equipment. Members had the right to use facilities and other infrastructure that Natcast was standing up (for a fee) and participate in research. One goal for the infrastructure was to reduce the time from concept to prototype by 30 percent. Access to the tools and processes NSTC would provide is out of reach for even some of the country’s top researchers, IEEE Spectrum ’s source said. Without that access, startups and researchers won’t be able to afford to do relevant work using 300-millimeter wafers and advanced chip-technology nodes. Despite the fact that not all of its facilities were complete, Natcast had plans to proceed with research by chaining together existing infrastructure around the country. In addition, the document from 8 September detailed the expert in-house team at Natcast, and said that expertise allowed it to evaluate R&D awards at twice the rate of federal agencies while requiring considerably less overhead than government, universities, and non-profit research organizations like Imec. (It forecast a 10 percent overhead on expenses over 10 years.) “Natcast was supposed to do what industry couldn’t solve by itself,” said a source familiar with the situation. Its R&D program was aimed at solving big problems, such as collapsing the time and energy required to move data between memory and computing , in a way that the whole industry could benefit from.",
    "published": "Thu, 11 Sep 2025 12:00:03 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Machine Learning Tests Keep Getting Bigger",
    "link": "https://spectrum.ieee.org/mlperf-inference-51",
    "summary": "The machine learning field is moving fast, and the yardsticks used to measure its progress are having to race to keep up. A case in point: MLPerf, the biannual machine learning competition sometimes termed “the Olympics of AI,” has introduced three new benchmark tests, reflecting new directions in the field. “Lately, it has been very difficult trying to follow what happens in the field,” says Miro Hodak , an Advanced Micro Devices engineer and MLPerf Inference working-group cochair. “We see that the models are becoming progressively larger, and in the last two rounds we have introduced the largest models we’ve ever had.” The chips that tackled these new benchmarks came from the usual suspects—Nvidia, Arm, and Intel. Nvidia topped the charts, introducing its new Blackwell Ultra GPU, packaged in a GB300 rack-scale design. AMD put up a strong performance, introducing its latest MI325X GPUs. Intel proved that one can still do inference on CPUs with its Xeon submissions, but it also entered the GPU game with an Intel Arc Pro submission. New benchmarks Last round, MLPerf introduced its largest benchmark yet, a large language model based on Llama 3.1-403B. In this round, MLPerf topped itself yet again, introducing a benchmark based on the DeepSeek-R1 671B model—more than 1.5 as many parameters as the previous largest benchmark. As a reasoning model, DeepSeek-R1 goes through several steps of chain-of-thought prompting when approaching a query. This means that much more of the computation happens during inference than in normal LLM operation, making this benchmark even more challenging. Reasoning models are claimed to be the most accurate, making them the technique of choice for science, math, and complex programming queries. In addition to the largest LLM benchmark yet, MLPerf also introduced the smallest, based on Llama 3.1-8B. There is growing industry demand for low latency yet high-accuracy reasoning, explained Taran Iyengar, the MLPerf Inference task-force chair. Small LLMs can supply this, and they’re an excellent choice for tasks such as text summarization and edge applications. This brings the total count of LLM-based benchmarks to a confusing four. They include the new, smallest Llama 3.1-8B benchmark; a preexisting Llama 2-70B benchmark; last round’s introduction of the Llama 3.1-403B benchmark; and the largest, the new DeepSeek-R1 model. If nothing else, this signals that LLMs are not going anywhere. In addition to the myriad LLMs, this round of MLPerf Inference included a new voice-to-text model, based on Whisper-large-v3. This benchmark is a response to the growing number of voice-enabled applications, whether they’re smart devices or speech-based AI interfaces. The MLPerf Inference competition has two broad categories: “closed,” which requires using the reference neural-network model as-is without modifications, and “open,” where some modifications to the model are allowed. Within those, there are several subcategories related to how the tests are done and in what sort of infrastructure. We will focus on the “closed” data-center server results for the sake of sanity. Nvidia leads Surprising no one, the best performance per accelerator on each benchmark, at least in the server category, was achieved by an Nvidia GPU-based system. Nvidia also unveiled the Blackwell Ultra, topping the charts in the two largest benchmarks: Llama 3.1-405B and DeepSeek-R1 reasoning. Blackwell Ultra is a more-powerful iteration of the Blackwell architecture, featuring significantly more memory capacity, double the acceleration for attention layers, 1.5 times more AI compute, and faster memory and connectivity compared with the standard Blackwell. It is intended for larger AI workloads, like the two benchmarks it was tested on. In addition to the hardware improvements, Dave Salvator , director of accelerated computing products at Nvidia, attributes the success of Blackwell Ultra to two key changes. First, the use of Nvidia’s proprietary 4-bit floating-point number format , NVFP4 . “We can deliver comparable accuracy to formats like BF16,” Salvator says, while using a lot less computing power. The second is so-called disaggregated serving . The idea behind disaggregated serving is that there are two main parts to the inference workload: prefill, where the query (“Please summarize this report”) and its entire context window (the report) are loaded into the LLM, and generation/decoding, where the output is actually calculated. These two stages have different requirements. While prefill is compute heavy, generation/decoding is much more dependent on memory bandwidth. Salvator says that by assigning different groups of GPUs to the two different stages, Nvidia achieves a performance gain of nearly 50 percent. AMD close behind AMD’s newest accelerator chip, MI355X, launched in July. The company offered results only in the “open” category, where software modifications to the model are permitted. Like Blackwell Ultra, MI355X features 4-bit floating-point support, as well as expanded high-bandwidth memory. The MI355X beat its predecessor, the MI325X, in the open Llama 2.1-70B benchmark by a factor of 2.7, says Mahesh Balasubramanian , senior director of data-center GPU product marketing at AMD. AMD’s “closed” submissions included systems powered by AMD MI300X and MI325X GPUs. The more advanced MI325X computer performed similarly to those built with Nvidia H200s on the Llama 2-70b, the “mixture of experts” test, and image-generation benchmarks. This round also included the first hybrid submission, where both AMD MI300X and MI325X GPUs were used for the same inference task, the Llama 2-70b benchmark. The use of hybrid GPUs is important, because new GPUs are coming at a yearly cadence , and the older models, deployed en masse, are not going anywhere. Being able to spread workloads among different kinds of GPUs is an essential step. Intel enters the GPU game In the past, Intel has remained steadfast that one does not need a GPU to do machine learning. Indeed, submissions using Intel’s Xeon CPU still performed on par with the Nvidia L4 on the object-detection benchmark but trailed on the recommender-system benchmark. In this round, for the first time, an Intel GPU also made a showing. The Intel Arc Pro was first released in 2022. The MLPerf submission featured a graphics card called the MaxSun Intel Arc Pro B60 Dual 48G Turbo , which contains two GPUs and 48 gigabytes of memory. The system performed on par with Nvidia’s L40S on the small LLM benchmark and trailed it on the Llama 2-70b benchmark.",
    "published": "Wed, 10 Sep 2025 15:00:03 +0000",
    "author": "Dina Genkina",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Teach 5G Hands-On with TIMS Lab Experiments",
    "link": "https://content.knowledgehub.wiley.com/unravelling-5g-complexity-engaging-students-with-tims-powered-hands-on-education/",
    "summary": "Boost Student Comprehension in Telecoms with Interactive 5G Labs. Teaching complex 5G and telecommunications concepts can be challenging – students often struggle to connect theory with real-world applications. Traditional lecture-based methods may fail to engage, leaving gaps in understanding critical technologies like OFDM, channel coding, and signal modulation. The Telecommunications Instructional Modelling System (TIMS) bridges this gap by transforming abstract concepts into tangible, hands-on experiments. Designed for EE/ECE/EET educators, TIMS enables students to model 5G systems, measure real signals, and validate theory through interactive labs – boosting engagement and retention. Download this free whitepaper now!",
    "published": "Thu, 28 Aug 2025 16:10:47 +0000",
    "author": "Emona Instruments",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Trump Seeks to Cancel CHIPS Act R&D Organization’s Funds",
    "link": "https://spectrum.ieee.org/natcast",
    "summary": "The U.S. Commerce Department says it will not abide by an agreement to fund the U.S. CHIPS and Science Act ’s R&D through the nonprofit set up to administer the program, called Natcast. Instead, it handed operational control to the National Institute of Standards and Technology (NIST). Natcast was created in 2023 to oversee the National Semiconductor Technology Center (NSTC), which the law established to conduct “research and prototyping of advanced semiconductor technology and grow the domestic semiconductor workforce to strengthen the economic competitiveness and security of the domestic supply chain.” The nonprofit was contracted to receive a total of US $7.4 billion in annual payments and when the organization reaches milestones. But Commerce Secretary Howard Lutnick claimed that Natcast doesn’t meet certain legal requirements, and therefore the contract, inked less than a week before Donald J. Trump took office for the second time, is illegal. Several NSTC proponents whom IEEE Spectrum spoke to are concerned that the move could squander U.S. semiconductor leadership in the long term. The goal of the NSTC, those involved say, is to make gains in semiconductors from the CHIPS Act durable through continued advances. Semiconductor Workforce Development Since its establishment, Natcast has been working to bring up three key centers to execute those functions . In Silicon Valley, it’s established a workforce-development and design-enablement center. In New York, it opened a center for extreme-ultraviolet lithography for cutting-edge chipmaking . And in Arizona, it plans to build a prototyping and packaging facility . The centers are intended to help startups and other companies more easily bridge the lab-to-fab gap that currently prevents new technologies from making it into commercial products. “There were people from day one…who viewed [Natcast] as very much a political entity and wanted to undo it.” The CHIPS Act requires that the NSTC be operated as a “public private-sector consortium with participation from the private sector” instead of by a government agency. During the Biden administration, the Commerce Department created Natcast to fill that role, deliberately setting it up in a way to help maintain its independence from political interference. In a public letter to Natcast CEO Deirdre Hanford , Lutnick cast the actions of Hanford, her staff, and the volunteer advisors involved in the organization’s creation as giving “the appearance of impropriety” and flouting “federal law.” “From the very beginning Natcast served as a semiconductor slush fund that did nothing but line the pockets of Biden loyalists with American tax dollars,” he said in a press release . Spectrum sought additional comment from the Commerce Department and from Natcast but did not receive a reply by press time. Very little funding has actually been delivered, sources say, in part because Commerce has held up its dispersal. (Despite this, NSTC does have a list of accomplishments .) Lutnick’s legal argument for refusing payment now is that Natcast wasn’t established in accordance with the Government Corporation Control Act, which lays out how government agencies establish or purchase corporations. One person familiar with the situation who asked not to be named says that the structure of Natcast is typical of public–private partnerships and that its underpinnings were thoroughly reviewed by the Commerce Department before its establishment. What’s really at issue, this person says, is Natcast’s independence. “What was set up…was always designed with a long-term strategy in mind. I don’t think they’ll get that back…. I think all of that has gone away with this decision.” “There were people from day one…who viewed [Natcast] as very much a political entity and wanted to undo it,” says this person. In the letter, Lutnick takes aim at Hanford , formerly a top executive at electronic design automation giant Synopsys , as well as at Natcast staffers who came over from government during the Biden administration or from a volunteer industrial advisory committee that included IEEE Fellows and other chip-industry leaders. Targeting such people is concerning, says one expert who preferred not to be named, because chip experts who choose to work in government or at Natcast are usually giving up more lucrative work to serve their country. It has the effect of “punishing patriotic behavior,” the expert said. Delaying the work of the NSTC by attacking Natcast is counterproductive for the U.S. chip industry, the expert added. “We are in a race, and these delays make it all the more urgent.” Commerce will likely find some way to spend the money on semiconductor R&D eventually, sources agreed. One expert told Spectrum they have faith in NIST’s ability to administer the research funding. Mark Granahan, an early proponent of the CHIPS Act and CEO of Ideal Semiconductor , in Bethlehem, Penn., went further. “If the administration has a different tactic but the same goal… not just independence in semiconductors but leadership…then NIST and other existing infrastructure is capable of handling things,” he said. But other sources were skeptical it would have the same impact as Natcast. “What was set up…was always designed with a long-term strategy in mind,” said one person. “I don’t think they’ll get that back…. I think all of that has gone away with this decision.” This post was updated on 26 August 2025 to remove mention of NSTC’s September Symposium, which is now canceled.",
    "published": "Wed, 27 Aug 2025 15:05:15 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Meta’s New Flat-Panel Laser Display Is Ultrathin",
    "link": "https://spectrum.ieee.org/meta-laser-display-system",
    "summary": "By shining lasers onto a tiny screen roughly as wide as a pencil-top eraser, scientists at Meta have created an ultrathin, high-definition flat-panel display they suggest might be used in augmented reality applications. Flat-panel displays are now commonplace, often illuminated by LEDs. When contemplating the next step beyond these displays, scientists have explored replacing the LEDs with lasers. Ideally, laser-based displays can offer superior brightness and a wider range of colors than conventional LED displays, researchers at Meta say. “High brightness is important particularly for see-through and outdoor applications, such as augmented reality use cases,” due to the bright lighting conditions in which people often find themselves, explains Guohua Wei , an optical scientist at Meta’s Reality Labs in Redmond, Wash. “Rich color provides a much better user experience, as we see in traditional cinemas.” However, laser displays typically rely on complex, bulky optical systems to deliver laser light precisely where it is needed on screens. Previous attempts to develop flat-panel laser displays have required complicated laser arrays or low-throughput fabrication methods, greatly limiting their performance and scalability, the researchers say. Now, the team at Meta has developed a flat-panel laser display only 2 millimeters thick. With it, they developed a prototype see-through augmented reality system that could blend virtual images with real-world scenes in an office environment. The photonic chip driving the new display may also one day be used in everything from smartphone screens to slim-panel 3D holography. “After three years of research and thousands of illuminator chips prototyped, we are happy to have the opportunity to share with the tech community the results of our team’s work,” says Giuseppe Calafiore , head of the augmented reality waveguides group at Meta’s Reality Labs. Photonic Chip Slims Meta’s Display At the heart of the new display is a centimeter-scale photonic integrated circuit. This combines thousands of components with different optical functions onto a single photonic chip, avoiding the need for the complex, bulky optical systems required with conventional optics. “It’s an integrated optical chip that can generate almost any arbitrary optical function,” Calafiore says. The new display weds the photonic chip with a 5- by 5-millimeter liquid-crystal-on-silicon (LCoS) panel, which has a resolution of 1,920 by 1,080 pixels. The resulting device is less than one-eightieth the thickness of conventional LCoS displays while achieving 211 percent of the color gamut. The new photonic chip was fabricated using standard CMOS-compatible processes, making it scalable for mass production, the scientists note. The devices may one day find use in a wide range of new kinds of displays, including slim-panel holographic displays , high-resolution light-field displays , and more, they add. “This work could pave the way to a future where photonic integrated circuits create the brightest, most compact new kind of displays,” Calafiore says. Calafiore notes that current LCoS displays are limited to pixel sizes of about 3 micrometers. This could mean any system based on LCoS—including the new display—may fall behind competitors in the augmented reality market, such as micro-LEDs, unless LCoS research is not pushed more aggressively. The scientists detailed their findings 20 August in the journal Nature .",
    "published": "Wed, 20 Aug 2025 15:00:04 +0000",
    "author": "Charles Q. Choi",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Semiconductor Rivalry Rages on in High-Temperature Chips",
    "link": "https://spectrum.ieee.org/high-temperature-transistor",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Two semiconductors— silicon carbide and gallium nitride —are the rivals in a (quite literally) heated competition to make circuits capable of performing at the highest temperatures. Silicon carbide chips had taken the lead, operating at 600 °C. But gallium nitride, which possesses unique features that make it more functional at high temperatures, has now surpassed SiC. Researchers at Pennsylvania State University led by Rongming Chu , a professor of electrical engineering, have designed a gallium nitride chip capable of operating at 800 °C —hot enough to melt table salt. The development could be critical to future space probes, jet engines, pharmaceutical processes, and a host of other applications that need circuits for extreme conditions. Silicon carbide high-temperature chips have allowed scientists to put sensors in places they weren’t able to before, says Alan Mantooth , a professor of electrical engineering and computer science at the University of Arkansas, who was not involved in the new gallium nitride result. He explains that the gallium nitride chip could do the same in monitoring the health of natural gas turbines, energy-intensive manufacturing processes in chemical plants and refineries, and systems no one has even thought of yet. “We can put this kind of electronics in places silicon simply can’t even imagine going,” he says. Both silicon carbide and gallium nitride’s potential to perform under such extreme conditions comes from their wide bandgaps. Those are the energy gaps between the materials’ valence bands, where electrons are bound to the molecule, and the conduction band, where they are free to contribute to the flow of electricity. At high temperatures, electrons in materials with a narrower bandgap are always excited enough to reach the conduction band. This presents a problem for transistors, because they will then be unable to switch off. The wide bandgaps of silicon carbide and gallium nitride require more energy to excite electrons to the conduction band, so that the transistors aren’t unintentionally always switched on in high-temperature environments. Gallium nitride also has unique features compared to silicon carbide which allow its chips to perform better under high-heat conditions. Chu’s group’s IC, which they described this month in IEEE Electron Device Letters , is composed of what are called gallium nitride high electron mobility transistors (HEMT) . The structure of GaN HEMTs involves an aluminum gallium nitride film on top of a layer of gallium nitride. The structure draws electrons to the interface between the two materials. This layer of electrons—called a two-dimensional electron gas (2DEG)—is highly concentrated and moves with little resistance. This means charge moves much faster in the 2DEG, leading the transistor to be able to respond to changes in voltage and switch between its on and off states more quickly. Faster electron movement also allows the transistor to carry more current in response to a given voltage. The 2DEG is harder to produce using silicon carbide, making it more difficult for its chips to match the performance of gallium nitride devices. To coax a GaN HEMT into operating at 800 °C took some alterations to its structure, explains Yixin Xiong , Chu’s graduate student. Some of those measures involved minimizing leakage current, charge that sneaks across even when the transistor is supposed to be off. They did this by using a tantalum silicide barrier to protect the device’s components from the environment and by preventing the outer layer of the metal on the sides of the device from touching the 2DEG, which would have further increased leakage current and instability in the transistor. Penn State engineers tested high electron mobility transistors at 800 °C. Rongming Chu/Pennsylvania State University Chu says that the research and fabrication process of the chip went much faster than he had anticipated. The team had been confident that the experiment would work, he says. But it was “faster than my best guess,” he says. Despite the notable benefits it presents, Mantooth is concerned about gallium nitride’s long-term reliability compared to silicon carbide. “One of the things that people have been concerned about with GaN at those extreme temperatures, 500 ℃ and above, is microfractures or microcracking [which is] not something that we’re necessarily seeing in silicon carbide, so there may be reliability issues” with GaN, he explains. Chu agrees that long-term reliability is an area for improvement, saying “there are a few technical improvements we can make: One is making it more reliable at a high temperature. Right now, I think we can hold at 800 ℃ for probably 1 hour.” Gallium Nitride vs. Silicon Carbide There is still a lot of work to be done to improve the device, says Xiong. He explains that other than minimizing leakage current, one function of the tantalum silicide barrier is to prevent titanium in the device from potentially reacting with the AlGaN film, which could destroy the 2DEG. Eventually, Xiong wants to remove titanium from the device altogether. “The ultimate goal, I would say, is to not rely on titanium,” he concludes. Despite its potential longevity challenges, the group’s chip is pushing the limits of where electronics can operate, such as on the surface of Venus . “If you can hold it for 1 hour at 800 ℃, that means that at 600 or 700 ℃, you can hold it for much longer,” Chu explains. Venus’s ambient temperature is 470 ℃, so GaN’s new temperature record could be useful for electronics in a Venus probe . The 800 ℃ figure is also important for hypersonic aircraft and weapons, explains Mantooth. Their extreme speeds generate friction that can heat up the surface to 1,500 ℃ or more. “One of the things a lot of people don’t realize is that when you’re flying at Mach 2, or Mach 3, the air friction creates an extreme environment on the leading edge of the wing…. And guess what? That’s where your radar is located. That’s where other processing equipment is located. These applications are why the U.S. Defense Department is interested in electronics for extreme temperatures,” says Mantooth. As far as plans for the future, Chu says the next steps are to “scale the device to make it run faster.” He also thinks that the chip may be ready for commercialization not too far down the line, because there are so few suppliers for chips capable of operating at such extreme temperatures. “I think it’s quite ready. It requires some improvements, but the nice thing about high-temperature electronics is there’s nothing else there,” he says. The gallium nitride circuit’s victory against its silicon carbide companions may not last long, however. Mantooth’s lab also fabricates high-temperature chips, and is working on getting silicon carbide to hit the heat levels that Chu’s chips have. “We’ll be fabricating circuitry to try to attack the same temperatures with silicon carbide,” says Mantooth. Though it’s unclear who will eventually finish on top, at least one thing is certain: The competition is still heating up. This article appears in the October 2025 print issue as “Gallium Nitride Tops 800 °C in High-Temp Rivalry.”",
    "published": "Mon, 11 Aug 2025 14:00:02 +0000",
    "author": "Velvet Wu",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Advancing Semiconductor Interconnects for Next-Gen Transistors",
    "link": "https://spectrum.ieee.org/kamal-rudra-transistors",
    "summary": "Kamal Rudra found the topic of semiconductors pretty boring when he encountered it in high school. But that all changed when he took a college course on semiconductor optoelectronics. The IEEE member credits the class’s professor with sparking his interest in the technology. “His teaching style was engaging and incredibly effective,” Rudra says. “It made me genuinely fall in love with the subject. The combination of hands-on lab experiments and deep theoretical learning finally gave me clarity that this was the field I wanted to pursue.” Kamal Rudra Employer: IBM Research in Albany, N.Y. Title: Research and development integration engineer Member grade: Member Alma maters: Motilal Nehru National Institute of Technology in Allahabad, India; University of Michigan in Ann Arbor A research and development integration engineer at IBM Research in Albany, N.Y., Rudra is now working on back-end-of-line (BEOL) integrated circuits to create copper wiring structures for logic technologies. “With the advent of novel transistor architectures, we are paving the way for faster and more-energy-efficient processors for AI and other uses,” Rudra says. “By improving the BEOL interconnects, RC delay and electromigration can be reduced, ensuring transistor-level gains translate into system-level performance.” For his work, he was recognized last year on the IEEE Computer Society ’s Top 30 Early Career Professionals in Computing list and received US $2,500. The Computer Society recognition “reinforces my belief that I’m moving in the right direction,” Rudra says, “and it encourages me to keep pushing the boundaries of what’s possible in semiconductor technology and computing.” Inspired by a physics teacher Like many engineers, Rudra was fascinated by how things worked as a youngster. Growing up in India, he would take apart remote-controlled cars and use the motors and batteries to build something new. No one in his family worked in science, technology, engineering, or math, he says, but science has always felt “completely natural.” His fascination with STEM subjects deepened in high school, and his teachers fueled his passion. But it was M.R. Shenoy, a physics professor at the Indian Institute of Technology, Delhi , who inspired him to pursue research in the semiconductor field. As an undergraduate at the Motilal Nehru National Institute of Technology , in Allahabad, India, Rudra took an online semiconductor optoelectronics certification course offered by NPTEL that was taught by Shenoy. Motivated to gain experience in the field, from 2017 to 2019 Rudra completed several internships working on semiconductor fabrication at organizations including the Indian Institute of technology (BHU) , Varanasi; the Indian Institute of Technology , Kharagpur; and the Central Electronics Engineering Research Institute . “These internships were my first real exposure to experimental research and gave me a foundation in device physics and material science,” he says. He earned a bachelor’s degree in electronics and communications engineering from MNNIT in 2019. Working in semiconductor fabrication and FinFET After graduating he became a research assistant at the Indian Institute of Science (IISc), in Bengaluru. There he developed a thin film of manganese vanadium oxide using the epitaxy process. The method grows the manganese vanadium oxide on top of the crystal substrate—which gives engineers more control over the film’s thickness, composition, and crystal structure, according to Photonics Media . Rudra used the film to develop a photodetector for infrared light. After a year he joined semiconductor manufacturer GlobalFoundries , also in Bengaluru, as an integration and yield engineer. He continued his work at IISc on the weekends. “Before joining, I had modest expectations—largely because of the limited semiconductor manufacturing ecosystem in India,” he says. “But I realized that hardware-focused work was indeed happening in the industry. That experience planted the seed of transitioning from academia to cutting-edge industrial R&D.” “For any young professional in STEM, IEEE isn’t just a resource; it’s a launchpad.” While at GlobalFoundries, he worked on BEOL interconnects to enhance the yield of the company’s FinFET chips, which are used in automotive technology, smartphones, and smart speakers. In conventional planar transistors, the gate sits atop a flat silicon channel, controlling the flow of current between the source and drain from only one side. As transistors shrank in size, however, they became less reliable and leaked current, wasting power. FinFET development was led by Chenming Hu , who received the 2020 IEEE Medal of Honor for the invention. FinFET’s 3D structure provides better control of the current. In 2021 Rudra decided to continue his education and was accepted into the master’s degree program in electrical and computer engineering at the University of Michigan in Ann Arbor. “I chose this school because there was a particular professor working on LED devices whose research really resonated with me,” he says. The professor, IEEE Fellow Zetian Mi , was working on III-V semiconductor optoelectronic devices. Rudra was part of Mi’s research team for the first two semesters of his graduate program, working on the fabrication and characterization of III-Nitride-based microLEDs. Rudra also completed an internship at Meta in Redmond, Wash., where he developed integration processes for waveguide-based photonic devices for AR/VR systems. “That experience helped me understand how photonics intersects with semiconductor manufacturing,” he says, “particularly in emerging applications like next-gen displays and wearable optics.” After earning his degree in 2023, he joined Samsung Semiconductor in Austin, Texas, as a device integration engineer. There he returned to working on FinFETs, but this time analyzing and optimizing device and front-end-of-line integration for 14-nm node technology and how different processes affect the electrical performance and yield of FinFETs. After a year, he left to join IBM Research. “Being part of such a high-impact, globally collaborative initiative has been a fantastic experience and one that continues to push me technically and professionally,” he says. Since working at the company, he has filed 22 U.S. patents. He received a slew of honors last year, including being named to Semicon West ’s 20 Under 30 list, the Society of Manufacturing Engineers ’s 30 Under 30, and Semicon Europa ’s 20 Under 30. Recently, he made the Albany Business Review 40 Under 40 list. “These recognitions,” he says, “have been deeply motivating—not just as personal milestones but as validation of the collective work I’ve been part of, and the mentors who’ve helped shape my path.” Making important connections at IEEE Rudra joined IEEE in 2020 after his “Visible Light Response in Defect Engineered Wrinkle Network Nanostructured ZnO” research paper was accepted by the IEEE Electron Devices Technology and Manufacturing Conference . He continues to renew his membership, he says, because of the networking opportunities it provides, as well as technical content that helps him stay up to date on semiconductors. In addition to the IEEE Computer Society, he is an active member of the IEEE Electron Devices (EDS), Electronics Packaging , and Photonics societies. Each “is home to a vibrant network of engineers, scientists, and innovators who are accomplished in their respective fields,” Rudra says. In 2022 he received an Electron Devices Society master’s fellowship , which awarded him $2,000 to use toward research within the society’s fields of interest. Receiving the honor, he says, was a powerful motivator. He is active with IEEE Young Professionals and is a member of the Electron Devices Society’s YP committee. Being a part of the community, he says, gives him access to world-class expertise and provides resources to help him make career decisions and solve technical challenges. He is also a part of the organizing committee for this year’s IEEE EDS Summer School , a two-day lecture program for university seniors, graduate students, postdoctoral fellows, and young professionals. “For any young professional in STEM, I believe IEEE isn’t just a resource; it’s a launchpad,” Rudra says. “Getting involved early helps you grow technically, professionally, and personally in a way few organizations can offer.”",
    "published": "Thu, 07 Aug 2025 18:00:03 +0000",
    "author": "Joanna Goodrich",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Protecting Electronics Against Electrostatic Discharge",
    "link": "https://spectrum.ieee.org/electrostatic-discharge",
    "summary": "Today’s semiconductor devices are built with nanometer-scale features and operate at increasingly lower voltages—which makes them more susceptible to even minor electrical overstress. Electrostatic discharge (ESD) is a persistent, costly challenge in the electronics industry. A discharge of just 100 volts can degrade or destroy sensitive components designed to operate at voltages as low as 1.2 V. According to the EOS/ESD Association , ESD rapidly and spontaneously transfers an electrostatic charge induced by a high electrostatic field. It typically occurs when two objects with different electrical potentials come into contact or close proximity, allowing electrons to jump between them, often through a small spark. ESD can cause immediate physical damage to circuit paths or introduce latent defects that lead to failures later in the product life cycle. As electrical devices become smaller and more sensitive, the ESD risks grow, along with their financial impact. Industry experts estimate that ESD is responsible for more than 30 percent of semiconductor failures during manufacturing and handling. The financial impact can add up quickly. The cost of the discharge damage can range from a few US cents for a simple diode to thousands of dollars for complex integrated circuits, according to the EOS/ESD Association. When factoring in revisions, labor, shipping, and overhead, the total cost to manufacturers can be substantial. A new ESD protection design program To equip engineers, technicians, and quality-assurance professionals with knowledge and tools to mitigate electrostatic discharge, IEEE has launched a Practical ESD Protection Design course and certificate program. The new training is suitable for individuals and organizations seeking to improve their ESD control. The standards-based instruction is aligned with ANSI/ESD S20.20–21: Protection of Electrical and Electronic Parts and other industry guidelines. The interactive modules cover theory, real-world case studies, and practical mitigation techniques. “An understanding of ESD is valued in multiple areas, ranging from design to testing and handling equipment in the field,” says Zachariah Peterson , an IEEE member and ESD industry expert and executive consultant for Northwest Engineering Solutions , in Portland, Ore. “Equipment failure due to ESD results in more than just rework costs, the damage is also to a company’s brand. The ability to anticipate ESD gives engineers a critical leg up in building reliable products and a durable business.” After successfully completing the training program, learners earn an IEEE certificate for 89 professional development hours and 8.9 continuing education units. As the electronics industry evolves, the importance of ESD control is likely to increase. With the rise of artificial intelligence , 5G , and edge computing, the demand for high-performance, reliable chips is growing while the margin for error is shrinking. The IEEE Practical ESD Protection Design program is not just a preventative measure; it’s a strategic choice that can support innovation, quality, and long-term success.",
    "published": "Tue, 05 Aug 2025 18:00:03 +0000",
    "author": "Angelique Parashis",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "What the CHIPS Act Looks Like Now",
    "link": "https://spectrum.ieee.org/chips-act-map",
    "summary": "The U.S. CHIPS and Science Act of 2022 aimed to reestablish advanced manufacturing for logic and memory in the United States, as well as boost or establish other chipmaking activities. The job is far from complete, but a look at where the money is expected to go points to a potentially broad geographic boost for the domestic chip industry. That’s assuming it continues. Not long after the law took effect, the federal government began careful negotiations and had in hand proposed deals for more than 30 projects by the end of October 2024. After Donald Trump won the 2024 election, the CHIPS Office went into high gear, converting those proposed deals into awards. It agreed to more than $30 billion in the roughly two months before Trump took office. But things have gotten deathly quiet since then. Proponents of the CHIPS Act shouldn’t panic…yet, says Russell Harrison, managing director of IEEE-USA and an expert on the workings of Washington. New administrations often press pause to examine what they want to keep and change—and to find ways to take credit for successes. In the meantime, Harrison’s team is focused on getting Congress to fund the parts of the act meant to solidify any manufacturing gains —such as the R&D and workforce-development programs.",
    "published": "Mon, 28 Jul 2025 13:00:03 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Ferroelectric Helps Break Transistor Limits",
    "link": "https://spectrum.ieee.org/negative-capacitance-schottky-limit",
    "summary": "Integrating an electronic material that exhibits a strange property called negative capacitance can help high-power gallium nitride transistors break through a performance barrier, say scientists in California. Research published in Science suggests that negative capacitance helps sidestep a physical limit that typically enforces trade-offs between how well a transistor performs in the “on” state versus how well it does in the “off” state. The researchers behind the project say this shows that negative capacitance, which has been extensively studied in silicon, may have broader applications than previously appreciated. Electronics based on GaN power 5G base stations and compact power adapters for cellphones. When trying to push the technology to higher frequency and higher power operations, engineers face trade-offs. In GaN devices used to amplify radio signals, called high-electron-mobility transistors (HEMTs) , adding an insulating layer called a dielectric prevents them from wasting energy when they’re turned off, but it also suppresses the current flowing through them when they are on, compromising their performance. To maximize energy efficiency and switching speed, HEMTs use a metal component called a Schottky gate, which is set directly on top of a structure made up of layers of GaN and aluminum gallium nitride. When a voltage is applied by the Schottky gate, a 2D electron cloud forms inside the transistor. These electrons are zippy and help the transistor switch rapidly, but they also tend to travel up toward the gate and leak out. To prevent them from escaping, the device can be capped with a dielectric. But this additional layer increases the distance between the gate and the electron cloud. And that distance decreases the ability of the gate to control the transistor, hampering performance. This inverse relationship between the degree of gate control and the thickness of the device is called the Schottky limit. “Getting more current from the device by adding an insulator is extremely valuable. This cannot be achieved in other cases without negative capacitance.” —Umesh Mishra, University of California, Santa Barbara In place of a conventional dielectric, Sayeef Salahuddin , Asir Intisar Khan , and Urmita Sikder , electrical engineers at University of California, Berkeley, collaborated with Srabanti Chowdhury and Jeongkyu Kim at Stanford University to test a special coating on GaN devices with Schottky gates. This coating is made up of a zirconium oxide layer frosted with a thin topping of hafnium oxide. The 1.8-nanometer-thick bilayer material is called HZO for short, and it’s engineered to display negative capacitance. HZO is a ferroelectric . That is, it has a crystal structure that allows it to maintain an internal electrical field even when no external voltage is applied. (Conventional dielectrics don’t have this inherent electrical field.) When a voltage is applied to the transistor, HZO’s inherent electric field opposes it. In a transistor, this leads to a counterintuitive effect: A decrease in voltage causes an increase in the charge stored in HZO. This negative capacitance response effectively amplifies the gate control, helping the transistor’s 2D electron cloud accumulate charge and boosting the on-state current. At the same time, the thickness of the HZO dielectric suppresses leakage current when the device is off, saving energy. “When you put another material, the thickness should go up, and the gate control should go down,” Salahuddin says. However, the HZO dielectric seems to break the Schottky limit. “This is not conventionally achievable,” he says. “Getting more current from the device by adding an insulator is extremely valuable,” says Umesh Mishra , a specialist in GaN high-electron-mobility transistors at the University of California, Santa Barbara, who was not involved with the research. “This cannot be achieved in other cases without negative capacitance.” Leakage current is a well-known problem in these kinds of transistors, “so integrating an innovative ferroelectric layer into the gate stack to address this has clear promise,” says Aaron Franklin , an electrical engineer at Duke University, in Durham, N.C. “It certainly is an exciting and creative advancement.” Going Further With Negative Capacitance Salahuddin says the team is currently seeking industry collaborations to test the negative capacitance effect in more advanced GaN radio-frequency transistors. “What we see scientifically breaks a barrier,” he says. Now that they can break down the Schottky limit in GaN transistors under lab conditions, he says, they need to test whether it works in the real world. Mishra agrees, noting that the devices described in the paper are relatively large. “It will be great to see this in a device that’s highly scaled,” says Mishra. “That’s where this will really shine.” He says the work is “a great first step.” Salahuddin has been studying negative capacitance in silicon transistors since 2007 . And for much of that time, says Mishra, Salahuddin has been subject to intense questioning after every conference presentation. Nearly 20 years later, Salahuddin’s team has made a strong case for the physics of negative capacitance, and the GaN work shows it may help push power electronics and telecom equipment to higher powers in the future, says Mishra. The Berkeley team also hopes to test the effect in transistors made from other kinds of semiconductors including diamond, silicon carbide, and other materials. This post was corrected on 1 August 2025 to fix the spelling of Urmita Sikder’s name and the order of the ferroelectric coatings’ constituent parts.",
    "published": "Mon, 28 Jul 2025 12:00:03 +0000",
    "author": "Katherine Bourzac",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Startup Claims up to 100x Better Embedded Computing Efficiency",
    "link": "https://spectrum.ieee.org/efficient-computer-dataflow-architecture",
    "summary": "There’s a growing need for CPUs that can live life on the edge. That is, computing for a long time embedded in hard-to-get-to places and surviving on battery power or energy they can scrounge from the environment. Frustrated with inherent inefficiencies in the architecture of ultralow-power microprocessors , the founders of startup Efficient Computer decided to reinvent the general-purpose processor from the ground up for energy efficiency. “We’re doing something that has the capability of a CPU but is one or two orders of magnitude more efficient,” says cofounder Brandon Lucia . The result, the Electron E1 and its accompanying compiler, is now heading to developers and early partners. According to Lucia, the C -programmable processor is delivering between 10- and 100-fold better efficiency than commercial ultralow-power CPUs on typical embedded systems tasks, like performing a fast Fourier transform on sensor data or doing convolutions for machine learning. The key innovation was to invent an architecture that can lay out any program’s instructions spatially on a chip rather than delivering them sequentially from memory as is done now in processors that follow the von Neumann architecture , says Lucia. A Fabric for Dataflow The von Neumann architecture has dominated computing for decades. It basically takes in an instruction from memory that tells the processor what to do with data—add it to something, flip it around, whatever—and puts the result in memory. Then it picks the next instruction, and the next, and so on. It sounds simple, but it actually comes with a lot of overhead. “Several billion times per second, you’re pulling an instruction in from memory. That operation costs some energy,” says Lucia. Additionally, to prevent the process from stalling, modern CPUs have to guess at what instruction comes next, requiring logic called branch prediction and still more overhead. Instead, the E1 maps out the sequence of instructions as a spatial pathway through which data moves. Fundamentally, the E1 is an array of “tiles.” Each is like a stripped-down processor core—capable of performing a set of instructions but lacking instruction fetching, branch prediction, and other overhead. The tiles are linked together in a specially designed, programmable network. The E1’s compiler, called the effcc Compiler , reads the program, which can be written in C or other common languages and platforms, and assigns each instruction in the program to a tile. It then sets up the network so that data enters one tile, is processed, and the result becomes the input to the next tile all in the right sequence to run the program. When the sequence branches, such as when the program encounters an if/then/else, so too does the spatial pattern of tiles. “It’s like a switch track in a railroad,” says Lucia. “There have been other dataflow-style architectures,” Lucia notes. Google’s TPUs and Amazon’s Inferentia chips, for example, are designed around a dataflow architecture called a systolic array. But systolic arrays and other dataflow efforts are restricted to a subset of all the possible data paths software might demand, Lucia says. In contrast, the E1’s network fabric allows any arbitrary path a program could ask for. Critical to that is the fabric’s ability to support so-called arbitrary recurrences, such as the “while loop.” (Think: “while the light is red, depress the brake.”) Such loops require a feedback path. “It turns out that’s harder than it seems when you first look at it,” says Lucia. The E1 fabric can carry values around the feedback paths in a way that allows for general purpose computing. “A lot of other dataflow architectures don’t do general purpose because they couldn’t crack that nut.… It took us years to get it right.” According to Efficient Computer, the E1 consumes less energy than two competing ARM processors at three common tasks: matrix multiplication for machine learning, the fast Fourier transform, and convolution for computer vision. Efficient Computer According to University of Michigan computer science and engineering professor Todd Austin , chips like the E1 are a good example of an efficient architecture, because they minimize parts of the silicon engaged in things that are not purely computation, such as fetching instructions, temporarily stashing data, and checking if a network route is in use. Lucia’s team “is doing a lot of clever work to allow you to get extremely low power for general purpose computing,” says Rakesh Kumar , a computer architect at the University of Illinois Urbana-Champaign. The challenge for the startup will be economics, he predicts. “Ultralow-power companies have had a hard time because of strong competition in low-power, very cheap microcontrollers. The key challenge is in identifying a new capability” and getting customers to pay for it.",
    "published": "Thu, 24 Jul 2025 13:00:03 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Researchers Stabilize Novel State of Matter for Faster Compute",
    "link": "https://spectrum.ieee.org/charge-density-wave",
    "summary": "A team of researchers working in different universities across the United States found a new technique to unlock a hidden state of matter that could make electronics times faster in the future. This novel state of matter is a metallic phase in an otherwise insulating compound that can only be accessed by using ultrafast lasers. This capability to change phases quickly makes them very appealing for potential technological applications. New research shows that quick changes in temperature can access a new phase, in which the insulating and hidden phases coexist. In switching fast from a conducting to an insulating state, tantalum disulfide (1T-TaS₂, the material used in the research) has the potential to replace silicon components in electronic devices, allowing them to become faster. Unlike conventional electronics, where data is stored in one place and processed in another — causing energy loss during constant movement — materials like tantalum disulfide can potentially store and process information in the same location , aided by this hidden phase where insulating and conducting properties exist in close proximity. This could make them much more energy-efficient. What’s more, instead of just flipping between two states like binary silicon (0 or 1), these materials may stabilize multiple distinct states, which opens the door to denser and more efficient data encoding. “Artificial intelligence with regular semiconductors is extremely power hungry—and this could be an interesting, promising approach [that could potentially consume a lot less energy],” says John Miller , a physicist with the University of Houston who did not participate in the study. Because today’s AI systems burn enormous amounts of energy moving data between memory and processors, researchers are increasingly drawn to materials like 1T-TaS₂, Miller says. “These quantum materials could enable ‘in-memory computing’—storing and processing information within the same atomic landscape—which might slash power consumption while preserving speed,” he adds. Bringing stability to an exotic state The study also showed it is possible to stabilize this new state at higher temperatures for longer than ever before. Alberto De la Torre used controlled heating and cooling to make a quantum material switch between a conductive state and an insulating state. Matthew Modoono/Northeastern University The key was to heat and freeze single crystals of tantalum disulfide, a material known for its ability to switch quickly between insulating and conducting states with light or heat. This technique is called thermal quenching. “The idea is to heat the system above a phase transition and then cool it fast enough that it doesn’t have time to fully reorganize,” says lead author Alberto De la Torre , a physicist with Northeastern University. De la Torre’s team heated the material well above the boiling temperature of water (147 °C) and then cooled it—or quenched it—at a fast rate (-153 °C per second) to see how it would behave. At about 77 °C, tantalum disulfide transitions from a metallic-like state to an insulating phase—and at -63 °C, the material’s crystals stabilized in a mix of insulating and metallic phases. This happened because temperature changes shifted the way electrons organize in tantalum disulfide. Instead of being evenly spread out along the material’s lattice (or atomic arrangement), electrons bunched together in certain regions and spread out in others, forming a wave pattern—or a charge density wave (CDW) phase. “Where the electrons are more mobile, you get conduction; where the CDW locks them into place, you get insulation,” De la Torre explains. This is the first time researchers managed to get a mixed phase of the material by using temperature control alone. The change of states in tantalum disulfide by the use of temperature only is important because laser technologies are more expensive and harder to embed in electronics than non-cryogenic cooling techniques, says De la Torre. “If you change the temperature very slowly [...] the system is going to migrate toward thermal equilibrium. But if you go down very, very fast [...] you can stabilize things into a kind of a non-equilibrium state,” says Miller. This mixed phase can be repeatedly and reliably re-created as many times as needed since the thermal quenching process is reversible. Previous studies managed to reach this mixed state in tantalum disulfide by using ultrashort laser or voltage pulses. The problem is this state would only last for a few microseconds at almost impossibly cold temperatures (-213 °C), requiring cryogenic cooling to work. The fact that it is now possible to keep a mixed state stable at much higher temperatures for hours on end—the system stays stable until reheated at about 77 °C—means this is approaching a much more device-friendly technology that could transform computers in the future. Real-life developments might be still far along the road, but not needing liquid nitrogen to stabilize mixed states in CDW materials is a large stride ahead, De la Torre says.",
    "published": "Wed, 23 Jul 2025 18:04:51 +0000",
    "author": "Meghie Rodrigues",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "2D Transistors Could Come Sooner Than Expected",
    "link": "https://spectrum.ieee.org/cdimensions-2d-semiconductors",
    "summary": "Chipmaking giants like Intel, Samsung, and TSMC see a future where key parts of silicon transistors are replaced with semiconductors that are only a few atoms thick. Although they’ve reported progress toward that goal, that future is generally thought to be more than a decade away . Now, a startup spun out of MIT thinks it has cracked the code for making commercial-scale 2D semiconductors and expects chipmakers to have integrated them in advanced chips in half that time. CDimension has developed a process for growing molybdenum disulfide (MoS 2 ), a 2D semiconductor, on silicon at a low enough temperature that it will not damage underlying silicon circuits. That could allow the integration of layers of 2D transistors above existing silicon circuits and eventually multitiered 3D chips made from 2D devices. “A lot of people think of 2D semiconductors as something that’s still in the laboratory,” says CDimension CEO and co-founder Jiadi Zhu . “But CDimension has a proprietary tool designed for 2D material growth…and we’ve addressed a lot of critical [2D materials] problems regarding wafer-scale uniformity, regarding device performance and variation, regarding device reliability, and regarding compatibility with silicon manufacturing processes.” Taken together, 2D semiconductors are ready to enter an industrial phase of development, he says. Much of CDimension’s plans hinge on a proprietary process that it uses to grow a single layer of MoS 2 on silicon and other substrates at only about 200 °C across entire 300-millimeter wafers. 2D materials are formed by chemical vapor deposition, wherein vaporized precursor chemicals react on a surface to coat it. But typically the reactions for making 2D materials requires temperatures upward of 1,000 °C. That’s so high it would damage any underlying structures needed to make transistors. Today, researchers get around that problem by depositing the 2D semiconductor separately and then delicately transferring it to a silicon wafer. But CDimension’s system can grow the materials right on the silicon wafer without damage. The 2D Semiconductor Business Part of the startup’s business right now is to ship silicon wafers with 2D material grown on it so customers can evaluate it and build devices. Alternatively, customers can send wafers that have already been processed so that they have silicon circuits or structures on them. CDimension can then grow MoS 2 or other 2D materials atop that and send it back to the customers, so they can integrate a layer of 2D devices with their silicon circuits. A test wafer made with CDimension’s process sits underneath a microscope. CDimension The latter might be 2D semiconductor’s first industrial entry. “We’re showing the possibilities with silicon plus 2D material,” Zhu says. “But 2D material might be used for the highly scaled logic devices as well. That can be the next step.” Chipmakers like Intel , Samsung , and TSMC reported research aimed at replacing silicon nanosheets in their future transistors with MoS 2 and other 2D semiconductors at the IEEE International Electron Device Meeting in December 2024. At the same conference, Zhu and his colleagues from the MIT laboratories of IEEE Fellow Tomás Palacios and Jing Kong showed that the low-temperature synthesis could produce MoS 2 transistors with multiple stacked channels, akin to nanosheet transistors. (Palacios is a strategic advisor to CDimension.) By scaling down the device, the team predicted that such devices could meet and exceed the requirements of the future 10A (1-nanometer) node in terms of power consumption, performance, and the area they occupy. A big motivation to go with 2D semiconductors is to reduce power consumption, says Zhu. Power is lost in transistors both when they are on (dynamic power) and when they are off (static power). Because it’s just over 0.6 nm thick, 2D transistors have qualities that could let them operate using about half the voltage of today’s silicon devices, saving dynamic power. When they are off, it’s leakage current you have to worry about most. But MoS 2 has a bandgap that’s more than twice the value of silicon’s, meaning it takes much more energy for charge to leak across the device. Zhu says devices made using CDimension’s materials consumed as little as one-thousandth the energy of silicon devices. In addition to MoS 2 , which is an electron-conducting (n-type) semiconductor, the startup also provides tungsten diselenide, a p-type semiconductor, as well as 2D insulating films, such as hexagonal boron nitride. The whole combination will be needed if 2D semiconductors are to ever take over in future CMOS chips. This article appears in the September 2025 print issue as “ 2D Transistors Could Come Much Sooner Than Expected .”",
    "published": "Thu, 17 Jul 2025 13:00:04 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Funding Cuts Jeopardize U.S. Chip Supply Chain Study",
    "link": "https://spectrum.ieee.org/funding-cuts-chip-security-research",
    "summary": "As part of the Trump administration’s push to reduce government spending, the U.S. Department of Defense has rolled back more than US $580 million in funding, including $360 million in grants, some of which were tied to universities . The cuts pumped the brakes on troves of projects across the country, including national security–focused research that President Trump deemed unaligned with national priorities. Among the projects placed on indefinite hold was a multiyear, $3 million study at Cornell University funded through the National Defense Authorization Act (NDAA) to examine cyber vulnerabilities in the U.S. semiconductor supply chain. It was one of more than 75 DoD-funded projects at Cornell that received a stop-work order. The research came amid rising geopolitical tensions and a global AI arms race. Chips are critical to both AI infrastructure and military systems. Without understanding risks—such as design software attacks and IP theft—some experts warn that halting this work could leave the chip supply chain open to sabotage, posing threats to U.S. economic and national security. Sarah Kreps , a professor of government at Cornell and director of the university’s Tech Policy Institute , led the research. In a conversation with IEEE Spectrum , Kreps explained what her team set out to study, how the project came to a halt, and why securing the chip supply chain is more urgent than ever. The following has been edited for length and clarity. Sarah Kreps on: The origin of her research Vulnerabilities of the chip supply chain Her reaction to the funding cuts How did this project come about? Sarah Kreps: About two years ago, Micron reached out to the Cornell Tech campus. They were building a $100 billion manufacturing facility near Syracuse and were concerned about the security of their entire supply chain. They said they didn’t have the mental bandwidth or in-house expertise to figure out where the vulnerabilities lay, and therefore couldn’t figure out how to address them. Cornell Tech said this was an interdisciplinary project they couldn’t handle on their own, so they reached out to us. My work focuses on emergent technologies—AI, semiconductors—with a national security focus, which is what Micron was most interested in, especially with chips that ultimately end up with DoD as the customer. We put together one-pagers about the project, and eventually it made it into the 2025 NDAA. We finally got the grant in December 2024, and the work was set to run for the calendar year 2025. Why did this feel so urgent? Kreps : We started this around the time the AI boom really took off. I’ve worked in the AI space since 2018, but it’s completely accelerated. The geopolitics of AI matter. If there’s a goal to be a global leader in AI, then we have to secure the infrastructure underlying it. Part of the problem is that infrastructure isn’t very sexy. It’s easy to take for granted. But if you want to be a leader in AI, meet national security needs, and fulfill domestic manufacturing goals, you need a resilient supply chain. Our early work showed we’re not there yet. What was the scope of the research, and what were you hoping to achieve? Kreps : We were compressing quite a bit of work into a one-year window. There were three phases, each addressing key vulnerabilities in the defense semiconductor supply chain. The first phase involved mapping out material and information networks: identifying critical nodes, choke points, and redundancies to assess supply chain resilience. The material side traced manufacturing dependencies, while the information side looked at data flows, security protocols, and alternative communication routes to prevent theft or disruption. The second phase brought in the cyber vulnerability angle. We were going to conduct penetration and stress testing of key manufacturing stages like wafer fab and lithography to identify security gaps and high-risk nodes based on the testing results. The third phase in the fall was going to integrate the findings through network simulations and tabletop exercises. That would’ve modeled how disruptions cascade, analyzed the dual vulnerabilities of the material and information flows, and tested mitigation strategies. We were going to bring in industry, government, and academic stakeholders to validate the risk scenarios and shape policy recommendations. In the end, we owed Congress both an unclassified and a classified version of these findings. We were also starting with a proof of concept—looking at analogous use and misuse cases, like the Israeli-Hezbollah pager attacks and Stuxnet . The idea was to understand what the threat environment actually looks like by pointing to real-world examples of the kinds of risks we were modeling. That formed the foundation for a typology of threats we were developing. What vulnerabilities were you uncovering, and what did early insights reveal? Kreps : We got a stop-work order in the middle of March, so we were still in the scaling-up phase of the project—bringing the team together, building out the lab, setting up the hardware. We’d spent a few hundred thousand dollars on advanced servers and compute to build the cyber test range. One early insight was just how little different parts of the supply chain really understand their vulnerabilities, let alone know what their own supply chain looks like. If you don’t know who your tier-three suppliers are, you can’t figure out what the vulnerabilities are. We were supposed to visit Micron in Boise the week we got our stop-work order to begin mapping what they know, what they don’t know, and what they don’t know they don’t know. But we couldn’t go. It’s clear they’re under attack every day, and they don’t have the expertise to systematically assess and mitigate those threats. Walk me through some of the real-world cyber risks in the supply chain. Kreps : We have a dizzying array of possible attacks that could come from this. Upstream, in the design phase, you can have IP theft or hardware Trojans inserted. Your adversary can put something malicious into the software at the front end that could lead to cloned or sabotaged chips. At the fabrication stage, which is often offshore—Taiwan, South Korea, China—the foundry itself could be compromised. You can imagine software altering doping levels or embedding hidden functionalities without the designer even knowing. That could lead to sabotage, low-yield manufacturing, or chips that don’t work. During post-fab testing and assembly, contractors could introduce subtle changes—malicious code in the firmware or test environments that are hard to detect after deployment. In the toolchain phase, you have risks of malware in design software that could infect the chips. These tools are often updated in the field, which becomes another attack vector. At packaging and distribution, malicious updates could be inserted at the last mile. And then there’s the idea that dormant code could allow a chip to be disabled in the future—remote kill switch–type risks. These are all real concerns. What was your reaction to the funding cut, and how did it affect your team? Kreps : Frankly, I was blindsided. In the initial weeks of DOGE, we were feeling like we had dodged a bullet. We felt like the project seemed so central to national security, and that anyone who cares about the country would see that pausing this kind of work is shooting yourself in the foot. But by mid-March, we got caught in the crosshairs of the political fallout with higher education—$1 billion pulled from Cornell. The PhD students on the project had to stop work right away because their support is tied to the grant. We had some admin help, undergrads, and all of that went away. It really made me think about the funding and training pipeline from universities to industry. Students in these programs often go on to work for SpaceX, Micron, and ASML. These industries—which are so central now to the U.S. economy and national security—are going to be losing their training pipeline. I think that’s a real problem. Federal grants are what make that training possible. If you cut them, you’re not just disrupting research; you’re creating a talent deficit in national security and aerospace that will show up in three to five years. Is there any chance the funding could be reinstated? Kreps : We’re working on a few different angles. But this was an executive-level decision, so it’s not something Cornell can change unilaterally. Columbia, for example, received a list of conditions they needed to meet to restart their funding. We didn’t get anything like that, which has made it harder. Are you optimistic the project will resume? Kreps : Depends on the day. Sometimes it feels like we’re making progress. Other times, the conflict between higher ed and the executive branch feels unresolved. We’re doing what we can—pushing forward with dissertations, keeping the infrastructure in place—with the hopes there’s a willingness to turn the project back on. So if and when we get the green light, we’ll be ready.",
    "published": "Wed, 18 Jun 2025 14:55:36 +0000",
    "author": "Aaron Mok",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Why the Semiconductor Industry Can’t Abandon Women",
    "link": "https://spectrum.ieee.org/women-in-semiconductors-workforce",
    "summary": "The percentage of women in the semiconductor industry is stubbornly low. According to a report released in April, 51 percent of companies report having less than 20 percent of their technical roles filled by women. At the same time, fewer of these companies were publicly committed to equal opportunity measures in 2024 than the year prior, the same report found. This lack of support comes at the same time that major workforce shortages are expected, says Andrea Mohamed , COO and co-founder of QuantumBloom , which helps companies attract, retain, and advance early career women in STEM. The company focuses on the transition from higher education to the workforce, a critical point during which many women leave STEM. IEEE Spectrum spoke to Mohamed about supporting women in semiconductor jobs, and why a retreat from these initiatives is at odds with the needs of the industry. Andrea Mohamed on: The current state of the semiconductor industry How a lack of support for women will impact the industry Whether the industry is regressing in its hiring practices What the semiconductor industry can learn from other industries Tell me about your perspective as a returning veteran of the semiconductor industry. Andrea Mohamed: I worked for a semiconductor startup company over 20 years ago, and it was very male dominated. Now, it’s still very male dominated. Seeing the semiconductor industry with fresh eyes, what I see is an industry that hasn’t evolved as quickly as other STEM-intensive industries. I’ve worked for science and research-oriented organizations, and the progress that’s been made in other sectors just hasn’t been made in this particular sector. Return to top How might the lack of support for women in the U.S. semiconductor industry create additional problems? Mohamed: On a macro scale, you have an industry that is facing a lot of geopolitical and economic forces that are disrupting the whole supply chain ecosystem around semiconductors, and there’s a push to reshore and onshore . There are a lot of infrastructure gaps in doing that, one of them being the workforce component . It’s not just semiconductors that are poised to be reshored and onshored to the United States; it’s also pharmaceuticals and automotive. And all of that is going to continue to put pressure on the supply and demand curve, if you will, around labor. There’s been an enormous amount of attention on the STEM education pipeline, and rightfully so. China and India are producing STEM graduates at a rate that we are not keeping pace with. While we’ve had that focus on the STEM education pipeline, there’s been very little focused attention on what industry is doing inside companies to address the workforce challenges. There is a lot of additional concern around corporate cultures, burn-and-churn cyclical nature, policies that seem out of date relative to other industries, including as it relates to child care. Industry is very clearly articulating to education what it needs the next generation to have from a skills perspective . But we don’t see the voice of the next generation worker influencing how industry is attracting them. We’ve got to start to see the industry recognize how it’s in its own way when it comes to workforce development. It sounds like the problem goes beyond the “leaky pipeline” that’s often discussed. Mohamed: Right. We keep talking about the leaky pipeline for all these stages of women dropping out. It starts in middle school, when girls’ interest and confidence in STEM start to wane. At every stage there’s a leak. And then you get to this early career stage, which QuantumBloom is focused on, and that bucket is gushing. We’re losing a ton, and we’re all thinking about just putting more water in the bucket, when really, we need to fix the holes. There’s a lot of discussion about what it’s going to take to attract women, people of color, other communities into the semiconductor workforce, and very little on fixing the holes. Oftentimes the early career experience is pretty much sink or swim for everybody, regardless of gender. We know with women, it’s more likely that they leave. Return to top I understand that the semiconductor industry may actually be regressing in these areas. Can you talk about that? Mohamed: The latest report that came out from Global Semiconductor Alliance and Accenture on the state of women and semiconductors , to me, is like a canary in a coal mine. We’re seeing a decrease in public commitments for diversity and the progress that we’ve made around programs that support women. It’s counterintuitive that we are decreasing support at exactly the time we need to be attracting this audience into the industry. I understand the pressures that companies are facing around anything that’s related to DEI. We need to change the conversation from DEI to talent management. This is retention and avoiding turnover costs. This is about needing every available brilliant mind in the United States that wants to be in semiconductors. We have offshored this industry for so long. Other countries have existing talent bases. We have to build it. So the industry should work on these initiatives to build better workplaces, regardless of whether they’re labeled as promoting diversity? Mohamed: I think a lot of DEI activity was performative. A lot of companies were really not committed to creating great workplaces for everybody. I think that’s part of the reason DEI has gotten politicized. There’s this notion that people were given opportunities that weren’t based on merit. What I’m saying is that this is not a merit conversation, right? Women are graduating with bachelor’s degrees at a rate higher than men and increasing. Really, this is about human capital development. You have women who are opting out of your industry, a nd you have to recognize and pay attention to the unique lived experience of women in these environments in order to solve the problem. So there are semantics in all of this, but it’s not just relabeling. This is about business. You are not going to be able to compete on a global stage in the United States if you are not finding ways to attract and retain new communities of workers, and women are one of those communities. That means understanding what women need from their employer, because if you do not provide it, they will go somewhere else that does. The concern by companies about, if they run a program like QuantumBloom, does that create a risk? It’s the wrong question about risk. Your big risk is that your fab is empty, because you can’t find workers and retain them. Return to top What have you observed in other industries, and what can semiconductor leaders learn from them? Mohamed: Many women whose roots are in engineering end up working potentially in a technical organization, but not in a technical role. You see them also pivot into completely different industries. They go to business school, they become a consultant, they go to law school. In other industries, there are organizations that are very intentional about attracting and retaining their youngest talent. They are dedicating resources to investing in them, which is very rare—most organizations invest more the higher up you go. Really, we need to be thinking about flipping that script and investing more sooner. Andrea Mohamed is COO and co-founder of QuantumBloom, a professional development company focused on women in STEM. Andrea Mohamed When I think about employer-led solutions around early career talent, what comes to mind are apprenticeships, rotational programs, and leadership skill development —all the things you’re not taught in school but that are really important to your success. These are skills that you take with you for an entire career. When you invest in the top, most of the time people say, “I wish I had this in my 20s.” I don’t see m any of th ese solutions being used in this industry. I heard recently one of the big semiconductor giants in this country used to have an engineering rotational program and stopped it five years ago. I was talking to a person who had been in that program and how pivotal it was in their early career experience. Are there other steps that you think are important for semiconductor leaders to take? Mohamed: The things that QuantumBloom solves are very early career and focused on individuals. At the same time, companies need to be thinking about top-down culture change and industry transformation. Those are longer-term horizon things to fix. People join companies and quit bosses. The relationship with your boss is so important. You can be in a relatively terrible organization culturally and have a wonderful boss, and you can have career success. Vice versa, you could be in an awesome corporate culture with a terrible boss and not thrive. If we can improve that primary work relationship, build more empathy for each other’s experiences at a local level, we can improve work outcomes and retention. And then things start to spread. That manager who may be supporting a particular woman in our program, they learn skills and tools to be more inclusive leaders that extends beyond just that woman. We’re doing that more at that local level, but man, companies really need to be addressing top-down transformation and culture change. At the end of the day, we need semiconductor leaders to envision becoming a magnet for all talent, and then commit the resources and organizational changes needed to make that vision reality. Return to top",
    "published": "Mon, 16 Jun 2025 14:56:52 +0000",
    "author": "Gwendolyn Rak",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Intel Upgrades Chip Packaging for Bigger AI",
    "link": "https://spectrum.ieee.org/intel-advanced-packaging-for-ai",
    "summary": "This week at the IEEE Electronic Components and Packaging Technology Conference , Intel unveiled that it is developing new chip-packaging technology that will allow for bigger processors for AI. With Moore’s Law slowing down, makers of advanced GPUs and other data-center chips are having to add more silicon area to their products to keep up with the relentless rise of AI’s computing needs. But the maximum size of a single silicon chip is fixed at around 800 square millimeters (with one exception ), so manufacturers have had to turn to advanced packaging technologies that integrate multiple pieces of silicon in a way that lets them act like a single chip. Three of the innovations Intel unveiled at ECTC were aimed at tackling limitations in just how much silicon you can squeeze into a single package and how big that package can be. They include improvements to the technology Intel uses to link adjacent silicon dies together, a more-accurate method for bonding silicon to the package substrate, and a system to expand the size of a critical part of the package that removes heat. Together, the technologies enable the integration of more than 10,000 square millimeters of silicon within a package that can be bigger than 21,000 mm 2 —a massive area about the size of four and a half credit cards. EMIB gets a 3D upgrade One of the limitations on how much silicon can fit in a single package has to do with connecting a large number of silicon dies at their edges. Using an organic polymer package substrate to interconnect the silicon dies is the most affordable option, but a silicon substrate allows you to make more dense connections at these edges. Intel’s solution, introduced more than five years ago, is to embed a small sliver of silicon in the organic package beneath the adjoining edges of the silicon dies. That sliver of silicon, called EMIB, is etched with fine interconnects that increase the density of connections beyond what the organic substrate can handle. At ECTC, Intel unveiled the latest twist on the EMIB technology, called EMIB-T. In addition to the usual fine horizontal interconnects, EMIB-T provides relatively thick vertical copper connections called through-silicon vias, or TSVs. The TSVs allow power from the circuit board below to directly connect to the chips above instead of having to route around the EMIB, reducing power lost by a longer journey. Additionally, EMIB-T contains a copper grid that acts as a ground plane to reduce noise in the power delivered due to process cores and other circuits suddenly ramping up their workloads. “It sounds simple, but this is a technology that brings a lot of capability to us,” says Rahul Manepalli, vice president of substrate packaging technology at Intel. With it and the other technologies Intel described, a customer could connect silicon equivalent to more than 12 full-size silicon dies—10,000 mm 2 of silicon—in a single package using 38 or more EMIB-T bridges. Thermal control Another technology Intel reported at ECTC that helps increase the size of packages is low-thermal-gradient thermal compression bonding. It’s a variant of the technology used today to attach silicon dies to organic substrates. Micrometer-scale bumps of solder are positioned on the substrate where they will connect to a silicon die. The die is then heated and pressed onto the microbumps, melting them and connecting the package’s interconnects to the silicon’s. Because the silicon and the substrate expand at different rates when heated, engineers have to limit the inter-bump distance, or pitch. Additionally, the expansion difference makes it difficult to reliably make very large substrates full of lots of silicon dies, which is the direction AI processors need to go. The new Intel tech makes the thermal expansion mismatch more predictable and manageable, says Manepalli. The result is that very large substrates can be populated with dies. Alternatively, the same technology can be used to increase the density of connections to EMIB down to about one every 25 micrometers. A flatter heat spreader These bigger silicon assemblages will generate even more heat than today’s systems. So it’s critical that the heat’s pathway out of the silicon isn’t obstructed. An integrated piece of metal called a heat spreader is key to that, but making one big enough for these large packages is difficult. The package substrate can warp, and the metal heat spreader itself might not stay perfectly flat, so it might not touch the tops of the hot dies it’s supposed to be sucking the heat from. Intel’s solution was to assemble the integrated heat spreader in parts rather than as one piece. This allowed the company to add extra stiffening components, among other things, to keep everything flat and in place. “Keeping it flat at higher temperatures is a big benefit for reliability and yield,” says Manepalli. Intel says the technologies are still in the R&D stage and would not comment on when these technologies would debut commercially. However, they will likely have to arrive in the next few years for the Intel foundry to compete with TSMC’s planned packaging expansion .",
    "published": "Sun, 08 Jun 2025 13:00:03 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Spiking Neural Network Chip for Smarter Sensors",
    "link": "https://spectrum.ieee.org/innatera-neuromorphic-chip",
    "summary": "By mimicking the way the brain operates, neuromorphic processors can expend dramatically less energy than conventional technology for certain applications. Now, the Dutch firm Innatera has launched what it calls the world’s first commercially available neuromorphic microcontroller, in the hope of spurring mass-market adoption of this emerging technology. Innatera says its new chip, Pulsar , can lower latency to as little as one-one-hundredth that of conventional processors and consume only one-five-hundredth the power they use for artificial intelligence applications. “Most AI accelerators today have to deal with a tradeoff between performance and power,” says Sumeet Kumar, cofounder and CEO of Innatera. “They either run simplified AI models to consume less power, or ramp up their accuracy and the amount of power they need. With Pulsar, you don’t have to give up anything.” Neuromorphic Chips Mimic Brain Function Neuromorphic devices often imitate the workings of the brain in a variety of ways. For instance, whereas conventional microchips use clock signals fired at regular intervals to coordinate the actions of circuits, neuromorphic architecture often “ spikes ”—that is, generates an output signal—only after it receives a certain amount of input signals over a given time. A key application often envisioned for neuromorphic technology is to implement similarly brain-inspired neural networks , the main AI systems in use today. In addition, spiking neuromorphic devices fire spikes only rarely, so they shuffle around much less data than the electronics that typically run neural networks. As such, neuromorphic hardware in principle requires much less power and communication bandwidth for artificial intelligence applications. So far, neuromorphic devices have not found widespread use. Now Innatera hopes that Pulsar, launched on 21 May, can overcome barriers that neuromorphic computing has long faced to commercialization. The Pulsar chip possesses a hybrid analog-digital architecture. In addition to 12 digital cores for spiking neural networks, it also has four analog ones, with silicon circuits making up the spiking neurons and interconnecting synapses of each core. “The analog spiking fabric offers extremely high energy efficiency, while the digital spiking fabric offers more programmability and configurability while still offering very good energy efficiency,” Kumar says. Developers can pick which set of cores they want to load their models onto depending on their needs, he explains. Each Pulsar chip also has an accelerator for convolutional neural networks (which are often used for image recognition and natural-language processing) that supports 32-bit multiply-accumulate (MAC) operations . In addition, each chip possesses a fast Fourier transform accelerator for efficient low-power signal processing. Each Pulsar also incorporates a 32-bit RISC-V CPU that can run at up to 160 megahertz for systems management, as well as a range of standard sensor interfaces and other components. “All of this is integrated into a tiny chip of 2.8 by 2.6 millimeters,” Kumar says. What Makes Pulsar Unique in AI Sensors? What sets Pulsar apart from other neuromorphic devices, such as BrainChip’s Akida Pico , “is not just building a neuromorphic core, but also the rest of the system around it,” Kumar says. “In the industry, there’s a lot of emphasis on inference , but when their neuromorphic cores speak with the rest of their systems, you see them burning power moving data in and out, and all the energy gains they can bring to the table quickly become irrelevant. We built Pulsar as an engine for efficient processing, not just efficient inference .” By integrating all these functions together, “it’s the only chip a sensor needs to process data,” Kumar says. This can simplify overall device design, which can reduce the need for complex data signal-processing pipelines, speed up development and time to market, lower maintenance costs, extend battery life and enable submillisecond analysis times. With submilliwatt power consumption, “Pulsar enables always-on processing of sensor data, even in devices radically constrained by power,” Kumar says. For example, it can enable radar-based presence detection with as little as 600 microwatts and audio scene classification with just 400 µW. In comparison, similar applications using conventional electronics consume 10 to 100 milliwatts, he notes. Pulsar is designed for ultralow-power AI sensor applications in consumer, industrial, and IoT settings. For example, it may find use in smart doorbells , which currently detect people by using cameras or infrared sensors to pick up motion. “This makes them susceptible to being triggered by a flag fluttering in front or a pair of headlights going down the street, draining their batteries,” Kumar says. “Most smart doorbells are advertised as having a battery life of three months, when realistically they need to be recharged every two to three weeks.” Innatera is partnering with the Japanese system-on-a-chip company Socionext to develop a radar-based sensor that can accurately detect people even if they are standing perfectly still, based on their body motions as they breathe. “It can ignore things like bushes moving in the wind,” Kumar says. “It can extend smart doorbell operations to 18 months per recharge. And since it’s not camera-based and doesn’t store data in the cloud, it helps protect privacy.” A key obstacle that neuromorphic computing faces is the steep learning curve that confronts developers as they seek to run their models on these devices. As such, Innatera has released its Talamo software development kit to reduce this barrier to entry, with which developers can build spiking models from scratch in a PyTorch -based environment. “You should not need a neuromorphics Ph.D. to run a neuromorphics solution on chips like these,” Kumar says. The company is also launching a developer program, now open to early adopters, to provide hardware and software kits to a growing community of researchers. “The hope is to grow an ecosystem of neuromorphics applications, and to discover things so far not even thought about,” Kumar says.",
    "published": "Thu, 05 Jun 2025 15:00:04 +0000",
    "author": "Charles Q. Choi",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Nvidia’s Blackwell Conquers Largest LLM Training Benchmark",
    "link": "https://spectrum.ieee.org/nvidia-blackwell-mlperf-training-5",
    "summary": "For those who enjoy rooting for the underdog, the latest MLPerf benchmark results will disappoint: Nvidia’s GPUs have dominated the competition yet again . This includes chart-topping performance on the latest and most demanding benchmark, pretraining the Llama 3.1 403B large language model. That said, the computers built around the newest AMD GPU, MI325X, matched the performance of Nvidia’s H200, Blackwell’s predecessor, on the most popular LLM fine-tuning benchmark. This suggests that AMD is one generation behind Nvidia. MLPerf training is one of the machine learning competitions run by the MLCommons consortium. “AI performance sometimes can be sort of the Wild West. MLPerf seeks to bring order to that chaos,” says Dave Salvator , director of accelerated computing products at Nvidia. “This is not an easy task.” The competition consists of six benchmarks, each probing a different industry-relevant machine learning task. The benchmarks are content recommendation, large language model pretraining, large language model fine-tuning, object detection for machine vision applications, image generation, and graph node classification for applications such as fraud detection and drug discovery. The large language model pretraining task is the most resource intensive, and this round it was updated to be even more so. The term “pretraining” is somewhat misleading—it might give the impression that it’s followed by a phase called “training.” It’s not. Pretraining is where most of the number crunching happens, and what follows is usually fine-tuning, which refines the model for specific tasks. In previous iterations, the pretraining was done on the GPT3 model. This iteration, it was replaced by Meta’s Llama 3.1 403B, which is more than twice the size of GPT3 and uses a four times larger context window. The context window is how much input text the model can process at once. This larger benchmark represents the industry trend for ever larger models, as well as including some architectural updates. Blackwell Tops the Charts, AMD on Its Tail For all six benchmarks, the fastest training time was on Nvidia’s Blackwell GPUs. Nvidia itself submitted to every benchmark (other companies also submitted using various computers built around Nvidia GPUs). Nvidia’s Salvator emphasized that this is the first deployment of Blackwell GPUs at scale and that this performance is only likely to improve. “We’re still fairly early in the Blackwell development life cycle,” he says. This is the first time AMD has submitted to the training benchmark, although in previous years other companies have submitted using computers that included AMD GPUs. In the most popular benchmark, LLM fine-tuning, AMD demonstrated that its latest Instinct MI325X GPU performed on par with Nvidia’s H200s. Additionally, the Instinct MI325X showed a 30 percent improvement over its predecessor, the Instinct MI300X . (The main difference between the two is that MI325X comes with 30 percent more high-bandwidth memory than MI300X.) For it’s part, Google submitted to a single benchmark, the image-generation task, with its Trillium TPU . The Importance of Networking Of all submissions to the LLM fine-tuning benchmarks, the system with the largest number of GPUs was submitted by Nvidia, a computer connecting 512 B200s. At this scale, networking between GPUs starts to play a significant role. Ideally, adding more than one GPU would divide the time to train by the number of GPUs. In reality, it is always less efficient than that, as some of the time is lost to communication. Minimizing that loss is key to efficiently training the largest models. This becomes even more significant on the pretraining benchmark, where the smallest submission used 512 GPUs, and the largest used 8,192. For this new benchmark, the performance scaling with more GPUs was notably close to linear, achieving 90 percent of the ideal performance. Nvidia’s Salvator attributes this to the NVL72, an efficient package that connects 36 Grace CPUs and 72 Blackwell GPUs with NVLink , to form a system that “acts as a single, massive GPU,” the datasheet claims. Multiple NVL72s were then connected with InfiniBand network technology. Notably, the largest submission for this round of MLPerf—at 8192 GPUs—is not the largest ever, despite the increased demands of the pretraining benchmark. Previous rounds saw submissions with over 10,000 GPUs. Kenneth Leach , principal AI and machine learning engineer at Hewlett Packard Enterprise, attributes the reduction to improvements in GPUs, as well as networking between them. “Previously, we needed 16 server nodes [to pretrain LLMs], but today we’re able to do it with 4. I think that’s one reason we’re not seeing so many huge systems, because we’re getting a lot of efficient scaling.” One way to avoid the losses associated with networking is to put many AI accelerators on the same huge wafer, as done by Cerebras , which recently claimed to beat Nvidia’s Blackwell GPUs by more than a factor of two on inference tasks. However, that result was measured by Artificial Analysis , which queries different providers without controlling how the workload is executed. So its not an apples-to-apples comparison in the way the MLPerf benchmark ensures. A Paucity of Power The MLPerf benchmark also includes a power test, measuring how much power is consumed to achieve each training task. This round, only a single submitter—Lenovo—included a power measurement in its submission, making it impossible to make comparisons across performers. The energy it took to fine-tune an LLM on two Blackwell GPUs was 6.11 gigajoules, or 1,698 kilowatt-hours, or roughly the energy it would take to heat a small home for a winter. With growing concerns about AI’s energy use, the power efficiency of training is crucial, and this author is perhaps not alone in hoping more companies submit these results in future rounds.",
    "published": "Wed, 04 Jun 2025 15:59:50 +0000",
    "author": "Dina Genkina",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Startup’s Analog AI Promises Power for PCs",
    "link": "https://spectrum.ieee.org/analog-ai-chip-architecture",
    "summary": "Naveen Verma’ s lab at Princeton University is like a museum of all the ways engineers have tried to make AI ultra-efficient by using analog phenomena instead of digital computing. At one bench lies the most energy-efficient magnetic-memory-based neural-network computer ever made. At another you’ll find a resistive-memory-based chip that can compute the largest matrix of numbers of any analog AI system yet. Neither has a commercial future, according to Verma. Less charitably, this part of his lab is a graveyard. Analog AI has captured chip architects’ imagination for years. It combines two key concepts that should make machine learning massively less energy intensive. First, it limits the costly movement of bits between memory chips and processors. Second, instead of the 1s and 0s of logic, it uses the physics of the flow of current to efficiently do machine learning’s key computation. As attractive as the idea has been, various analog AI schemes have not delivered in a way that could really take a bite out of AI’s stupefying energy appetite. Verma would know. He’s tried them all. But when IEEE Spectrum visited a year ago, there was a chip at the back of Verma’s lab that represents some hope for analog AI and for the energy-efficient computing needed to make AI useful and ubiquitous. Instead of calculating with current, the chip sums up charge. It might seem like an inconsequential difference, but it could be the key to overcoming the noise that hinders every other analog AI scheme. This week, Verma’s startup EnCharge AI unveiled the first chip based on this new architecture, the EN100. The startup claims the chip tackles various AI work with performance per watt up to 20 times better than competing chips. It’s designed into a single processor card that adds 200 trillion operations per second at 8.25 watts, aimed at conserving battery life in AI-capable laptops. On top of that, a 4-chip, 1,000-trillion-operations-per-second card is targeted for AI workstations. Current and Coincidence In machine learning, “it turns out, by dumb luck, the main operation we’re doing is matrix multiplies,” says Verma. That’s basically taking an array of numbers, multiplying it by another array, and adding up the result of all those multiplications. Early on, engineers noticed a coincidence: Two fundamental rules of electrical engineering can do exactly that operation. Ohm’s Law says that you get current by multiplying voltage and conductance. And Kirchoff’s Current Law says that if you have a bunch of currents coming into a point from a bunch of wires, the sum of those currents is what leaves that point. So basically, each of a bunch of input voltages pushes current through a resistance (conductance is the inverse of resistance), multiplying the voltage value, and all those currents add up to produce a single value. Math, done. Sound good? Well, it gets better. Much of the data that makes up a neural network are the “weights,” the things by which you multiply the input. And moving that data from memory into a processor’s logic to do the work is responsible for a big fraction of the energy GPUs expend. Instead, in most analog AI schemes, the weights are stored in one of several types of nonvolatile memory as a conductance value (the resistances above). Because weight data is already where it needs to be to do the computation, it doesn’t have to be moved as much, saving a pile of energy. The combination of free math and stationary data promises calculations that need just thousandths of a trillionth of joule of energy . Unfortunately, that’s not nearly what analog AI efforts have been delivering. The Trouble With Current The fundamental problem with any kind of analog computing has always been the signal-to-noise ratio. Analog AI has it by the truckload. The signal, in this case the sum of all those multiplications, tends to be overwhelmed by the many possible sources of noise. “The problem is, semiconductor devices are messy things,” says Verma. Say you’ve got an analog neural network where the weights are stored as conductances in individual RRAM cells. Such weight values are stored by setting a relatively high voltage across the RRAM cell for a defined period of time. The trouble is, you could set the exact same voltage on two cells for the same amount of time, and those two cells would wind up with slightly different conductance values. Worse still, those conductance values might change with temperature. The differences might be small, but recall that the operation is adding up many multiplications, so the noise gets magnified. Worse, the resulting current is then turned into a voltage that is the input of the next layer of neural networks, a step that adds to the noise even more. Researchers have attacked this problem from both a computer science perspective and a device physics one. In the hope of compensating for the noise, researchers have invented ways to bake some knowledge of the physical foibles of devices into their neural network models. Others have focused on making devices that behave as predictably as possible. IBM, which has done extensive research in this area , does both. Such techniques are competitive, if not yet commercially successful, in smaller-scale systems, chips meant to provide low-power machine learning to devices at the edges of IoT networks. Early entrant Mythic AI has produced more than one generation of its analog AI chip, but it’s competing in a field where low-power digital chips are succeeding. The EN100 card for PCs is a new analog AI chip architecture. EnCharge AI Capacitors All the Way Down EnCharge’s solution strips out the noise by measuring the amount of charge instead of flow of charge in machine learning’s multiply-and-accumulate mantra. In traditional analog AI, multiplication depends on the relationship among voltage, conductance, and current. In this new scheme, it depends on the relationship among voltage, capacitance, and charge—where basically, charge equals capacitance times voltage. Why is that difference important? It comes down to the component that’s doing the multiplication. Instead of using some finicky, vulnerable device like RRAM, EnCharge uses capacitors. A capacitor is basically two conductors sandwiching an insulator. A voltage difference between the conductors causes charge to accumulate on one of them. The thing that’s key about them for the purpose of machine learning is that their value, the capacitance, is determined by their size. (More conductor area or less space between the conductors means more capacitance.) “The only thing they depend on is geometry, basically the space between wires,” Verma says. “And that’s the one thing you can control very, very well in CMOS technologies.” EnCharge builds an array of precisely valued capacitors in the layers of copper interconnect above the silicon of its processors. The data that makes up most of a neural network model, the weights, are stored in an array of digital memory cells, each connected to a capacitor. The data the neural network is analyzing is then multiplied by the weight bits using simple logic built into the cell, and the results are stored as charge on the capacitors. Then the array switches into a mode where all the charges from the results of multiplications accumulate and the result is digitized. While the initial invention , which dates back to 2017, was a big moment for Verma’s lab, he says the basic concept is quite old. “It’s called switched capacitor operation; it turns out we’ve been doing it for decades,” he says. It’s used, for example, in commercial high-precision analog-to-digital converters. “Our innovation was figuring out how you can use it in an architecture that does in-memory computing.” Competition Verma’s lab and EnCharge spent years proving that the technology was programmable and scalable and co-optimizing it with an architecture and software stack that suits AI needs that are vastly different than they were in 2017. The resulting products are with early-access developers now, and the company—which recently raised US $100 million from Samsung Venture, Foxconn, and others—plans another round of early access collaborations. But EnCharge is entering a competitive field, and among the competitors is the big kahuna, Nvidia. At its big developer event in March, GTC, Nvidia announced plans for a PC product built around its GB10 CPU-GPU combination and workstation built around the upcoming GB300 . And there will be plenty of competition in the low-power space EnCharge is after. Some of them even use a form of computing-in-memory. D-Matrix and Axelera , for example, took part of analog AI’s promise, embedding the memory in the computing, but do everything digitally. They each developed custom SRAM memory cells that both store and multiply and do the summation operation digitally, as well. There’s even at least one more-traditional analog AI startup in the mix, Sagence . Verma is, unsurprisingly, optimistic. The new technology “means advanced, secure, and personalized AI can run locally, without relying on cloud infrastructure,” he said in a statement . “We hope this will radically expand what you can do with AI.”",
    "published": "Mon, 02 Jun 2025 14:00:04 +0000",
    "author": "Samuel K. Moore",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  },
  {
    "title": "Giving Plants E-Tattoos Could Be Good for Their Health",
    "link": "https://spectrum.ieee.org/plant-health-monitoring-electronic-tattoo",
    "summary": "Imagine a future in which farmers can tell when plants are sick even before they start showing symptoms. That ability could save a lot of crops from disease and pests—and potentially save a lot of money as well. A team of researchers in Singapore and China have taken a step toward that possibility with their development of ultrathin electronic tattoos—dubbed e-tattoos—to study plant immune responses without the need for piercing, cutting, or bruising leaves. The e-tattoo is a silver nanowire film that attaches to the surface of plant leaves. It conducts a harmless alternating current—in the microampere range—to measure a plant’s electrochemical impedance to that current. That impedance is a telltale sign of the plant’s health. Lead author Tianyiyi He , an associate professor of the Shenzhen MSU-BIT University’s Artificial Intelligence Research Institute, says that a healthy plant has a characteristic impedance spectrum—it’s as unique to the plant as a person’s fingerprints. “If the plant is stressed or its cells are damaged, this spectrum changes in shape and magnitude. Different stressors—dehydration, immune response—cause different changes.” This is because plant cells, He explains, are like tiny chambers with fluids passing through them. The membranes of plant cells act like capacitors, resisting the flow of electrical current. “When cells break down—like in an immune response—the current flows more easily, and impedance drops,” He adds. Detecting Plant Stress Early with E-Tattoos Different problems yield different electrical responses: Dehydration, for example, looks different than an infection. Changes in a plant’s impedance spectrum means that something is not right—and by looking at where and how that spectrum changed, He’s team could spot what the problem was, up to three hours before physical symptoms started appearing. The researchers conducted the work in a controlled environment. He says that a lot more research is needed to help scientists spot a wider array of responses to stressors in the real-world environment. But this is a good step in that direction, says Eleni Stavrinidou, principal investigator in the electronic plants research group from Linköping University’s Laboratory of Organic Electronics in Sweden, who was not involved in the work. He’s team published its work on 4 April in Nature Communications . The team tested the film on lab-grown thale cress ( Arabidopsis thaliana ) for 14 days. They mixed the nanowires in water so that they could transfer smoothly to the plant, by simply dripping the mix onto the leaves. Then they applied the e-tattoo in two different positions—side by side on a single leaf and on opposite faces of a leaf—to see how the current would flow. Then, with a droplet of galistan (a liquid metal alloy composed of gallium , indium, and tin), they attached a copper wire with the diameter of a human hair to the e-tattoo’s surface to apply an AC current from a small generator. He’s team collected data every day to see how plants would react. Control plants showed a consistent spectrum over the course of two weeks, but plants that received immune-response stimulants (such as ethanol) or were wounded or dehydrated showed different patterns of electrical impedance spectra. He says liquid-carried silver nanowires worked better than other highly conductive metals such as copper or nickel because they were not soft enough to entirely “glue” to plants’ leaves and stay perfectly plastered even as the leaf bends or wrinkles. And in the case of thale cresses, they also have tricomas, tiny hairlike structures that usually protect and keep leaves from losing too much water. Tricomas, He explains, hinder perfect attachment since they make a leaf’s surface uneven—but silver nanowires managed to get around the problem in a better way than other materials. “Even the smallest gaps between the film and the leaf can mess with electrical impedance spectroscopy readings,” He says. The silver nanowire e-tattoo proved to be versatile, too. It also worked with coleus, polka-dot plants, and benth—a close relative to tobacco, field mustard, and sweet potato. The team noticed the material did not block sun rays, which means it did not interfere with photosynthesis. Advancements in Plant Impedance Spectroscopy This isn’t the first time tattoos or electrical impedance spectroscopy have been used for plants, says Stavrinidou. What’s new in the study, Stavrinidou says, “is the validation—they show this approach works on delicate plants like Arabidopsis and links clearly to immune responses.” Stavrinidou says that ensuring that impedance spectrum changes tell exactly what is wrong with a plant in an unknown scenario is still a challenge. “But this paper is a strong step in that direction.” At scale, the technique could be another tool to help farmers spot problems in their crops. But the technique will need improvement to get there, He says. Researchers can, for example, redesign the circuits to optimize them. “We can further shrink it to smaller sizes and add wireless communication to build IoT (Internet of Things) systems so we don’t have to link every plant to a wire. Everything is going to be wireless, connected, and transmitted to the cloud,” He says. To Stavrinidou, this work is a step toward a long-term goal: the development of sensors that correlate biological signals to physiological states—stress, disease, or growth—non-invasively. “As more of these studies are done, we’ll be able to map out what different impedance signals mean biologically. That opens the door to sensors that are not just diagnostic, but predictive—a game-changer for agriculture,” Stavrinidou says.",
    "published": "Mon, 02 Jun 2025 12:00:04 +0000",
    "author": "Meghie Rodrigues",
    "topic": "semiconductors",
    "collected_at": "2025-10-08T14:03:16"
  }
]