[
  {
    "title": "Video Friday: Drone Easily Lands on Speeding Vehicle",
    "link": "https://spectrum.ieee.org/video-friday-speedy-drone-landing",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! We demonstrate a new landing system that lets drones safely land on moving vehicles at speeds up to 110 kilometers per hour. By combining lightweight shock absorbers with reverse thrust, our approach drastically expands the landing envelope, making it far more robust to wind, timing, and vehicle motion. This breakthrough opens the door to reliable high-speed drone landings in real-world conditions. [ Createk Design Lab ] Thanks, Alexis! This video presents an academic parody inspired by KAIST’s humanoid robot moonwalk. While KAIST demonstrated the iconic move with robot legs, we humorously reproduced it using the Tesollo DG-5F robot hand. A playful experiment to show that not only humanoid robots but also robotic fingers can “dance.” [ Hangyang University ] Twenty years ago, Universal Robots built the first collaborative robot . You turned it into something bigger. Our cobot was never just technology. In your hands, it became something more: a teammate, a problem-solver, a spark for change. From factories to labs, from classrooms to warehouses. That’s the story of the past 20 years. That’s what we celebrate today. [ Universal Robots ] The assistive robot Maya, newly developed at DLR, is designed to enable people with severe physical disabilities to lead more independent lives. The new robotic arm is built for seamless wheelchair integration, with optimized kinematics for stowing, ground-level access, and compatibility with standing functions. [ DLR ] Contoro and HARCO Lab have launched an open-source initiative, ROS-MCP-Server, which connects AI models (for example, Claude, GPT, Gemini) with robots using a robot operating system and the Model Context Protocol. This software enables AI to communicate with multiple ROS nodes in the language of robots. We believe it will allow robots to perform tasks previously impossible due to limited intelligence, help robotics engineers program robots more efficiently, and enable nonexperts to interact with robots without deep robotics knowledge. [ GitHub ] Thanks, Mok! Here’s a quick look at the Conference on Robotic Learning (CoRL) exhibit hall, thanks to PNDbotics. [ PNDbotics ] Old and busted: sim to real. New hotness: real to sim! [ Paper ] Any humanoid video with tennis balls should be obligated to show said humanoid failing to walk over them. [ LimX ] Thanks, Jinyan! The correct answer to the question “Can you beat a robot arm at tic-tac-toe?” should be “No. No, you cannot.” And you can’t beat a human, either, if they know what they’re doing. [ AgileX ] It was an honor to host the team from Microsoft AI as part of their larger educational collaboration with the University of Texas at Austin. During their time here, they shared this wonderful video of our lab facilities. Moody lighting is second only to random primary-colored lighting when it comes to making a lab look science-y. [ The University of Texas at Austin HCRL ] Robots aren’t just sci-fi anymore. They’re evolving fast. AI is teaching them how to adapt, learn, and even respond to open-ended questions with advanced intelligence. Aaron Saunders, chief technology officer of Boston Dynamics, explains how this leap is transforming everything, from simple controls to full-motion capabilities. While there are some challenges related to safety and reliability, AI is significantly helping robots become valuable partners at home and on the job. [ IBM ]",
    "published": "Fri, 03 Oct 2025 16:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Why the World Needs a Flying Robot Baby",
    "link": "https://spectrum.ieee.org/ironcub-jet-powered-flying-robot",
    "summary": "One of the robotics projects that I’ve been most excited about for years now is iRonCub , from Daniele Pucci’s Artificial and Mechanical Intelligence Lab at the Italian Institute of Technology (IIT) in Genoa, Italy. Since 2017 , Pucci has been developing a jet-propulsion system that will enable an iCub robot (originally designed in 2004 to be the approximate shape and size of a 5-year-old child) to fly like Iron Man. Over the summer, after nearly 10 years of development, iRonCub3 achieved lift-off and stable flight for the first time , with its four jet engines lifting it 50 centimeters off the ground for several seconds. The long-term vision is for iRonCub (or a robot like it) to operate as a disaster response platform, Pucci tells us. In an emergency situation like a flood or a fire, iRonCub could quickly get to a location without worrying about obstacles, and then on landing, start walking for energy efficiency while using its hands and arms to move debris and open doors. “We believe in contributing to something unique in the future,” says Pucci. “We have to explore new things, and this is wild territory at the scientific level.” Obviously, this concept for iRonCub and the practical experimentation attached to it is really cool. But coolness in and of itself is usually not enough of a reason to build a robot, especially a robot that’s a (presumably rather expensive) multi-year project involving a bunch of robotics students, so let’s get into a little more detail about why a flying robot baby is actually something that the world needs. In an emergency situation like a flood or a fire, iRonCub could quickly get to a location without worrying about obstacles, and then on landing, start walking for energy efficiency while using its hands and arms to move debris and open doors. IIT Getting a humanoid robot to do this sort of thing is quite a challenge. Together, the jet turbines mounted to iRonCub’s back and arms can generate over 1000 N of thrust, but because it takes time for the engines to spool up or down, control has to come from the robot itself as it moves its arm-engines to maintain stability. “What is not visible from the video,” Pucci tells us, “is that the exhaust gas from the turbines is at 800 °C and almost supersonic speed. We have to understand how to generate trajectories in order to avoid the fact that the cones of emission gases were impacting the robot.” Even if the exhaust doesn’t end up melting the robot, there are still aerodynamic forces involved that have until this point really not been a consideration for humanoid robots at all—in June, Pucci’s group published a paper in Nature Engineering Communications , offering a “comprehensive approach to model and control aerodynamic forces [for humanoid robots] using classical and learning techniques.” “The exhaust gas from the turbines is at 800 °C and almost supersonic speed.” —Daniele Pucci, IIT Whether or not you’re on board with Pucci’s future vision for iRonCub as a disaster-response platform, derivatives of current research can be immediately applied beyond flying humanoid robots. The algorithms for thrust estimation can be used with other flying platforms that rely on directed thrust, like eVTOL aircraft. Aerodynamic compensation is relevant for humanoid robots even if they’re not airborne, if we expect them to be able to function when it’s windy outside. More surprising, Pucci describes a recent collaboration with an industrial company developing a new pneumatic gripper. “At a certain point, we had to do force estimation for controlling the gripper, and we realized that the dynamics looked really similar to those of the jet turbines, and so we were able to use the same tools for gripper control. That was an ‘ah-ha’ moment for us: first you do something crazy, but then you build the tools and methods, and then you can actually use those tools in an industrial scenario. That’s how to drive innovation.” What’s Next for iRonCub: Attracting Talent and Future Enhancements There’s one more important reason to be doing this, he says: “It’s really cool.” In practice, a really cool flagship project like iRonCub not only attracts talent to Pucci’s lab, but also keeps students and researchers passionate and engaged. I saw this firsthand when I visited IIT last year, where I got a similar vibe to watching the DARPA Robotics Challenge and DARPA SubT —when people know they’re working on something really cool , there’s this tangible, pervasive, and immersive buzzing excitement that comes through. It’s projects like iRonCub that can get students to love robotics. In the near future, a new jetpack with an added degree of freedom will make yaw control of iRonCub easier, and Pucci would also like to add wings for more efficient long-distance flight. But the logistics of testing the robot are getting more complicated—there’s only so far that the team can go with their current test stand (which is on the roof of their building), and future progress will likely require coordinating with the Genoa airport. It’s not going to be easy, but as Pucci makes clear, “This is not a joke. It’s something that we believe in. And that feeling of doing something exceptional, or possibly historical, something that’s going to be remembered—that’s something that’s kept us motivated. And we’re just getting started.”",
    "published": "Tue, 30 Sep 2025 12:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Gemini Robotics Improves Motor Skills",
    "link": "https://spectrum.ieee.org/video-friday-google-gemini-robotics",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Gemini Robotics 1.5 is our most capable vision-language-action (VLA) model, which turns visual information and instructions into motor commands for a robot to perform a task. This model thinks before taking action and shows its process, helping robots assess and complete complex tasks more transparently. It also learns across embodiments, accelerating skill learning . [ Google DeepMind ] A simple “force pull” gesture brings Carter straight into her hand. This is a fantastic example of how an intuitive interaction can transform complex technology into an extension of our intent. [ Robust.ai ] I can’t help it, I feel bad for this poor little robot. [ Urban Robotics Laboratory, KAIST ] Hey look, no legs! [ Kinisi Robotics ] Researchers at the University of Michigan and Shanghai Jiao Tong University have developed a soft robot that can crawl along a flat path and climb up vertical surfaces using its unique origami structure. The robot can move with an accuracy typically seen only in rigid robots. [ University of Michigan Robotics ] Unitree G1 has learned the “antigravity” mode: Stability is greatly improved under any action sequence, and even if it falls, it can quickly get back up. [ Unitree ] Kepler Robotics has commenced mass production of the K2 Bumblebee, the world’s first commercially available humanoid robot powered by Tesla’s hybrid architecture. [ Kepler Robotics ] Reinforcement learning (RL)-based legged locomotion controllers often require meticulous reward tuning to track velocities or goal positions while preserving smooth motion on various terrains. Motion imitation methods via RL using demonstration data reduce reward engineering but fail to generalize to novel environments. We address this by proposing a hierarchical RL framework in which a low-level policy is first pretrained to imitate animal motions on flat ground, thereby establishing motion priors. Real-world experiments with an ANYmal-D quadruped robot confirm our policy’s capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amid challenging terrains with obstacles. [ ETHZ RSL ] I think we have entered the “differentiation through novelty” phase of robot vacuums. [ Roborock ] In this work, we present Kinethreads: a new full-body haptic exosuit design built around string-based motor-pulley mechanisms, which keeps our suit lightweight (less than 5 kilograms), soft and flexible and quick-to-wear (in less than 30 seconds), comparatively low-cost (about US $400), and yet capable of rendering expressive, distributed, and forceful (up to 120 newtons) effects. [ ACM Symposium on User Interface and Software Technology ] In this episode of the IBM AI in Action podcast, Aaron Saunders, chief technology officer of Boston Dynamics, delves into the transformative potential of AI-powered robotics, highlighting how robots are becoming safer, more cost-effective and widely accessible through robotics as a service (RaaS). [ IBM ] This Carnegie Mellon RI Seminar is by Michael T. Tolley from the University of California, San Diego, on biologically inspired soft robotics. Robotics has the potential to address many of today’s pressing problems in fields ranging from health care to manufacturing to disaster relief. However, the traditional approaches used on the factory floor do not perform well in unstructured environments. The key to solving many of these challenges is to explore new, nontraditional designs. Fortunately, nature surrounds us with examples of novel ways to navigate and interact with the real world. Dr. Tolley’s Bioinspired Robotics and Design Lab seeks to borrow the key principles of operation from biological systems and apply them to robotic design. [ Carnegie Mellon University Robotics Institute ]",
    "published": "Fri, 26 Sep 2025 15:30:02 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Exploit Allows for Takeover of Fleets of Unitree Robots",
    "link": "https://spectrum.ieee.org/unitree-robot-exploit",
    "summary": "A critical vulnerability in the Bluetooth Low Energy (BLE) Wi-Fi configuration interface used by several different Unitree robots can result in a root-level takeover by an attacker, security researchers disclosed on 20 September . The exploit impacts Unitree’s Go2 and B2 quadrupeds and G1 and H1 humanoids. Because the vulnerability is wireless, and the resulting access to the affected platform is complete, the vulnerability becomes wormable, say the researchers , meaning “a n infected robot can simply scan for other Unitree robots in BLE range and automatically compromise them, creating a robot botnet that spreads without user intervention.” Initially discovered by security researchers Andreas Makris and Kevin Finisterre, UniPwn takes advantage of several security lapses that are still present in the firmware of Unitree robots as of 20 September 2025. As far as IEEE Spectrum is aware, this is the first major public exploit of a commercial humanoid platform. Unitree Robots’ BLE Security Flaw Exposed Like many robots, Unitree’s robots use an initial BLE connection to make it easier for a user to set up a Wi-Fi network connection. The BLE packets that the robot accepts are encrypted, but those encryption keys are hardcoded and were published on X (formerly Twitter) by Makris in July. Although the robot does validate the contents of the BLE packets to make sure that the user is authenticated, the researchers say that all it takes to become an authenticated user is to encrypt the string “unitree” with the hardcoded keys and the robot will let someone in. From there, an attacker can inject arbitrary code masquerading as the Wi-Fi SSID and password, and when the robot attempts to connect to Wi-Fi, it will execute that code without any validation and with root privileges. “A simple attack might be just to reboot the robot, which we published as a proof of concept,” explains Makris. “But an attacker could do much more sophisticated things: It would be possible to have a trojan implanted into your robot’s startup routine to exfiltrate data while disabling the ability to install new firmware without the user knowing. And as the vulnerability uses BLE, the robots can easily infect each other, and from there the attacker might have access to an army of robots.” Makris and Finisterre first contacted Unitree in May in an attempt to responsibly disclose this vulnerability. After some back and forth with little progress, Unitree stopped responding to the researchers in July, and the decision was made to make the vulnerability public. “We have had some bad experiences communicating with them,” Makris tells us, citing an earlier backdoor vulnerability he discovered with the Unitree Go1. “So we need to ask ourselves—are they introducing vulnerabilities like this on purpose, or is it sloppy development? Both answers are equally bad.” Unitree has not responded to a request for comment from IEEE Spectrum as of press time. On 29 September, Unitree posted a statement on LinkedIn addressing the security concerns: “We have become aware that some users have discovered security vulnerabilities and network-related issues while using our robots,” the company wrote. “We immediately began addressing these concerns and have now completed the majority of the fixes. These updates will be rolled out to you in the near future.” “Unitree, as other manufacturers do, has simply ignored prior security disclosures and repeated outreach attempts,” says Víctor Mayoral-Vilches, the founder of robotics cybersecurity company Alias Robotics . “This is not the right way to cooperate with security researchers.” Mayoral-Vilches was not involved in publishing the UniPwn exploit, but he has found other security issues with Unitree robots, including undisclosed streaming of telemetry data to servers in China which could potentially include audio, visual, and spatial data. Mayoral-Vilches explains that security researchers are focusing on Unitree primarily because the robots are available and affordable. This makes them not just more accessible for the researchers, but also more relevant, since Unitree’s robots are already being deployed by users around the world who are likely not aware of the security risks. For example, Makris is concerned that the Nottinghamshire police in the United Kingdom have begun testing a Unitree Go2 , which can be exploited by UniPwn. “We tried contacting them and would have disclosed the vulnerability upfront to them before going public, but they ignored us. What would happen if an attacker implanted themselves into one of these police dogs?” How to Secure Unitree Robots In the short term, Mayoral-Vilches suggests that people using Unitree robots can protect themselves by connecting the robots to only isolated Wi-Fi networks and disabling their Bluetooth connectivity. “You need to hack the robot to secure it for real,” he says. “This is not uncommon and why security research in robotics is so important.” Both Mayoral-Vilches and Makris believe that fundamentally it’s up to Unitree to make their robots secure in the long term, and that the company needs to be much more responsive to users and security researchers. But Makris says: “There will never be a 100 percent secure system.” Mayoral-Vilches agrees: “Robots are very complex systems, with wide attack surfaces to protect, and a state-of-the-art humanoid exemplifies that complexity.” Unitree, of course, is not the only company offering complex state-of-the-art quadrupeds and humanoids, and it seems likely (if not inevitable) that similar exploits will be discovered in other platforms. The potential consequences here can’t be overstated—the idea that robots can be taken over and used for nefarious purposes is already a science-fiction trope, but the impact of a high-profile robot hack on the reputation of the commercial robotics industry is unclear. Robots companies are barely talking about security in public, despite how damaging even the perception of an unsecured robot might be. A robot that is not under control has the potential to be a real physical danger. For the IEEE Humanoids Conference in Seoul from 30 September to 2 October, Mayoral-Vilches has organized a workshop on Cybersecurity for Humanoids , where he will present a brief (coauthored with Makris and Finisterre) titled Humanoid Robots as Attack Vectors . Despite the title, their intent is not to overhype the problem but instead to encourage roboticists (and robotics companies) to take security seriously, and not treat it as an afterthought. As Mayoral-Vilches points out, “Robots are only safe if secure.” Story updated 29 Sep 2025 with statement released by Unitree.",
    "published": "Thu, 25 Sep 2025 13:36:52 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Robot Navigates Tough Terrain With New 3D Mapping Technique",
    "link": "https://spectrum.ieee.org/robot-navigation-3d-mapping",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Four-legged robots are making great strides in their ability to safely traverse complex terrains. In a recent advance, researchers in Hong Kong have developed a novel mapping model for quadrupedal robots that allows them to autonomously crawl under and leap over significant obstacles in order to arrive at its desired endpoint. The researchers describe the robot, which uses multilayered mapping to understand its environment, in a study published 4 August in IEEE Robotics and Automation Letters . Agile two- and four-legged animals can adapt to diverse terrains. Robots that can traverse similarly complex environments are appealing because they could complete missions that would be dangerous for humans to do, like monitoring and assessing unstable rubble sites after an earthquake. But ensuring that robots can effectively map out complex environments, as well as handle the various types of obstacles (leaping across gaps and or scaling high objects, for example) is challenging. Advanced Terrain Mapping for Robots Peng Lu , an assistant professor at the University of Hong Kong, along with his postdoctoral student Yeke Chen and other team members sought to create a robot capable of overcoming these hurdles. To help their robot perceive its surroundings in detail, they developed a model that creates a multilayer elevation map from the robot’s sensor data. The map can capture the characteristics of a wide range of terrains using lidar data. - YouTube youtu.be The team used simulations to train the robot to recognize different terrains it may encounter in the real world. This includes very challenging terrains to navigate, such as a gap that it must jump across or crawling under obstacles with an overhang jutting out. If the robot has missing sensor data, it can compensate to some extent with estimations, based on its training data. “Through learning different skills in simulation and knowledge distillation, the robot is able to switch among different skills to traverse through different obstacles,” says Lu. In their study, the researchers tested their mapping technique using a Unitree Go1 robot in a series of indoor and outdoor experiments, where it had to autonomously crawl, climb, or jump to overcome obstacles. “The results show that the multilayer elevation map can effectively represent various complex terrains, which allow a robot to easily understand the environment,” says Lu, noting the robot also succeeded in autonomously switching between modes—crawling, jumping, and climbing—as needed. He adds that the robot inadvertently has path-planning abilities, even though it was not programmed to have them. For example, when the robot encounters obstacles that are too high to pass, it moves around the obstacle, and therefore finds a path through the environment on its own through trial and error. Lu notes that while a key strength of this robot is its ability to navigate diverse and difficult terrain, it can rely only on the data that it has already been trained on, and cannot learn directly from real-world data. Lu says his team may commercialize the robot for inspection scenarios, such as construction sites, and plans on using real-world data to further enhance the robot’s ability to cope with any type of terrain.",
    "published": "Wed, 24 Sep 2025 14:00:02 +0000",
    "author": "Michelle Hampson",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: A Billion Dollars for Humanoid Robots",
    "link": "https://spectrum.ieee.org/video-friday-billion-humanoid-robots",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! A billion dollars is a lot of money. And this is actual money, not just a valuation. but Figure already had a lot of money. So what are they going to be able to do now that they weren’t already doing, I wonder? [ Figure ] Robots often succeed in simulation but fail in reality. With PACE, we introduce a systematic approach to sim-to-real transfer. [ Paper ] Anthropomorphic robotic hands are essential for robots to learn from humans and operate in human environments. While most designs loosely mimic human hand kinematics and structure, achieving the dexterity and emergent behaviors present in human hands, anthropomorphic design must extend to also match passive compliant properties while simultaneously strictly having kinematic matching. We present ADAPT-Teleop, a system combining a robotic hand with human-matched kinematics, skin, and passive dynamics, along with a robotic arm for intuitive teleoperation. [ Paper ] This robot can walk without any electronic components in its body, because the power is transmitted through wires from motors concentrated outside of its body. Also, this robot’s front and rear legs are optimally coupled and can walk with just four wires. [ JSK Lab ] Thanks, Takahiro! Five teams of Los Alamos engineers competed to build the ultimate hole-digging robot dog in a recent engineering sprint. In just days, teams programmed their robot dogs to dig, designing custom “paws” from materials like sheet metal, foam, and 3D-printed polymers. The paws mimicked animal digging behaviors—from paddles and snowshoes to dew claws—and helped the robots avoid sinking into a 30-gallon soil bucket. Teams raced to see whose dog could dig the biggest hole and dig under a fence the fastest. [ Los Alamos ] This work presents UniPilot, a compact hardware-software autonomy payload that can be integrated across diverse robot embodiments to enable resilient autonomous operation in GPS-denied environments. The system integrates a multimodal sensing suite including lidar, radar, vision, and inertial sensing for robust operation in conditions where unimodal approaches may fail. A large number of experiments are conducted across diverse environments and on a variety of robot platforms to validate the mapping, planning, and safe navigation capabilities enabled by the payload. [ NTNU ] Thanks, Kostas! KAIST Humanoid v0.5. Developed at the DRCD Lab, KAIST, with a control policy trained via reinforcement learning. [ KAIST ] I just like the determined little hops. [ AgileX ] I’m always a little bit suspicious of robotics labs that are exceptionally clean and organized. [ PNDbotics ] Er, has PAL Robotics ever actually seen a kangaroo...? [ PAL ] See Spots push. Push, Spots, push. [ Tufts ] Training humanoid robots to hike could accelerate development of embodied AI for tasks like autonomous search and rescue, ecological monitoring in unexplored places, and more, say University of Michigan researchers who developed an AI model that equips humanoids to hit the trails. [ Michigan ] I am dangerously close to no longer being impressed by breakdancing humanoid robots. [ Fourier ] This, though, would impress me. [ Inria ] In this interview, Clone’s co-founder and CEO Dhanush Radhakrishnan discusses the company’s path to creating the synthetic humans straight out of science fiction. (If YouTube brilliantly attempts to auto-dub this for you, switch the audio track to original [which YouTube thinks is Polish] and the video will still be in English.) [ Clone ] This documentary takes you behind the scenes of the HMND 01 Alpha release: the breakthroughs, the failures, and the late nights of building the U.K.’s first industrial humanoid robot. [ Humanoid ] What is the role of ethical considerations in the development and deployment of robotic and automation technologies, and what are the responsibilities of researchers to ensure that these technologies advance in ways that are transparent, fair, and aligned with the broader well-being of society? [ ICRA@40 ] This UPenn GRASP SFI lecture is from Tairan He at Nvidia on “Scalable Sim-to-Real Learning for General-Purpose Humanoid Skills.” Humanoids represent the most versatile robotic platform, capable of walking, manipulating, and collaborating with people in human-centered environments. Yet despite recent advances, building humanoids that can operate reliably in the real world remains a fundamental challenge. Progress has been hindered by difficulties in whole-body control, robust perceptive reasoning, and bridging the sim-to-real gap. In this talk, I will discuss how scalable simulation and learning can systematically overcome these barriers. [ UPenn ]",
    "published": "Fri, 19 Sep 2025 15:30:05 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: A Soft Robot Companion",
    "link": "https://spectrum.ieee.org/video-friday-soft-robot-companion",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Fourier’s first Care-bot GR-3. This full-size “care bot” is designed as an interactive companion. Its soft-touch outer shell and multimodal emotional interaction system bring the concept of “warm tech companionship” to life. I like that it’s soft to the touch , although I’m not sure that encouraging touch is safe. Reminds me a little bit of Valkyrie , where NASA put a lot of thought into the soft aspects of the robot. [ Fourier ] TAKE MY MONEY This 112-gram micro air vehicle (MAV) features foldable propeller arms that can lock into a compact rectangular profile comparable to the size of a smartphone. The vehicle can be launched by simply throwing it in the air, at which point the arms will unfold and autonomously stabilize to a hovering state. Multiple flight tests demonstrated the capability of the feedback controller to stabilize the MAV based on different initial conditions, including tumbling rates of up to 2,500 degrees per second. [ AVFL ] The U.S. Naval Research Laboratory (NRL), in collaboration with NASA, is advancing space robotics by deploying reinforcement-learning algorithms onto Astrobee , a free-flying robotic assistant on board the International Space Station. This video highlights how NRL researchers are leveraging artificial intelligence to enable robots to learn, adapt, and perform tasks autonomously. By integrating reinforcement learning, Astrobee can improve maneuverability and optimize energy use. [ NRL ] Every day I’m scuttlin.’ [ Ground Control Robotics ] Trust is built. Every part of our robot Proxie—from wheels to eyes—is designed with trust in mind. Cobot CEO Brad Porter explains the intent behind its design. [ Cobot ] Phase 1: Build lots of small quadruped robots. Phase 2: ? Phase 3: Profit! [ DEEP Robotics ] LAPP USA partnered with Corvus Robotics to solve a long-standing supply-chain challenge: labor-intensive, error-prone inventory counting. [ Corvus ] I’m pretty sure that 95 percent of all science consists of moving small amounts of liquid from one container to another. [ Flexiv ] Raffaello D’Andrea , interviewed at ICRA 2025. [ Verity ] Tessa Lau, interviewed at ICRA 2025. [ Dusty Robotics ] Ever wanted to look inside the mind behind a cutting-edge humanoid robot? In this special episode, we have Dr. Aaron Zhang, the product manager at LimX Dynamics, for an exclusive deep dive into the LimX Oli. [ LimX Dynamics ]",
    "published": "Fri, 12 Sep 2025 17:04:10 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Reality Is Ruining the Humanoid Robot Hype",
    "link": "https://spectrum.ieee.org/humanoid-robot-scaling",
    "summary": "Over the next several years, humanoid robots will change the nature of work. Or at least, that’s what humanoid robotics companies have been consistently promising, enabling them to raise hundreds of millions of dollars at valuations that run into the billions. Delivering on these promises will require a lot of robots. Agility Robotics expects to ship “ hundreds ” of its Digit robots in 2025 and has a factory in Oregon capable of building over 10,000 robots per year. Tesla is planning to produce 5,000 of its Optimus robots in 2025, and at least 50,000 in 2026. Figure believes “ there is a path to 100,000 robots ” by 2029. And these are just three of the largest companies in an increasingly crowded space. This article is part of The Scale Issue . Amplifying this message are many financial analysts: Bank of America Global Research , for example, predicts that global humanoid robot shipments will reach 18,000 units in 2025. And Morgan Stanley Research estimates that by 2050 there could be over 1 billion humanoid robots, part of a US $5 trillion market. But as of now, the market for humanoid robots is almost entirely hypothetical. Even the most successful companies in this space have deployed only a small handful of robots in carefully controlled pilot projects. And future projections seem to be based on an extraordinarily broad interpretation of jobs that a capable, efficient, and safe humanoid robot—which does not currently exist—might conceivably be able to do. Can the current reality connect with the promised scale? What Will It Take to Scale Humanoid Robots? Physically building tens of thousands, or even hundreds of thousands, of humanoid robots is certainly possible in the near term. In 2023, on the order of 500,000 industrial robots were installed worldwide . Under the basic assumption that a humanoid robot is approximately equivalent to four industrial arms in terms of components, existing supply chains should be able to support even the most optimistic near-term projections for humanoid manufacturing. But simply building the robots is arguably the easiest part of scaling humanoids, says Melonee Wise , who served as chief product officer at Agility Robotics until this month. “The bigger problem is demand—I don’t think anyone has found an application for humanoids that would require several thousand robots per facility.” Large deployments, Wise explains, are the most realistic way for a robotics company to scale its business, since onboarding any new client can take weeks or months. An alternative approach to deploying several thousand robots to do a single job is to deploy several hundred robots that can each do 10 jobs, which seems to be what most of the humanoid industry is betting on in the medium to long term. While there’s a belief across much of the humanoid robotics industry that rapid progress in AI must somehow translate into rapid progress toward multipurpose robots, it’s not clear how, when, or if that will happen. “I think what a lot of people are hoping for is they’re going to AI their way out of this,” says Wise. “But the reality of the situation is that currently AI is not robust enough to meet the requirements of the market.” Bringing Humanoid Robots to Market Market requirements for humanoid robots include a slew of extremely dull, extremely critical things like battery life, reliability, and safety. Of these, battery life is the most straightforward—for a robot to usefully do a job, it can’t spend most of its time charging. The next version of Agility’s Digit robot, which can handle payloads of up to 16 kilograms, includes a bulky “backpack” containing a battery with a charging ratio of 10 to 1: The robot can run for 90 minutes, and fully recharge in 9 minutes. Slimmer humanoid robots from other companies must necessarily be making compromises to maintain their svelte form factors. In operation, Digit will probably spend a few minutes charging after running for 30 minutes. That’s because 60 minutes of Digit’s runtime is essentially a reserve in case something happens in its workspace that requires it to temporarily pause, a not-infrequent occurrence in the logistics and manufacturing environments that Agility is targeting. Without a 60-minute reserve, the robot would be much more likely to run out of power mid-task and need to be manually recharged. Consider what that might look like with even a modest deployment of several hundred robots weighing over a hundred kilograms each. “No one wants to deal with that,” comments Wise. Potential customers for humanoid robots are very concerned with downtime. Over the course of a month, a factory operating at 99 percent reliability will see approximately 5 hours of downtime. Wise says that any downtime that stops something like a production line can cost tens of thousands of dollars per minute, which is why many industrial customers expect a couple more 9s of reliability: 99.99 percent. Wise says that Agility has demonstrated this level of reliability in some specific applications, but not in the context of multipurpose or general-purpose functionality. Humanoid Robot Safety A humanoid robot in an industrial environment must meet general safety requirements for industrial machines. In the past, robotic systems like autonomous vehicles and drones have benefited from immature regulatory environments to scale quickly. But Wise says that approach can’t work for humanoids, because the industry is already heavily regulated—the robot is simply considered another piece of machinery. There are also more specific safety standards currently under development for humanoid robots, explains Matt Powers, associate director of autonomy R&D at Boston Dynamics. He notes that his company is helping develop an International Organization for Standardization (ISO) safety standard for dynamically balancing legged robots . “We’re very happy that the top players in the field, like Agility and Figure, are joining us in developing a way to explain why we believe that the systems that we’re deploying are safe,” Powers says. These standards are necessary because the traditional safety approach of cutting power may not be a good option for a dynamically balancing system. Doing so will cause a humanoid robot to fall over, potentially making the situation even worse. There is no simple solution to this problem, and the initial approach that Boston Dynamics expects to take with its Atlas robot is to keep the robot out of situations where simply powering it off might not be the best option. “We’re going to start with relatively low-risk deployments, and then expand as we build confidence in our safety systems,” Powers says. “I think a methodical approach is really going to be the winner here.” In practice, low risk means keeping humanoid robots away from people. But humanoids that are restricted by what jobs they can safely do and where they can safely move are going to have more trouble finding tasks that provide value. Are Humanoids the Answer? The issues of demand, battery life, reliability, and safety all need to be solved before humanoid robots can scale. But a more fundamental question to ask is whether a bipedal robot is actually worth the trouble. Dynamic balancing with legs would theoretically enable these robots to navigate complex environments like a human. Yet demo videos show these humanoid robots as either mostly stationary or repetitively moving short distances over flat floors. The promise is that what we’re seeing now is just the first step toward humanlike mobility. But in the short to medium term, there are much more reliable, efficient, and cost-effective platforms that can take over in these situations: robots with arms, but with wheels instead of legs. Safe and reliable humanoid robots have the potential to revolutionize the labor market at some point in the future. But potential is just that, and despite the humanoid enthusiasm, we have to be realistic about what it will take to turn potential into reality. This article appears in the October 2025 print issue as “Why Humanoid Robots Aren’t Scaling.”",
    "published": "Thu, 11 Sep 2025 13:00:00 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "How Robotics Is Powering the Future of Innovation",
    "link": "https://content.knowledgehub.wiley.com/from-concept-to-reality-how-robotics-is-transforming-our-world/",
    "summary": "The future of robotics is being shaped by powerful technologies like AI, edge computing, and high-speed connectivity, driving smarter, more responsive machines across industries. Robots are no longer confined to static environments—they are evolving to interact dynamically with humans and their surroundings. This eBook explores the impact of robotics in diverse fields, from home automation and medical technology to automotive, data centers, and industrial applications. It highlights challenges like power efficiency, miniaturization, and ruggedization, while showcasing Molex’s innovative solutions tailored for each domain. Additionally, the eBook covers: Ruggedized connectors for harsh industrial settings Advanced power management for home robots Miniaturized systems for precision medical robotics 5G/6G-enabled autonomous vehicles High-speed data solutions for cloud infrastructure Download this free whitepaper now!",
    "published": "Thu, 11 Sep 2025 10:00:02 +0000",
    "author": "Heilind Electronics",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Large Behavior Models Are Helping Atlas Get to Work",
    "link": "https://spectrum.ieee.org/boston-dynamics-atlas-scott-kuindersma",
    "summary": "Boston Dynamics can be forgiven, I think, for the relative lack of acrobatic prowess displayed by the new version of Atlas in ( most of ) its latest videos. In fact, if you look at this Atlas video from late last year and compare it to Atlas’s most recent video , it’s doing what looks to be more or less the same logistics-y stuff—all of which is far less visually exciting than backflips. But I would argue that the relatively dull tasks Atlas is working on now, moving car parts and totes and whatnot, are just as impressive. Making a humanoid that can consistently and economically and safely do useful things over the long term could very well be the hardest problem in robotics right now, and Boston Dynamics is taking it seriously. Last October, Boston Dynamics announced a partnership with Toyota Research Institute with the goal of general-purpose-izing Atlas. We’re now starting to see the results of that partnership, and Boston Dynamics’ vice president of robotics research, Scott Kuindersma , takes us through the progress they’ve made. Building AI Generalist Robots While the context of this work is “building AI generalist robots,” I’m not sure that anyone really knows what a “generalist robot” would actually look like, or how we’ll even know when someone has achieved it. Humans are generalists, sort of—we can potentially do a lot of things, and we’re fairly adaptable and flexible in many situations, but we still require training for most tasks. I bring this up just to try and contextualize expectations, because I think a successful humanoid robot doesn’t have to actually be a generalist, but instead just has to be capable of doing several different kinds of tasks, and to be adaptable and flexible in the context of those tasks. And that’s already difficult enough. The approach that the two companies are taking is to leverage large behavior models (LBMs), which combine more general world knowledge with specific task knowledge to help Atlas with that adaptability and flexibility thing. As Boston Dynamics points out in a recent blog post , “the field is steadily accumulating evidence that policies trained on a large corpus of diverse task data can generalize and recover better than specialist policies that are trained to solve one or a small number of tasks.” Essentially, the goal is to develop a foundational policy that covers things like movement and manipulation, and then add more specific training (provided by humans) on top of that for specific tasks. This video below shows how that’s going so far. Boston Dynamics/YouTube What the video doesn’t show is the training system that Boston Dynamics uses to teach Atlas to do these tasks. Essentially imitation learning, an operator wearing a motion tracking system teleoperates Atlas through motion and manipulation tasks. There’s a one-to-one mapping between the operator and the robot, making it fairly intuitive, although as anyone who has tried to teleoperate a robot with a surfeit of degrees of freedom can attest, it takes some practice to do it well. A motion-tracking system provides high-quality task training data for Atlas. Boston Dynamics This interface provides very high-quality demonstration data for Atlas, but it’s not the easiest to scale—just one of the challenges of deploying a multipurpose (different than generalist!) humanoid. For more about what’s going on behind the scenes in this video and Boston Dynamics’ strategy with Atlas, IEEE Spectrum spoke with Kuindersma. Scott Kuindersma on: What’s new from Boston Dynamics and Toyota Research Institute The role of large behavior models Learning through human imitation The potential limitations of imitating humans The importance of high-quality data The future for Atlas In a video from last October , just as your partnership with Toyota Research Institute was beginning, Atlas was shown moving parts around and performing whole-body manipulation. What’s the key difference between that demonstration and what we’re seeing in the new video? Scott Kuindersma: The big difference is how we programmed the behavior. The previous system was a more traditional robotics stack involving a combination of model-based controllers, planners, and machine-learning models for perception all architected together to do end-to-end manipulation. Programming a new task on that system generally required roboticists or system integrators to touch code and tell the robot what to do. For this new video, we replaced most of that system with a single neural network that was trained on demonstration data. This is much more flexible because there’s no task-specific programming or other open-ended creative engineering required. Basically, if you can teleoperate the robot to do a task, you can train the network to reproduce that behavior. This approach is more flexible and scalable because it allows people without advanced degrees in robotics to “program” the robot. Back to top We’re talking about a large behavior model (LBM) here, right? What would you call the kind of learning that this model does? Kuindersma: It is a kind of imitation learning. We collect many teleoperation demonstrations and train a neural network to reproduce the input-output behaviors in the data. The inputs are things like raw robot camera images, natural language descriptions of the task, and proprioception, and the outputs are the same teleop commands sent by the human interface. What makes it a large behavior model is that we collect data from many different tasks and, in some cases, many different robot embodiments, using all of that as training data for the robot to end up with a single policy that knows how to do many things. The idea is that by training the network on a much wider variety of data and tasks and robots, its ability to generalize will be better. As a field, we are still in the early days of gathering evidence that this is actually the case (our [Toyota Research Institute] collaborators are among those leading the charge ), but we expect it is true based on the empirical trends we see in robotics and other AI domains. So the idea with the behavior model is that it will be more generalizable, more adaptable, or require less training because it will have a baseline understanding of how things work? Kuindersma: Exactly, that’s the idea. At a certain scale, once the model has seen enough through its training data, it should have some ability to take what it’s learned from one set of tasks and apply those learnings to new tasks. One of the things that makes these models flexible is that they are conditioned on language. We collect teleop demonstrations and then post-annotate that data with language, having humans or language models describing in English what is happening. The network then learns to associate these language prompts with the robot’s behaviors. Then, you can tell the model what to do in English, and it has a chance of actually doing it. At a certain scale, we hope it won’t take hundreds of demonstrations for the robot to do a task—maybe only a couple—and maybe way in the future, you might be able to just tell the robot what to do in English, and it will know how to do it, even if the task requires dexterity beyond simple object pick-and-place. Back to top There are a lot of robot videos out there of robots doing stuff that might look similar to what we’re seeing here. Can you tell me how what Boston Dynamics and Toyota Research Institute are doing is unique? Kuindersma: Many groups are using AI tools for robot demos, but there are some differences in our strategic approach. From our perspective, it’s crucial for the robot to perform the full breadth of humanoid manipulation tasks. That means if you use a data-driven approach, you need to somehow funnel those embodied experiences into the dataset you’re using to train the model. We spent a lot of time building a highly expressive teleop interface for Atlas, which allows operators to move the robot around quickly, take steps, balance on one foot, reach the floor and high shelves, throw and catch things, and so on. The ability to directly mirror a human body in real time is vital for Atlas to act like a real humanoid laborer. If you’re just standing in front of a table and moving things around, sure, you can do that with a humanoid, but you can do it with much cheaper and simpler robots, too. If you instead want to, say, bend down and pick up something from between your legs, you have to make careful adjustments to the entire body while doing manipulation. The tasks we’ve been focused on with Atlas over the last couple of months have been focused more on collecting this type of data, and we’re committed to making these AI models extremely performant so the motions are smooth, fast, beautiful, and fully cover what humanoids can do. Is it a constraint that you’re using imitation learning, given that Atlas is built to move in ways that humans can’t? How do you expand the operating envelope with this kind of training? Kuindersma: That’s a great question. There are a few ways to think about it: Atlas can certainly do things like continuous joint rotation that people can’t. While those capabilities might offer efficiency benefits, I would argue that if Atlas only behaved exactly like a competent human, that would be amazing, and we would be very happy with that. We could extend our teleop interface to make available types of motions the robot can do but a person can’t. The downside is this would probably make teleoperation less intuitive, requiring a more highly trained expert, which reduces scalability. We may be able to co-train our large behavior models with data sources that are not just teleoperation-based. For example, in simulation, you could use rollouts from reinforcement learning policies or programmatic planners as augmented demonstrations that include these high-range-of-motion capabilities. The LBM can then learn to leverage that in conjunction with teleop demonstrations. This is not just a hypothetical; we’ve actually found that co-training with simulation data has improved performance in the real robot, which is quite promising. Can you tell me what Atlas was directed to do in the video? Is it primarily trying to mirror its human-based training, or does it have some capacity to make decisions? Kuindersma: In this case, Atlas is responding primarily to visual and language cues to perform the task. At our current scale and with the model’s training, there’s a limited ability to completely innovate behaviors. However, you can see a lot of variety and responsiveness in the details of the motion, such as where specific parts are in the bin or where the bin itself is. As long as those experiences are reflected somewhere in the training data, the robot uses its real-time sensor observations to produce the right type of response. So, if the bin was too far away for the robot to reach, without specific training, would it move itself to the bin? Kuindersma: We haven’t done that experiment, but if the bin was too far away, I think it might take a step forward because we varied the initial conditions of the bin when we collected data, which sometimes required the operator to walk the robot to the bin. So there is a good chance that it would step forward, but there is also a small chance that it might try to reach and not succeed. It can be hard to make confident predictions about model behavior without running experiments, which is one of the fun features of working with models like this. Back to top It’s interesting how a large behavior model, which provides world knowledge and flexibility, interacts with this instance of imitation learning, where the robot tries to mimic specific human actions. How much flexibility can the system take on when it’s operating based on human imitation? Kuindersma: It’s primarily a question of scale. A large behavior model is essentially imitation learning at scale, similar to a large language model. The hypothesis with large behavior models is that as they scale, generalization capabilities improve, allowing them to handle more real-world corner cases and require less training data for new tasks. Currently, the generalization of these models is limited, but we’re addressing that by gathering more data not only through teleoperating robots but also by exploring other scaling bets like non-teleop human demonstrations and sim/synthetic data. These other sources might have more of an “embodiment gap” to the robot, but the model’s ability to assimilate and translate between data sources could lead to better generalization. How much skill or experience does it take to effectively train Atlas through teleoperation? Kuindersma: We’ve had people on day tours jump in and do some teleop, moving the robot and picking things up. This ease of entry is thanks to our teams building a really nice interface: The user wears a VR headset, where they’re looking at a re-projection of the robot’s stereo RGB cameras, which are aligned to provide a 3D sense of vision, and there are built-in visual augmentations like desired hand locations and what the robot is actually doing to give people situational awareness. So novice users can do things fairly easily; they’re probably not generating the highest-quality motions for training policies. To generate high-quality data, and to do that consistently over a period of several hours, it typically takes a couple of weeks of onboarding. We usually start with manipulation tasks and then progress to tasks involving repositioning the entire robot. It’s not trivial, but it’s doable. The people doing it now are not roboticists; we have a team of “robot teachers” who are hired for this, and they’re awesome. It gives us a lot of hope for scaling up the operation as we build more robots. Back to top How is what you’re doing different from other companies that might lean much harder on scaling through simulation? Are you focusing more on how humans do things? Kuindersma: Many groups are doing similar things, with differences in technical approach, platform, and data strategy. You can characterize the strategies people are taking by thinking about a “data pyramid,” where the top of the pyramid is the highest quality, hardest-to-get data, which is typically teleoperation on the robot you’re working with. The middle of the pyramid might be egocentric data collected on people (e.g., by wearing sensorized gloves), simulation data, or other synthetic world models. And the bottom of the pyramid is data from YouTube or the rest of the Internet. Different groups allocate finite resources to different distributions of these data sources. For us, we believe it’s really important to have as large a baseline of actual on-robot data (at the top of the pyramid) as possible. Simulation and synthetic data are almost certainly part of the puzzle, and we’re investing resources there, but we’re taking a somewhat balanced data strategy rather than throwing all of our eggs in one basket. Ideally you want the top of the pyramid to be as big as possible, right? Kuindersma: Ideally, yes. But you won’t get to the scale you need by just doing that. You need the whole pyramid, but having as much high-quality data at the top as possible only helps. But it’s not like you can just have a super-large bottom to the pyramid and not need the top? Kuindersma: I don’t think so. I believe there needs to be enough high-quality data for these models to effectively translate into the specific embodiment that they are executing on. There needs to be enough of that “top” data for the translation to happen, but no one knows the exact distribution, like whether you need 5 percent real robot data and 95 percent simulation, or some other ratio. Back to top Is that a box of “ Puny-os ” on the shelf in the video? Part of this self-balancing robot. Boston Dynamics Kuindersma: Yeah! Alex Alspach from [Toyota Research Institute] brought it in to put in the background as an easter egg. What’s next for Atlas? Kuindersma: We’re really focused on maximizing the performance manipulation behaviors. I think one of the things that we’re uniquely positioned to do well is reaching the full behavioral envelope of humanoids, including mobile bimanual manipulation, repetitive tasks, and strength, and getting the robot to move smoothly and dynamically using these models. We’re also developing repeatable processes to climb the robustness curve for these policies—we think reinforcement learning may play a key role in achieving this. We’re also looking at other types of scaling bets around these systems. Yes, it’s going to be very important that we have a lot of high-quality on-robot on-task data that we’re using as part of training these models. But we also think there are real opportunities and being able to leverage other data sources, whether that’s observing or instrumenting human workers or scaling up synthetic and simulation data, and understanding how those things can mix together to improve the performance of our models. Back to top",
    "published": "Sun, 07 Sep 2025 13:00:02 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Robot Vacuum Climbs Stairs",
    "link": "https://spectrum.ieee.org/video-friday-eufy-robot-vacuum",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! This is ridiculous, and I love it. [ Eufy ] At ICRA 2024, We met Paul Nadan to learn about how his LORIS robot climbs up walls by sticking itself to rocks. [ CMU ] If a humanoid robot is going to load my dishwasher, I expect it to do so optimally, not all haphazardly like a puny human. [ Figure ] Humanoid robots have recently achieved impressive progress in locomotion and whole-body control, yet they remain constrained in tasks that demand rapid interaction with dynamic environments through manipulation. Table tennis exemplifies such a challenge: With ball speeds exceeding 5 m/s, players must perceive, predict, and act within sub-second reaction times, requiring both agility and precision. To address this, we present a hierarchical framework for humanoid table tennis that integrates a model-based planner for ball trajectory prediction and racket target planning with a reinforcement learning–based whole-body controller. [ Hybrid Robotics ] Despite their promise, today’s biohybrid robots typically underperform their fully synthetic counterparts and their potential as predicted from a reductionist assessment of constituents. Many systems represent enticing proofs of concept with limited practical applicability. Most remain confined to controlled laboratory settings and lack feasibility in complex real-world environments. Developing biohybrid robots is currently a painstaking, bespoke process, and the resulting systems are routinely inadequately characterized. Complex, intertwined relationships between component, interface, and system performance are poorly understood, and methodologies to guide informed design of biohybrid systems are lacking. The HyBRIDS ARC opportunity seeks ideas to address the question: How can synthetic and biological components be integrated to enable biohybrid platforms that outperform traditional robotic systems? [ DARPA ] Robotic systems will play a key role in future lunar missions, and a great deal of research is currently being conducted in this area. One such project is SAMLER-KI (Semi-Autonomous Micro Rover for Lunar Exploration Using Artificial Intelligence), a collaboration between the German Research Center for Artificial Intelligence (DFKI) and the University of Applied Sciences Aachen (FH Aachen), Germany. The project focuses on the conceptual design of a semi-autonomous micro rover that is capable of surviving lunar nights while remaining within the size class of a micro rover. During development, conditions on the moon such as dust exposure, radiation, and the vacuum of space are taken into account, along with the 14-Earth-day duration of a lunar night. [ DFKI ] ARMstrong Dex is a human-scale dual-arm hydraulic robot developed by the Korea Atomic Energy Research Institute (KAERI) for disaster response applications. It is capable of lifting its own body through vertical pull-ups and manipulating objects over 50 kilograms, demonstrating strength beyond human capabilities. In this test, ARMstrong Dex used a handheld saw to cut through a thick 40×90 millimeter wood beam. Sawing is a physically demanding task involving repetitive force application, fine trajectory control, and real-time coordination. [ KAERI ] This robot stole my “OMG I HAVE JUICE” face. [ Pudu Robotics ] The best way of dodging a punch to the face is to just have a big hole where your face should be. I do wish they wouldn’t call it a combat robot, though. [ Unitree ] It really might be fun to have a DRC-style event for quadrupeds. [ DEEP Robotics ] CMU researchers are developing new technology to enable robots to physically interact with people who are not able to care for themselves. These breakthroughs are being deployed in the real world, making it possible for individuals with neurological diseases, stroke, multiple sclerosis, ALS, and dementia to be able to eat, clean, and get dressed fully on their own. [ CMU ] Caracol’s additive manufacturing platforms use KUKA robotic arms to produce large-scale industrial parts with precision and flexibility. This video outlines how Caracol integrates multi-axis robotics, modular extruders, and proprietary software to support production in sectors like aerospace, marine, automotive, and architecture. [ KUKA ] There were a couple of robots at ICRA 2025, as you might expect. [ ICRA ] On June 6, 1990, following the conclusion of Voyager’s planetary explorations, mission representatives held a news conference at NASA’s Jet Propulsion Laboratory in Southern California to summarize key findings and answer questions from the media. In the briefing, Voyager’s longtime project scientist Ed Stone, along with renowned science communicator Carl Sagan, also revealed the mission’s “Solar System Family Portrait,” a mosaic comprising images of six of the solar system’s eight planets. Sagan was a member of the Voyager imaging team and instrumental in capturing these images and bringing them to the public. Carl Sagan, man. Carl Sagan. Blue Dot unveil was right around 57:00, if you missed it. [ JPL ]",
    "published": "Fri, 05 Sep 2025 16:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Do People Really Want Humanoid Robots in Their Homes?",
    "link": "https://spectrum.ieee.org/home-humanoid-robots-survey",
    "summary": "I’ve been teaching robotics at the University of Washington for more than a decade. Every class begins with “robotics news of the week.” For years, humanoid robots appeared only occasionally—usually in the form of viral clips of Boston Dynamics’ Atlas doing parkour or RoboCup humanoid league bloopers that served more as comic relief than serious news. But over the past few years, things have shifted. Each week brings another humanoid demo, each flashier than the last, as companies race to showcase new capabilities. And behind those slick videos lies a flood of venture capital. Humanoid robotics has become a billion-dollar frenzy. The scale of investment is astonishing. Just a year ago, Figure AI’s $2.6 billion valuation seemed extraordinary—until its latest funding round catapulted it to $39.5 billion . Investors large and small are rushing in, and tech giants like Microsoft, Amazon, OpenAI, and Nvidia are scrambling to get a foothold for fear of missing out. Tesla is pouring resources into its Optimus robot , while China has committed more than $10 billion in government funding to drive down costs and seize market dominance. Goldman Sachs now projects the global humanoid market could reach $38 billion by 2035 . This surge of interest reflects a long-standing dream in robotics: If machines could match human form and function, they could simply step into human jobs without requiring us to change our environments. If humanoids could do everything people can, then in theory they could replace workers on the factory floor or in warehouse aisles. It’s no surprise, then, that many humanoid companies are targeting what they believe are sectors with labor shortages and undesirable jobs— manufacturing , logistics , distribution , retail —as near-term markets. Factories first, homes next? A subset of humanoid companies see homes as the next frontier. Figure AI claims humanoids will revolutionize “assisting individuals in the home” and “caring for the elderly.” Its marketing materials show robots handing an apple to a human, making coffee, putting away groceries and dishes, pouring drinks, and watering plants. Tesla’s Optimus , similarly branded as an “autonomous assistant, humanoid friend,” is shown folding clothes, cracking eggs, unloading groceries, receiving packages, and even playing family games. The Neo humanoid by 1X Technologies appears targeted solely at in-home use, with the company declaring that “1X bets on the home” and is “building a world where we do more of what we love, while our humanoid companions handle the rest.” Neo is depicted vacuuming, serving tea, wiping windows and tables, and carrying laundry and grocery bags. All these glossy marketing videos struck a personal chord with me. I have always dreamed of robots in homes—and I know I am not alone. Like many roboticists of my generation, my earliest memories of robots trace back to Rosie the Robot from The Jetsons . I dedicated my career to getting assistive robots into homes. In 2014, my students and I placed a PR2 robot in a home in Arizona, where it failed miserably at most tasks—though we learned a great deal in the process. Later, I was part of more successful in-home deployments of a Stretch robot and an assistive feeding robot . I even found myself enjoying housework because it gave me a chance to analyze the tasks it entailed with an eye toward someday automating them. For years, I promoted my work under a personal motto: “I want robots to do all the chores by the time I retire,” often joking that I might never retire. Yet when billion-dollar companies began chasing the same dream, I found myself reacting with unease. I had always imagined that home robots would be more like Rosie—robotic and cartoonish—and my own research moved further and further away from the human form because non-humanoid robots were more practical and preferred by users. I struggled to picture a humanoid in my own house—or any of the homes where I had deployed robots. And after years of human-centered research in robotics, I could not imagine users welcoming humanoids into their homes without hesitation. Still, I assumed someone must want them. Surely some fraction of those billions had gone into market research and customer insight. And I wanted to know what they knew. [Left] Three real-world humanoids shown to participants in our study (Figure, Optimus, Neo). [Right] Examples of three general-purpose robots with few or no humanlike features (PR2, Fetch, Stretch). Maya Cakmak What people actually think To find out, my students and I set out to better understand what the public thinks about humanoid robots in the home. We surveyed 76 participants from the U.S. and the U.K., asking whether they considered humanoids in the home acceptable, which designs they preferred, and why. We also presented them with imagined scenarios where either a humanoid or a special-purpose robot assisted an older adult with tasks like eating, dressing, or vacuuming, and asked which they would choose. The results are detailed in our paper “ Attitudes Towards Humanoid Robots for In-Home Assistance ,” presented this week at the IEEE International Conference on Robot and Human Interactive Communication (RO-MAN). Our survey showed that people generally prefer special-purpose robots over humanoids. They see special-purpose robots as safer, more private, and ultimately more comfortable to have in their homes and around loved ones. So, while humanoid companies (and their investors) dream of a single humanoid capable of doing it all, our survey participants seem to be more on board with a toolbox of smaller, specialized machines for most tasks: a Roomba for cleaning, a medication dispenser for pills, a stairlift for stairs. Nevertheless, most survey participants considered humanoids in the home acceptable. Some even preferred humanoids for certain tasks, especially when the special-purpose alternative was more speculative—like a dressing assistant robot. When shown images of Neo, Figure 02, and Optimus performing household tasks, they agreed the robots looked useful and well-suited for homes. Many said they would feel comfortable having one in their own home—or in the home of a loved one. Of course, we had framed the scenarios optimistically: Participants were told to assume the robots had passed extensive safety testing, were approved by regulators, and would be covered by insurance—assumptions that may be decades away from reality. And we can safely assume that finding humanoids “acceptable” doesn’t mean people actually want them—or that they’d be willing to pay for one. AI-generated images of humanoid and special-purpose robots across eight tasks used in our questionnaires. Cakmak et al, 2025 Are home humanoids safe? Unsurprisingly, the task context impacted whether people were open to humanoids in the home. Participants balked at imaginary scenarios involving safety-critical assistance—such as being carried down a staircase—responding with visceral rejections like “absolutely not in a million years.” Whereas for tasks that require little interaction—such as folding laundry—most were willing to imagine a humanoid lending a hand. Even with our reassurances about safety, people readily imagined hazards: Humanoids could trip, stumble, or tip over; they might glitch, run out of battery, or malfunction. The idea of a robot handling hot surfaces or sharp objects was also mentioned by multiple participants as a serious concern. Privacy was another major concern. Participants worried about camera data being sent to the cloud or robots being remotely controlled by strangers. Several pointed out the security risks—any Internet-connected device, they noted, could be hacked. Even participants who saw clear benefits often described a lingering unease. Several described the robots as “creepy” or “unsettling,” and a few explicitly mentioned the uncanny valley effect , pointing in particular to the black face masks common on this new generation of humanoids. One participant described the masks as creating an “eerie sensation, the idea that something might be watching you.” I felt a similar conflict watching a video of Neo (1X) sitting on a couch after finishing its chores —a scene that was meant to be comforting but instead left me unsettled. A common reason participants preferred special-purpose robots was space. Humanoids were described as “bulky” and “unnecessary,” while specialized robots were seen as “less intrusive” and “more discreet.” These comments reminded me of user research conducted in Japan by the Toyota Research Institute, which led to a ceiling-mounted robot design after finding that limited floor space was a major barrier to adoption. The same thought struck me at home when I showed an in-home humanoid video to my 9-year-old and asked if we should get one. He replied: “But we don’t have an extra bed.” His answer nailed the point: If your home doesn’t have room for another human, it probably doesn’t have room for a humanoid. Very big ifs In the end, the study didn’t fully answer my question about what these companies know that I don’t. Participants said they would accept humanoids—if they were safe, worked reliably, and didn’t cost more than the alternatives. Those are very big ifs. And of course, our study asked people to use their imaginations. Looking at a picture is not the same as sharing your living room with a six-foot metal figure that moves—in reality, their reactions might be very different. Likewise, picturing yourself someday needing help with eating, dressing, or walking is very different from already relying on that help every day. Perhaps for those already living with these needs, the immediacy of their situation would make the promise of humanoids more compelling. To probe further, I asked the same question at the HRI 2025 Physical Caregiving Robots Workshop to a panel of six people with motor limitations who are experienced users of assistive robots. Not one of them wanted a humanoid. Their concerns ranged from “it’s creepy” to “it has to be 100 percent safe because I cannot escape it.” One panelist summed it up perfectly: “Trying to make assistive robots with humanoids would be like trying to make autonomous cars by putting humanoids in the driver’s seat and asking them to drive like a human.” After all, it was obvious to investors that the better path to autonomous vehicles was to modify or redesign vehicles for autonomy, rather than replicate human drivers. So why are they convinced that replicating humans is the right solution for the home? What’s the alternative? Special-purpose robots may be preferable to humanoids, but building a dedicated machine for every possible task is unrealistic. Homes involve a long tail of chores, and general-purpose robots could indeed provide enormous value. However, the humanoid form is likely overkill, since much simpler designs—such as wheeled robots with basic pinch grippers—can already accomplish a great deal and are far more attainable. And people will likely accept modest changes to their homes to expand what these robots can do, just as Roomba owners move furniture to let their vacuums work. After all, our homes have already transformed around new technologies—cars, appliances, televisions—so why not for robots, if they prove just as valuable? But beyond the unnecessary complexity, a more important issue about the humanoid form may be that users find it less desirable than simpler alternatives. Research has long shown that highly humanlike robots can trigger negative emotional responses , and our study suggests that is true of the latest generation of humanoids. Simpler designs with more cartoonlike features are more likely to be accepted as companions. We may even want home robots with no humanlike features at all, so they can be viewed as tools rather than social agents. I believe those who would benefit most from in-home robots—including the rapidly growing population of older adults —would prefer robots that empower them to do things for themselves, rather than ones that attempt to replace human caregivers. Yet humanoid companies are openly pursuing the latter. Only time will tell whether humanoid companies can deliver on their promises—and whether people, myself included, will welcome them into their homes. I hope our findings encourage these companies to dig deeper and share their insights about in-home humanoid customers. I’d also like to see more capital directed toward alternative robot designs for the home. In the meantime, my students and I can’t wait to get our hands on one of these humanoids—purely in the name of science—bring it to older adults in our communities, and hear their unfiltered reactions. I can already imagine someone saying, “It better not sit in my recliner when I’m not looking,” or, “If it’s going to live here, it better pay rent.”",
    "published": "Wed, 03 Sep 2025 12:00:02 +0000",
    "author": "Maya Cakmak",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Connecting Africa’s Next Generation of Engineers",
    "link": "https://spectrum.ieee.org/africa-s-next-generation-engineers",
    "summary": "I get a lot of email from people asking to contribute to IEEE Spectrum . Usually, they want to write an article for us. But one bold query I received in January 2024 went much further: An undergraduate engineering student named Oluwatosin Kolade , from Obafemi Awolowo University, in Ilé-Ifẹ̀, Nigeria, volunteered to be our robotics editor. Kolade—Tosin to his friends—had been the newsletter editor for his IEEE student branch, but he’d never published an article professionally. His earnestness and enthusiasm were endearing. I explained that we already have a robotics editor , but I’d be glad to work with him on writing, editing, and ultimately publishing an article. Back in 2003, I had met plenty of engineering students when I traveled to Nigeria to report on the SAT-3/WASC cable, the first undersea fiber-optic cable to land in West Africa. I remember seeing students gathering around obsolete PCs at Internet cafés connected to the world via a satellite dish powered by a generator. I challenged Tosin to tell Spectrum readers what it’s like for engineering students today. The result is “ Lessons from a Janky Drone .” I decided to complement Tosin’s piece with the perspective of a more established engineer in sub-Saharan Africa. I reached out to G. Pascal Zachary , who has covered engineering education in Africa for us, and Zachary introduced me to Engineer Bainomugisha , a computer science professor at Makerere University, in Kampala, Uganda. In “ Learning More With Less ,” Bainomugisha draws out the things that were common to his and Tosin’s experience and suggests ways to make the hardware necessary for engineering education more accessible. In fact, the region’s decades-long struggle to develop its engineering talent hinges on access to the three things we focus on in this issue: reliable electricity, ubiquitous broadband, and educational resources for young engineers. “During my weekly video calls with Tosin...the connection was pretty good— except when it wasn’t.” Zachary’s article in this issue, “ What It Will Really Take to Electrify All of Africa ” tackles the first topic, with a focus on an ambitious initiative to bring electricity to an additional 300 million people by 2030. Contributing editor Lucas Laursen ’s article, “ In Nigeria, Why Isn’t Broadband Everywhere? ” investigates the slow rollout of fiber-optic connectivity in the two decades since my first visit. As he learned when he traveled to Nigeria earlier this year, the country now has eight undersea cables delivering 380 terabits of capacity, yet less than half of the population has broadband access. I got a sense of Nigeria’s bandwidth issues during my weekly video calls with Tosin to discuss his article. The connection was pretty good, except when it wasn’t. Still, I reminded myself, two decades ago such calls would have been nearly impossible. Through those weekly chats, we established a professional connection, which made it that much more meaningful when I got to meet Tosin in person this past May at the IEEE ICRA robotics conference , in Atlanta. Tosin, a RAS member, was attending thanks to a scholarship from the IEEE Robotics and Automation Society to support his participation in robotics standards activities at the conference. Like a kid in a candy shop, he kibbutzed with fellow scholarship winners, attended talks, checked out robots, and met the engineers who built them. As Tosin embarks on the next leg in his career journey, he is supported by the IEEE community, which not only recognizes his promise but gives him access to a network of professionals who can help him and his cohort realize their potential.",
    "published": "Mon, 01 Sep 2025 10:00:02 +0000",
    "author": "Harry Goldstein",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Spot’s Got Talent",
    "link": "https://spectrum.ieee.org/video-friday-synchronized-dancing-robots",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Boston Dynamics is back and their dancing robot dogs are bigger, better, and bolder than ever! Watch as they bring a “dead” robot to life and unleash a never-before-seen synchronized dance routine to “Good Vibrations.” And what’s much more interesting, here’s a discussion of how they made it work: [ Boston Dynamics ] I don’t especially care whether a robot falls over . I care whether it gets itself back up again. [ LimX Dynamics ] The robot autonomously connects multiple wires to the environment using small flying anchors—drones equipped with anchoring mechanisms at the wire tips. Guided by an onboard RGB-D camera for control and environmental recognition, the system enables wire attachment in unprepared environments and supports simultaneous multiwire connections, expanding the operational range of wire-driven robots. [ JSK Robotics Laboratory ] at [ University of Tokyo ] Thanks, Shintaro! For a robot that barely has a face, this is some pretty good emoting. [ Pollen ] Learning skills from human motions offers a promising path toward generalizable policies for whole-body humanoid control, yet two key cornerstones are missing: (1) a scalable, high-quality motion-tracking framework that faithfully transforms kinematic references into robust, extremely dynamic motions on real hardware, and (2) a distillation approach that can effectively learn these motion primitives and compose them to solve downstream tasks. We address these gaps with BeyondMimic, a real-world framework to learn from human motions for versatile and naturalistic humanoid control via guided diffusion. [ Hybrid Robotics ] Introducing our open-source metal-made bipedal robot MEVITA. All components can be procured through e-commerce, and the robot is built with a minimal number of parts. All hardware, software, and learning environments are released as open source. [ MEVITA ] Thanks, Kento! I’ve always thought that being able to rent robots (or exoskeletons) to help you move furniture or otherwise carry stuff would be very useful. [ DEEP Robotics ] A new study explains how tiny water bugs use fanlike propellers to zip across streams at speeds up to 120 body lengths per second. The researchers then created a similar fan structure and used it to propel and maneuver an insect-size robot. The discovery offers new possibilities for designing small machines that could operate during floods or other challenging situations. [ Georgia Tech ] Dynamic locomotion of legged robots is a critical yet challenging topic in expanding the operational range of mobile robots. To achieve generalized legged locomotion on diverse terrains while preserving the robustness of learning-based controllers, this paper proposes an attention-based map encoding conditioned on robot proprioception, which is trained as part of the end-to-end controller using reinforcement learning. We show that the network learns to focus on steppable areas for future footholds when the robot dynamically navigates diverse and challenging terrains. [ Paper ] from [ ETH Zurich ] In the fifth installment of our Moonshot Podcast Deep Dive video interview series, X’s Captain of Moonshots Astro Teller sits down with Google DeepMind’s chief scientist, Jeff Dean, for a conversation about the origin of Jeff’s pioneering work scaling neural networks. They discuss the first time AI captured Jeff’s imagination, the earliest Google Brain framework, the team’s stratospheric advancements in image recognition and speech-to-text, how AI is evolving, and more. [ Moonshot Podcast ]",
    "published": "Fri, 29 Aug 2025 16:30:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Inaugural World Humanoid Robot Games Held",
    "link": "https://spectrum.ieee.org/world-humanoid-robot-games",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, THE NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! The First World Humanoid Robot Games Conclude Successfully! Unitree Strikes Four Golds (1500m, 400m, 100m Obstacle, 4×100m Relay). [ Unitree ] Steady! PNDbotics Adam has become the only full-size humanoid robot athlete to successfully finish the 100m Obstacle Race at the World Humanoid Robot Games! [ PNDbotics ] Introducing Field Foundation Models (FFMs) from FieldAI—a new class of “physics-first” foundation models built specifically for embodied intelligence. Unlike conventional vision or language models retrofitted for robotics, FFMs are designed from the ground up to grapple with uncertainty, risk, and the physical constraints of the real world. This enables safe and reliable robot behaviors when managing scenarios that they have not been trained on, navigating dynamic, unstructured environments without prior maps, GPS, or predefined paths. [ Field AI ] Multiply Labs, leveraging Universal Robots’ collaborative robots, has developed a groundbreaking robotic cluster that is fundamentally transforming the manufacturing of life-saving cell and gene therapies. The Multiply Labs solution drives a staggering 74% cost reduction and enables up to 100x more patient doses per square foot of cleanroom. [ Universal Robots ] In this video, we put Vulcan V3, the world’s first ambidextrous humanoid robotic hand capable of performing the full American Sign Language (ASL) alphabet, to the ultimate test—side by side with a real human! [ Hackaday ] Thanks, Kelvin! More robots need to have this form factor. [ Texas A&M University ] Robotic vacuums are so pervasive now that it’s easy to forget how much of an icon the iRobot Roomba has been. [ iRobot ] This is quite possibly the largest robotic hand I’ve ever seen. [ CAFE Project ] via [ BUILT ] Modular robots built by Dartmouth researchers are finding their feet outdoors. Engineered to assemble into structures that best suit the task at hand, the robots are pieced together from cube-shaped robotic blocks that combine rigid rods and soft, stretchy strings whose tension can be adjusted to deform the blocks and control their shape. [ Dartmouth ] Our quadruped robot X30 has completed extreme-environment missions in Hoh Xil—supporting patrol teams, carrying vital supplies and protecting fragile ecosystems. [ DEEP Robotics ] We propose a base-shaped robot named “koboshi” that moves everyday objects. This koboshi has a spherical surface in contact with the floor, and by moving a weight inside using built-in motors, it can rock up and down, and side to side. By placing everyday items on this koboshi, users can impart new movement to otherwise static objects. The koboshi is equipped with sensors to measure its posture, enabling interaction with users. Additionally, it has communication capabilities, allowing multiple units to communicate with each other. [ Paper ] Bi-LAT is the world’s first Vision-Language-Action (VLA) model that integrates bilateral control into imitation learning, enabling robots to adjust force levels based on natural language instructions. [ Bi-LAT ] to be presented at [ IEEE RO-MAN 2025 ] Thanks, Masato! Look at this jaunty little guy! Although, they very obviously cut the video right before it smashes face-first into furniture more than once. [ Paper ] to be presented at [ 2025 IEEE-RAS International Conference on Humanoid Robotics ] This research has been conducted at the Human Centered Robotics Lab at UT Austin. The video shows our latest experimental bipedal robot, dubbed Mercury, which has passive feet. This means that there are no actuated ankles, unlike humans, forcing Mercury to gain balance by dynamically stepping. [ University of Texas at Austin Human Centered Robotics Lab ] We put two RIVR delivery robots to work with an autonomous vehicle—showing how Physical AI can handle the full last mile, from warehouse to consumers’ doorsteps. [ Rivr ] The KR TITAN ultra is a high-performance industrial robot weighing 4.6 tonnes and capable of handling payloads up to 1.5 tonnes. [ Kuka ] CMU MechE’s Ding Zhao and Ph.D. student Yaru Niu describe LocoMan, a robotic assistant they have been developing. [ Carnegie Mellon University ] Twenty-two years ago, Silicon Valley executive Henry Evans had a massive stroke that left him mute and paralyzed from the neck down. But that didn’t prevent him from becoming a leading advocate of adaptive robotic tech to help disabled people—or from writing country songs, one letter at a time. Correspondent John Blackstone talks with Evans about his upbeat attitude and unlikely pursuits. [ CBS News ]",
    "published": "Fri, 22 Aug 2025 15:30:04 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "What I Learned From a Janky Drone",
    "link": "https://spectrum.ieee.org/stem-education-in-africa",
    "summary": "The package containing the ArduCopter 2.8 board finally arrived from China, bearing the weight of our anticipation. I remember picking it up, the cardboard box weathered slightly from its journey. As I tore through the layers of tape, it felt like unwrapping a long-awaited gift. But as I lifted the ArduCopter 2.8 board out of the box, my heart sank. The board, which was to be the cornerstone of our project, looked worn out and old, with visible scuffs and bent pins. This was just one of a cascade of setbacks my team would face. It all started when I was assigned a project in machine design at Obafemi Awolowo University (OAU), located in the heart of Ilé-Ifẹ̀, an ancient Yoruba city in Osun State, in southwest Nigeria, where I am a mechanical engineering student entering my final year of a five-year program. OAU is one of Nigeria’s oldest and most prestigious universities, known for its beautiful campus and architecture. Some people I know refer to it as the “Stanford of Nigeria” because of the significant number of brilliant startups it has spun off. Despite its reputation, though, OAU—like every other federally owned institution in Nigeria—is underfunded and plagued by faculty strikes , leading to interruptions in academics. The lack of funding means students must pay for their undergraduate projects themselves, making the success of any project heavily dependent on the students’ financial capabilities. The Student & the Professor Two perspectives on engineering education in Africa Johnson I. Ejimanya is a one-man pony express. Walking the exhaust-fogged streets of Owerri, Nigeria, Ejimanya, the engineering dean of the Federal University of Technology, Owerri, carries with him a department’s worth of communications, some handwritten, others on disk. He’s delivering them to a man with a PC and an Internet connection who converts the missives into e-mails and downloads the responses. To Ejimanya, broadband means lugging a big bundle of printed e-mails back with him to the university, which despite being one of the country’s largest and most prestigious engineering schools, has no reliable means of connecting to the Internet. I met Ejimanya when I visited Nigeria in 2003 to report on how the SAT-3/WASC, the first undersea fiber-optic cable to connect West Africa to the world, was being used. (The passage above is from my February 2004 IEEE Spectrum article “ Surf Africa .”) Beyond the lack of computers and Internet access, I saw labs filled with obsolete technology from the 1960s. If students needed a computer or to get online, they went to an Internet cafe, their out-of-pocket costs a burden on them and their families. So is the situation any better 20-plus years on? The short answer is yes. But as computer science professor Engineer Bainomugisha and IEEE student member Oluwatosin Kolade attest in the following pages, there’s still a long way to go. Both men are engineers but at different stages of their academic journey: Bainomugisha went to college in the early 2000s and is now a computer science professor at Makerere University in Kampala, Uganda. Kolade is in his final semester as a mechanical engineering student at Obafemi Awolowo University in Ilé-Ifẹ̀, Nigeria. They describe the challenges they face and what they see as the path forward for a continent brimming with aspiring engineers but woefully short on the resources necessary for a robust education. —Harry Goldstein Dr. Oluwaseun K. Ajayi , an expert in computer-aided design (CAD), machine design, and mechanisms, gave us the freedom to choose our final project. I proposed a research project based on a paper titled “ Advance Simulation Method for Wheel-Terrain Interactions of Space Rovers: A Case Study on the UAE Rashid Rover ” by Ahmad Abubakar and coauthors . But due to the computational resources required, it was rejected. Dr. Ajayi instead proposed that my fellow students and I build a surveillance drone, as it aligned with his own research. Dr. Ajayi, a passionate and driven researcher, was motivated by the potential real-world applications of our project. His constant push for progress, while sometimes overwhelming, was rooted in his desire to see us produce meaningful work. As my team finished scoping out the preliminary concepts of the drone in CAD designs, we were ready to contribute money toward implementing our idea. We conducted a cost analysis and decided to use a third-party vendor to help us order our components from China. We went this route due to shipping and customs issues we’d previously experienced. Taking the third-party route was supposed to solve the problem. Little did we suspect what was coming. By the time we finalized our cost analysis and started to gather funds, the price of the components we needed had skyrocketed due to a sudden economic crisis and depreciation of the Nigerian naira by 35 percent against the U.S. dollar at the end of January 2024. This was the genesis of our problem. Related: Learning More With Less Initially, we were a group of 12, but due to the high cost per person, Dr. Ajayi asked another group, led by Tonbra Suoware , to merge with mine. Tonbra’s team had been planning a robotic arm project until Dr. Ajayi merged our teams and instructed us to work on the drone, with the aim of exhibiting it at the National Space Research and Development Agency , in Abuja, Nigeria. The merger increased our group to 25 members, which helped with the individual financial burden but also meant that not everyone would actively participate in the project. Many just contributed their share of the money. Tonbra and I drove the project forward. Supply Chain Challenges in African Engineering Education With Dr. Ajayi’s consent, my teammates and I scrapped the “surveillance” part of the drone project and raised the money for developing just the drone, totaling approximately 350,000 naira (approximately US $249). We had to cut down costs, which meant straying away from the original specifications of some of the components, like the flight controller, battery, and power-distribution board. Otherwise, the cost would have been way more unbearable. We were set to order the components from China on 5 February 2024. Unfortunately, it was a long holiday in China, we were told, so we wouldn’t get the components until March. This led to tense discussions with Dr. Ajayi, despite having briefed him about the situation. Why the pressure? Our school semester ends in March, and having components arrive in March would mean that the project would be long overdue by the time we finished it. At the same time, we students had a compulsory academic-industrial training at the end of the semester. Oluwatosin Kolade, a mechanical engineering student at Nigeria’s Obafemi Awolowo University, says the drone project taught him the value of failure. Andrew Esiebo But what choice did we have? We couldn’t back down from the project—that would have cost us our grade. We got most of our components by mid-March, and immediately started working on the drone. We had the frame 3D-printed at a cost of 50 naira (approximately US $0.03) per gram for a 570-gram frame, for a total cost of 28,500 naira (roughly US $18). Next, we turned to building the power-distribution system for the electrical components. Initially, we’d planned to use a power-distribution board to evenly distribute power from the battery to the speed controllers and the rotors. However, the board we originally ordered was no longer available. Forced to improvise, we used a Veroboard instead. We connected the battery in a configuration parallel to the speed controllers to ensure that each rotor received equal power. This improvisation did mean additional costs, as we had to rent soldering irons, hand drills, hot glue, cables, a digital multimeter, and other tools from an electronics hub in downtown Ilé-Ifẹ̀. Everything was going smoothly until it was time to configure the flight controller—the ArduCopter 2.8 board—with the assistance of a software program called Mission Planner . We toiled daily, combing through YouTube videos, online forums, Stack Exchange, and other resources for guidance, all to no avail. We even downgraded the Mission Planner software a couple of times, only to discover that the board we’d waited for so patiently was obsolete. It was truly heartbreaking, but we couldn’t order another one because we didn’t have time to wait for it to arrive. Plus, getting another flight controller would’ve cost an additional sum—240,000 naira (about US $150) for a Pixhawk 2.4.8 flight controller —which we didn’t have. We knew our drone would be half-baked without the flight controller. Still, given our semester-ending time constraint, we decided to proceed with the configuration of the transmitter and receiver. We made the final connections and tested the components without the flight controller. To ensure that the transmitter could control all four rotors simultaneously, we tested each rotor individually with each transmitter channel. The goal was to assign a single channel on the transmitter that would activate and synchronize all four rotors, allowing them to spin in unison during flight. This was crucial, because without proper synchronization, the drone would not be able to maintain a stable flight. “This experience taught me invaluable lessons about resilience, teamwork, and the harsh realities of engineering projects done by students in Nigeria.” After the final configuration and components testing, we set out to test our drone in its final form. But a few minutes into the testing, our battery failed. This failure meant the project had failed, and we were incredibly disappointed. When we finally submitted our project to Dr. Ajayi, the deadline had passed. He told us to charge the battery so he could see the drone come alive, even though it couldn’t fly. But circumstances didn’t allow us to order a battery charger, and we were at a loss as to where to get help with the flight controller and battery. There are no tech hubs available for such things in Ilé-Ifẹ̀. We told Dr. Ajayi we couldn’t do as he’d asked and explained the situation to him. He finally allowed us to submit our work, and all team members received course credit. Resourcefulness is not a substitute for funding This experience taught me invaluable lessons about resilience, teamwork, and the harsh realities of engineering projects done by students in Nigeria. It showed me that while technical knowledge is crucial, the ability to adapt and improvise when faced with unforeseen challenges is just as important. I also learned that failure, though disheartening, is not an ending but a stepping stone toward growth and improvement. In my school, the demands on mechanical engineering students are exceptionally high. For instance, in a single semester, I was sometimes assigned up to four different major projects, each from a different professor. Alongside the drone project, I worked on two other substantial projects for other courses. The reality is that a student’s ability to score well in these projects is often heavily dependent on financial resources. We are constantly burdened with the costs of running numerous projects. The country’s ongoing economic challenges, including currency devaluation and inflation, only exacerbate this burden. In essence, when the world, including graduate-school-admission committees and industry recruiters, evaluates transcripts from Nigerian engineering graduates, it’s crucial to recognize that a grade may not fully reflect a student’s capabilities in a given course. They can also reflect financial constraints, difficulties in sourcing equipment and materials, and the broader economic environment. This understanding must inform how transcripts are interpreted, as they tell a story not just of academic performance but also of perseverance in the face of significant challenges. As I advance in my education, I plan to apply these lessons to future projects, knowing that perseverance and resourcefulness will be key to overcoming obstacles. The failed drone project has also given me a realistic glimpse into the working world, where unexpected setbacks and budget constraints are common. It has prepared me to approach my career with both a practical mindset and an understanding that success often comes from how well you manage difficulties, not just how well you execute plans.",
    "published": "Wed, 20 Aug 2025 13:00:03 +0000",
    "author": "Oluwatosin Kolade",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Smart Glasses Help Train General-Purpose Robots",
    "link": "https://spectrum.ieee.org/smart-glasses-robot-training",
    "summary": "General-purpose robots are hard to train. The dream is to have a robot like the Jetson’s Rosie that can performing a range of household tasks, like tidying up or folding laundry. But for that to happen, the robot needs to learn from a large amount of data that match real-world conditions—that data can be difficult to collect. Currently, most training data is collected from multiple static cameras that have to be carefully set up to gather useful information. But what if bots could learn from the everyday interactions we already have with the physical world? That’s a question that the General-purpose Robotics and AI Lab at New York University, led by Assistant Professor Lerrel Pinto , hopes to answer with EgoZero , a smart-glasses system that aids robot learning by collecting data with a souped-up version of Meta’s glasses . In a recent preprint , which serves as a proof of concept for the approach, the researchers trained a robot to complete seven manipulation tasks, such as picking up a piece of bread and placing it on a nearby plate. For each task, they collected 20 minutes of data from humans performing these tasks while recording their actions with glasses from Meta’s Project Aria . (These sensor-laden glasses are used exclusively for research purposes.) When then deployed to autonomously complete these tasks with a robot, the system achieved a 70 percent success rate. The Advantage of Egocentric Data The “ego” part of EgoZero refers to the “egocentric” nature of the data, meaning that it is collected from the perspective of the person performing a task. “The camera sort of moves with you,” like how our eyes move with us, says Raunaq Bhirangi , a postdoctoral researcher at the NYU lab. This has two main advantages: First, the setup is more portable than external cameras. Second, the glasses are more likely to capture the information needed because wearers will make sure they—and thus the camera—can see what’s needed to perform a task. “For instance, say I had something hooked under a table and I want to unhook it. I would bend down, look at that hook and then unhook it, as opposed to a third-person camera, which is not active,” says Bhirangi. “With this egocentric perspective, you get that information baked into your data for free.” The second half of EgoZero’s name refers to the fact that the system is trained without any robot data, which can be costly and difficult to collect; human data alone is enough for the robot to learn a new task. This is enabled by a framework developed by Pinto’s lab that tracks points in space, rather than full images. When training robots on image-based data, “the mismatch is too large between what human hands look like and what robot arms look like,” says Bhirangi. This framework instead tracks points on the hand, which are mapped onto points on the robot. The EgoZero system takes data from humans wearing smart glasses and turns it into usable 3D-navigation data for robots to do general manipulation tasks. Vincent Liu, Ademi Adeniji, Haotian Zhan, et al. Reducing the image to points in 3D space means the model can track movement the same way, regardless of the specific robotic appendage. “As long as the robot points move relative to the object in the same way that the human points move, we’re good,” says Bhirangi. All of this leads to a generalizable model that would otherwise require a lot of diverse robot data to train. If the robot was trained on data picking up one piece of bread—say, a deli roll—it can generalize that information to pick up a piece of ciabatta in a new environment. A Scalable Solution In addition to EgoZero, the research group is working on several projects to help make general-purpose robots a reality, including open-source robot designs, flexible touch sensors , and additional methods of collecting real-world training data. For example, as an alternative to EgoZero, the researchers have also designed a setup with a 3D-printed handheld gripper that more closely resembles most robot “hands.” A smartphone attached to the gripper captures video with the same point-space method that’s used in EgoZero. The team, by having people collect data without bringing a robot into their homes, provide two approaches that could be more scalable for collecting training data. That scalability is ultimately the researcher’s goal. Large language models can harness the entire Internet, but there is no Internet equivalent for the physical world. Tapping into everyday interactions with smart glasses could help fill that gap.",
    "published": "Tue, 19 Aug 2025 14:00:03 +0000",
    "author": "Gwendolyn Rak",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: SCUTTLE",
    "link": "https://spectrum.ieee.org/video-friday-scuttle-robot",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, THE NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Check out our latest innovations on SCUTTLE, advancing multilegged mobility anywhere. [ GCR ] That laundry-folding robot we’ve been working on for 15 years is still not here. Honestly I think Figure could learn a few tricks from vintage UC Berkeley PR2, though. - YouTube [ Figure ] Tensegrity robots are so cool, but so hard—it’s good to see progress. [ Michigan Robotics ] We should find out next week how quick this is. [ Unitree ] We introduce a methodology for task-specific design optimization of multirotor Micro Aerial Vehicles. By leveraging reinforcement learning, Bayesian optimization, and covariance matrix adaptation evolution strategy, we optimize aerial robot designs guided only by their closed-loop performance in a considered task. Our approach systematically explores the design space of motor pose configurations while ensuring manufacturability constraints and minimal aerodynamic interference. Results demonstrate that optimized designs achieve superior performance compared to conventional multirotor configurations in agile waypoint navigation tasks, including against fully actuated designs from the literature. We build and test one of the optimized designs in the real world to validate the sim2real transferability of our approach. [ ARL ] Thanks, Kostas! I guess legs are required for this inspection application because of the stairs right at the beginning? But sometimes, that’s how the world is. [ DEEP Robotics ] The Institute of Robotics and Mechatronics at DLR has a long tradition in developing multifingered hands, creating novel mechatronic concepts as well as autonomous grasping and manipulation capabilities. The range of hands spans from Rotex, a first two-fingered gripper for space applications, to the highly anthropomorphic Awiwi Hand and variable stiffness end effectors. This video summarizes the developments of DLR in this field over the past 30 years, starting with the Rotex experiment in 1993. [ DLR RM ] The quest for agile quadrupedal robots is limited by handcrafted reward design in reinforcement learning. While animal motion capture provides 3D references, its cost prohibits scaling. We address this with a novel video-based framework. The proposed framework significantly advances robotic locomotion capabilities. [ Arc Lab ] Serious question: Why don’t humanoid robots sit down more often? [ EngineAI ] And now, this. [ LimX Dynamics ] NASA researchers are currently using wind tunnel and flight tests to gather data on an electric vertical takeoff and landing (eVTOL) scaled-down small aircraft that resembles an air taxi that aircraft manufacturers can use for their own designs. By using a smaller version of a full-sized aircraft called the RAVEN Subscale Wind Tunnel and Flight Test (RAVEN SWFT) vehicle, NASA is able to conduct its tests in a fast and cost-effective manner. [ NASA ] This video details the advances in orbital manipulation made by DLR’s Robotic and Mechatronics Center over the past 30 years, paving the way for the development of robotic technology for space sustainability. [ DLR RM ] This summer, a team of robots explored a simulated Martian landscape in Germany, remotely guided by an astronaut aboard the International Space Station. This marked the fourth and final session of the Surface Avatar experiment, a collaboration between ESA and the German Aerospace Center (‪DLR) to develop how astronauts can control robotic teams to perform complex tasks on the Moon and Mars. [ ESA ]",
    "published": "Fri, 15 Aug 2025 16:00:04 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Designing for Functional Safety: A Developer's Introduction",
    "link": "https://events.bizzabo.com/749990?utm_source=Wiley&utm_medium=Spectrum",
    "summary": "Welcome to your essential guide to functional safety, tailored specifically for product developers. In a world where technology is increasingly integrated into every aspect of our lives—from industrial robots to autonomous vehicles—the potential for harm from product malfunctions makes functional safety not just important, but critical. This webinar cuts through the complexity to provide a clear understanding of what functional safety truly entails and why it’s critical for product success. We’ll start by defining functional safety not by its often-confusing official terms, but as a structured methodology for managing risk through defined engineering processes, essential product design requirements, and probabilistic analysis. The “north star” goals? To ensure your product not only works reliably but, if it does fail, it does so in a safe and predictable manner. We’ll dive into two fundamental concepts: the Safety Lifecycle, a detailed engineering process focused on design quality to minimize systematic failures, and Probabilistic, Performance-Based Design using reliability metrics to minimize random hardware failures. You’ll learn about IEC 61508, the foundational standard for functional safety, and how numerous industry-specific standards derive from it. The webinar will walk you through the Engineering Design phases: analyzing hazards and required risk reduction, realizing optimal designs, and ensuring safe operation. We’ll demystify the Performance Concept and the critical Safety Integrity Level (SIL), explaining its definition, criteria (systematic capability, architectural constraints, PFD), and how it relates to industry-specific priorities. Discover key Design Verification techniques like DFMEA/DDMA and FMEDA, emphasizing how these tools help identify and address problems early in development. We’ll detail the FMEDA technique showing how design decisions directly impact predictions like safe and dangerous failure rates, diagnostic coverage, and useful life. Finally, we’ll cover Functional Safety Certification, explaining its purpose, process, and what adjustments to your development process can set you up for success. Register now for this free webinar!",
    "published": "Fri, 15 Aug 2025 11:52:07 +0000",
    "author": "exida",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Bug-size Bots Get More Nimble With Flexible Actuators",
    "link": "https://spectrum.ieee.org/soft-robot-actuators-bugs",
    "summary": "Small, autonomous robots that can access cramped environments could help with future search-and-rescue operations and inspecting infrastructure details that are difficult to access by people or larger bots. However, the conventional, rigid motors that many robots rely on are difficult to miniaturize to these scales, because they easily break when made smaller or can no longer overcome friction forces. Now, researchers have developed a muscle-inspired elasto-electromagnetic system to build insect-size “soft” robots made of flexible materials. “It became clear that existing soft robotic systems at this scale still lack actuation mechanisms that are both efficient and autonomous,” says Hanqing Jiang , a professor of mechanical engineering at Westlake University in Hangzhou, China. Instead, they “often require harsh stimuli such as high voltage, strong external fields, or intense light that hinder their real-world deployment.” Muscles function similarly to actuators, where body parts move through the contraction and relaxation of muscle fibers. When connected to the rest of the body, the brain and other electrical systems in the body allow animals to make a range of movements, including movement patterns that generate disproportionately large forces relative to their body mass. Muscle-Inspired Actuator Technology The new actuator is made of a flexible silicone polymer called polydimethylsiloxane, a neodymium magnet , and an electrical coil intertwined with soft magnetic iron spheres. The researchers fabricated the actuators using a 2D molding process that can manufacture them at millimeter, centimeter, and decimeter scales. It is also scalable for larger, more powerful soft devices. “We shifted focus from material response to structural design in soft materials and combined it with static magnetic forces to create a novel actuation mechanism,” says Jiang. The researchers published their work in Nature Communications . The new actuator is able to contract like a muscle using a balance between elastic and magnetic forces. When the actuator contracts, it generates an electrical current to create a Lorentz force between the electrical coil and the neodymium magnet. The actuator then deforms as the iron spheres respond to the increased force, which can be used to provide movement for the robot itself. The flexible polymer ensures that the system can both deform and recover back to its original state when the current is no longer applied. The system tested by the researchers achieved an output force of 210 newtons per kilogram, a low operational voltage below 4 volts, and is powered by onboard batteries. It can also undergo large deformations, up to a 60 percent contraction ratio. The researchers made it more energy efficient by not requiring continuous power to maintain a stable state when the actuator isn’t moving—a technique similar to how mollusks stay in place using their catch muscles, which can maintain high tension over long periods of time by latching together thick and thin muscle filaments to conserve energy. Autonomous Insect-Size Soft Robots The researchers used the actuators to develop a series of insect-size soft robots that could exhibit autonomous adaptive crawling, swimming, and jumping movements in a range of environments. One such series of bug-size bots was a group of compact soft inchworm crawlers, just 16-by-10-by-10 millimeters in size and weighing only 1.8 grams. The robots were equipped with a translational joint, a 3.7-volt (30-milliampere-hour) lithium-ion battery, and an integrated control circuit. This setup enabled the robots to crawl using sequential contractions and relaxation—much like a caterpillar. Despite its small size, the crawler exhibited an output force of 0.41 N, which is 8 to 45 times as powerful as existing insect-scale soft crawler robots. This output force enabled the robot to traverse difficult-to-navigate terrain—including soil, rough stone, PVC, glass, wood, and inclines between 5 and 15 degrees—while keeping a consistent speed. The bug bots were also found to be very resilient to impacts and falling. They suffered no damage and continued to work even after a 30-meter drop off the side of a building. The researchers also developed 14-by-20-by-19-mm legged crawlers, weighing 1.9 g with an output force of 0.48 N, that crawled like inchworms. These used rotational elasto-electromagnetic joints to move the legs backward and forward and weighed just 1.9 g. The researchers also built a 19-by-19-by-11-mm swimming robot that weighed 2.2 g with an output force of 0.43 N. Alongside testing how the bots move on different surfaces, the researchers built a number of obstacle courses for them to navigate while performing sensing operations. The inchworm bot was put into an obstacle course featuring narrow and complex paths and used a humidity sensor to detect sources of moisture. The swimming bots were tested in both the lab and a river. A course was built in the lab, where the swimmer had to perform chemical sensing operations in a narrow chamber using an integrated miniature ethanol gas detector. Jiang says the researchers are now looking at developing sensor-rich robotic swarms capable of distributed detection, decision-making, and collective behavior. “By coordinating many small robots, we aim to create systems that can cover wide areas, adapt to dynamic environments, and respond more intelligently to complex tasks.” Jiang says they’re also looking into flying and other swimming movements enabled by the elasto-electromagnetic system, including a jellyfish-like soft robot for deep-sea exploration and marine research.",
    "published": "Tue, 12 Aug 2025 12:00:03 +0000",
    "author": "Liam Critchley",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Unitree’s A2 Quadruped Goes Exploring",
    "link": "https://spectrum.ieee.org/video-friday-exploration-robots",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. World Humanoid Robot Games : 15–17 August 2025, BEIJING RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, THE NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! The A2 sets a new standard in quadruped robots, balancing endurance, strength, speed, and perception. The A2 weighs 37 kg (81.6 lbs) unloaded. Fully loaded with a 25 kg (55 lb) payload, it can continuously walk for 3 hours or approximately 12.5 km. Unloaded, it can continuously walk for 5 hours or approximately 20 km. Hot-swappable dual batteries enable seamless battery swap and continuous runtime for any mission. [ Unitree ] Thanks, William! ABB is working with Cosmic Buildings to reshape how communities rebuild and transform construction after disaster. In response to the 2025 Southern California wildfires, Cosmic Buildings are deploying mobile robotic microfactories to build modular homes on-site—cutting construction time by 70% and costs by 30%. [ ABB ] Thanks, Caitlin! How many slightly awkward engineers can your humanoid robot pull? [ MagicLab ] The physical robot hand does some nifty stuff at about 1 minute in. [ ETH Zurich Soft Robotics Lab ] Biologists, you can all go home now. [ AgileX ] The World Humanoid Robot Games start next week in Beijing, and of course Tech United Eindhoven are there. [ Tech United ] Our USX-1 Defiant is a new kind of autonomous maritime platform , with the potential to transform the way we design and build ships. As the team prepares Defiant for an extended at-sea demonstration, program manager Greg Avicola shares the foundational thinking behind the breakthrough vessel. [ DARPA ] After loss, how do you translate grief into creation? Meditation Upon Death is Paul Kirby’s most personal and profound painting—a journey through love, loss, and the mystery of the afterlife. Inspired by a conversation with a Native American shaman and years of artistic exploration, Paul fuses technology and traditional art to capture the spirit’s passage beyond. With 5,796 brushstrokes, a custom-built robotic painting system, and a vision shaped by memory and devotion, this is the most important painting he has ever made. [ Dulcinea ] Thanks, Alexandra! In the fourth installment of our Moonshot Podcast Deep Dive video interview series, X’s Captain of Moonshots Astro Teller sits down with Andrew Ng, the founder of Google Brain and DeepLearning.AI, for a conversation about the history of neural network research and how Andrew’s pioneering ideas led to some of the biggest breakthroughs in modern-day AI. [ Moonshot Podcast ]",
    "published": "Fri, 08 Aug 2025 15:30:04 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Dance With CHILD",
    "link": "https://spectrum.ieee.org/video-friday-child-humanoid-robot",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Many parents naturally teach motions to their child while using a baby carrier. In this setting, the parent’s range of motion fully encompasses the child’s, making it intuitive to scale down motions in a puppeteering manner. This inspired UIUC KIMLAB to build CHILD: Controller for Humanoid Imitation and Live Demonstration. The role of teleoperation has grown increasingly important with the rising interest in collecting physical data in the era of Physical/Embodied AI. We demonstrate the capabilities of CHILD through loco-manipulation and full-body control experiments using the Unitree G1 and other PAPRAS dual-arm systems. To promote accessibility and reproducibility, we open-source the hardware design. [ KIMLAB ] This costs less than US $6,000. [ Unitree ] If I wasn’t sold on one of these little Reachy Minis before. I definitely am now. [ Pollen ] In this study, we propose a falconry-like interaction system in which a flapping-wing drone performs autonomous palm-landing motion on a human hand. To achieve a safe approach toward humans, our motion planning method considers both physical and psychological factors. I should point out that palm landings are not falconry-like at all, and that if you’re doing falconry right, the bird should be landing on your wrist instead. I have other hobbies besides robots, you know! [ Paper ] I’m not sure that augmented reality is good for all that much, but I do like this use case of interactive robot help. [ MRHaD ] Thanks, Masato! LimX Dynamics officially launched its general-purpose full-size humanoid robot LimX Oli. It’s currently available only in Mainland China. A global version is coming soon. Standing at 165 cm and equipped with 31 active degrees of freedom (excluding end-effectors), LimX Oli adopts a general-purpose humanoid configuration with modular hardware-software architecture and is supported by a development tool chain. It is built to advance embodied AI development from algorithm research to real-world deployment. [ LimX Dynamics ] Thanks, Jinyan! Meet Treadward – the newest robot from HEBI Robotics, purpose-built for rugged terrain, inspection missions, and real-world fieldwork. Treadward combines high mobility with extreme durability, making it ideal for challenging environments like waterlogged infrastructure, disaster zones, and construction sites. With a compact footprint and treaded base, it can climb over debris, traverse uneven ground, and carry substantial payloads. [ HEBI ] PNDbotics made a stunning debut at the 2025 World Artificial Intelligence Conference (WAIC) with the first-ever joint appearance of its full-sized humanoid robot Adam and its intelligent data-collection counterpart Adam-U. [ PNDbotics ] This paper presents the design, development, and validation of a fully autonomous dual-arm aerial robot capable of mapping, localizing, planning, and grasping parcels in an intra-logistics scenario. The aerial robot is intended to operate in a scenario comprising several supply points, delivery points, parcels with tags, and obstacles, generating the mission plan from voice the commands given by the user. [ GRVC ] We left the room. They took over. No humans. No instructions. Just robots...moving, coordinating, showing off. It almost felt like…they were staging something. [ AgileX ] TRI’s internship program offers a unique opportunity to work closely with our researchers on technologies to improve the quality of life for individuals and society. Here’s a glimpse into that experience from some of our 2025 interns! [ TRI ] In the third installment of our Moonshot Podcast Deep Dive video interview series, X’s Captain of Moonshots Astro Teller sits down with Dr. Catie Cuan, robot choreographer and former artist in residence at Everyday Robots, for a conversation about how dance can be used to build beautiful and useful robots that people want to be around. [ Moonshot Podcast ]",
    "published": "Fri, 01 Aug 2025 17:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Robots That Learn to Fear Like Humans Survive Better",
    "link": "https://spectrum.ieee.org/robot-risk-assessment-fear",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Imagine walking downtown when you hear a loud bang coming from the construction site across the street—you may have the impulse to freeze or even duck down. This type of quick, instinctual reaction is one of the most basic but important evolutionary processes we have to protect ourselves and survive in unfamiliar settings. Now, researchers are beginning to explore how a similar, fast-reacting thought process can be translated into robots. The idea is to program robots to make decisions the same way that humans do, based on our innate emotional responses to unknown stimuli—and in particular our fear response. The results , published 27 June in IEEE Robotics and Automation Letters , show that the approach can significantly enhance robots’ ability to assess risk and avoid dangerous situations. Alessandro Rizzo , an associate professor in automation engineering and robotics at the Polytechnic University of Turin in Italy, led the study. He notes that robots currently face many challenges in adapting to dynamic environments while enacting self-preserving strategies. This is in large part because their control systems are often designed to accomplish very specific tasks. “As a result, robots may struggle to operate effectively in complex and changing conditions,” Rizzo says. How the Human Brain Responds to Risk Humans, on the other hand, are able to respond to many different and unique stimuli that we encounter. It’s theorized that our brains have two different ways to calculate, assess, and respond to risk in these scenarios. The first involves a very innate response where we detect external stimuli (for example, a loud bang from a construction site) and our brains make very quick, emotional decisions (such as to freeze or duck). In a way, our brains are swiftly responding to raw data in these scenarios, rather than taking the time to more thoroughly process it. According to a theory on how our brains work, called the dual-pathway hypothesis, this reaction is elicited by the “ low road ,” neural circuitry responsible for emotions, driven by the amygdala. But when our brains instead use experience and more articulated reasoning involving our prefrontal cortex, this is the second, “high road” pathway to respond to stimuli. Rizzo and a doctoral candidate in his lab, Andrea Usai , were curious to see how these two different approaches for confronting risky situations would play out in robots that have to navigate unfamiliar environments. They began by designing a control system for robots that emulates a fear response via the low road. “We focused on fear, as it is one of the most studied emotions in neuroscience and, in our view, the one with the greatest potential for robotics,” says Usai. “Fear is closely related to self-preservation and rapid responses to danger, both of which are critical for adaptive behavior.” Reinforced Learning in Robotics To emulate the fear response in their robot, the researchers designed a controller based on reinforced learning, which helps the robot dynamically adjust its priorities and constraints in real time based on raw data of its surroundings. These results inform the behavior of a second algorithm called a nonlinear model predictive controller, which sets a corresponding motor pattern to the robot’s locomotion. Through simulations, Rizzo and Usai tested how their robot navigates unfamiliar environments, comparing it to other robot control systems without the fear element. The simulations involved different scenarios, with various dangerous and nondangerous obstacles, which are either static or moving around the simulated environment. The results show that the robot with the low-road programming was able to navigate a smoother and safer path toward its goal compared to conventional robot designs. For example, in one of the scenarios with hazards dynamically moving around, the low-road robot navigated around dangerous objects with a wider berth of about 3.1 meters, whereas the other two conventional robots tested in this study came within a harrowing 0.3 and 0.8 meters of dangerous objects. Usai says there are many different scenarios where this low-road approach to robotics could be useful, including cases of object manipulation, surveillance, and rescue operations, where robots must deal with hazardous conditions and may need to adopt more cautious behavior. But as Usai notes, the low-road approach is very reactive in nature and is better suited for very quick decisions that are needed in the short term. Therefore, the research team is working on a control design that mimics the high road that, while complementing the low road, could help robots make more rational, long-term decisions. The researchers are considering doing this using multimodal large language models , like ChatGPT. As Rizzo explains, “These models could help simulate some of the core functions of the human prefrontal cortex, such as decision-making, strategic planning, and context evaluation, allowing us to emulate more cognitively driven responses in robots.” “Looking ahead, it would also be interesting trying to extend the architecture to incorporate multiple emotions,” Rizzo adds, “enabling a richer and more nuanced form of adaptive behavior in robotic systems.” This article appears in the October 2025 print issue as “Robots Do Better When They Can Fear.”",
    "published": "Sat, 26 Jul 2025 13:00:01 +0000",
    "author": "Michelle Hampson",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Skyfall Takes on Mars With Swarm Helicopter Concept",
    "link": "https://spectrum.ieee.org/video-friday-skyfall-mars-helicopter",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! AeroVironment revealed Skyfall—a potential future mission concept for next-generation Mars Helicopters developed with NASA’s Jet Propulsion Laboratory (JPL) to help pave the way for human landing on Mars through autonomous aerial exploration. The concept is heavily focused on rapidly delivering an affordable, technically mature solution for expanded Mars exploration that would be ready for launch by 2028. Skyfall is designed to deploy six scout helicopters on Mars, where they would explore many of the sites selected by NASA and industry as top candidate landing sites for America’s first Martian astronauts. While exploring the region, each helicopter can operate independently, beaming high-resolution surface imaging and subsurface radar data back to Earth for analysis, helping ensure crewed vehicles make safe landings at areas with maximum amounts of water, ice, and other resources. The concept would be the first to use the “Skyfall Maneuver”—an innovative entry, descent, and landing technique whereby the six rotorcraft deploy from their entry capsule during its descent through the Martian atmosphere. By flying the helicopters down to the Mars surface under their own power, Skyfall would eliminate the necessity for a landing platform–traditionally one of the most expensive, complex, and risky elements of any Mars mission. [ AeroVironment ] By far the best part of videos like these is watching the expressions on the faces of the students when their robots succeed at something. [ RaiLab ] This is just a rendering of course, but the real thing should be showing up on 6 August. [ Fourier ] Top performer in its class! Less than two weeks after its last release, MagicLab unveils another breakthrough — MagicDog-W, the wheeled quadruped robot. Cyber-flex, dominate all terrains! [ MagicLab ] Inspired by the octopus’s remarkable ability to wrap and grip with precision, this study introduces a vacuum-driven, origami-inspired soft actuator that mimics such versatility through self-folding design and high bending angles. Its crease-free, 3D-printable structure enables compact, modular robotics with enhanced grasping force—ideal for handling objects of various shapes and sizes using octopus-like suction synergy. [ Paper ] via [ IEEE Transactions on Robots ] Thanks, Bram! Is it a plane? Is it a helicopter? Yes. [ Robotics and Intelligent Systems Laboratory, City University of Hong Kong ] You don’t need wrist rotation as long as you have the right gripper . [ Nature Machine Intelligence ] ICRA 2026 will be in Vienna next June! [ ICRA 2026 ] Boing, boing, boing! [ Robotics and Intelligent Systems Laboratory, City University of Hong Kong ] ROBOTERA Unveils L7: Next-Generation Full-Size Bipedal Humanoid Robot with Powerful Mobility and Dexterous Manipulation! [ ROBOTERA ] Meet UBTECH New-Gen of Industrial Humanoid Robot—Walker S2 makes multiple industry-leading breakthroughs! Walker S2 is the world’s first humanoid robot to achieve 3-minute autonomous battery swapping and 24/7 continuous operation. [ UBTECH ] ARMstrong Dex is a human-scale dual-arm hydraulic robot developed by the Korea Atomic Energy Research Institute (KAERI) for disaster response. It can perform vertical pull-ups and manipulate loads over 50 kilograms, demonstrating strength beyond human capabilities. However, disaster environments also require agility and fast, precise movement. This test evaluated ARMstrong Dex’s ability to throw a 500-milliliter water bottle (0.5 kg) into a target container. The experiment assessed high-speed coordination, trajectory control, and endpoint accuracy, which are key attributes for operating in dynamic rescue scenarios. [ KAERI ] This is not a humanoid robot, it’s a data-acquisition platform. [ PNDbotics ] Neat feature on this drone to shift the battery back and forth to compensate for movement of the arm. [ Paper ] via [ Drones journal ] As residential buildings become taller and more advanced, the demand for seamless and secure in-building delivery continues to grow. In high-end apartments and modern senior living facilities where couriers cannot access upper floors, robots like FlashBot Max are becoming essential. In this featured elderly care residence, FlashBot Max completes 80 to 100 deliveries daily, seamlessly navigating elevators, notifying residents upon arrival, and returning to its charging station after each delivery. [ Pudu Robotics ] “How to Shake Trees With Aerial Manipulators.” [ GRVC ] We see a future where seeing a cobot in a hospital delivering supplies feels as normal as seeing a tractor in a field. Watch our CEO Brad Porter share what robots moving in the world should feel like. [ Cobot ] Introducing the Engineered Arts UI for robot Roles, it’s now simple to set up a robot to behave exactly the way you want it to. We give a quick overview of customization for languages, personality, knowledge, and abilities. All of this is done with no code. Just simple LLM prompts, drop-down list selections and some switches to enable the features you need. [ Engineered Arts ] Unlike most quadrupeds, CARA doesn’t use any gears or pulleys. Instead, her joints are driven by rope through capstan drives. Capstan drives offer several advantages: zero backlash, high torque transparency, low inertia, low cost, and quiet operation. These qualities make them an ideal speed reducer for robotics. [ CARA ]",
    "published": "Fri, 25 Jul 2025 16:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "DeepMind’s Quest for Self-Improving Table Tennis Agents",
    "link": "https://spectrum.ieee.org/deepmind-table-tennis-robots",
    "summary": "Hardly a day goes by without impressive new robotic platforms emerging from academic labs and commercial startups worldwide. Humanoid robots in particular look increasingly capable of assisting us in factories and eventually in homes and hospitals. Yet, for these machines to be truly useful, they need sophisticated “brains” to control their robotic bodies. Traditionally, programming robots involves experts spending countless hours meticulously scripting complex behaviors and exhaustively tuning parameters, such as controller gains or motion-planning weights, to achieve desired performance. While machine learning (ML) techniques have promise, robots that need to learn new complex behaviors still require substantial human oversight and reengineering. At Google DeepMind , we asked ourselves: How do we enable robots to learn and adapt more holistically and continuously, reducing the bottleneck of expert intervention for every significant improvement or new skill? This question has been a driving force behind our robotics research. We are exploring paradigms where two robotic agents playing against each other can achieve a greater degree of autonomous self-improvement, moving beyond systems that are merely preprogrammed with fixed or narrowly adaptive ML models toward agents that can learn a broad range of skills on the job. Building on our previous work in ML with systems like AlphaGo and AlphaFold , we turned our attention to the demanding sport of table tennis as a testbed . We chose table tennis precisely because it encapsulates many of the hardest challenges in robotics within a constrained, yet highly dynamic, environment. Table tennis requires a robot to master a confluence of difficult skills: Beyond just perception, it demands exceptionally precise control to intercept the ball at the correct angle and velocity and involves strategic decision-making to outmaneuver an opponent. These elements make it an ideal domain for developing and evaluating robust learning algorithms that can handle real-time interaction, complex physics, high-level reasoning and the need for adaptive strategies — capabilities that are directly transferable to applications like manufacturing and even potentially unstructured home settings. The Self-Improvement Challenge Standard machine learning approaches often fall short when it comes to enabling continuous, autonomous learning. Imitation learning, where a robot learns by mimicking an expert, typically requires us to provide vast numbers of human demonstrations for every skill or variation; this reliance on expert data collection becomes a significant bottleneck if we want the robot to continually learn new tasks or refine its performance over time. Similarly, reinforcement learning, which trains agents through trial-and-error guided by rewards or punishments, often necessitates that human designers meticulously engineer complex mathematical reward functions to precisely capture desired behaviors for multifaceted tasks, and then adapt them as the robot needs to improve or learn new skills, limiting scalability. In essence, both of these well-established methods traditionally involve substantial human involvement, especially if the goal is for the robot to continually self-improve beyond its initial programming. Therefore, we posed a direct challenge to our team: Can robots learn and enhance their skills with minimal or no human intervention during the learning-and-improvement loop? Learning Through Competition: Robot vs. Robot One innovative approach we explored mirrors the strategy used for AlphaGo: Have agents learn by competing against themselves. We experimented with having two robot arms play table tennis against each other, an idea that is simple yet powerful. As one robot discovers a better strategy, its opponent is forced to adapt and improve, creating a cycle of escalating skill levels. DeepMind To enable the extensive training needed for these paradigms, we engineered a fully autonomous table-tennis environment. This setup allowed for continuous operation, featuring automated ball collection as well as remote monitoring and control, allowing us to run experiments for extended periods without direct involvement. As a first step, we successfully trained a robot agent (replicated on both the robots independently) using reinforcement learning in simulation to play cooperative rallies. We fine-tuned the agent for a few hours in the real-world robot-versus-robot setup, resulting in a policy capable of holding long rallies. We then switched to tackling the competitive robot-versus-robot play. Out of the box, the cooperative agent didn’t work well in competitive play. This was expected, because in cooperative play, rallies would settle into a narrow zone, limiting the distribution of balls the agent can hit back. Our hypothesis was that if we continued training with competitive play, this distribution would slowly expand as we rewarded each robot for beating its opponent. While promising, training systems through competitive self-play in the real world presented significant hurdles. The increase in distribution turned out to be rather drastic given the constraints of the limited model size. Essentially, it was hard for the model to learn to deal with the new shots effectively without forgetting old shots, and we quickly hit a local-minima in the training where after a short rally, one robot would hit an easy winner, and the second robot was not able to return it. While robot-on-robot competitive play has remained a tough nut to crack, our team also investigated how the robot could play against humans competitively . In the early stages of training, humans did a better job of keeping the ball in play, thus increasing the distribution of shots that the robot could learn from. We still had to develop a policy architecture consisting of low-level controllers with their detailed skill descriptors and a high-level controller that chooses the low-level skills, along with techniques for enabling a zero-shot sim-to-real approach to allow our system to adapt to unseen opponents in real time. In a user study, while the robot lost all of its matches against the most advanced players, it won all of its matches against beginners and about half of its matches against intermediate players, demonstrating solidly amateur human-level performance. Equipped with these innovations, plus a better starting point than cooperative play, we are in a great position to go back to robot-versus-robot competitive training and continue scaling rapidly. DeepMind The AI Coach: VLMs Enter the Game A second intriguing idea we investigated leverages the power of vision language models (VLMs) , like Gemini. Could a VLM act as a coach, observing a robot player and providing guidance for improvement? DeepMind An important insight of this project is that VLMs can be leveraged for explainable robot policy search. Based on this insight, we developed the SAS Prompt (summarize, analyze, synthesize), a single prompt that enables iterative learning and adaptation of robot behavior by leveraging the VLM’s ability to retrieve, reason, and optimize to synthesize new behavior. Our approach can be regarded as an early example of a new family of explainable policy-search methods that are entirely implemented within an LLM. Also, there is no reward function—the VLM infers the reward directly from the observations given in the task description. The VLM can thus become a coach that constantly analyzes the performance of the student and provides suggestions for how to get better. DeepMind Toward Truly Learned Robotics: An Optimistic Outlook Moving beyond the limitations of traditional programming and ML techniques is essential for the future of robotics. Methods enabling autonomous self-improvement, like those we are developing, reduce the reliance on painstaking human effort. Our table-tennis projects explore pathways toward robots that can acquire and refine complex skills more autonomously. While significant challenges persist—stabilizing robot-versus-robot learning and scaling VLM-based coaching are formidable tasks—these approaches offer a unique opportunity. We are optimistic that continued research in this direction will lead to more capable, adaptable machines that can learn the diverse skills needed to operate effectively and safely in our unstructured world. The journey is complex, but the potential payoff of truly intelligent and helpful robotic partners make it worth pursuing. The authors express their deepest appreciation to the Google DeepMind Robotics team and in particular David B. D’Ambrosio, Saminda Abeyruwan, Laura Graesser, Atil Iscen, Alex Bewley, and Krista Reymann for their invaluable contributions to the development and refinement of this work.",
    "published": "Mon, 21 Jul 2025 15:00:03 +0000",
    "author": "Pannag Sanketi",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Robot Metabolism",
    "link": "https://spectrum.ieee.org/video-friday-robot-metabolism",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics . We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, THE NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Columbia University researchers introduce a process that allows machines to “grow” physically by integrating parts from their surroundings or from other robots, demonstrating a step toward self-sustaining robot ecologies. [ Robot Metabolism ] via [ Columbia ] We challenged ourselves to see just how far we could push Digit’s ability to stabilize itself in response to a disturbance. Utilizing state-of-the-art AI technology and robust physical intelligence, Digit can adapt to substantial disruptions, all without the use of visual perception. [ Agility Robotics ] We are presenting the Figure 03 (F.03) battery—a significant advancement in our core humanoid robot technology roadmap. The effort that was put into safety for this battery is impressive. But I would note two things: The battery life is “5 hours of run time at peak performance” without saying what “peak performance” actually means, and 2-kilowatt fast charge still means over an hour to fully charge. [ Figure ] Well this is a nifty idea. [ UBTECH ] PAPRLE is a plug-and-play robotic limb environment for flexible configuration and control of robotic limbs across applications. With PAPRLE, users can use diverse configurations of leader-follower pair for teleoperation. In the video, we show several teleoperation examples supported by PAPRLE. [ PAPRLE ] Thanks, Joohyung! Always nice to see a robot with a carefully thought-out commercial use case in which it can just do robot stuff like a robot. [ Cohesive Robotics ] Thanks, David! We are interested in deploying autonomous legged robots in diverse environments, such as industrial facilities and forests. As part of the DigiForest project, we are working on new systems to autonomously build forest inventories with legged platforms, which we have deployed in the UK, Finland, and Switzerland. [ Oxford ] Thanks, Matias! In this research we introduce a self-healing, biocompatible strain sensor using Galinstan and a Diels-Alder polymer, capable of restoring both mechanical and sensing functions after repeated damage. This highly stretchable and conductive sensor demonstrates strong performance metrics—including 80% mechanical healing efficiency and 105% gauge factor recovery—making it suitable for smart wearable applications. [ Paper ] Thanks, Bram! The “Amazing Hand” from Pollen Robotics costs less than $250. [ Pollen ] Welcome to our Unboxing Day! After months of waiting, our humanoid robot has finally arrived at Fraunhofer IPA in Stuttgart. I used to take stretching classes from a woman who could do this backwards in 5.43 seconds . [ Fraunhofer ] At the Changchun stop of the VOYAGEX Music Festival on July 12, PNDbotics’ full-sized humanoid robot Adam took the stage as a keytar player with the famous Chinese musician Hu Yutong’s band. [ PNDbotics ] Material movement is the invisible infrastructure of hospitals, airports, cities–everyday life. We build robots that support the people doing this essential, often overlooked work. Watch our CEO Brad Porter reflect on what inspired Cobot. [ Cobot ] Yes please. [ Pollen ] I think I could get to the point of being okay with this living in my bathroom. [ Paper ] Thanks to its social perception, high expressiveness, and out-of-the-box integration, TIAGo Head offers the ultimate human-robot interaction experience. [ PAL Robotics ] Sneak peek: Our No Manning Required Ship (NOMARS) Defiant unmanned surface vessel is designed to operate for up to a year at sea without human intervention. In-water testing is preparing it for an extended at-sea demonstration of reliability and endurance. Excellent name for any ship. [ DARPA ] At the 22nd International Conference on Ubiquitous Robots (UR2025), high school student and robotics researcher Ethan Hong was honored as a Special Invited Speaker for the conference banquet and “Robots With Us” panel. In this heartfelt and inspiring talk, Ethan shares the story behind Food Angel—a food delivery robot he designed and built to support people experiencing homelessness in Los Angeles. Motivated by the growing crises of homelessness and food insecurity, Ethan asked a simple but profound question: “Why not use robots to help the unhoused?” [ UR2025 ]",
    "published": "Fri, 18 Jul 2025 15:30:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Reachy Mini Brings the Cute",
    "link": "https://spectrum.ieee.org/video-friday-reachy-mini",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. IFAC Symposium on Robotics : 15–18 July 2025, PARIS RoboCup 2025 : 15–21 July 2025, BAHIA, BRAZIL RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, THE NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Reachy Mini is an expressive, open-source robot designed for human-robot interaction, creative coding, and AI experimentation. Fully programmable in Python (and soon JavaScript, Scratch) and priced from $299, it’s your gateway into robotics AI: fun, customizable, and ready to be part of your next coding project. I’m so happy that Pollen and Reachy found a home with Hugging Face, but I hope they understand that they are never, ever allowed to change that robot’s face. O-o [ Reachy Mini ] via [ Hugging Face ] General-purpose robots promise a future where household assistance is ubiquitous and aging in place is supported by reliable, intelligent help. These robots will unlock human potential by enabling people to shape and interact with the physical world in transformative new ways. At the core of this transformation are Large Behavior Models (LBMs)—embodied AI systems that take in robot sensor data and output actions. LBMs are pretrained on large, diverse manipulation datasets and offer the key to realizing robust, general-purpose robotic intelligence. Yet despite their growing popularity, we still know surprisingly little about what today’s LBMs actually offer—and at what cost. This uncertainty stems from the difficulty of conducting rigorous, large-scale evaluations in real-world robotics. As a result, progress in algorithm and dataset design is often guided by intuition rather than evidence, hampering progress. Our work aims to change that. [ Toyota Research Institute ] Kinisi Robotics is advancing the frontier of physical intelligence by developing AI-driven robotic platforms capable of high-speed, autonomous pick-and-place operations in unstructured environments. This video showcases Kinisi’s latest wheeled-base humanoid performing dexterous bin stacking and item sorting using closed-loop perception and motion planning. The system combines high-bandwidth actuation, multi-arm coordination, and real-time vision to achieve robust manipulation without reliance on fixed infrastructure. By integrating custom hardware with onboard intelligence, Kinisi enables scalable deployment of general-purpose robots in dynamic warehouse settings, pushing toward broader commercial readiness for embodied AI systems. [ Kinisi Robotics ] Thanks, Bren! In this work, we develop a data collection system where human and robot data are collected and unified in a shared space and propose a modularized cross-embodiment Transformer that is pretrained on human data and fine-tuned on robot data. This enables high data efficiency and effective transfer from human to quadrupedal embodiments, facilitating versatile manipulation skills for unimanual and bimanual, non-prehensile and prehensile, precise tool-use, and long-horizon tasks, such as cat litter scooping! [ Human2LocoMan ] Thanks, Yaru! LEIYN is a quadruped robot equipped with an active waist joint. It achieves the world’s fastest chimney climbing through dynamic motions learned via reinforcement learning. [ JSK Lab ] Thanks, Keita! Quadrupedal robots are really just bipedal robots that haven’t learned to walk on two legs yet. [ Adaptive Robotic Controls Lab, University of Hong Kong ] This study introduces a biomimetic self-healing module for tendon-driven legged robots that uses robot motion to activate liquid metal sloshing, which removes surface oxides and significantly enhances healing strength. Validated on a life-sized monopod robot, the module enables repeated squatting after impact damage, marking the first demonstration of active self-healing in high-load robotic applications. [ University of Tokyo ] Thanks, Kento! That whole putting wheels on quadruped robots thing was a great idea that someone had way back when. [ Pudu Robotics ] I know nothing about this video except that it’s very satisfying and comes from a YouTube account that hasn’t posted in six years. [ Young-jae Bae YouTube ] Our AI WORKER now comes in a new Swerve Drive configuration, optimized for logistics environments. With its agile and omnidirectional movement, the swerve-type mobile base can efficiently perform various logistics tasks such as item transport, shelf navigation, and precise positioning in narrow aisles. Wait, you can have a bimanual humanoid without legs? I am shocked. [ ROBOTIS ] I can’t tell whether I need an office assistant, or if I just need snacks. [ PNDbotics ] “MagicBot Z1: Atomic kinetic energy, the brave are fearless,” says the MagicBot website. Hard to argue with that! [ MagicLab ] We’re excited to announce our new HQ in Palo Alto [Calif.]. As we grow, consolidating our Sunnyvale [Calif.] and Moss [Norway] team under one roof will accelerate our speed to ramping production and getting NEO into homes near you. I’m not entirely sure that moving from Norway to California is an upgrade, honestly. [ 1X ] Jim Kernan, chief product officer at Engineered Arts, shares how they’re commercializing humanoid robots—blending AI, expressive design, and real-world applications to build trust and engagement. [ Humanoids Summit ] In the second installment of our Moonshot Podcast Deep Dive video interview series, X’s Captain of Moonshots Astro Teller sits down with André Prager, former chief engineer at Wing, for a conversation about the early days of Wing and how the team solved some of their toughest engineering challenges to develop simple, lightweight, inexpensive delivery drones that are now being used every day across three continents. [ Moonshot Podcast ]",
    "published": "Fri, 11 Jul 2025 16:00:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: Cyborg Beetles May Speed Disaster Response One Day",
    "link": "https://spectrum.ieee.org/video-friday-cyborg-beetles",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. IEEE World Haptics : 8–11 July 2025, SUWON, SOUTH KOREA IFAC Symposium on Robotics : 15–18 July 2025, PARIS RoboCup 2025 : 15–21 July 2025, BAHIA, BRAZIL RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, THE NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA ACTUATE 2025 : 23–24 September 2025, SAN FRANCISCO CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Common beetles equipped with microchip backpacks could one day be used to help search-and-rescue crews locate survivors within hours instead of days following disasters such as building and mine collapses. The University of Queensland’s Dr. Thang Vo-Doan and Research Assistant Lachlan Fitzgerald have demonstrated they can remotely guide darkling beetles ( Zophobas morio ) fitted with the packs via video-game controllers. [ Paper ] via [ University of Queensland ] Thanks, Thang! This is our latest work about six-doF hand-based teleoperation for omnidirectional aerial robots, which shows an intuitive teleoperation system for advanced aerial robot. This work has been presented in 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025). [ DRAGON Lab ] Thanks, Moju! Pretty sure we’ve seen this LimX humanoid before, and we’re seeing it again right now, but hey, the first reveal is just ahead! [ LimX Dynamics ] Thanks, Jinyan! Soft robot arms use soft materials and structures to mimic the passive compliance of biological arms that bend and extend. Here, we show how relying on patterning structures instead of inherent material properties allows soft robotic arms to remain compliant while continuously transmitting torque to their environment. We demonstrate a soft robotic arm made from a pair of mechanical metamaterials that act as compliant constant-velocity joints. [ Paper ] via [ Transformative Robotics Lab ] Selling a platform is really hard, but I hope K-Scale can succeed with its open source humanoid. [ K-Scale ] MIT CSAIL researchers combined GenAI and a physics simulation engine to refine robot designs . The result: a machine that outjumped a robot designed by humans. [ MIT News ] ARMstrong Dex is a human-scale dual-arm hydraulic robot under development at the Korea Atomic Energy Research Institute (KAERI) for disaster-response applications. Designed with dimensions similar to those of an adult human, it combines human-equivalent reach and dexterity with force output that exceeds human physical capabilities, enabling it to perform extreme heavy-duty tasks in hazardous environments. [ Korea Atomic Energy Research Institute ] This is a demonstration of in-hand object rotation with Torobo Hand. Torobo Hand is modeled in simulation, and a control policy is trained within several hours using large-scale parallel reinforcement learning in Isaac Sim. The trained policy can be executed without any additional training in both a different simulator (MuJoCo) and on the real Torobo Hand. [ Tokyo Robotics ] Since 2005, Ekso Bionics has been developing and manufacturing exoskeleton bionic devices that can be strapped on as wearable robots to enhance the strength, mobility, and endurance of soldiers, patients, and workers. These robots have a variety of applications in the medical, military, industrial, and consumer markets, helping rehabilitation patients walk again and workers preserve their strength. [ Ekso Bionics ] Sponsored by Raytheon, an RTX business, the 2025 east coast Autonomous Vehicle Competition was held at XElevate in Northern Virginia. Student Engineering Teams from five universities participated in a two-semester project to design, develop, integrate, and compete two autonomous vehicles that could identify, communicate, and deliver a medical kit with the best accuracy and time. [ RTX ] This panel is from the Humanoids Summit in London: “Investing in the Humanoids Robotics Ecosystem—a VC Perspective.” [ Humanoids Summit ]",
    "published": "Sat, 05 Jul 2025 15:04:05 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Robotic Arm “Feels” Using Sound",
    "link": "https://spectrum.ieee.org/farm-robots-sound-based-sensing",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore . Agricultural robots could help farmers harvest food under tough environmental conditions, especially as temperatures continue to rise. However, creating affordable robotic arms that can gracefully and accurately navigate the thick network of branches and trunks of plants can be challenging. In a recent study, researchers developed a sensing system , called SonicBoom, which allows autonomous robots to use sound to sense the objects it touches. The approach, which can accurately localize or “feel” the objects it encounters with centimeter-level precision, is described in a study published 2 June in IEEE Robotics and Automation Letters . Moonyoung (Mark) Lee is a fifth-year Ph.D. student at Carnegie Mellon University’s Robotics Institute who was involved in developing SonicBoom. He notes that many autonomous robots currently rely on a collection of tiny camera-based tactile sensors . Minicameras beneath a protective gel pack that lines the robot’s surface let the sensors visually estimate the gel’s deformation to gain tactile information. However, this approach isn’t ideal in agricultural settings, when branches are likely to occlude the visual sensors. What’s more, camera-based sensors can be expensive and could be easily damaged in this context. Another option is pressure sensors, Lee notes, but these would need to cover much of the surface area of the robot in order to effectively sense when it comes into contact with branches. “Imagine covering the entire robot arm surface with that kind of [sensor]. It would be expensive,” he says. Instead, Lee and his colleagues are proposing a completely different approach that relies on sound for sensing. The system involves an array of contact microphones, which detect physical touch as sound signals that propagate through solid materials. How Does SonicBoom Work? When a robotic arm touches a branch, the resulting sound waves travel down the robotic arm until they encounter the array of contact microphones. Tiny differences in sound-wave properties (such as signal intensity and phase) across the array of microphones are used to localize where the sound originated, and thus the point of contact. In this video, see SonicBoom in action during laboratory testing. youtu.be Lee notes that this approach allows microphones to be embedded deeper in the robotic arm. This means they are less prone to damage compared to traditional visual sensors on the exterior of a robotic arm. “The contact microphones can be easily protected from very harsh, abrasive contacts,” he explains. As well, the approach uses a small handful of microphones dispersed across the robotic arm, rather than many visual or pressure sensors more densely coating it. To help SonicBoom better localize points of contact, the researchers used an AI model, trained on data collected by tapping the robotic arm more than 18,000 times with a wooden rod. As a result, SonicBoom was able to localize contact on the robotic arm with an error rate of just 0.43 centimeters for objects it was trained to detect. It was also able to detect novel objects, for example ones made of plastic or aluminum, with an error rate of 2.22 cm. In a subsequent study pending publication, Lee and his colleagues are using new data to train SonicBoom to identify what kind of object its encountering — for example, a leaf, branch, or trunk. “With SonicBoom, you can blindly tap around and know where the [contact happens], but at the end of the day, for the robot, the really important information is: Can I keep pushing, or am I hitting a strong trunk and should rethink how to move my arm?” he explains. Of note, SonicBoom has yet to be tested in real-world agricultural settings, Lee says.",
    "published": "Sat, 28 Jun 2025 13:00:02 +0000",
    "author": "Michelle Hampson",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  },
  {
    "title": "Video Friday: This Quadruped Throws With Its Whole Body",
    "link": "https://spectrum.ieee.org/robot-arm-thrower",
    "summary": "Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. IAS 2025 : 30 June–4 July 2025, GENOA, ITALY ICRES 2025 : 3–4 July 2025, PORTO, PORTUGAL IEEE World Haptics : 8–11 July 2025, SUWON, SOUTH KOREA IFAC Symposium on Robotics : 15–18 July 2025, PARIS RoboCup 2025 : 15–21 July 2025, BAHIA, BRAZIL RO-MAN 2025 : 25–29 August 2025, EINDHOVEN, THE NETHERLANDS CLAWAR 2025 : 5–7 September 2025, SHENZHEN, CHINA CoRL 2025 : 27–30 September 2025, SEOUL IEEE Humanoids : 30 September–2 October 2025, SEOUL World Robot Summit : 10–12 October 2025, OSAKA, JAPAN IROS 2025 : 19–25 October 2025, HANGZHOU, CHINA Enjoy today’s videos! Throwing is a fundamental skill that enables robots to manipulate objects in ways that extend beyond the reach of their arms. We present a control framework that combines learning and model-based control for prehensile whole-body throwing with legged mobile manipulators. This work provides an early demonstration of prehensile throwing with quantified accuracy on hardware, contributing to progress in dynamic whole-body manipulation. [ Paper ] from [ ETH Zurich ] As it turns out, in many situations humanoid robots don’t necessarily need legs at all. [ ROBOTERA ] Picking-in-Motion is a brand new feature of Autopicker 2.0. Instead of remaining stationary while picking an item, Autopicker begins traveling toward its next destination immediately after retrieving a storage tote, completing the pick while on the move. The robot then drops off the first storage tote at an empty slot near the next pick location before collecting the next tote. [ Brightpick ] Thanks, Gilmarie! I am pretty sure this is not yet real, but boy is it shiny. [ SoftBank ] via [ RobotStart ] Why use one thumb when you can use two instead? [ TU Berlin ] Kirigami offers unique opportunities for guided morphing by leveraging the geometry of the cuts. This work presents inflatable kirigami crawlers created by introducing cut patterns into heat-sealable textiles to achieve locomotion upon cyclic pneumatic actuation. We found that the kirigami actuators exhibit directional anisotropic friction properties when inflated, having higher friction coefficients against the direction of the movement, enabling them to move across surfaces with varying roughness. We further enhanced the functionality of inflatable kirigami actuators by introducing multiple channels and segments to create functional soft robotic prototypes with versatile locomotion capabilities. [ Paper ] from [ SDU Soft Robotics ] Lockheed Martin wants to get into the Mars Sample Return game for a mere US $3 billion. [ Lockheed Martin ] This is pretty gross and exactly what you want a robot to be doing: dealing with municipal solid waste. [ ZenRobotics ] Drag your mouse or move your phone to explore this 360-degree panorama provided by NASA’s Curiosity Mars rover. This view shows some of the rover’s first looks at a region that has only been viewed from space until now, and where the surface is crisscrossed with spiderweb-like patterns. [ NASA Jet Propulsion Laboratory ] In case you were wondering, iRobot is still around. [ iRobot ] Legendary roboticist Cynthia Breazeal talks about the equally legendary Personal Robots Group at the MIT Media Lab. [ MIT Personal Robots Group ] In the first installment of our Moonshot Podcast Deep Dive video interview series, X’s Captain of Moonshots Astro Teller sits down with Sebastian Thrun , cofounder of the Moonshot Factory, for a conversation about the history of Waymo and Google X, the ethics of innovation, the future of AI, and more. [ Google X, The Moonshot Factory ]",
    "published": "Fri, 27 Jun 2025 16:30:03 +0000",
    "author": "Evan Ackerman",
    "topic": "robotics",
    "collected_at": "2025-10-08T14:03:13"
  }
]