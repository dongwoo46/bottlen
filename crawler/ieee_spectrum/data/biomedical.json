[
  {
    "title": "Quantum Sensors Sidestep the Heisenberg Uncertainty Principle",
    "link": "https://spectrum.ieee.org/sidestep-heisenberg-uncertainty",
    "summary": "A cornerstone of quantum physics is uncertainty. Heisenberg’s uncertainty principle states that the more precisely you pinpoint the position of a particle, the less precisely you can know its momentum at the same time, and vice versa. However, a new study reveals that scientists have now discovered a way to sidestep this quantum trade-off. This could lead to next-generation quantum sensors that can simultaneously measure both the position and momentum of particles with unprecedented precision. “We take it for granted that the Heisenberg uncertainty principle is a fundamental law that cannot be broken,” says Tingrei Tan , a research fellow at the University of Sydney Nano Institute and School of Physics. “I want to be clear that we have not broken the Heisenberg uncertainty principle. But in certain cases, we can get around it.” Other kinds of uncertainty are known in quantum physics as well. For example, if an atom is excited to a higher orbital, one cannot precisely know both the energy and the duration of its excited state at the same time. Previously, scientists have taken advantages of these kinds of trade-offs to “ squeeze ” or reduce the uncertainty in the measurements of a given variable while increasing the uncertainty in the measurement of another variable the researchers can ignore. For example, researchers have shown they can make qubits—the key components in quantum computers—highly resistant to a common source of error known as bit flip , when a qubit’s state flips from 1 to 0 or the reverse, while making them more vulnerable to a different common source of error known as phase flip, when a qubit switches between one of two opposite phases. They make this trade-off because having just one common source of error to correct instead of two can drastically simplify quantum-computer design. Now Tan and his colleagues have found they can make a similar swap to sidestep Heisenberg’s uncertainty principle. “We are increasing the precision with which we can measure momentum and position at the same time within a small sensing range, while increasing the uncertainty with which we can measure those properties simultaneously outside of that sensing range,” Tan says. Since the kind of quantum sensing applications the researchers want to carry out with this new technique are roughly on the atomic scale anyhow, gaining precision on a tiny scale while losing it on larger ones is a worthwhile trade-off. It’s a bit like using a magnifying glass—you care about what you want to focus on under the lens, not around it. Inspiration from quantum computing In the new study, the researchers experimented with a single ytterbium ion held in place and controlled with electric and magnetic fields. They generated sets of specific patterns of vibrations in the ion known as grid states or Gottesman-Kitaev-Preskill states. Scientists have long investigated GKP states for quantum error-correction strategies in quantum computing . In the case of trapped ions, because of the way in which the patterns of vibrations are entangled together, any small disturbance would generate changes in these patterns, which quantum error-correction techniques can detect and account for in quantum computations. In the new study, by preparing grid states in the vibrations of a ytterbium ion, the researchers found they could simultaneously measure both its position and momentum with a precision beyond the standard quantum limit—the best achievable using only classical sensors. Tingrei Tan in the Sydney Nanoscience Hub at the University of Sydney, where he manages the Quantum Control Laboratory. Fiona Wolf/The University of Sydney “We are borrowing techniques from quantum error correction to do quantum sensing,” Tan says. Tan says a key application for this research is spectroscopy — the analysis of atoms or molecules based on the specific wavelengths of light they absorb or reflect, which finds wide use in medical research, navigation in environments where GPS does not work , and searches for dark matter . In general, however, “this opens up a whole new way to do precision measurements,” Tan says. “It was taken for granted that there were pairs of variables that you could not measure precisely at the same time, and our work provides a framework for how we can get around those limitations.” The scientists detailed their findings on 24 September in the journal Science Advances .",
    "published": "Mon, 06 Oct 2025 14:00:05 +0000",
    "author": "Charles Q. Choi",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "How Wi-Fi Signals Can Be Used to Detect Your Heartbeat",
    "link": "https://spectrum.ieee.org/wi-fi-signal-heartbeat-detection",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore . Wi-Fi signals today primarily transmit data. But these signals can also be used for other innovative purposes. For instance, one California-based team has proposed using ambient Wi-Fi signals to monitor a person’s heart rate. The new approach, called Pulse-Fi, offers advantages over existing heart-rate-monitoring methods. It’s low-cost and easily deployable, and it sidesteps the need for people to strap a device to their body. Katia Obraczka is a professor at the University of California, Santa Cruz , who led the development of Pulse-Fi. She notes that continuous tracking of vital signs, including heat rate, can help flag health concerns such as stress, dehydration, cardiac disease, and other illnesses. “But using wearables to monitor vitals can be uncomfortable, have weak adherence, and have limited accessibility due to cost,” she says. Camera-based methods are one option for remote, contactless tracking of a person’s heart rate without a wearable device. However, these approaches may be compromised in poor lighting conditions and may also raise privacy concerns. In the search for a better option, Obraczka, along with postdoc student Nayan Sanjay Bhatia and high-school intern Pranay Kocheta working in her lab, sought to create Pulse-Fi. “Pulse-Fi uses ordinary Wi-Fi signals to monitor your heartbeat without touching you. It captures tiny changes in the Wi-Fi signal waves caused by heartbeats,” says Obraczka. How Can Wi-Fi Signals Measure Someone’s Pulse? Specifically, the team designed Pulse-Fi to filter out background noise and detect the changes in signal amplitude brought about by heartbeats. They developed an AI model —capable of running on a simple computing device, such as a Raspberry Pi —which then reads the filtered signals and estimates heart rate in real time. The team tested their approach in two different experiments, which are described in a study published in August at the 2025 International Conference on Distributed Computing in Smart Systems and the Internet of Things . First, the researchers had seven volunteers sit in a chair at various distances of 1, 2, and 3 meters from two ESP32 microcontrollers that used Pulse-Fi to estimate the volunteers’ heart rates, comparing these data to heart-rate measurements taken by a pulse oximeter. In the second experiment, Pulse-Fi was used on Raspberry Pi devices to monitor the heart rates of more than 100 participants in different positions, including walking, running in place, sitting down, and standing up. The results show that the system performs on par with other reference sensors, and Pulse-Fi’s less than 1.5-beats-per-minute error rate compares favorably to other vital-sign-monitoring technologies. Pulse-Fi also maintained sufficient accuracy despite the person’s posture (for example, sitting or walking) or distance from the recording device (up to 10 feet away). Based on these results, Obraczka says the team plans to establish a company to commercialize the technology. Prof. Katia Obraczka and her Ph.D. student Nayan Sanjay Bhatia—both of the University of California, Santa Cruz—discuss research on their Pulse-Fi technology, which remotely measures people’s pulse rates using Wi-Fi signals. Erika Cardema/UC Santa Cruz Obraczka adds that Pulse-Fi can work in new environments that its underlying AI model hadn’t trained for. “The model generalized well in [a new] setting, showing it’s not just memorizing, but actually learning patterns that transfer to new situations,” she says. Obraczka also notes that the devices that Pulse-Fi runs on are affordable–with ESP32 chips costing about US $5 to $10, with Raspberry Pis costing about $30. As yet, the researchers have tested Pulse-Fi on only a single user in the room at a time. The team is now beginning to pilot the approach on multiple users simultaneously. “In addition to working on multiuser environments, we are also exploring other wellness and health care applications for Pulse-Fi,” Obraczka says, citing sleep apnea and breathing rates as examples.",
    "published": "Sun, 05 Oct 2025 13:00:04 +0000",
    "author": "Michelle Hampson",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "The Quest to Sequence the Genomes of Everything",
    "link": "https://spectrum.ieee.org/whole-genome-sequencing",
    "summary": "A gibbous moon hangs over a lonely mountain trail in the Italian Alps, above the village of Malles Venosta, whose lights dot the valley below. Benjamin Wiesmair stands next to a moth trap as tall as he is, his face, bushy beard, and hair bun lit by its purple glow. He’s wearing a headlamp, a dusty and battered smartwatch, cargo shorts, and a blue zip sweater with the sleeves pulled up. Countless moths beat frenetically around the trap’s white, diaphanous panels, which are swaying with ghostly ripples in a gentle breeze. Wiesmair squints at his smartphone, which is logged on to a database of European moth species. “ Chersotis multangula ,” he says. “Yes, we need that,” comes the crisp reply from Clara Spilker , consulting a laptop. This article is part of The Scale Issue . Wiesmair, an entomologist at the Tyrolean State Museums , in Innsbruck, Austria, and Spilker, a technical assistant at the Senckenberg German Entomological Institute , in Müncheberg, are taking part in one of the most far-reaching biological initiatives ever: obtaining a genome sequence for nearly every named species of eukaryotic organism on the planet. All 1.8 million of them. The researchers are part of an expedition for Project Psyche , which is sampling European butterflies and moths and will feed its data into the global initiative, called the Earth BioGenome Project (EBP). Eukaryotes are organisms whose cells contain a nucleus. From protozoa to human beings, all have the same basic biological mechanism for building, maintaining, and propagating their form of life: a genome. It’s the sum total of the genes carried by the creature. Twenty-two years ago, researchers announced that for the first time they had mapped, or “sequenced,” nearly all of the genes in a human genome . The project cost more than US $3 billion and took 13 years, but it eventually transformed medical practice. In the new era of genomic medicine , doctors can take a patient’s specific genetic makeup into consideration during diagnosis and treatment. The EBP aims to reach its monumental goal by 2035. As of July 2024, its tally of genomes sequenced stood at about 4,200 . Success will undoubtedly depend on researchers’ ability to scale several biotech technologies. “We need to scale, from where we’re at, more than a hundredfold in terms of the number of genomes per year that we’re producing worldwide,” says Harris Lewin , who leads the EBP and is a professor and genetics researcher at Arizona State University . One of the most crucial technologies that must be scaled is a technique called long-read genome sequencing. Specialists on the front lines of the genomic revolution in biology are confident that such scaling will be possible, their conviction coming in part from past experience. “Compared to 2001,” when the Human Genome Project was nearing completion, “it is now approximately 500,000 times cheaper to sequence DNA,” says Steven Salzberg , a Bloomberg Distinguished Professor at Johns Hopkins University and director of the school’s Center for Computational Biology . “And it is also about 500,000 times faster to sequence,” he adds. “That is the scale, over the past 25 years, a scale of acceleration that has vastly outstripped any improvements in computational technology, either in memory or speed of processors.” There are many reasons to cheer on the EBP and the technological advances that will underpin it. Having established a genome for every eukaryotic creature, researchers will gain deep new insights into the connections among the threads in Earth’s web of life, and into how evolution proceeded for its myriad life forms. That knowledge will become increasingly important as climate change alters the ecosystems on which all of those creatures, including us, depend. And although the project is a scientific collaboration, it could spin off sizable financial windfalls. Many drugs, enzymes, catalysts, and other chemicals of incalculable value were first identified in natural samples . Researchers expect many more to be discovered in the process of identifying, in effect, each of the billions of eukaryotic genes on Earth, many of which encode a protein of some kind. “One idea is that by looking at plants, which have all sorts of chemicals, often which they make in order to fight off insects or pests, we might find new molecules that are going to be important drugs,” says Richard Durbin , professor of genetics at the University of Cambridge and a veteran of several genome sequencing initiatives. The immunosuppressant and cancer drug rapamycin , to cite just one of countless examples, came from a microbe genome. Your Genes Are a Big Reason Why You’re You The EBP is an umbrella organization for some 60 projects (and counting) that are sequencing species in either a region or in a particular taxonomic group. The overachiever is the Darwin Tree of Life Project , which is sequencing all species in Britain and Ireland, and has contributed about half of all of the genomes recorded by the EBP so far. Project Psyche was spun out of the Darwin Tree of Life initiative, and both have received generous support from the Wellcome Trust . To get an idea of the magnitude of the overall EBP, consider what it takes to sequence a species. First, an organism must be found or captured and sampled, of course. That’s what brought Wiesmair, Spilker, and 41 other lepidopterists to the Italian Alps for the Project Psyche expedition this past July. Over five days, they collected more than 200 new species for sequencing, which will augment the 1,000 finished Lepidoptera genome sequences already completed and the roughly 2,000 samples awaiting sequencing. There’s still plenty of work to be done; there are around 11,000 species of moths and butterflies across Europe and Britain. After sampling, genetic material—the creature’s DNA—is collected from cells and then broken up into fragments that are short enough to be read by the sequencing machines. After sequencing, the genome data is analyzed to determine where the genes are and, if possible, what they do. Over the past 25 years, the acceleration of gene-sequencing tech has vastly outstripped any improvements in computational technology, either in memory or speed of processors. DNA is a molecule whose structure is the famous double helix . It resides in the nucleus of every cell in the body of every living thing. If you think of the molecule as a twisted ladder, the rungs of the ladder are formed by pairs of chemical units called bases. There are four different bases: adenine (A), guanine (G), cytosine (C), and thymine (T). Adenine always pairs with thymine, and guanine always pairs with cytosine. So a “rung” can be any of four things: A–T, T–A, C–G, or G–C. Those four base-pair permutations are the symbols that comprise the code of life. Strings of them make up the genome as segments of various lengths called genes . Your genes at least partially control most of your physical and many of your mental traits—not only what color your eyes are and how tall you are but also what diseases you are susceptible to, how difficult it is for you to build muscle or lose weight, and even whether you’re prone to motion sickness. How Long-Read Genome Sequencing Works Long-read sequencing starts by breaking up a sample of genetic material into pieces that are often about 20,000 base pairs long. Then the sequencing technology reads the sequence of base pairs on those DNA strands to produce random segments, called “reads,” of DNA that are at least 10,000 pairs in length. Once those long reads are obtained, powerful bioinformatics software is used to build longer stretches of contiguous sequence by overlapping reads that share the same sequence of bases. To understand the process, think of a genome as a novel, and each of its separate chromosomes as a chapter in the novel. Imagine shredding the novel into pieces of paper, each about 5 square centimeters. Your job is to reassemble them into the original novel (unfortunately for you, the pages aren’t numbered). What makes this task possible is overlap—you shredded multiple copies of the novel, and the pieces overlap, making it easier to see where one leaves off and another begins. Making it much harder, however, are the many sections of the book filled with repetitive nonsense: the same word repeated hundreds or even thousands of times. At least half of a typical mammalian genome consists of these repetitive sequences, some of which have regulatory functions and others regarded as “junk” DNA that’s descended from ancient genes or viral infections and no longer functional. Long-read technology is adept at handling these repetitive sequences. Going back to the novel-shredding analogy, imagine trying to reassemble the book after it was shredded into pieces only 1 centimeter square rather than 5. That’s analogous to the challenge that researchers formerly faced trying to assemble million-base-pair DNA sequences using older, “short-read” sequencing technology . The Two Approaches to Long-Read Sequencing The long-read sequencing market has two leading companies— Oxford Nanopore Technologies (ONT) and Pacific Biosciences of California (PacBio)—which compete intensely. The two companies have developed utterly different systems. The heart of ONT’s system is a flow cell that contains 2,000 or more extremely tiny apertures called, appropriately enough, nanopores. The nanopores are anchored in an electrically resistant membrane, which is integrated onto a sensor chip. In operation, each end of a segment of DNA is attached to a molecule called an adapter that contains a helicase enzyme . A voltage is applied across the nanopore to create an electric field, and the field captures the DNA with the attached adapter. The helicase begins to unzip the double-stranded DNA, with one of the DNA strands passing through the nanopore, base by base, and the other released into the medium. OPTICAL SEQUENCING (Pacific Biosciences) A polymerase enzyme replicates the DNA strand, matching and connecting each base to a specially engineered, complementary nucleotide. That nucleotide flashes light in a characteristic color that identifies which base is being connected. Each DNA strand is immobilized at the bottom of a well. As the DNA strand is replicated, each base while being incorporated emits a tiny flash of light in a color that is characteristic of the base. The sequence of light flashes indicates the sequence of bases. What propels the strand through the nanopore is that voltage—it’s only about 0.2 volts, but the nanopore is only 5 nanometers wide, so the electric field is several hundred thousand volts per meter. “It’s like a flash of lightning going through the pore,” says David Deamer , one of the inventors of the technology. “At first, we were afraid we would fry the DNA, but it turned out that the surrounding water absorbed the heat.” That kind of field strength would ordinarily propel the DNA-based molecule through the pore at speeds far too fast for analysis. But the helicase acts like a brake, causing the molecule to go through with a ratcheting motion, one base at a time, at a still-lively rate of about 400 bases per second. Meanwhile, the electric field also propels a flow of ions across the nanopore. This current flow is decreased by the presence of a base in the nanopore—and, crucially, the amount of the decrease depends on which of the four bases, A, T, G, or C, is entering the pore. The result is an electrical signal that can be rapidly translated into a sequence of bases. NANOPORE SEQUENCING (Oxford Nanopore) The helicase enzyme unzips and unravels the double-stranded DNA, and one strand enters the nanopore. The enzyme feeds the strand through the nanopore with a ratcheting motion, base by base. The ionic current is reduced by a characteristic amount, depending on the base. The current signal indicates the sequence of bases. PacBio’s machines rely on an optical rather than an electronic means of identifying the bases. PacBio’s latest process , which it calls HiFi, begins by capping both ends of the DNA segment and untwisting it to create a single-stranded loop. Each loop is then placed in an infinitesimally tiny well in a microchip, which can have 25 million of those wells. Attached to each loop is a polymerase enzyme, which serves a critical function every time a cell divides. It attaches to single-stranded DNA and adds the complementary bases, making each rung of the ladder whole again. PacBio uses special versions of the four bases that have been engineered to fluoresce in a characteristic color when exposed to ultraviolet light. A UV laser shines through the bottom of the tiny well, and a photosensor at the top detects the faint flashes of light as the polymerase goes around the DNA sample loop, base by base. The upshot is that there is a sequence of light flashes, at a rate of about three per second, that reveals the sequence of base pairs in the DNA sample. Because the DNA sample has been converted into a loop, the whole process can be repeated, to achieve higher accuracy, by simply going around the loop another time. PacBio’s flagship Revio machine typically makes five to 10 passes, achieving median accuracy rates as high as 99.9 percent, according to Aaron Wenger , senior director of product marketing at the company. How Researchers Will Scale Up Long-Read Sequencing That kind of accuracy doesn’t come cheap. A Revio system , which has four chips, each with 25 million wells, costs around $600,000, according to Wenger. It weighs 465 kilograms and is about the size of a large household refrigerator. PacBio says a single Revio can sequence about four entire human genomes in a 24-hour period for less than $1,000 per genome. ONT claims accuracy above 99 percent for its flagship machine, called PromethION 24 . It costs around $300,000, according to Rosemary Sinclair Dokos , chief product and marketing officer at ONT. Another advantage of the ONT PromethION system is its ability to process fragments of DNA with as many as a million base pairs. ONT also offers an entry-level system, called MinION Mk1D , for just $3,000. It’s about the size of two smartphones stacked on top of each other, and it plugs into a laptop, offering researchers a setup that can easily be toted into the field. Although researchers often have strong preferences, it’s not uncommon for a state-of-the-art genetics laboratory to be equipped with machines from both companies. At Barcelona’s Centro Nacional de Análisis Genómico, for example, researchers have access to both PacBio Revio machines as well as PromethION 24 and GridION machines from ONT. Durbin, at Cambridge University, sees lots of upside in the current situation. “It’s very good to have two companies,” he declares. “They’re in competition with each other for the market.” And that competition will undoubtedly fuel the tech advances that the EBP’s backers are counting on to get the project across the finish line. PacBio’s Wenger notes that the 25-million-well chips that underpin its Revio system are still being fabricated on 200-millimeter semiconductor wafers. A move to 300-mm wafers and more advanced lithographic techniques, he says, would enable them to get many more chips per wafer and put hundreds of millions of wells on each of those chips—if the market demands it. At ONT, Dokos describes similar math. A single flow cell now consists of more than 2,000 nanopores, and a state-of-the-art PromethION 24 system can have 24 flow cells (or upward of 48,000 nanopores) running in parallel. But a future system could have hundreds of thousands of nanopores, she says—again, if the market demands it. The EBP will need all of those advances, and more. EBP director Lewin notes that after seven years, the three-phase initiative is wrapping up phase one and preparing for phase two. The goal for phase two is to sequence 150,000 genomes between 2026 and 2030. For phase two, “We’ve got to get to 37,500 genomes per year,” Lewin says. “Right now, we’re getting close to 3,000 per year.” In phase two, the cost per genome sequenced will also have to decline from roughly $26,000 per genome in phase one to $6,100, according to the EBP’s official road map . That $6,100 figure includes all costs—not just sequencing but also sampling and the other stages needed to produce a finished genome, with all of the genes identified and assigned to chromosomes. Phase three will up the ante even higher. The road map calls for more than 1.65 million genome sequences between 2030 and 2035 at a cost of $1,900 per genome. If they can pull it off, the entire project will have cost roughly $4.7 billion—considerably less in real terms than what it cost to do just the human genome 22 years ago. All of the data collected—the genome sequences for all named species on Earth—will occupy a little over 1 exabyte (1 billion gigabytes) of digital storage. It will arguably be the most valuable exabyte in all of science. “With this genomic data, we can get to one of the questions that Darwin asked a long time ago, which is, How does a species arise? What is the origin of species? That’s his famous book where he never actually answered the question,” says Mark Blaxter , who leads the Darwin Tree of Life Project at the Wellcome Sanger Institute near Cambridge and who also conceived and started Project Psyche. “We’ll get a much, much better idea about what it is that makes a species and how species are distinct from each other.” A portion of that knowledge will come from the many moths collected on those summer nights in the Italian Alps. Lepidoptera “go back around 300 million years,” says Charlotte Wright , a co-leader, along with Blaxter, of Project Psyche. Analyzing the genomes of huge numbers of species will help explain why some branches of the Lepidoptera order have evolved far more species than others, she says. And that kind of knowledge should eventually accumulate into answers to some of biology’s most profound questions about evolution and the mechanisms by which it acts. “The amazing thing is that by doing this for all of the lepidoptera of Europe, we aren’t just learning about individual cases,” says Wright. “We’ve learned across all of it.”",
    "published": "Thu, 02 Oct 2025 13:00:04 +0000",
    "author": "Glenn Zorpette",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Array-Scale MUT Simulations Powered by the Cloud",
    "link": "https://events.bizzabo.com/764786",
    "summary": "Designing and optimizing ultrasound transducers—whether PMUTs or CMUTs—requires accuracy at scale. Yet traditional simulation approaches are often constrained to individual cells or limited structures, leaving important array-level effects poorly understood until expensive and time-consuming testing begins. This gap can lead to longer development cycles and higher risk of failed devices. In this webinar, we will introduce the improved approach: full array-scale MUT simulations with fully coupled multiphysics. By leveraging Quanscient’s cloud-native platform, engineers can model entire transducer arrays with all relevant physical interactions (electrical, mechanical, acoustic, and more) capturing system-level behaviors such as beam patterns and cross-talk that single-cell simulations miss. Cloud scalability also enables extensive design exploration. Through parallelization, users can run Monte Carlo analyses, parameter sweeps, and large-scale models in a fraction of the time, enabling rapid optimization and higher throughput in the design process. This not only accelerates R&D but ensures more reliable designs before fabrication. The session will feature real-world case examples with detailed insights of the methodology and key metrics. Attendees will gain practical understanding of how array-scale simulation can greatly improve MUT design workflows reducing reliance on costly prototypes, minimizing risk, and delivering better device performance. Join us to learn how array-scale MUT simulations in the cloud can improve MUT design accuracy, efficiency, and reliability. Register now for this free webinar!",
    "published": "Mon, 29 Sep 2025 14:47:06 +0000",
    "author": "Quanscient",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Google’s AI Co-Scientist Scores Two Wins in Biology",
    "link": "https://spectrum.ieee.org/ai-co-scientist",
    "summary": "Hey Google! What if, instead of setting reminders or fetching restaurant reviews, you helped crack the mysteries of biology? That playful question hints at a radical vision now being tested in labs. AI systems are being recast not as digital secretaries but as scientific partners—copilots built to dream up bold, testable ideas. The pitch sounds revolutionary. But it also makes many scientists bristle. How much true novelty can a machine conjure? Isn’t it more likely to remix the past than to uncover something genuinely new? For months, the controversy over “AI scientists” has simmered : hype versus hope, parroting versus discovery. But two new studies offer some of the strongest evidence to date that large language models (LLMs) can generate truly novel scientific ideas, leaping to nonobvious insights that might otherwise require many years of painstaking lab work. Both studies showcase Google’s AI-powered scientific-research assistant, known as the AI co-scientist. “These early examples are unbelievable—it’s so compelling,” says Dillan Prasad , a neurosurgery researcher at Northwestern University and an outside observer who has written about the potential for AI co-scientists to supercharge hypothesis generation . “You have AI agents that are producing scientific discovery! It’s absolutely exciting.” AI Takes on Drug Repurposing In one of these proof-of-concept demonstrations, a team led by Gary Peltz , a liver-disease researcher at Stanford Medicine, tasked the AI assistant with finding drugs already on the market that could be repurposed to treat liver fibrosis, an organ-scarring condition with few effective therapies. He prompted the tool to look for medicines directed at epigenetic regulators—proteins that control how genes are switched on or off without altering the underlying DNA—and the AI, after mining the biomedical literature, came back with three reasonable suggestions. Peltz added two candidates of his own, and put all five drugs through a battery of tests on lab-grown liver tissue. Two of the AI’s picks—but none of Peltz’s—reduced fibrosis and even showed signs of promoting liver regeneration in the lab tests. Peltz, who published the findings 14 September in the journal Advanced Science , hopes the results will pave the way for a clinical trial of one standout candidate, the cancer drug vorinostat, in patients with liver fibrosis. Bacterial Mystery Solved In the second validation study, a team led by microbiologists José Penadés and Tiago Costa at Imperial College London challenged the AI co-scientist with a thorny question about bacterial evolution. The researchers had shown in 2023 that parasitic scraps of DNA could spread within bacterial populations by hitching rides on the tails of infecting viruses. But that mechanism seemed confined to one host species. How, then, did identical bits of DNA surface in entirely different types of bacteria? So they tasked the AI with solving the mystery. They fed the system their data, background papers, and a pointed question about what hidden mechanism might explain the jump. The AI, after “thinking” and processing for two days, proposed a handful of solutions—the leading one being that the DNA fragments could snatch viral tails not just from their own host cell but also from neighboring bacteria to complete their journey. It was uncannily correct. What the system could not know was that Penadés and Costa already had unpublished data hinting at exactly this mechanism. The AI had, in effect, leapt to the same conclusion that it had taken the researchers years of benchwork to devise, a convergence that astonished the Imperial team and lent credibility to the tool. “I was really shocked,” says Penadés, who at first thought the AI had hacked into his computer and accessed additional data to arrive at the correct result. Reassured that it hadn’t, he delved into the logic the AI co-scientist used for its various hypotheses and found surprising rigor. “Even for the ones that were not correct,” Penadés says, “the thinking was extremely good.” An AI Scientific Method That sound logic prompted the Imperial team to explore one of the AI’s runner-up ideas—one in which bacteria might directly pass the DNA fragments to one another. Working with microbial geneticists in France, the group is now probing that possibility further, with promising early results. “Our preliminary data seem to be pointing toward that hypothesis [also] being correct,” says Costa. He and Penadés published both the AI’s predictions and their experimental results in the journal Cell earlier this month. Notably, the Imperial researchers also tried various LLMs not specifically designed for scientific reasoning. These included systems from OpenAI, Anthropic, DeepSeek, and Google’s general-purpose Gemini 2.0 model. None of those jack-of-all-trades models came up with the hypotheses that proved experimentally correct. Vivek Natarajan from Google DeepMind, who helped develop the co-scientist platform, thinks he knows what explains that edge. He points to the system’s multiagent design, which assigns different AI roles to generate, critique, refine, and rank hypotheses in iterative loops, all overseen by a “supervisor” that manages goals and resources. Unlike a generic LLM, it grounds ideas in external tools and literature, strategically scales up compute for deeper reasoning, and vets hypotheses through automated tournaments. According to Natarajan, academic institutions around the world are now piloting the system , with plans to expand access—though the company’s “trusted tester program” is currently at capacity and not accepting new applications. “Clearly we see a lot of potential,” he says. “We imagine that, every time you’re going to try and solve a new problem, you’re going to use the co-scientist to come along on the journey with you.” Constellation of Co-Scientists Google is not alone in chasing this vision. In July, computer scientist Kyle Swanson and his colleagues at Stanford University described their Virtual Lab , an LLM-based system that strings together reasoning steps across biology datasets to propose new experiments. Rivals are moving fast, too: Biomni , another Stanford-led system, is helping to autonomously execute a wide range of research tasks in the life sciences, while the nonprofit FutureHouse is building a comparable platform. Each is vying to show that its approach can turn language models into real engines of discovery. Many onlookers have been impressed, noting that the studies offer some of the clearest evidence yet that LLMs can generate ideas worth testing at the bench. “This is going to make our jobs much easier,” says Rodrigo Ibarra Chávez , a microbiologist at the University of Copenhagen who studies the kind of bacterial genetic hitchhiking explored by the Imperial team. But critics warn that an overreliance on AI-generated hypotheses in science risks creating a closed loop that recycles old information instead of producing new discoveries. “We need tools that augment our creativity and critical thinking, not repackage existing information using alternative language,” Kriti Gaur of the life sciences analytics firm Elucidata wrote in a white paper that evaluated the Google platform. “Until this ‘AI co-scientist’ can demonstrate original, verifiable, and meaningful insights that stand up to scientific scrutiny, it remains a powerful assistant, but certainly not a co-scientist.” The blue section of the figure shows an experimental research pipeline that led to a discovery of DNA transfer among bacterial species. The orange section shows how AI rapidly reached the same conclusions. José R. Penadés, Juraj Gottweis, et al. Reasoning, Not Just Recall Supporters counter that the latest generation of models show glimmers of what scientists might reasonably call “ intelligence .” Systems like Google’s co-scientist not only recall and synthesize vast libraries but also reason through competing possibilities, discard weaker ideas, and refine stronger ones in ways that can feel strikingly human. “I find it very invigorating,” says Peltz. “It’s like having a conversation with someone who knows more than you.” Still, the magic doesn’t happen automatically. Extracting valuable hypotheses requires careful prompting, iterative feedback, and a willingness to engage in a kind of dialogue with the AI, notes Swanson. It’s less like pressing a button for an answer and more like mentoring a junior colleague—asking the right questions, pushing back on shallow reasoning, and nudging the system toward sharper insights. “For now, you still need to be a bit of an expert to get the most use out of these systems,” Swanson says. “But if you ask a well-designed question, you can get really good answers.”",
    "published": "Thu, 25 Sep 2025 15:00:03 +0000",
    "author": "Elie Dolgin",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Smartphone Cameras Go Hyperspectral",
    "link": "https://spectrum.ieee.org/hyperspectral-imaging",
    "summary": "The human eye is mostly sensitive to only three bands of the electromagnetic spectrum—red, green, and blue (RGB)—in the visible range. In contrast, off-the-shelf smartphone camera sensors are potentially hyperspectral in nature, meaning that each pixel is sensitive to far more spectral bands. Now scientists have found a simple way for any conventional smartphone camera to serve as a hyperspectral sensor—by placing a card with a chart on it within its view. The new patent-pending technique may find applications in defense, security, medicine, forensics, agriculture, environmental monitoring, industrial quality control, and food and beverage quality analysis, the researchers add. “At the heart of this work is a simple but powerful idea—a photo is never just an image,” says Semin Kwon , a postdoctoral research associate of biomedical engineering Purdue University in West Lafayette, Ind. “Every photo carries hidden spectral information waiting to be uncovered. By extracting it, we can turn everyday photography into science.” Using a smartphone camera and a spectral color chart, researchers can image the transmission spectrum of high-end whiskey, thus determining its authenticity. Semin Kwon/Purdue University Every molecule has a unique spectral signature—the degree to which it absorbs or reflects each wavelength of light. The extreme sensitivity to distinguishing color seen in scientific-grade hyperspectral sensors can help them identify chemicals based on their spectral signatures , for applications in a wide range of industries, such as medical diagnostics, distinguishing authentic versus counterfeit whiskey, monitoring air quality, and nondestructive analysis of pigments in artwork, says Young Kim , a professor of biomedical engineering at Purdue. Previous research has pursued a number of different ways to recover spectral details from conventional smartphone RGB camera data. However, machine learning models developed for this purpose typically rely heavily on the task-specific data on which they are trained. This limits their generalizability and makes them susceptible to errors resulting from variations in lighting, image file formats, and more. Another possible avenue involved special hardware attachments, but these can prove expensive and bulky. In the new study, the scientists designed a special color reference chart that can be printed on a card. They also developed an algorithm that can analyze smartphone pictures taken with this card and account for factors such as lighting conditions. This strategy can extract hyperspectral data from raw images with a sensitivity of 1.6 nanometers of difference in wavelength of visible light, comparable to scientific-grade spectrometers. “In short, this technique could turn an ordinary smartphone into a pocket spectrometer,” Kim says. The scientists are currently pursuing applications for their new technique in digital and mobile-health applications in both domestic and resource-limited settings. “We are truly excited that this opens the door to making spectroscopy both affordable and accessible,” Kwon says. The scientists recently detailed their findings in the journal IEEE Transactions on Image Processing .",
    "published": "Tue, 23 Sep 2025 14:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Pill-Size Robot Helps Assess Gut Health",
    "link": "https://spectrum.ieee.org/swallowable-robotic-pill-gut-health",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore . Medical providers have a wide swath of imaging options to help peer inside a patient’s abdomen and assess a health concern — but sometimes a deeper dive is needed in the form of a biopsy. Recently, researchers in China developed a novel, swallowable capsule that is magnetically guided within the gut to perform liquid biopsies. In a study published in the September issue of IEEE Transactions on Biomedical Engineering , the researchers put the cleverly designed capsule through a series of tests, including in a real intestinal cavity, demonstrating it could potentially be viable in humans one day. Currently, gut biopsies are collected through invasive procedures, like an endoscopy or a colonoscopy, in which tubes must be inserted into the patient’s body and anesthesia is required. This has prompted scientists to search for less invasive techniques, including swallowable capsules. Shuang Song , a professor at the School of Robotics and Advanced Manufacture at the Harbin Institute of Technology Shenzhen, notes that many swallowable capsules developed to date have been limited to internal imaging of the gut . Their team wanted to create a minirobot capable of actually sampling liquid in the gut—which, although not as common as a tissue sample, can still yield important insights into gut health. Taking these samples yields more biochemical information, such as gut microbiota composition, metabolites, inflammatory markers, and digestive enzyme activity. “By collecting intestinal fluid samples, this robot may provide critical information for gut microbiota-related diseases, malabsorption disorders, gastrointestinal bleeding, and early cancer screening,” Song explains. They add that this approach, compared to traditional tissue biopsies, avoids the risk of tissue damage or bleeding, and is particularly suitable for hard-to-reach areas like the small intestine. How the Capsule Robot Works The robot design is simultaneously simple and effective. After the patient swallows the capsule, an external magnetic-sensor-array system tracks the robot’s position and orientation in real time to determine whether it has reached the target sampling area. Once the target area is reached, an external magnetic field is applied to orient the sampling port of the capsule robot toward the intestinal fluid. The magnetic field is then increased to trigger a magnetic spring mechanism inside the capsule. By changing the orientation and strength of the applied magnetic field, researchers can direct the capsules and trigger the mechanism that draws in a liquid sample. Shuo Zhang, Shaohui Song, et al. “This mechanism would then expand the internally compressed flexible membrane chamber, creating negative pressure within the chamber that draws liquid into the capsule robot,” Song explains. In a way, it’s like reopening a plastic water bottle after a flight, where the changes in pressure cause air to be sucked into the water bottle, reinflating it. In the case of this robotic capsule, the pressure changes cause liquid samples in the gut to be sucked into the capsule. After the capsule collects its sample and passes through the gut, its outer shell can be removed and the sample analyzed. The researchers tested a prototype of the robot, which measures 16.3 by 24.4 millimeters, through a series of experiments. In an isolated pig intestine, they show that the capsule can be effectively guided to desired locations using the magnetic field. They also demonstrate that the capsule is able to suck in liquids of different viscosities — including the range of viscosity common in the human gut — and effectively seal the liquid inside. “The proposed capsule robot has the advantages of small size and large sampling capacity,” Song says, adding that no internal power supply is needed. “However, our capsule robot is only applicable to liquid sampling and has certain limitations when dealing with solid and solid-liquid mixed samples.” The team plans to further optimize the size and structure of the capsule robot, improving the shell material to ensure its safety in humans, and test the capsule via experiments in live animals.",
    "published": "Thu, 18 Sep 2025 13:00:04 +0000",
    "author": "Michelle Hampson",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "How Robotics Is Powering the Future of Innovation",
    "link": "https://content.knowledgehub.wiley.com/from-concept-to-reality-how-robotics-is-transforming-our-world/",
    "summary": "The future of robotics is being shaped by powerful technologies like AI, edge computing, and high-speed connectivity, driving smarter, more responsive machines across industries. Robots are no longer confined to static environments—they are evolving to interact dynamically with humans and their surroundings. This eBook explores the impact of robotics in diverse fields, from home automation and medical technology to automotive, data centers, and industrial applications. It highlights challenges like power efficiency, miniaturization, and ruggedization, while showcasing Molex’s innovative solutions tailored for each domain. Additionally, the eBook covers: Ruggedized connectors for harsh industrial settings Advanced power management for home robots Miniaturized systems for precision medical robotics 5G/6G-enabled autonomous vehicles High-speed data solutions for cloud infrastructure Download this free whitepaper now!",
    "published": "Thu, 11 Sep 2025 10:00:02 +0000",
    "author": "Heilind Electronics",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "New Quantum Sensor Can Identify Molecules Through Vibrations",
    "link": "https://spectrum.ieee.org/molecular-quantum-sensors",
    "summary": "By harnessing virtual particles that constantly blink in and out of existence, a new type of sensor can detect infinitesimal vibrations to identify molecules. The novel device may one day help identify diseases and detect trace levels of contaminants in factories and the environment, researchers say. The way in which atoms move within a molecule can supply details about the kinds of bonds that connect these atoms. By shining light onto molecules to analyze these vibrations, techniques such as infrared spectroscopy or Raman spectroscopy can identify these molecules. Such insights have a wide range of applications, such as revealing the presence of diseases ranging from infections to cancer. However, conventional techniques for analyzing molecular vibrations are limited by weak interactions between the light they use and the matter they are probing. This leads to signals that are often faint, easily drowned out by background noise, and difficult to isolate in complex biological environments such as blood or tissue. In a new study , researchers aimed to create strong interactions between light and molecular vibrations. They started with highly reflective gold mirrors about 12 nanometers thick to create optical cavities about 6 micrometers in diameter. To understand why that’s helpful, we have to peer into the quantum world of atoms. Harnessing Quantum Physics The strange nature of quantum physics suggests that the universe is inherently fuzzy. For instance, you can never know a subatomic particle’s momentum and position at the same time. A consequence of this uncertainty is that space—such as the area within an optical cavity—is never completely empty but instead buzzes with so-called virtual particles that constantly pop in and out of existence. The optical cavities forced virtual photons, or particles of light, to reflect back and forth, helping them couple with the vibrations of molecules that were also enclosed within the receptacles. The virtual photons and the molecular vibrations became so intertwined, they formed a new kind of hybrid quantum state, a quasiparticle called a vibropolariton. The researchers could then use infrared light to analyze these vibropolaritons. “This advance required three ingredients—precise nanophotonic engineering to confine light strongly enough to couple with vibrations, theoretical advances in understanding quantum hybrid states, and modern spectroscopic tools capable of resolving very small shifts in molecular signals,” says Peng Zheng, an associate research scientist in the department of mechanical engineering at Johns Hopkins University, in Baltimore, who worked on the project. “Only recently have these technologies matured to the point where all three could be combined.” In experiments, by analyzing the spectral features of these vibropolaritons, the new quantum sensor was able to identify an organic molecule known as 4-mercaptobenzonitrile dissolved in an organic solvent. “Quantum hybrid light-matter states, something often thought of as highly abstract, can actually make molecules easier to detect in practical conditions,” says Ishan Barman, a professor of mechanical engineering at Johns Hopkins. “By tapping into these states, we found a way to amplify molecular sensitivity beyond what classical optics can do.” A Path to Real-World Applications These experiments achieved this feat under ambient, real-world conditions, without the need for the kind of high-vacuum, cryogenic, or other extreme environments typically required to preserve fragile quantum states. “We now have a pathway toward molecular detection using quantum states in practical conditions,” Barman says. “The big-picture message is that quantum physics isn’t just a curiosity here; it can be harnessed to build real-world sensors for health, safety, and the environment.” Ultimately, Barman envisions compact, microchip-scale quantum sensors. Potential applications include medical diagnostics that can detect trace levels of disease-linked molecules at the very early stages of a condition, real-time analysis in drug or vaccine production, and environmental monitoring to detect harmful chemicals at extremely low levels where one molecule matters, Zheng adds. Future research needs to show these quantum sensors can work in real-world clinically relevant conditions. “We want to integrate these sensors into portable, point-of-care devices,” Barman says. “That will take clever materials engineering and smart device design. The scientists detailed their findings 15 August in the journal Science Advances .",
    "published": "Thu, 04 Sep 2025 14:00:02 +0000",
    "author": "Charles Q. Choi",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "New Eye-Shaping Technique Could Replace LASIK",
    "link": "https://spectrum.ieee.org/electrochemistry-for-eye-surgeries",
    "summary": "A new, promising technique has the potential to replace laser surgeries in ophthalmologists’ offices in the future, for a fraction of the cost. Called electromechanical reshaping (EMR), the technique offers a gentler approach to correcting the cornea than Laser-Assisted in Situ Keratomileusis (LASIK), today’s gold standard for treating vision issues including nearsightedness, farsightedness, and astigmatism. The eye develops these and other conditions when the cornea’s curvature is off—too steep, too flat, or too uneven. To solve the problem, surgeons generally use laser techniques such as LASIK to “sculpt” the eye surface by cutting away small parts of corneal tissue. The results can be life-changing, but the procedure has its risks, as LASIK permanently reduces corneal strength, raising the risk of new vision problems. Alternative nonsurgical methods such as specially designed contact lenses can temporarily mold the cornea , but these require nightly wear and can cause infection. Now, engineers and eye doctors are trying to find a way to permanently reshape collagen-rich tissues like the cornea without cutting, burning, or removing material. Brian Wong , a surgeon-engineer at the University of California, Irvine , stumbled upon a possible solution about a decade ago. He had long worked with thermal techniques for reshaping cartilage tissues—which include the cornea—but found a puzzling “Goldilocks problem” during his research: The heating needed to change shapes often killed too many tissue cells. Then a “happy accident” opened a different perspective, he says. “My postdoctoral fellow connected a pair of electrodes and a Coke can to a power supply…and out of spite, fried a piece of cartilage,” Wong recalls. The cartilage began to bubble, which the postdoc thought was from heat. “But it wasn’t hot. We touched it and thought, this is getting a shape change. This must be electrolysis ,” he says. That surprise pointed to electrochemistry rather than heat as the mechanism. To explore further, Wong partnered with Michael Hill , a chemist at Occidental College . Together, they began exploring the chemistry behind EMR and testing it in different tissues. In mid-August, they presented results from their most recent tests at the American Chemical Society ’s fall meeting that took place in Washington, D.C. How Electricity Reshapes Tissue EMR uses small electrical pulses to split water at the tissue surface into hydrogen and oxygen, releasing protons that spread into the part of the corneal tissue that gives it structural integrity, the ability to hydrate, and other mechanical properties. Once protons are spread throughout the cornea’s surface, they disrupt the chemical bonds that hold collagen fibers in place, also changing the corneal tissue’s pH. This, Wong explains, is the moment when the cornea becomes moldable. Once shaped with a metal contact lens–like mold, it “locks in” to the new shape as the electric pulses are turned off and the body’s natural physiological response returns the cornea’s pH back to its normal value. In 2023, Wong and Hill coauthored a proof-of-concept paper in ACS Biomaterials Science & Engineering , showing that EMR could reshape rabbit corneas without compromising transparency. “That paper was really about asking, is it even possible? Can we change the shape of a cornea without gross damage?” Hill says. “Now, after two more years of work, we’ve systematically gone through the parameters—and we can say yes, it is possible, and we can do it safely,” he adds. Their team built custom platinum contact lenses, press-molded to precise curvatures, and connected them to electrodes. Mounted onto rabbit eyes immersed in a saline solution, the electrodes delivered pulses of around 1.5 volts. X-ray imaging tests confirmed the corneas had indeed matched the mold’s shape. Microscopy tests also confirmed the collagen tissue remained organized post-surgery. “Fine control is the key,” Wong observes. The cost of procedures using the new technique can be significantly lower than laser eye surgery, according to Wong. That’s because, unlike LASIK, EMR doesn’t rely on “laser platforms that cost as much as luxury cars.” The new technique could also be more affordable for clinics and regions priced out of LASIK. While the technique has a long way to go before being used in eye surgeries, the research is advancing to in-vivo animal tests to prove safety and durability—and for long-term tracking to ensure the results last . “Nobody’s getting this at the optometrist next year,” Hill cautions. “Now comes the hard work—refining parameters, confirming long-term viability, and making sure treated eyes don’t revert back,” he adds. That hard work, Hill adds, depends a lot on funding for basic science. EMR was born not from a targeted medical-device program but from curiosity-driven experiments in electrochemistry. “You don’t always know where basic research will lead,” Hill says. “We were looking at electroanalytical chemistry, not eye surgery. But those foundational insights are what made this possible. If you cut off that basic research, you don’t get these kinds of unexpected, transformative opportunities,” he adds.",
    "published": "Tue, 02 Sep 2025 14:00:03 +0000",
    "author": "Meghie Rodrigues",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "The First Inkjet Printer Was a Medical Device",
    "link": "https://spectrum.ieee.org/rune-elmqvist",
    "summary": "Millions of people worldwide have reason to be thankful that Swedish engineer Rune Elmqvist decided not to practice medicine. Although qualified as a doctor, he chose to invent medical equipment instead. In 1949, while working at Elema-Schonander (later Siemens-Elema), in Stockholm, he applied for a patent for the Mingograph , the first inkjet printer. Its movable nozzle deposited an electrostatically controlled jet of ink droplets on a spool of paper. Rune Elmqvist qualified to be a physician, but he devoted his career to developing medical equipment, like this galvanometer. Håkan Elmqvist/Wikipedia Elmqvist demonstrated the Mingograph at the First International Congress of Cardiology in Paris in 1950. It could record physiological signals from a patient’s electrocardiogram or electroencephalogram in real time, aiding doctors in diagnosing heart and brain conditions. Eight years later, he worked with cardiac surgeon Åke Senning to develop the first fully implantable pacemaker. So whether you’re running documents through an inkjet printer or living your best life due to a pacemaker, give a nod of appreciation to the inventive Dr. Elmqvist. The world’s first inkjet printer Rune Elmqvist was an inquisitive person. While still a student, he invented a specialized potentiometer to measure pH and a portable multichannel electrocardiograph. In 1940, he became head of development at the Swedish medical electronics company Elema-Schonander. Before the Mingograph, electrocardiograph machines relied on a writing stylus to trace the waveform on a moving roll of paper. But friction between the stylus and the paper prevented small changes in the electrical signal from being accurately recorded. Elmqvist’s initial design was a modified oscillograph. Traditionally, an oscillograph used a mirror to reflect a beam of light (converted from the electrical signal) onto photographic film or paper. Elmqvist swapped out the mirror for a small, moveable glass nozzle that continuously sprayed a thin stream of liquid onto a spool of paper. The electrical signal electrostatically controlled the jet. The Mingograph was originally used to record electrocardiograms of heart patients. It soon found use in many other fields. Siemens Healthineers Historical Institute By eliminating the friction of a stylus, the Mingograph (which the company marketed as the Mingograf) was able to record more detailed changes of the heartbeat. The machine had three paper-feed speeds: 10, 25, and 50 millimeters per second. The speed could be preset or changed while in operation. RELATED: The Inventions That Made Heart Disease Less Deadly An analog input jack on the Mingograph could be used to take measurements from other instruments. Researchers in disciplines far afield from medicine took advantage of this input to record pressure or sound. Phoneticians used it to examine the acoustic aspects of speech, and zoologists used it to record birdsongs. Throughout the second half of the 20th century, scientists cited the Mingograph in their research papers as an instrument for their experiments. Today, the Mingograph isn’t that widely known, but the underlying technology, inkjet printing, is ubiquitous. Inkjets dominate the home printer market, and specialized printers print DNA microarrays in labs for genomics research, create electrical traces for printed circuit boards, and much more, as Phillip W. Barth and Leslie A. Field describe in their 2024 IEEE Spectrum article “ Inkjets Are for More Than Just Printing .” The world’s first implantable pacemaker Despite the influence of the Mingograph on the evolution of printing, it is arguably not Elmqvist’s most important innovation. The Mingograph helped doctors diagnose heart conditions, but it couldn’t save a patient’s life by itself. One of Elmqvist’s other inventions could and did: the first fully implantable, rechargeable pacemaker. The first implantable pacemaker [left] from 1958 had batteries that needed to be recharged once a week. The 1983 pacemaker [right] was programmable, and its batteries lasted several years. Siemens Healthineers Historical Institute Like many stories in the history of technology, this one was pushed into fruition at the urging of a woman, in this case Else-Marie Larsson. Else-Marie’s 43-year-old husband, Arne, suffered from scarring of his heart tissue due to a viral infection. His heart beat so slowly that he constantly lost consciousness, a condition known as Stokes-Adams syndrome. Else-Marie refused to accept his death sentence and searched for an alternative. After reading a newspaper article about an experimental implantable pacemaker being developed by Elmqvist and Senning at the Karolinska Hospital in Stockholm, she decided that her husband would be the perfect candidate to test it out, even though it had been tried only on animals up until that point. External pacemakers—that is, devices outside the body that regulated the heart beat by applying electricity—already existed, but they were heavy, bulky, and uncomfortable. One early model plugged directly into a wall socket, so the user risked electric shock. By comparison, Elmqvist’s pacemaker was small enough to be implanted in the body and posed no shock risk. Fully encased in an epoxy resin, the disk-shaped device had a diameter of 55 mm and a thickness of 16 mm—the dimensions of the Kiwi Shoe Polish tin in which Elmqvist molded the first prototypes. It used silicon transistors to pace a pulse with an amplitude of 2 volts and duration of 1.5 milliseconds, at a rate of 70 to 80 beats per minute (the average adult heart rate). The pacemaker ran on two rechargeable 60-milliampere-hour nickel-cadmium batteries arranged in series. A silicon diode connected the batteries to a coil antenna. A 150-kilohertz radio loop antenna outside the body charged the batteries inductively through the skin. The charge lasted about a week, but it took 12 hours to recharge. Imagine having to stay put that long. In 1958, over 30 years before this photo, Arne Larsson [right] received the first implantable pacemaker, developed by Rune Elmqvist [left] at Siemens-Elema. Åke Senning [center] performed the surgery. Sjöberg Bildbyrå/ullstein bild/Getty Images Else-Marie’s persuasion and persistence pushed Elmqvist and Senning to move from animal tests to human trials, with Arne as their first case study. During a secret operation on 8 October 1958, Senning placed the pacemaker in Arne’s abdomen wall with two leads implanted in the myocardium, a layer of muscle in the wall of the heart. The device lasted only a few hours. But its replacement, which happened to be the only spare at the time, worked perfectly for six weeks and then off and on for several more years. Arne Larsson lived another 43 years after his first pacemaker was implanted. Shown here are five of the pacemakers he received. Sjöberg Bildbyrå/ullstein bild/Getty Images Arne Larsson clearly was happy with the improvement the pacemaker made to his quality of life because he endured 25 more operations over his lifetime to replace each failing pacemaker with a new, improved iteration. He managed to outlive both Elmqvist and Senning, finally dying at the age of 86 on 28 December 2001. Thanks to the technological intervention of his numerous pacemakers, his heart never gave out. His cause of death was skin cancer. Today, more than a million people worldwide have pacemakers implanted each year, and an implanted device can last up to 15 years before needing to be replaced. (Some pacemakers in the 1980s used nuclear batteries, which could last even longer, but the radioactive material was problematic. See “ The Unlikely Revival of Nuclear Batteries .”) Additionally, some pacemakers also incorporate a defibrillator to shock the heart back to a normal rhythm when it gets too far out of sync. This lifesaving device certainly has come a long way from its humble start in a shoe polish tin. Rune Elmqvist’s legacy Whenever I start researching the object of the month for Past Forward , I never know where the story will take me or how it might hit home. My dad lived with congestive heart failure for more than two decades and absolutely loved his pacemaker. He had a great relationship with his technician, Francois, and they worked together to fine-tune the device and maximize its benefits. And just like Arne Larsson, my dad died from an unrelated cause. An engineer to the core, he would have delighted in learning about the history of this fantastic invention. And he probably would have been tickled by the fact that the same person also invented the inkjet printer. My dad was not a fan of inkjets, but I’m sure he would have greatly admired Rune Elmqvist, who saw problems that needed solving and came up with elegantly engineered solutions. Part of a continuing series looking at historical artifacts that embrace the boundless potential of technology. An abridged version of this article appears in the September 2025 print issue. References There is frustratingly little documented information about the Mingograph’s origin story or functionality other than its patent . I pieced together how it worked by reading the methodology sections of various scientific papers, such as Alf Nachemson’s 1960 article in Acta Orthopaedica Scandinavica, “ Lumbar Intradiscal Pressure: Experimental Studies on Post-mortem Material ”; Ingemar Hjorth’s 1970 article in the Journal of Theoretical Biology , “A Comment on Graphic Displays of Bird Sounds and Analyses With a New Device, the Melograph Mona”; and Paroo Nihalani’s 1975 article in Phonetica , “Velopharyngeal Opening in the Formation of Voiced Stops in Sindhi.” Such sources reveal how this early inkjet printer moved from cardiology into other fields. Descriptions of Elmqvist’s pacemaker were much easier to find, with Mark Nicholls’s 2007 profile “ Pioneers of Cardiology: Rune Elmqvist, M.D. ,” in Circulation: Journal of the American Heart Association, being the main source. Siemens also pays tribute to the pacemaker on its website; see, for example, “ A Lifesaver in a Plastic Cup .”",
    "published": "Thu, 28 Aug 2025 13:00:03 +0000",
    "author": "Allison Marsh",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Stretchy, Self-Healing Sensor Survives Being Cut in Half",
    "link": "https://spectrum.ieee.org/self-healing-flexible-sensor",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Stretchy sensors are useful for many applications, including monitoring human health and emulating artificial muscles in soft robots. One big problem with these sensors is that they don’t last very long as they twist, stretch, and otherwise deform. Now, a team of researchers in Belgium have created a highly durable, stretchable sensor with remarkable self-healing capabilities—to the point that it can heal itself after being cut completely in half and still work at near-perfect performance. The results are described in a study published 16 July in IEEE Sensors Journa l . Lead author Rathul Sangma is a Ph.D. candidate at Vrije Universiteit Brussel who is affiliated with Imec . He says his team was motivated to develop a reliable, stretchable sensor for health monitoring, rehabilitation, and motion tracking because “these systems often endure repeated strain or accidental damage. Existing stretchable sensors can fail under such conditions, leading to unreliability and waste.” Self-Healing Polymer Sensors for Wearables To create their durable sensor, Sangma and his colleagues decided to use a polymer with a chemical bonding mechanism called Diels–Alder crosslinking. These chemical bonds are reversible, meaning they can break when damaged and re-form upon recontact. “When the material is cut, the broken bonds become reactive and, when properly realigned, [they] reconnect, restoring the polymer’s original structure,” says Sangma. In experiments, the researchers showed that the polymer could be cut in half and self-heal at room temperature over the course of roughly 24 hours. The self-healing process could be sped up to just 4 hours when the sensor was placed in an oven at 60 °C. Even after being stretched to the point of breaking and then healed six times, the sensor worked at 80 percent capacity. Smart wearables could benefit from stretchable sensors capable of recovering their functionality even after sustaining significant damage. BruBotics/YouTube Embedded within the polymer is a liquid metal called Galinstan , which acts as conductor. While you might expect the liquid metal to spill out when the polymer is severely damaged, the researchers found that the loss of Galinstan was minimal. They suspect that the liquid metal oxidizes when exposed to air, and the resulting oxide creates a thin, protective barrier that prevents the liquid from escaping. The oxide barrier is broken down once the two pieces of the sensor are mechanically reconnected. “This mechanism is remarkably analogous to how human veins form a clot after rupture to prevent further blood loss,” Sangma says. “Here, the oxide acts as a temporary seal that preserves the integrity of the system until healing is complete.” In a series of tests, the researchers explored how pristine and damaged sensors experienced drift, which is the gradual change in a sensor’s signal over a long period of continuous stretching and relaxing. The results show that a pristine sensor subjected to repeated stretching through 800 cycles drifted less than 5 percent, while a sensor that had been cut in half and stretched the same number of times drifted less than 10 percent. “This dual healing—both in structure and electrical functionality—is what makes our design stand out,” Sangma says. The tests also show that the materials can be recycled with high efficiency once the device finally reaches the end of its operational life. “Over 95 percent of the sensor material can be recovered and reprocessed—an important step toward eco-friendly wearables,” says Sangma. The research team is actively exploring opportunities to commercialize their sensor, with the aim of using it for medical rehabilitation, sports performance monitoring, and soft robotic systems. They have established a spin-off company, Valence Technologies , to commercialize the materials. Moving forward, the researchers are looking to scale the sensor so that it can track full body movements, and they would like to conduct long-term durability testing in real-world environments, such as seeing how the sensor performs when exposed to sweat.",
    "published": "Mon, 18 Aug 2025 15:00:03 +0000",
    "author": "Michelle Hampson",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Wonky Graphene Could Sharpen Proton Therapy",
    "link": "https://spectrum.ieee.org/proton-therapy",
    "summary": "A new twist on pencil graphite might be a key ingredient to better cancer treatment, scientists in Singapore say. Graphite is composed of stacked layers of graphene , a single-atom-thick sheet of carbon atoms arranged in repeating hexagonal rings. Now add pentagons, heptagons, and octagons of carbon atoms into the sheet, and you’re looking at a new form of ultrathin carbon that promises to sharpen beams of subatomic particles used in proton therapy . Ultrathin foils of carbon materials have been used for decades in proton therapy to filter particles into high-precision beams meant to kill tumors. But, they take time to make and often contain impurities from the manufacturing process that lower the precision of the beam. In research described in Nature Nanotechnology, Jiong Lu and his colleagues at National University of Singapore and in China developed a technique that can grow a 200-millimeter sheet of a new kind of ultrathin carbon material in just 3 seconds, with no detectable impurities. Proton therapy is a noninvasive radiation treatment in which hydrogen ions are accelerated through a cyclotron to form a high-energy beam used to destroy DNA in tumors. In a cyclotron, an electromagnetic field accelerates ions of molecular hydrogen, which spiral outward as they pick up speed. They then strike a carbon foil that strips away the hydrogen’s electrons, leaving protons that exit the machine as a high-energy beam. Proton therapy is often preferred as a treatment because of its precision. The sharp beam eliminates tumors while preserving healthy tissue. The new carbon promises an even sharper and more energy-intense beam, potentially making the treatment more potent. The benefits of the new material, called ultra-clean monolayer amorphous carbon (UC-MAC), are derived from its disordered ring structure, which contrasts with the perfect hexagonal rings in graphene. The structures present in UC-MAC create tiny pores in the material that are only one-tenth of a nanometer wide. The researchers have found a way to fine-tune these angstrom-scale pores to control how the material filters hydrogen ions, in order to produce proton beams with less scattering. Nanograins and Nanopores The new technique starts with depositing a thin film of copper on top of a sapphire wafer inside a chamber filled with high-density plasma. Depending on the temperature of the copper and the rate at which it’s deposited, irregular crystals a couple dozen nanometers in size called nanograins form. The nanograins provide the right conditions for UC-MAC to grow, and eventually, a complete layer of the atom-thick carbon material crystallizes on top of the copper. This growth happens in just three seconds, more than an order of magnitude faster than previous methods used to grow carbon foils. Huihui Lin , a research scientist at Singapore’s Agency for Science, Technology and Research who worked on the project, explains that the synthesis’s rapid speeds come from the high density of the nanograins that form on the copper, and from the plasma in the growth chamber, which provides high quantities of particles that react with the substrate to form the carbon structure. Despite its potential importance in cancer treatment though, Lin says that UC-MAC was originally designed with different applications in mind. “We tried it in electronics and optical devices, and after three years of work, we discovered its unique advantage as a membrane for producing precision proton beams,” he explains. Because of the angstrom-size pores in the material, the team discovered that UC-MAC was uniquely suited to turning molecular hydrogen ions into protons. Accelerating molecular hydrogen ions through the cyclotron instead of already-filtered protons increased the quantity of protons in the beam in a given amount of time, by an order of magnitude. Lin thinks it will still take time to get the material to the point of commercialization. He explains that like many other 2D materials, “you need tens of steps” to grow the carbon on the substrate. So, simplifying the process is crucial to getting closer to commercialization. Eventually though, the material may make proton therapy a more widely available treatment option. “The UC-MAC makes proton beams more tunable [and] affordable,” says Lin.",
    "published": "Mon, 18 Aug 2025 12:00:02 +0000",
    "author": "Velvet Wu",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Machine-Learning Contest Aims to Improve Speech BCIs",
    "link": "https://spectrum.ieee.org/speech-bci-machine-learning-competition",
    "summary": "For the next five months, machine-learning gurus can try to best predict the speech of a brain-computer interface (BCI) user who lost the ability to speak due to a neurodegenerative disease . Competitors will design algorithms that predict words from the patient’s brain data. The individual or team whose algorithm makes the fewest errors between predicted sentences and actual attempted sentences will win a US $5,000 prize. The competition, called Brain-to-text ’25 , is the second-annual public, open-source, brain-to-text competition hosted by a research lab part of the BrainGate consortium, which has been pioneering BCI clinical trials since the early 2000s. This year, the competition is being run by the University of California Davis’s Neuroprosthetics Lab . (A group from Stanford University hosted the first competition using brain data from a different BCI user.) For two years, the UC Davis research team has collected brain data from a 46-year-old man, Casey Harrell, whose speech is unintelligible except to his regular caregivers. Once the speech BCI was trained on Harrell’s brain data, it could decode what he was trying to say over 97 percent of the time and could instantly synthesize his own voice, as previously reported by IEEE Spectrum . Decoding Speech from Brain Data Parsing words from brain data is a two-step process: The algorithm must first predict speech sounds, called phonemes, from neural data. Then it must predict words from the phonemes. Competitors will train their algorithms on the brain data corresponding to 10,948 sentences with accompanying transcripts of what Harrell was attempting to say. Then comes the real test: The algorithms must predict the words in 1,450 sentences from brain data withheld from the training data. The difference between the final set of predicted words and the words that Harrell attempted to say is called the word error rate —the lower the word error rate, the better the speech BCI works, overall. Researchers reported a 6.7 percent word error rate, which they hope the public can beat. The goal of the competition is to attract machine-learning experts who may not realize how valuable their skills are to speech BCIs, says Nick Card , a postdoctoral researcher at UC Davis leading both the clinical trial and the competition. “We could sit on this data and hide it internally and make more discoveries with it over time,” says Card. “But if the goal is to help make this technology mature faster to help the people who need to benefit from this technology right now, then we want to share it, and we want people to help us solve this problem.” The public invite into the research world is “an awesome development” that is “long overdue” in the BCI space, said Konrad Kording , a professor at the University of Pennsylvania who researches the brain using machine learning, and who is not involved in the research or competition. This year, Card and his fellow researchers have raised the bar by lowering the starting word error rate with their own high-performing algorithm. The first brain-to-text competition in 2024 began with the Stanford University group posting an error rate of 11.06 percent and finished with the competition winner achieving 5.77 percent. Also new this year are cash prizes for lowest error rates and the most innovative approach, provided by BCI company Blackrock Neurotech, whose electrodes and recording hardware have been used by BrainGate clinical trials since 2006. Ethical Concerns in BCI Data Sharing BCIs have long served as a bridge between neuroscience, medicine, and machine learning. And while machine learning has a tradition of open-source research, medical research is bound by patient confidentiality. The main concern with public brain data is that the patient will be identified, says bioethicist Veljko Dubljević , a professor of both philosophy and science, technology, and society at North Carolina State University. That concern is moot in this case because Harrell went public in August 2024, roughly five years after he began losing muscle tone because of amyotrophic lateral sclerosis (also known as Lou Gehrig’s disease). In 2023, neurosurgeons at UC Davis implanted four electrode arrays with a total of 256 electrodes into the top layers of his brain . Harrell used his speech BCI in an interview with the New England Journal of Medicine last year to explain how the disease feels like being in a “slow-motion car crash.” Harrell said at the time that “it was very painful to lose the ability to communicate, especially with my daughter.” The speech BCI was trained on data collected while Harrell conducted in-lab experiments and while he spoke casually with family and friends. But competitors of Brain-to-text ’25 will not see any “personal use” data recorded while Harrell spoke casually and extemporaneously, Card says. While this is a “good precaution,” Dubljević says, he wonders if Harrell realizes what it means to have someone’s sensitive medical data in the public domain for years. The “noise” of today’s BCIs could be decoded into meaningful personal information in 50 years, for instance, in a way similar to how blood donated in 1955 can now also reveal details about a person’s DNA. (DNA profiling wasn’t established until the 1980s .) Dubljević recommends limiting the data storage to five years. Speech BCIs decode the intended movements of a person’s jaw and mouth muscles in the same way a BCI for an arm or hand prosthesis decodes intended movements. But speech BCIs feel more personal than BCIs that control a hand prosthesis, Dubljević says. Speech is closer to “the innermost sanctum of a person,” he says. “There’s quite a lot of fear about mind-reading, right?” “As a researcher who wants to see science technology deployed for the public good, I want the technology not to be hyped up” in order to avoid a backlash, Dubljević says. Cash Prizes for Innovative BCI Solutions The two lowest word error rates come with $5,000 and $3,000 cash prizes, respectively, and the most innovative approach will win $1,000. The last category is meant to encourage out-of-the-box ideas with great potential, if given more data or more time. Stacking 10 multiples of the same algorithm is a common way to force a more accurate overall performance, but it costs 10 times as much computational power and, “quite frankly, it’s not a very creative solution, right?” Card says. The innovative category is likely to attract the usual crowd of academic and industry BCI scientists who enjoy finding creative solutions, Kording says. But the top slots will likely go to coders with no background in BCIs and who sport a “street-fighting” style of machine learning, as Kording calls it. These “street fighters” focus on speed over ingenuity. In practice, the best BCI algorithms, Kording said, are “usually not really driving from a deep knowledge of how brains work. They’re driving from a deep understanding of how machine learning works.” That said, both the traditional BCIs and new entrants are important parts of the science engineering ecosystem, Kording says. With the corners full, the competition is slated to be an exciting battle.",
    "published": "Sat, 16 Aug 2025 13:00:03 +0000",
    "author": "Elissa Welle",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Superbugs Meet Their Match in Generative AI-Designed Drugs",
    "link": "https://spectrum.ieee.org/ai-drug-design-mit-antibiotics",
    "summary": "Some today fear that artificial intelligence will one day destroy humanity. But if the rise of the machines doesn’t get us, drug-resistant bacteria just might. These microscopic killers already claim millions of lives each year worldwide, and the world’s arsenal of effective antibiotics is dwindling . But could one threat be trained perhaps to help stave off the other? A study published today in the journal Cell certainly suggests the possibility. A team led by Jim Collins , MIT professor of biological engineering, showed how generative AI algorithms trained on vast datasets of antibacterial substances could dream up millions of previously unimagined molecules with predicted microbe-killing power—some of which proved potent in mouse experiments. The researchers synthesized a small subset of these AI-designed molecules and found them lethal to superbugs responsible for drug-resistant gonorrhea and stubborn staphylococcus skin infections. “It’s a great addition to this emerging field of using AI for antibiotic discovery,” says César de la Fuente , a synthetic biologist at the University of Pennsylvania who was not involved in the research. “It shows quite well how generative AI can produce molecules with real-world activity,” he adds. “It’s elegant and potentially clinically meaningful.” A social-enterprise nonprofit created by Collins, called Phare Bio , now plans to advance these and other AI-discovered antibiotics toward clinical development. The candidate antibiotics build on earlier finds from Collins’s lab—including halicin, a potent broad-spectrum antibiotic identified in 2020; a more targeted agent called abaucin , with activity against Acinetobacter baumannii , a major cause of hospital-acquired infections; and a novel structural class of molecules described last year that proved effective against the superbugs MRSA and VRE . With the team’s earlier discoveries, however, Collins and his colleagues were still mining existing chemical libraries, using deep-learning models to spot overlooked compounds with antibacterial potential. The new work sets down a new path altogether: Rather than searching for hidden gems in familiar territory, the generative AI platform starts from scratch, conjuring entirely new molecular structures absent from any database. “This is moving from using AI as a discovery tool to using AI as a design tool,” Collins says. The shift, he adds, opens new frontiers in antibiotic discovery—unexplored territory that could harbor the next generation of lifesaving drugs. Anti-Germ Intelligence Proves Its Mettle To train their generative-AI model, Collins and his colleagues first used a neural network framework to virtually screen more than 45 million chemical fragments—the building blocks of would-be drugs—looking for pieces predicted to have activity against Neisseria gonorrhoeae (the cause of sexually transmitted gonorrhea infections) and Staphylococcus aureus (the germ behind deadly bloodstream infections, pneumonia, and flesh-eating skin disease). Two algorithms then went to work: one assembling the fragments into complete molecular structures, the other predicting which of those structures would pack the strongest antibacterial punch. Together, the algorithms generated more than 10 million candidate molecules, none of which had ever existed before. But then came what MIT study author and computational biologist Aarti Krishnan describes as “a massive bottleneck”: Very few of these prophesied antibiotics could actually be made in the lab. The researchers manually sifted through the AI hits, filtering for properties suggestive of drug likeness and synthetic feasibility. They ultimately arrived at a shortlist of around 200 promising designs, 24 of which could be successfully generated. Seven proved to be bona fide antimicrobial agents, as confirmed by laboratory tests, with two showing particularly strong efficacy in mouse models of gonorrhea and staph infections. Notably, each seems to work through a distinct and novel mechanism of action not exploited by existing antibiotics. “That’s pretty cool,” says Phare co-founder Jonathan Stokes , an antimicrobial chemical biologist at Canada’s McMaster University , in Hamilton, Ont. He praises Collins’s team for unearthing two highly promising antibiotic leads but notes that the labor-intensive trial-and-error process underscores how far the technology still has to go in producing compounds that can be readily synthesized. “It’s a bit of an elephant in the room,” he says of synthetic tractability in GenAI drug discovery. “Antibiotics, because of the financial disincentives in this space, have to be cheap,” says Stokes, who was not involved in the research. “They have to be cheap to discover, cheap to develop, and cheap to make. So if there are opportunities to avoid all of these issues with synthetic feasibility, I feel like that is a major advantage.” Moving From Model to Molecule To tackle that challenge, Stokes and his colleagues developed a generative-AI tool that designs antibiotic candidates with chemical blueprints tailored for real-world manufacturing, not just computer screens. This tool, called SyntheMol , operates within a more limited chemical space than Collins’s GenAI model, choosing only molecules whose building blocks can be synthesized with known, lab-proven reaction steps. That narrows the search parameters to tens of billions of molecules, compared to the 10 60 possible structures that Collins’s model explored. It’s enough, however, for SyntheMol to have already yielded several drug candidates that Stokes and his colleagues, through a startup called Stoked Bio , hope to develop into treatments for bacteria linked to Crohn’s disease and other hard-to-treat conditions. The team aims to balance the sheer breadth of biochemical possibilities the models can explore with crucial metrics like drug potency, safety, low toxicity, and ease of synthesis. “It’s a multiobjective optimization problem,” says de la Fuente, who advises Phare and builds his own generative-AI models to design antimicrobial peptide drugs . But for now, the tools powering Phare’s discovery efforts—rooted in Collins’s approaches—are already delivering early wins, says Akhila Kosaraju , Phare Bio’s CEO and president. “We are getting substantially more potent and less toxic initial compounds,” she notes. And backed by the U.S. government’s Advanced Research Projects Agency for Health (ARPA-H), along with the philanthropic arm of Google —which is funding Phare to build open-source infrastructure around AI-guided antibiotic design—Kosaraju and her colleagues aim to move the most promising candidates into human trials. “We are building what we think is the most novel and robust pipeline of antibiotics in the world,” she says.",
    "published": "Thu, 14 Aug 2025 15:00:04 +0000",
    "author": "Elie Dolgin",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Meta’s New Bracelet Reads Hand Gestures",
    "link": "https://spectrum.ieee.org/meta-wristband-interface",
    "summary": "Imagine the ability to control machines with your mind instead of having to type on a keyboard or click on a mouse. Now Facebook’s parent company, Meta, is aiming for the next best thing—a new wristband that can, with the help of AI, infer electrical commands sent from the brain to muscles and convert them into computer signals, all in a noninvasive way. Although experts doubt it will replace keyboards and mice for traditional computing, it might have new uses for a wide range of applications, such as wearable interfaces for mobile devices, or thought-controlled assistive technologies for people with disabilities. The bracelet from Reality Labs at Meta uses metal contacts placed against the skin to detect electrical signals from muscles—a technique known as surface electromyography (sEMG)—which are generated in response to commands from the brain. The highly sensitive new system transmits this data to a computer using Bluetooth to help it recognize gestures such as pointing and pinching in real time, findings detailed in Nature on 23 July. The bracelet is not a direct interface with the brain . “It is not a mind-reading system. It cannot make you act in a different way as imposed by your will; it does not connect you to other people neurally; it does not predict your intentions,” says Dario Farina , chair in neurorehabilitation engineering at Imperial College, London, who did not take part in Meta’s research but has tested the technology. (Meta was unable to make anyone available for comment as of press time.) How AI Enables Meta’s Wristband Previous “neuromotor” devices, such as the discontinued Myo armband , also sought to use sEMG for computer interfaces. A key challenge these earlier devices faced was how they each needed time-consuming personalized calibration for each user to account for differences in human anatomy and behavior. See how the device detects thumb swipes, finger taps, and handwriting gestures. Reality Labs at Meta In contrast, Meta says its bracelet can essentially work off the shelf. The key was training deep-learning artificial intelligence systems on data from more than 6,000 paid volunteers who wore the device. This generated models that could accurately interpret user input across different people without requiring individual calibration, says Joe Paradiso , head of the Responsive Environments Research Group at the MIT Media Lab, who did not participate in this study. “The amount of information decoded with this device is very large, far larger than any previous attempt,” Farina says. “The device can recognize handwriting, for example, which would have not been conceivable before.” The wristband uses surface electromyography to noninvasively measure electrical activity of the muscles that control hand gestures. Reality Labs at Meta A possible concern with using this wristband is how users might not want every hand motion interpreted as input—say, if they had to scratch an itch, or pick up a glass of water. However, the device is trained to recognize only certain commands, while ignoring other gestures. “A potential user of the device can perform any activity of daily living without the risk to activate the device, and yet control the device with the specific gestures for which the device has been trained,” Farina says. In tests, volunteers who had never previously tried the bracelet could use handwriting gestures to input text at 20.9 words per minute. (In comparison, mobile phone keyboard typing speeds average roughly 36 words per minute.) Given this interface speed, “I doubt most computer users would rush out to buy this wristband if it becomes commercially available,” says Eben Kirksey , a professor of anthropology at the University of Oxford who studies the interplay of science, technology, and society. “Most people type at around 40 words a minute, and highly skilled typists can bang out upwards of 100 words a minute. Since this new wearable device only enables users to write 20 words a minute, I doubt many people will want to take the time to learn how to use this new way to interface with computers.” Instead, the new study argues the bracelet might prove useful in scenarios where keyboards or mice would be limiting, such as mobile and wearable applications. “It’s not a keyboard replacement. It’s something else, and I think things like this are needed for the computational paradigms that are coming,” Paradiso says. For example, when it comes to the virtual reality and augmented reality glasses that Meta, Google, and others have pursued, interfaces previously envisioned for these devices include vision-based trackers that track the motions of hands held up in front of users , Paradiso says. However, the fatigue that can set in with this “gorilla arm” approach limits long-term use, and “the need to use space in front of you has its drawbacks,” Paradiso notes. Because this new bracelet interprets electrical signals instead of motion, it could instead let users keep their hands to their sides and interact with devices using subtle finger motions. “Interfaces like these are needed for the everywhere-computing eyewear that’s emerging,” Paradiso says. “My favorite scenario is on a crowded train with my head-worn display catching up with the news, or communicating with friends. You just nudge it around with your hands by your side. Going for a walk, and so on—same deal.” Applications for Accessibility The researchers also suggest the wristband could help people with disabilities better interact with computers, particularly individuals with reduced mobility, muscle weakness, finger amputations, paralysis, and more. “Some members of the disabled community have difficulty typing on conventional keyboards, or using computer mice,” says Kirksey, who did not take part in this research. “This device offers a new option that might help some members of this community, who have very specific bodily challenges, interface with computers.” For such applications, a virtue of the new bracelet is that it is relatively easy to don and doff. In contrast, brain-computer interfaces (BCIs), which also rely on electrical signals from the brain, often require invasive brain surgery. Non-invasive BCIs do exist, such as ones that apply electrodes onto the scalp , “but a wristband is more ergonomic than a skullcap,” Paradiso says. However, many questions remain before this new technology might help people with disabilities. “This type of wearable assumes typical limb shape and fine motor control,” says Solomon Romney , formerly the head of Microsoft’s Inclusive Design Lab. “As a limb-different person, I am always looking for ways to move activities to my limblet rather than continue to overload my typical hand. How easily can it be adjusted to fit nontypical limbs and/or musculature? How does it filter tremors? How effectively would it work for someone with no hands to wear on their ankle?” Ultimately, “the main obstacle is large-scale deployment,” Farina says. “To be distributed to millions of individuals, the system needs to be robust across different anatomies and be used with minimal error rate. To match the error rate of a mouse or keyboard in a vast human population is certainly a huge challenge.”",
    "published": "Wed, 13 Aug 2025 13:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "“Electrical Genotyping” Simplifies the Detection of DNA Mutations",
    "link": "https://spectrum.ieee.org/dna-sequencing-electrical-impedance",
    "summary": "A new microfluidic chip simplifies detecting and quantifying DNA by analyzing electrical impedance signals that vary based on DNA flowing across a biosensor inside the chip. The proof-of-concept technique, called electrical genotyping, can accurately detect single-point mutations faster and more simply than traditional genotyping, which typically takes days or weeks and can be prohibitively expensive. The new tool, developed by scientists at Rutgers University and Yale University, won’t replace all genotyping, but it could still be a powerful, yet affordable, screening tool that one day expands the ability of any clinic’s early detection of rare diseases. “What we’re trying to make is the equivalent of a glucose meter, but for detecting genetic mutations,” says Mehdi Javanmard , a professor in the Rutgers electrical and computer engineering department. Detecting Hereditary Genetic Mutations The researchers focused on detecting a hereditary single mutation that causes transthyretin cardiac amyloidosis , a disease that can lead to congestive heart failure. It is underdiagnosed and disproportionately affects people with West African ancestry. The researchers ran c linical samples of six patients, four of whom had the mutation, through the tool. The initial sample preparation steps amplified the mutated section of DNA. When the four samples with amplified amounts of DNA containing the relevant mutation were run, the biosensor correctly detected the DNA through changes in electrical impedance. It also measured low concentrations of DNA in nonamplified samples lacking the mutation. Javanmard and his colleagues published their findings in June in Nature Communications Engineering . In general, electrochemical biosensors have advanced research for various applications, including detecting cancer biomarkers , and DNA data storage and synthesis. This study’s detection is done with label-free impedance biosensors. (Labels are like attaching baggage tags to DNA. They are commonly used for accurate detection, but require extra steps that cost time and money.) “The novelty of the sensing component is being able with high sensitivity to detect DNA without any fluorescent tags” or other labels , Javanmard says. “Instead of 15 steps and a 5-hour-long run with a DNA sequencer, we’re talking about something that’s significantly shorter, a couple of steps on the sample-preparation side and then just one step on the detection side. Within 5 minutes, the detection is performed,” says Javanmard. The simplified sample preparation was developed by Curt Scharfe , a professor of genetics at Yale, and his lab. DNA Detection Through Electrical Impedance Before DNA can be detected, it must be prepared using the targeted technique developed by Scharfe’s team. “It’s the amplification process that allows you to differentiate between the wild type [nonmutated] and the mutation,” says Javanmard. Then the amplified DNA solution, driven by passive capillary flow, washes through the microfluidic channel of the biosensor and across submerged gold electrodes. The channel and the gold electrodes form a sensing circuit. The solution in the channel is modeled as a resistor and the DNA is modeled as a dipole capacitance due to its negatively charged properties and inherent conductivity. Essentially, the method measures conductivity. The electrical conductivity of a solution containing DNA is different than a solution without it, so the signal can be differentiated. An external impedance spectroscope sends a multi-frequency stimulus (between 10 kilohertz and 3 megahertz) to the circuit, generating an electrical field. As the DNA solution flows through the channel, it changes the electrical conductivity, and thus the impedance in the sensing circuit. This rate of impedance change is measured by the sensor as a raw current signal. That signal is fed into off-chip equipment. First it goes into a transimpedance amplifier that boosts the signal to a more detectable level and converts it into a voltage. Then it enters a lock-in amplifier , which reduces noise and outputs a clear signal by extracting, digitizing, and filtering the input. Finally, the data is analyzed. As the DNA concentration in the sensing region increases, the rate of change of the output voltage increases too, and the system measures the amount of DNA present. This is what the researchers dub a “DNA quantification score.” To improve the score’s accuracy, they average the readouts from four low-frequency signals sent to the circuit. For both nonmutated and mutated samples, the gold standard biological readout’s fluorescence intensity “correlated with the electrical DNA quantification score,” notes Javanmard. Their simpler electrical-detection technique could replace the more manual biological readout and be more portable in resource-constrained settings. The label-free technique itself is not new , but “there’s no one-size-fits-all electrical impedance sensor,” says Javanmard. He also says it’s a combination of how the measurement was performed, selecting the right mix of excitation frequencies for impedance, and postmeasurement data analytics that makes their sensor optimized and novel. To get beyond a proof of concept, the chip’s sensitivity to detect DNA will still need to be improved. The study authors believe one approach is to reduce the sensing area, which creates a greater threshold difference between a DNA impedance signal and a no-DNA signal. They would also need to bring the electronics and biological amplification on-chip. Javanmard said they’ve already shown that it’s possible to miniaturize the off-chip electronics in a device made by RizLab Health, a company spun off from his lab. This technique could also have applications beyond medical diagnostics. Javanmard notes, “as DNA storage becomes more and more advanced, there will be a need for technologies for accurate readout and error control. Rapid and advanced DNA biosensors will become a necessity.” This story was updated on 28 August 2025 to correct some details of the impedance spectroscope used in the technique.",
    "published": "Mon, 11 Aug 2025 16:11:03 +0000",
    "author": "Diya Dwarakanath",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Chips With Neural Tissue Aim to Make AI More Energy Efficient",
    "link": "https://spectrum.ieee.org/biochip-organoid-intelligence-ai-processor",
    "summary": "As generative AI systems advance, so too does their appetite for energy. Training and running large language models consumes vast amounts of electricity. AI’s energy demand is projected to double in the next five years, gobbling up 3 percent of total global electricity consumption. But what if AI chips could function more like the human brain, processing complex tasks with minimal energy? A growing chorus of scientists and engineers believes that the key might lie in organoid intelligence . AI enthusiasts were introduced to the concept of brain-inspired chips in July at the United Nations’ AI for Good Summit in Geneva. There, David Gracias , a professor of chemical and biomolecular engineering at Johns Hopkins University, in Baltimore, gave a talk discussing the latest research he’s led on biochips and their applications to AI. Focused on nanotech, intelligent systems, and bioengineering, Gracias’s research team is among the first to build a functioning biochip that combines neural organoids with advanced hardware, enabling chips to run on and interact with living tissue. Organoid intelligence is an emerging field that blends lab-grown neurons with machine learning to create a new form of computing. (The term organoid intelligence was coined by a group of Johns Hopkins researchers that includes Thomas Hartung .) The neurons, called organoids, are more specifically three-dimensional clusters of lab-grown brain cells that mimic neural structures and functions. Some researchers believe that so-called biochips—organoid systems that integrate living brain cells into hardware—have the potential to outstrip silicon-based processors like CPUs and GPUs in both efficiency and adaptability. If the process is commercialized, experts say biochips could potentially reduce the staggering energy demands of today’s AI systems while enhancing their learning capabilities. “This is an exploration of an alternate way to form computers,” Gracias says. How Do Biochips Mimic the Brain? Traditional chips have long been confined to two-dimensional layouts, which can limit how signals flow through the system. This paradigm is starting to shift, as chipmakers are now developing 3D chip architectures to increase their devices’ processing power. Similarly, biochips are designed to emulate the brain’s own three-dimensional structure. The human brain can support neurons with up to 200,000 connections—levels of interconnectivity that Gracias says flat silicon chips can’t achieve. This spatial complexity allows biochips to transmit signals across multiple axes, which could enable more efficient information processing. Gracias’s team developed a 3D electroencephalogram (EEG) shell that wraps around an organoid, enabling richer stimulation and recording than conventional flat electrodes. This cap conforms to the organoid’s curved surface, creating a better interface for stimulating and recording electrical activity. To train organoids, the team uses reinforcement learning. Electrical pulses are applied to targeted regions. When the resulting neural activity matches a desired pattern, it’s reinforced with dopamine, the brain’s natural reward chemical. Over time, the organoid learns to associate certain stimuli with outcomes. Once a pattern is learned, it can be used to control physical actions, such as steering a miniature robot car through strategically placed electrodes. This demonstrates neuromodulation—the ability to produce predictable responses from the organoid. These consistent reactions lay the groundwork for more advanced functions, such as stimulus discrimination, which is essential for applications like facial recognition, decision-making, and generalized AI inference. Gracias’s team is in the early stages of developing miniature self-driving cars controlled by biochips: A proof of concept that the system can act as a controller. This experimental work suggests future roles in robotics, prosthetics, and bio-integrated implants that communicate with human tissue. These systems also hold promise in disease modeling and drug testing. Gracias’s group is developing organoids that mimic neurological diseases like Parkinson’s. By observing how these diseased tissues respond to various drugs, researchers can test new treatments in a dish rather than relying solely on animal models. They can also uncover potential mechanisms of cognitive impairment that current AI systems fail to simulate. Because these chips are alive, they require constant care: temperature regulation, nutrient feeding, and waste removal. Gracias’s team has kept integrated biochips alive and functional for up to a month with continuous monitoring. Fred Jordan [left] and Martin Kutter are the founders of FinalSpark, a Swiss startup developing biochips that the company claims can store data in living neurons. FinalSpark Challenges in Scaling Biochip Technology Yet significant challenges remain. Biochips are fragile and high maintenance, and current systems depend on bulky lab equipment. Scaling them down for practical use will require biocompatible materials and technologies that can autonomously manage life-supporting functions. Neural latency, signal noise, and the scalability of neuron training also present hurdles for real-time AI inference. “There are a lot of biological and hardware questions,” Gracias says. Meanwhile, some companies are testing the waters. Swiss startup FinalSpark claims its biochip can store data in living neurons—a milestone it calls a “bio bit,” says Ewelina Kurtys , a scientist and strategic advisor at the company. This step suggests biological systems could one day perform core computing functions traditionally handled by silicon hardware. FinalSpark aims to develop remote-accessible bioservers for general computing in about a decade. The goal is to match digital processors in performance while being exponentially more energy efficient. “The biggest challenge is programming neurons, as we need to figure out a totally new way of doing this,” Kurtys says. Still, transitioning from the lab to industry will require more than just technical breakthroughs. ”We have enough funding to keep the lab running,” Gracias says. “But for the research to take off, more funding is needed from Silicon Valley.” Whether biochips will augment or replace silicon remains to be seen. But as AI systems demand more and more power, the idea of chips that think—and sip energy—like brains is becoming increasingly attractive. For Gracias, that technology could be shipped to market sooner than we think. “I don’t see any major showstoppers on the way to implementing this,” he says.",
    "published": "Sat, 09 Aug 2025 13:00:02 +0000",
    "author": "Aaron Mok",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Multiphysics Simulation of Electromagnetic Heating for Post-Surgical Infection Treatment in Knee Replacements",
    "link": "https://events.bizzabo.com/747708?utm_source=Wiley&utm_medium=Spectrum",
    "summary": "Post-operative knee replacement infections present a significant clinical challenge with limited noninvasive treatment options. A medical device is being developed that offers a novel therapeutic approach utilizing electromagnetic heating to target infections localized around metal implants. This device, designed using multiphysics simulation, is engineered to place more heat on the infected region and less heat on healthy tissue, thereby eliminating the infection with minimal necrosis of surrounding healthy tissue. Engineers can use multiphysics software to simulate the complex thermal responses in medical devices. The in silico data generated from these simulations is critical for the FDA approval process, significantly reducing the need for data collection from in vitro and in vivo studies. In this presentation, Kyle Koppenhoefer and Joshua Thomas of AltaSim Technologies will discuss how multiphysics simulation can be used to predict tissue heating in medical devices and address critical challenges in this area of research. Register now for this free webinar!",
    "published": "Wed, 06 Aug 2025 18:40:17 +0000",
    "author": "COMSOL",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Scientists Shine a Laser Through a Human Head",
    "link": "https://spectrum.ieee.org/optical-brain-imaging",
    "summary": "For the most part, anyone who wants to see what’s going on inside someone else’s brain has to make a trade-off when it comes to which tools to use. The electroencephalograph (EEG) is cheap and portable, but it can’t read much past the outer layers of the brain, while the alternative, functional magnetic resonance imaging (fMRI) , is expensive and the size of a room, but can go deeper. Now, a research group in Glasgow has come up with a mechanism that could one day provide the depth of fMRI using equipment as affordable and portable as an EEG. The technology will rely on something that previously seemed impossible—shining light all the way through a person’s head. Obviously, the human head doesn’t let much light through it. For years, brain-imaging techniques using light, called optical brain imaging, have struggled against that barrier to becoming widely used in research and clinical practice. Optical brain imaging primarily uses near-infrared light , to which human tissue is relatively transparent. But human heads are so good at blocking even those wavelengths that the Glasgow research group found that only a billionth of a billionth of all near-infrared photons make it through an entire adult human head from one side to the other. Statistics like these had prompted many in the field to conclude that transporting light through the deep brain was impossible, until Daniele Faccio ’s group at the University of Glasgow recently did it. “Sometimes we went through phases of thinking, okay, maybe this is just impossible because we just didn’t see a signal for so many years.” —Jack Radford, University of Glasgow “There are a lot of optical techniques of monitoring brain activity which have laser detectors that are placed maybe 3 centimeters apart, maybe 5 centimeters apart. But nobody had really tried to go all the way through the head,” Jack Radford , the lead author of the study describing the work in Neurophotonics , explains. The team started with a slab of thick, light-scattering material, and found that light could pass through a human head’s width of the material to reach a photodetector. Then they designed an experiment to test the limits of near-infrared light transmission through a volunteer’s head. The group measured the different times that millions of photons took to travel from a 1.2-watt laser emitting 800-nanometer-wavelength light into one side of the head to a detector on the other side. Each time represented possible paths that individual photons could take through the subject’s head. The group also simulated the travel paths of the photons and constructed distributions of both the experimental and simulated times. Because the distributions were so similar, they were able to conclude that they weren’t just detecting random photons passing through the room. But it wasn’t just smooth sailing. It took many iterations of experimental setups to definitively find the one in a billion billion photons that make it through the head. Extreme Light group/University of Glasgow “What’s not in the paper is the five years of experiments that didn’t really work,” Radford says. One major improvement the team made to the experiment was to reduce background noise. Because so few photons make it all the way through, it’s more likely that the photons bouncing around the room will hit the detector than the photons that actually pass through the head. The group made adjustments like draping black cloth over the subject’s head, conducting the entire experiment in a black box, putting the subject in a sleeping bag–type arrangement, and fitting another black cover on top of all of that, before seeing good results. They also spent time trying different lasers, adjusting the beam size and wavelength, and inventing new setups to improve their signal, some of which involved bicycle helmets and chin straps. “Sometimes we went through phases of thinking, okay, maybe this is just impossible because we just didn’t see a signal for so many years,” says Radford. “But there was always some sort of inclination that we might be able to do something. So that’s kind of what kept the momentum going in the research project.” Now the possibility of measuring photons that have passed through the deep brain opens up a host of new possibilities for cheaper, more accessible, and deeper-penetrating brain-imaging technology, he suggests. Toward Deeper Optical Brain Imaging “Applications to date pretty much are just focused on the surface of the brain—that’s what current technology can do,” says Roarke Horstmeyer , a professor in Duke University’s biomedical engineering department, who was not involved in the Glasgow research. The research “helps to assess and establish whether or not this optical technology can begin to reach those deeper regions.” Radford is exploring ways that future deep-penetrating optical brain imaging can be applied in clinical and medical settings, particularly to help quantify brain health. For a set of wide-ranging, hard-to-quantify conditions like cognitive decline, neurodegenerative diseases, brain fog, and concussions, hospitals typically use questionnaires to determine brain function. But “[there are] no real biomarkers for how brain health is and how it evolves over time,” says Radford. Optical imaging tools that can reach the deeper brain could provide a more widely accessible and deterministic method of identifying those hard-to-quantify conditions. Another application Radford is interested in is the rapid diagnosis of strokes. Correctly identifying and treating strokes before serious neurological damage occurs currently relies on the ability to obtain a CT scan and MRI within several hours to determine the exact cause of the stroke. But such scans are expensive, making that treatment less accessible. Prescribing stroke treatment without knowing the cause, though, could lead to fatal consequences. A bedside brain scanner using optical brain-imaging methods could quickly and more cheaply identify the cause of the stroke, leading to rapid diagnosis and treatment. Radford is excited that the difficult trade-off of expensive, deeper-penetrating imaging equipment versus cheaper but shallower sensors is starting to break down. Physicians and researchers “don’t realize they could be using [brain imaging] because they’ve always thought that using an MRI is out of the question…now that [MRI] isn’t the question, it’s exciting to speak to clinicians and…explore different potential uses of it to help them in their diagnostics and their treatment,” he says. However, there are hurdles the technology still needs to overcome to be successful in a clinical setting. For one, the study itself didn’t image any of the deep brain; it just sent photons through. “The technology still has a long way to go; it’s still in its infancy,” says Horstmeyer. Another obstacle will be variations in the head anatomy of subjects—out of the eight volunteers the experiment conducted trials on, Radford’s group was able to detect a signal for only a participant with fair skin and no hair. “When you go all the way across the head, you’re at such low light levels that simply the color of your skin or thickness of your skull or the hairstyle that you have can make that difference of being able to detect it or not,” says Horstmeyer. Radford thinks that there might be a way to overcome variations in human anatomy by changing the power and beam size of the laser, but he admits those changes might cause problems with spatial resolution. It’s “still an unsolved problem, in my mind,” he says. Despite these challenges, Radford emphasizes that the purpose of the study was just to show that it is physically possible to transport photons through the entire human head. “The point of measurement is to show that what was thought impossible, we’ve shown to be possible. And hopefully…that could inspire the next generation of these devices,” he says.",
    "published": "Mon, 04 Aug 2025 12:00:02 +0000",
    "author": "Velvet Wu",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Can a Brain-Sensing Headband for Consumers Be a Research Tool?",
    "link": "https://spectrum.ieee.org/muse-headband",
    "summary": "Over 20 years ago, Muse headbands began as an experiment in neurofeedback: The gadget notified the wearer when an electroencephalography ( EEG ) signal from their brain reached a certain threshold. As the product developed , its maker, Toronto-based Interaxon , began marketing the device as a meditation tool. Although mindfulness tech remains part of the Muse brand, the potential scope of its wearable devices has now expanded into sleep and performance. The Muse S Athena headband , announced earlier this year, is the first device from the company to also incorporate light sensors able to measure blood oxygenation; the system uses fluctuations in that data as an indicator of blood flow and brain activity. This technique, called functional near-infrared spectroscopy , or fNIRS, is considered complementary to EEG. In June, Muse also announced a reworked data analysis tool that the company calls a “foundation brain model.” The AI model has a transformer architecture, similar to that of ChatGPT, and it was trained on over a billion minutes of brain data collected over the past decade from Muse users. Though the headbands are designed as consumer products and the company makes no claims about diagnostic ability , previous Muse headbands have been used for a range of scientific studies, from exploring its therapeutic applications to basic behavioral research. The company hopes this one will be too. Researchers familiar with fNIRS and EEG recording told IEEE Spectrum that the device has advantages in portability, affordability, and ease of use, but noted that its sensor limitations could preclude its use for some research questions. What can brain researchers do with a Muse headband? “We’re focused on bringing neurotechnology to the home,” says Chris Aimone, chief innovation officer and co-founder of Interaxon. He says many of Interaxon’s research partners are interested in sleep science, and he notes that the S Athena can be used as an at-home sleep monitor—its measurements of both brain activity and blood oxygenation could be useful for assessing disorders such as sleep apnea. An earlier version of the Muse S performed comparably to standard laboratory sleep-testing equipment. The Muse S Athena is not the only commercialized brain monitor to integrate fNIRS and EEG sensors, but it may occupy a niche in the trade-off between coverage and cost. It has four EEG sensors placed along the brow line, five light sensors for fNIRS measurements on the forehead, and costs around US $500 with additional subscription fees for some services. In comparison, a full-coverage helmet from Kernel , which boasts fNIRS and EEG sensors that can provide up to 3,500 measurement channels, costs more than $100,000. So what do researchers think of the gear? The Muse systems “have limited channels [and] very limited capability for imaging,” says Hubin Zhao , an engineer at University College London who works in neuromedical technologies. “If you have more sensors—more measurement channels—you get richer information with better resolution.” “You have to really plan your study accordingly, knowing that you may only have info from these few frontal sensors,” says Guiomar Niso, a neuroscientist at the Cajal Institute , in Madrid, who has written about other wearable EEG systems. Aimone says that rather than pinpointing brain activity, the S Athena records an “interesting macroscopic signal.” The Muse S Athena headband uses two kinds of sensors to measure brain activity. Muse “As a consumer-oriented portable device, it may present some challenges regarding data quality and artifact removal compared to more sensitive laboratory-grade systems,” says Mevhibe Saricoğlu , an assistant professor of neuroscience at Istanbul Medipol University who has used concurrent fNIRS and EEG recordings to compare cognitive performance across age groups. However, “the device’s practicality and ease of use offer significant advantages for field studies, neurofeedback applications, and preliminary pilot experiments.” A device designed for everyday use could translate well to studies of everyday activities. “If you do research and you’re stuck in a lab, you’re limiting the kind of questions you can ask,” says Olav Krigolson , a neuroscientist at the University of Victoria, in Canada, who has worked extensively with previous generations of Muse devices . For example, he monitored the brains of entire classrooms of high school students and studied mental fatigue in both emergency room workers and pilots in their natural settings. What’s next for Muse Muse headbands could further expand the scope of research through a new citizen science program that allows users to enroll in individual studies through an app. A similar approach has enabled Muse to collect what Aimone calls “an enormous pile of data,” which is critical for training a successful AI model. “You need to train something that has the complexity of a brain to understand one,” says Aimone. “We’re not doing that yet, but in some ways that’s the goal: something that has the expressive power to identify brain patterns and functions, and also differences in brain patterns and functions between people.” The overall package of device, software, and services “marks a shift from isolated physiological tracking toward integrated, AI-enhanced brain monitoring,” says Saricaoğlu. “Muse S Athena and similar systems are powerful tools—but they must be used ethically, transparently, and with a full awareness of both their capabilities and their limitations.” This story was updated on 15 July 2025 to correct Mevhibe Saricaoğlu’s title. She is an assistant professor, not a Ph.D. student.",
    "published": "Tue, 15 Jul 2025 13:00:02 +0000",
    "author": "Greg Uyeno",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Next-Gen Brain Implants Offer New Hope for Depression",
    "link": "https://spectrum.ieee.org/deep-brain-stimulation-depression",
    "summary": "Her relapse into depression felt like defeat—but it offered vital clues to achieving lasting psychiatric relief. The 67-year-old woman from Alabama had already endured four major depressive episodes in her decades-long battle with mental illness. After exhausting numerous medications and other therapies, in 2015 she turned to an experimental last resort: deep brain stimulation , or DBS. Neurosurgeons implanted electrodes a few inches below her skull, targeting a small bundle of neural fibers in a brain region behind the forehead that acts as a crucial hub for mood regulation. Thin wires connected the electrodes to a pulse generator discreetly inserted in her upper chest. Once activated, the device delivered a steady stream of high-frequency electricity, gently buzzing the targeted circuits to disrupt maladaptive patterns and, like a pacemaker for the brain, restore a healthier balance of neural activity. At first, the treatment seemed to be working. The woman’s despair lifted, and she edged closer to remission. Watching football for hours with her husband on Sundays started to feel tedious—in a good way. Her desire to get off the couch and pursue other activities had returned. An X-ray image shows two pairs of DBS electrodes implanted for depression treatment. Department of Neurosurgery, Baylor College of Medicine But four months on, the darkness crept back in. The woman’s sudden downturn blindsided the medical team that had been closely monitoring her recovery. The doctors had to make three adjustments to the implant’s stimulation parameters, slowly increasing the voltage, before her condition finally stabilized—an agonizing couple of months. When the clinicians reviewed the data later, they realized that the electrodes embedded in the woman’s brain had detected trouble brewing before she did. Subtle shifts in the electrical patterns coursing through her neural fibers had flagged the impending relapse weeks before her outward symptoms reappeared. If clinicians had acted on those signals, they might have adjusted the stimulation settings in time to prevent her relapse. It’s a thought that weighs on Patricio Riva Posse , the psychiatrist at Emory University School of Medicine, in Atlanta, who treated the woman. Looking back now, he says, had he known that the brain’s circuits were off-kilter, “I would have taken action earlier.” Fortunately, Riva Posse no longer has to dwell on what could have been. Together with colleagues at the Icahn School of Medicine at Mount Sinai, in New York City, and Georgia Tech, in Atlanta, he is now leveraging advances in DBS hardware and artificial intelligence (AI) to design more precise treatments for depression. The team’s goal is to base treatment on objective neural data rather than the subjective measures—patient accounts, clinical hunches, questionnaires, mood scales—that dominate psychiatry today. The pioneering neurologist Helen S. Mayberg co-led the team with Riva Posse and Christopher Rozell of Georgia Tech. Ultimately, they hope to enable preemptive interventions rather than regretful, after-the-fact adjustments. It’s a new frontier for psychiatry. The field has long been one of the few medical disciplines without objective measures to guide treatment decisions. But with the advent of real-time brain monitoring with AI-driven analytics, that could finally change. “It’s a whole different mindset now,” says Martijn Figee , a Mount Sinai psychiatrist involved in the research. “My intuition, unfortunately, is not 100 percent [accurate],” he acknowledges. “So ultimately, I would always trust the brain more.” Researchers are developing “an automatic alarm system”—an AI-driven tool designed to continuously monitor device output and flag warning signs of relapse. Other research groups are pursuing similar goals, aiming to move beyond the one-size-fits-all approach that has long defined DBS treatment for mental health and replace it with precise stimulation tailored to individual needs. While standardized protocols benefit around 60 percent of people with treatment-resistant depression, they still leave a substantial minority without meaningful relief. No DBS platform is yet approved for treating depression, although some first-generation devices are getting close. Those are rooted in decades-old technology, however, while the Mount Sinai team and others are breaking new ground. They are investigating analytical frameworks that harness brain data to predict relapses, optimize stimulation parameters, or dynamically adjust device output in a responsive, closed-loop manner. “The field is just at a super exciting place,” says Benjamin Davidson , a neurosurgeon at the Sunnybrook Health Sciences Centre, in Toronto. “Things are starting to move at a kind of dizzying pace.” The Origins of DBS for Depression That momentum is a relatively recent phenomenon in a field that, for the past two decades, has progressed through baby steps. Beset by commercial and clinical setbacks, little has changed over the years aside from the adoption of newer surgical techniques. The biggest advance was an imaging-guided surgical approach called tractography that allows for greater precision in electrode placement , informed by connectivity patterns between bundles of brain fibers rather than anatomical landmarks alone. “The story is one of iteration to optimize and refine the targeting using new neuroscience tools,” says Mayberg, who launched the world’s first DBS trial for treatment-resistant depression in the early 2000s at the University of Toronto. “The procedure, as envisioned and published in 2005, is, in essence, what we continue to do today,” she says. Standard method Images: Chris Philpot The standard method of deep brain stimulation (DBS) for depression takes a “set-it-and-forget-it” approach. Doctors set the stimulation parameters during initial visits and adjust them based on patients’ reports on their moods. Sensing system Researchers are also experimenting with new devices that can both stimulate and record signals from the brain. Doctors can then look at patients’ neural activity and adjust theparameters accordingly, sometimes catching signs of a relapse before a patient is aware of them. Auto response Taking the idea of adaptive treatment a step farther, one clinical team is testing a DBS device that records signals from the brain and adjusts the settings automatically. This closed-loop system can respond in real time to fluctuations of mood. DBS is primarily used to manage movement disorders such as essential tremor and Parkinson’s disease . For those ailments, it’s an established and approved therapy that can drastically reduce symptoms such as shaking and muscle rigidity. But Mayberg was inspired by the discovery of a brain region called the subgenual cingulate (SGC), which plays a key role in acute sadness and the effects of antidepressant treatments . She theorized that stimulating this area might alleviate severe, treatment-resistant depression. Her patients were people who had typically tried several types of antidepressant medications and more drastic measures, like electroconvulsive therapy, without finding any relief. While the treatment didn’t work for everyone, many did feel better. Six months after surgery, 12 of the 20-person cohort experienced a profound lifting of their depressive symptoms, with 7 going into full remission. The effect was lasting, with many of those individuals continuing to report benefits to this day, according to Andres Lozano , the University of Toronto neurosurgeon who performed the operations. Mayberg’s hypothesis, it would seem, had proved correct. Learning from DBS Failures Yet, for all its early potential, DBS never gained traction as a mainstream psychiatric treatment. It is occasionally used today for people with debilitating obsessive-compulsive disorder, but the technique remains unapproved for depression and is largely confined to research trials—some of which have ended in dispiriting, high-profile failure . One of the most notable setbacks occurred in 2013. The device company St. Jude Medical set out to replicate the findings of Mayberg’s study in a randomized trial, with plans to enlist 200 participants. But the study was halted prematurely after only 90 patients had been enrolled. An interim analysis had found the therapy was no more effective than sham stimulation. It was a crushing blow to the field. Mayberg and others struggled to continue their research, as funding agencies and the scientific community at large grew increasingly skeptical about the viability of DBS for depression. With the benefit of hindsight, however, many researchers now believe that the St. Jude failure owed more to the study’s design flaws than to any inherent shortcomings of DBS itself. A longer-term follow-up of participants indicated that the treatment’s antidepressant effect steadily strengthened. The trial may simply have measured responses on the wrong timeline. Plus, the neurosurgical placement of the DBS electrodes relied on an outdated understanding of brain connectivity, leading to suboptimal positioning. This may have delayed the therapeutic response past the initial 6- to 12-month assessment window. These missteps likely undermined the study’s results, the trial investigators later concluded . But with the right trial design, most experts anticipate that future studies will succeed. “That could make a huge difference,” says Darin Dougherty , a psychiatrist at Massachusetts General Hospital, in Boston. “Hopefully those lessons learned will be enough to get it over the top.” A patient identified as Sarah participates in a trial at UC San Francisco of the first fully closed-loop DBS system for depression. Maurice Ramirez The biomedical company Abbott (which acquired St. Jude in 2017) is now conducting a do-over study at 22 sites across the United States; Dougherty, Figee, Riva Posse, and other leaders in the field are involved in the effort. The 100-person trial , launched in September 2024, could finally lead to regulatory approval and wider-scale adoption of DBS as a treatment strategy for depression. But Abbott’s study takes a “set-it-and-forget-it” approach, in which stimulation parameters are programmed during initial visits and remain largely unchanged over time. The settings are generally standardized across patients, with a common pulse width and frequency fixed at around 90 microseconds and 130 hertz, respectively. Only the amplitude of stimulation, measured in volts, is typically adjusted to accommodate individual tolerances or symptom severity. While this treatment approach is simple and scalable, it lacks the adaptability to respond to the dynamic nature of depression and its varying symptoms from one individual to the next. This limitation stems in part from a technological shortcoming of the Abbott platform: It can deliver precisely tuned electricity, but it lacks the ability to sense and record neural activity. Without this feedback mechanism, the device cannot detect shifts in brain states that might signal a relapse or a need for parameter adjustments, leaving clinicians reliant on patients’ reports. In contrast, newer DBS devices for epilepsy and movement disorders can both stimulate and record signals. Medtronic’s Percept system and NeuroPace’s Responsive Neurostimulator , for example, offer real-time feedback capabilities, which could allow for more adaptive therapies. Researchers want to bring that flexibility to DBS for depression. How Responsive DBS for Depression Works Consider again the example of Riva Posse’s 67-year-old patient. As described in Nature two years ago , this woman received a research-grade version of the Percept platform that detected signs of neural instability five weeks before her clinical symptoms reappeared. “Before the patient knew anything was wrong—before there was even a hint of behavior that could seem symptomatic of a relapse—the brain signal was headed in the wrong direction,” says Rozell, the neuroengineer at Georgia Tech who developed the AI model used to interpret the woman’s brain activity patterns. Rozell’s model combined a neural network classification scheme (for analyzing brain signals) with a generative causal explainer (for identifying key activity patterns). His work uncovered a distinct biomarker that reliably differentiated between states of depression relapse and recovery. Intriguingly, the biomarker also reflected changes in sleep quality , a telling early indicator since poor sleep patterns often precede the return of depression symptoms. Depression can take many forms: Some people experience it as emotional despondency, while others struggle with obsessive thoughts or a loss of pleasure. But the insights provided by Rozell’s model came too late to help the patient in the moment—they were validated only after her relapse had occurred. To address this limitation, the researchers are now refining the approach for real-time use, aiming to develop what Mayberg calls “an automatic alarm system”—an AI-driven tool designed to continuously monitor device output and flag warning signs of relapse. Such a system could prompt clinicians to intervene before these brain signals escalate into a full-blown depressive episode. Simultaneously, it could filter out false alerts from patients, providing reassurance to users who might otherwise interpret normal stress or anxiety as signs of an impending relapse. Informed by this neurofeedback, psychiatrists might then choose to fine-tune stimulation settings. Or they might proactively recommend additional support, such as psychotherapy or medication adjustments. Closing the Loop for DBS Going one step further, researchers from the University of California, San Francisco, are exploring a fully closed-loop DBS system for depression that removes some of the need for human decision-making. Their approach empowers the device itself to automatically adjust stimulation parameters in real time based on brain activity. Reporting on their first patient—a woman in her 30s named Sarah, who withheld her last name for privacy—the UC San Francisco team documented transformative improvements in her mood, emotional balance, everyday functioning, and overall outlook on life, all in the first week after the implant was switched on. Sarah reports that the closed-loop DBS system restored pleasure and purpose to her life. John Lok “My life took an immediate upward turn,” Sarah said at a 2021 press conference announcing the study’s early findings. “Hobbies I used to distract myself from suicidal thoughts suddenly became pleasurable again. I was able to make small decisions about what to eat without becoming stuck in a morass of indecision for hours,” she said, adding, “the device has kept my depression at bay, allowing me to return to my best self and rebuild a life worth living.” According to Andrew Krystal , the UC San Francisco psychiatrist leading the effort, similar benefits have since been seen in at least two other recipients of the closed-loop DBS device. In each case, patients first undergo an intensive 10-day exploration of their typical neural activity, with 10 electrodes—targeting five locations on each side of the brain—temporarily implanted. During this period, researchers administer a battery of tests to identify the most effective sites for both stimulation and sensing. Once the optimal locations are determined, a second surgery is performed to implant the permanent DBS system, now simplified to just two electrodes: one dedicated to delivering stimulation and the other to recording neural activity. When the recording electrode detects brain activity associated with depression—an event that can happen hundreds of times per day—it prompts the other electrode to deliver a brief burst of electricity lasting a few seconds. This approach stands out not only because it operates automatically in response to real-time brain activity, but also because it employs intermittent, on-demand stimulation rather than the continuous stimulation more commonly employed in DBS for psychiatric conditions. This adaptive and dynamic feedback strategy may be especially well suited to addressing the day-to-day fluctuations in mood and emotional strain that can make depression so hard to live with, notes Katherine Scangos , a psychiatrist who participated in the study. Patients have told her that receiving stimulation at key moments—like during a stressful interaction at the checkout line of a grocery store—helped prevent them from spiraling into distress. “They could really tell that they were getting the stimulation when they needed it most,” says Scangos, who joined the staff of the University of Pennsylvania last year. Identifying the right sites and parameters is an intricate and labor-intensive process, and it’s not always immediately clear which settings will work best, according to UC San Francisco neurosurgeon Kristin Sellers . All the data they collect creates a “curse of bounty,” she says. Yet, in her view, the outcomes demonstrate the effectiveness of taking this personalized approach. “No one has an identical implant,” she says. New Ideas on DBS for Depression Meanwhile, a team at Baylor College of Medicine, in Houston, is pursuing a different approach to customized DBS for depression. The team’s standardized implant consists of two coordinated sets of electrodes: One targets the SGC brain region involved in profound sadness, while the other stimulates a reward-and-motivation hub deep in the brain’s basal ganglia. The customization happens on the front end during the initial surgical procedure, when clinicians temporarily place another 10 electrodes into the brain that take recordings via electroencephalography (EEG). This method tracks brain waves and, as patients undergo various tests and activities, allows the Baylor team to map relevant neural networks and connections. At the same time, the doctors can fiddle with the amplitude, pulse width, frequency, and shape of the stimulation field. “Then we can basically design bespoke stimulation parameters for that individual that are going to move that person’s network in the right direction,” explains Sameer Sheth , the neurosurgeon leading the project. Sheth and his colleagues have treated seven people, with promising initial results . Any of these highly individualized approaches will involve additional surgical procedures and lengthy stays in the hospital. But as Dougherty of Massachusetts General Hospital points out, “We need to do this invasive research first so that we might be able to use noninvasive approaches later.” He imagines a future in which electrodes on the scalp or advanced imaging techniques could identify optimal targets and guide treatment adjustments. Even then, however, if DBS requires highly personalized programming, it will be challenging to make it accessible to the millions of people worldwide in the throes of depression. “The question will always be about the scalability of things,” says Volker A. Coenen , a neurosurgeon at the University of Freiburg Medical Center, in Germany. Coenen is therefore focusing his energy on testing a standardized DBS protocol, one that involves implanting the Vercise Gevia system from Boston Scientific into an area of the brain known as the medial forebrain bundle. In his view, this brain region offers a more direct and efficient pathway to reward systems and emotional-regulation networks. Still, the various brain regions under consideration are all interconnected, which explains why they all seem to offer some degree of therapeutic benefit. “You can perturb the network from different angles,” Coenen says. The Road Ahead for DBS So, which site is best? The answer likely depends on the specific symptoms and underlying brain circuits unique to each individual, says Alik Widge , a psychiatrist and biomedical engineer at the University of Minnesota, in Minneapolis. “There’s no such thing as DBS for depression. There’s DBS for treating specific cognitive-emotional syndromes,” he argues—and different targets will be suited for accessing different aspects of the disorder. Depression can take many forms: Some people experience it as emotional despondency, while others struggle with obsessive thoughts or a loss of pleasure. The optimal stimulation method may also vary. Continuous stimulation may work best for people whose depression follows a steady, persistent course, while intermittent or responsive stimulation may be more appropriate for those whose symptoms fluctuate with daily ups and downs. “It’s like the difference between weather and climate,” says Riva Posse—some people may need an umbrella for passing showers, while others need to reinforce their homes against rising tides. Ultimately, whether they’re tweaking stimulation parameters, finding the best brain targets, or making stimulation respond to real-time brain signals, the goal for researchers in the field remains the same: to create a neurologically precise approach to treating depression in people who have found no relief. “There are so many levers we can press here,” says Nir Lipsman , who directs the Harquail Centre for Neuromodulation at Sunnybrook, in Toronto. He’s confident that at least some of these efforts will unlock new therapeutic possibilities. “The field is experiencing a kind of reset,” Lipsman adds. Now, with neural activity as a guide, the brains of people undergoing DBS should likewise experience a kind of reset as well. This article appears in the August 2025 print issue as “Breaking the Depression Circuit.”",
    "published": "Mon, 30 Jun 2025 13:00:02 +0000",
    "author": "Elie Dolgin",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "A New BCI Instantly Synthesizes Speech",
    "link": "https://spectrum.ieee.org/bci-speech-synthesis",
    "summary": "By analyzing neural signals, a brain-computer interface (BCI) can now almost instantaneously synthesize the speech of a man who lost use of his voice due to a neurodegenerative disease, a new study finds. The researchers caution it will still be a long time before such a device, which could restore speech to paralyzed patients, will find use in everyday communication. Still, the hope is this work “will lead to a pathway for improving these systems further—for example, through technology transfer to industry,” says Maitreyee Wairagkar , a project scientist at the University of California Davis’s Neuroprosthetics Lab. A major potential application for brain-computer interfaces is restoring the ability to communicate to people who can no longer speak due to disease or injury. For instance, scientists have developed a number of BCIs that can help translate neural signals into text . However, text alone fails to capture many key aspects of human speech, such as intonation, that help to convey meaning. In addition, text-based communication is slow, Wairagkar says. Now, researchers have developed what they call a brain-to-voice neuroprosthesis that can decode neural activity into sounds in real time. They detailed their findings 11 June in the journal Nature . “Losing the ability to speak due to neurological disease is devastating,” Wairagkar says. “Developing a technology that can bypass the damaged pathways of the nervous system to restore speech can have a big impact on the lives of people with speech loss.” Neural Mapping for Speech Restoration The new BCI mapped neural activity using four microelectrode arrays. In total, the scientists placed 256 microelectrode arrays in three brain regions, chief among them the ventral precentral gyrus, which plays a key role in controlling the muscles underlying speech. “This technology does not ‘read minds’ or ‘read inner thoughts,’” Wairagkar says. “We record from the area of the brain that controls the speech muscles. Hence, the system only produces voice when the participant voluntarily tries to speak.” The researchers implanted the BCI in a 45-year-old volunteer with amyotrophic lateral sclerosis (ALS), the neurodegenerative disorder also known as Lou Gehrig’s disease. Although the volunteer could still generate vocal sounds, he was unable to produce intelligible speech on his own for years before the BCI. The neuroprosthesis recorded the neural activity that resulted when the patient attempted to read sentences on a screen out loud. The scientists then trained a deep-learning AI model on this data to produce his intended speech. The researchers also trained a voice-cloning AI model on recordings made of the patient before his condition so the BCI could synthesize his pre-ALS voice. The patient reported that listening to the synthesized voice “made me feel happy, and it felt like my real voice,” the study notes. Neuroprosthesis Reproduces a Man’s Speech UC Davis In experiments, the scientists found that the BCI could detect key aspects of intended vocal intonation. They had the patient attempt to speak sets of sentences as either statements, which had no changes in pitch, or as questions, which involved rising pitches at the ends of the sentences. They also had the patient emphasize one of the seven words in the sentence “ I never said she stole my money ” by changing its pitch. (The sentence has seven different meanings, depending on which word is emphasized.) These tests revealed increased neural activity toward the ends of the questions and before emphasized words. In turn, this let the patient control his BCI voice enough to ask a question, emphasize specific words in a sentence, or sing three-pitch melodies. “Not only what we say but also how we say it is equally important,” Wairagkar says. “Intonation of our speech helps us to communicate effectively.” All in all, the new BCI could acquire neural signals and produce sounds with a delay of 25 milliseconds, enabling near-instantaneous speech synthesis, Wairagkar says. The BCI also proved flexible enough to speak made-up pseudo-words, as well as interjections such as “ahh,” “eww,” “ohh,” and “hmm.” The resulting voice was often intelligible, but not consistently so. In tests where human listeners had to transcribe the BCI’s words, they understood what the patient said about 56 percent of the time, up from about 3 percent from when he did not use the BCI. Neural recordings of the BCI participant shown on screen. UC Davis “We do not claim that this system is ready to be used to speak and have conversations by someone who has lost the ability to speak,” Wairagkar says. “Rather, we have shown a proof of concept of what is possible with the current BCI technology.” In the future, the scientists plan to improve the accuracy of the device—for instance, with more electrodes and better AI models. They also hope that BCI companies might start clinical trials incorporating this technology. “It is yet unknown whether this BCI will work with people who are fully locked in”—that is, nearly completely paralyzed, save for eye motions and blinking, Wairagkar adds. Another interesting research direction is to study whether such speech BCIs could be useful for people with language disorders, such as aphasia . “Our current target patient population cannot speak due to muscle paralysis,” Wairagkar says. “However, their ability to produce language and cognition remains intact.” In contrast, she notes, future work might investigate restoring speech to people with damage to brain areas that produce speech, or with disabilities that have prevented them from learning to speak since childhood.",
    "published": "Thu, 19 Jun 2025 16:00:04 +0000",
    "author": "Charles Q. Choi",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "“Cyborg Embryos” Give Insights on Frog, Mouse Brains",
    "link": "https://spectrum.ieee.org/embryo-electrode-array",
    "summary": "Scientists have created cyborg embryos by implanting electrode arrays into the developing brains of frogs, mice, and salamanders. Although the researchers reject implants in human embryos as unethical, they suggest their technology might one day help study and treat neurodevelopmental conditions in children. The stretchable technology at the core of the electrode arrays could record brain activity while remaining soft enough to accommodate the children’s growth. Recording the activity of neurons during brain development can help scientists “understand how the brain transforms from a piece of tissue into a computing machine,” says Jia Liu , an assistant professor of bioengineering at Harvard University. “Virtually all neural technology was developed for the adult brain, but there are so many questions left unanswered about the developing brain.” However, until now, there has been no way for researchers to record neural activity throughout the entire brain on the cellular level on millisecond time scales throughout development. For instance, functional magnetic resonance imaging (fMRI) can scan the entire brain, but it has low resolution in space and time. Previous research using electrode arrays implanted in the brain yielded high resolution in space and time across the brain, but the way in which embryonic brains can rapidly change meant that such devices could not stay accurate over time. Flexible Brain Probes for Embryos In 2015, Liu and his colleagues developed flexible, minimally invasive brain probes . This led Liu to wonder if these pliant electronics might keep up with the mutable nature of embryonic brains. However, our brains and those of other vertebrates are complex, three-dimensional structures, and so embedding any electrodes into their interiors would inflict at least some damage during implantation. In the new study, Liu and his colleagues overcame this problem by implanting electrodes into developing brains when these were still two-dimensional layers of stem cells. Over the course of development, these sheets of cells fold into 3D structures, and the electronic meshes fold with them, distributing electrodes throughout the brain and creating what the scientists dub cyborg embryos. Their research was published online today in Nature . In the team’s first experiments, the electrode arrays were flexible, but they were still too rigid to prove compatible with embryonic development “and easily cut the embryonic tissue,” Liu says. He and his colleagues had to rely on even softer materials called fluorinated elastomers, which could stretch and keep pace with the dynamic tissues. The resulting flexible mesh electronics are less than one micrometer thick. In experiments with frog and salamander embryos, the new arrays can record neuron electrical activity on millisecond scales across the brain throughout development in a stable, continuous manner. Exams of neural tissues and the activity of stress-related genes found these electrodes had no discernible impact on brain development or function, nor did behavioral tests, such as whether tadpoles with the arrays avoided incoming objects. In preliminary tests, the scientists also implanted their devices into mouse embryos and newborn rats and recorded neural activity in developing mammalian brains. Analysis of the frog embryos showed how neural activity varied over development. The early brain displayed slow, global synchronized activity, but over time, different brain regions showed distinct and faster dynamics, and ultimately cell-specific behavior. “Studying how neural dynamics change during development could prove useful for developing better machine learning algorithms,” Liu says. The salamanders the researchers examined, known as axolotls , possess the ability to regenerate virtually any part of their body, including neural systems. In experiments where the scientists cut the tails of axolotl tadpoles, they found that during regeneration , neural activity in the brain significantly increased, becoming similar to what was seen during the early stages of embryonic development. “This suggests that brain activity might play a role in regeneration,” Liu says. “It would be very interesting to see if we can control the central nervous system to facilitate this kind of tissue recovery or injury recovery.” Liu stresses that “we are not talking about implants in human embryos, which is completely unethical and not our intention at all.” However, he suggests their electronics could still find use in children to help study or even treat neurodevelopmental conditions “in applications where you need the technology to be stretched and extremely soft to accommodate brain development.”",
    "published": "Wed, 11 Jun 2025 15:00:04 +0000",
    "author": "Charles Q. Choi",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Doctors Could Hack the Nervous System With Ultrasound",
    "link": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes",
    "summary": "Inflammation: It’s the body’s natural response to injury and infection, but medical science now recognizes it as a double-edged sword. When inflammation becomes chronic, it can contribute to a host of serious health problems, including arthritis, heart disease, and certain cancers. As this understanding has grown, so too has the search for effective ways to manage harmful inflammation. Doctors and researchers are exploring various approaches to tackle this pervasive health issue, from new medications to dietary interventions. But what if one of the most promising treatments relies on a familiar technology that’s been in hospitals for decades? Enter focused ultrasound stimulation (FUS), a technique that uses sound waves to reduce inflammation in targeted areas of the body. It’s a surprising new application for ultrasound technology, which most people associate with prenatal checkups or diagnostic imaging. And FUS may help with many other disorders too, including diabetes and obesity. By modifying existing ultrasound technology, we might be able to offer a novel approach to some of today’s most pressing health challenges. Our team of biomedical researchers at the Institute of Bioelectronic Medicine (part of the Feinstein Institutes for Medical Research), in Manhasset, N.Y., has made great strides in learning the electric language of the nervous system. Rather than treating disease with drugs that can have broad side effects throughout the body, we’re learning how to stimulate nerve cells, called neurons, to intervene in a more targeted way. Our goal is to activate or inhibit specific functions within organs. The relatively new application of FUS for neuromodulation , in which we hypothesize that sound waves activate neurons, may offer a precise and safe way to provide healing treatments for a wide range of both acute and chronic maladies. The treatment doesn’t require surgery and potentially could be used at home with a wearable device. People are accustomed to being prescribing pills for these ailments, but we imagine that one day, the prescriptions could be more like this: “Strap on your ultrasound belt once per day to receive your dose of stimulation.” How Ultrasound Stimulation Works Ultrasound is a time-honored medical technology. Researchers began experimenting with ultrasound imaging in the 1940s, bouncing low-energy ultrasonic waves off internal organs to construct medical images, typically using intensities of a few hundred milliwatts per square centimeter of tissue. By the late 1950s, some doctors were using the technique to show expectant parents the developing fetus inside the mother’s uterus . And high-intensity ultrasound waves, which can be millions of milliwatts per square centimeter, have a variety of therapeutic uses, including destroying tumors . The use of low-intensity ultrasound (with intensities similar to that of imaging applications) to alter the activity of the nervous system, however, is relatively unexplored territory. To understand how it works, it’s helpful to compare FUS to the most common form of neuromodulation today, which uses electric current to alter the activity of neurons to treat conditions like Parkinson’s disease. In that technique, electric current increases the voltage inside a neuron, causing it to “fire” and release a neurotransmitter that’s received by connected neurons, which triggers those neurons to fire in turn. For example, the deep brain stimulation used to treat Parkinson’s activates certain neurons to restore healthy patterns of brain activity. How It Works In FUS, by contrast, the sound waves’ vibrations interact with the membrane of the neuron, opening channels that allow ions to flow into the cell, thus indirectly changing the cell’s voltage and causing it to fire. One promising use is transcranial ultrasound stimulation , which is being tested extensively as a noninvasive way to stimulate the brain and treat neurological and psychiatric diseases. We’re interested in FUS’s effect on the peripheral nerves—that is, the nerves outside the brain and spinal cord. We think that activating specific nerves in the abdomen that regulate inflammation or metabolism may help address the root causes of related diseases, rather than just treating the symptoms. FUS for Inflammation Inflammation is something that we know a lot about. Back in 2002, Kevin Tracey , currently the president and CEO of the Feinstein Institutes, upset the conventional wisdom that the nervous system and the immune system operate independently and serve distinct roles. He discovered the body’s inflammatory reflex : a two-way neural circuit that sends signals between the brain and body via the vagus nerve and the nerves of the spleen. These nerves control the release of cytokines , which are proteins released by immune cells to trigger inflammation. Tracey and colleagues found that stimulating nerves in this neural circuit suppressed the inflammatory response. The discoveries led to the first clinical trials of electrical neuromodulation devices to treat chronic inflammation and launched the field of bioelectronic medicine. Hacking the Immune System Tracey has been a pioneer in treating inflammation with vagus nerve stimulation (VNS), in which electrical stimulation of the vagus nerve activates neurons in the spleen. In animals and humans, VNS has been shown to reduce harmful inflammation in both chronic diseases such as arthritis and acute conditions such as sepsis. But direct VNS requires surgery to place an implant in the body, which makes it risky for the patient and expensive. That’s why we’ve pursued noninvasive ultrasound stimulation of the spleen. Working with Tracey, collaborators at GE Research , and others, we first experimented with rodents to show that ultrasound stimulation of the spleen affects an anti-inflammatory pathway , just as VNS does, and reduces cytokine production as much as a VNS implant does. We then conducted the first-in-human trial of FUS for controlling inflammation . We initially enrolled 60 healthy people, none of whom had signs of chronic inflammation. To test the effect of a 3-minute ultrasound treatment, we were measuring the amount of a molecule called tumor necrosis factor (TNF), which is a biomarker of inflammation that’s released when white blood cells go into action against a perceived pathogen. At the beginning of the study, 40 people received focused ultrasound stimulation, while 20 others, serving as the control group, simply had their spleens imaged by ultrasound. Yet, when we looked at the early data, everyone had lower levels of TNF, even the control group. It seemed that even imaging with ultrasound for a few minutes had a moderate anti-inflammatory effect! To get a proper control group, we had to recruit 10 more people for the study and devise a different sham experiment, this time unplugging the ultrasound machine. After the subjects received either the real or sham stimulation, we took blood samples from all of them. We next simulated an infection by adding a bacterial toxin to the blood in the test tubes, then measured the amount of TNF released by the white blood cells to fight the toxin. The results, which we published in the journal Brain Stimulation in 2023, showed that people who had received FUS treatments had lower levels of TNF than the true control group. We saw no problematic side effects of the ultrasound: The treatment didn’t adversely affect heart rate, blood pressure, or the many other biomarkers that we checked. The results also showed that when we repeated the blood draw and experiment 24 hours later, the treatment groups’ TNF levels had returned to baseline. This finding suggests that if FUS becomes a treatment option for inflammatory diseases, people might require regular, perhaps even daily, treatments. One surprising result was that it didn’t seem to matter which location within the spleen we targeted—all the locations we tried produced similar results. Our hypothesis is that hitting any target within the spleen activates enough nerves to produce the beneficial effect. What’s more, it didn’t matter which energy intensity we used. We tried intensities ranging from about 10 to 200 mW per cm 2 , well within the range of intensities used in ultrasound imaging; remarkably, even the lowest intensity level caused subjects’ TNF levels to drop. Our big takeaway from that first-in-human study was that targeting the spleen with FUS is not just a feasible treatment but could be a gamechanger for inflammatory diseases. Our next steps are to investigate the mechanisms by which FUS affects the inflammatory response, and to conduct more animal and human studies to see whether prolonged administration of FUS to the spleen can treat chronic inflammatory diseases. FUS for Obesity and Diabetes For much of our research on FUS, we’ve partnered with GE Research, whose parent company is one of the world’s leading makers of ultrasound equipment. One of our first projects together explored the potential of FUS as a treatment for the widespread inflammation that often accompanies obesity, a condition that now affects about 890 million people around the world. In this study, we fed lab mice a high-calorie and high-fat “Western diet” for eight weeks. During the following eight weeks, half of them received ultrasound stimulation while the other half received daily sham stimulation. We found that the mice that received FUS had lower levels of cytokines —and to our surprise, those mice also ate less and lost weight. In related work with our GE colleagues, we examined the potential of FUS as a treatment for diabetes , which now affects 830 million people around the world. In a healthy human body, the liver stores glucose as a reserve and releases it only when it registers that glucose levels in the bloodstream have dropped. But in people with diabetes, this sensing system is dysfunctional, and the liver releases glucose even when blood levels are already high, causing a host of health problems. Hacking the Metabolic System For diabetes, our ultrasound target was the network of nerves that transmit signals between the liver and the brain: specifically, glucose-sensing neurons in the porta hepatis , which is essentially the gateway to the liver. We gave diabetic rats 3-minute daily ultrasound stimulation over a period of 40 days. Within just a few days, the treatment brought down the rats’ glucose levels from dangerously high to normal range. We got similar results in mice and pigs, and published these exciting results in 2022 in Nature Biomedical Engineering . Those diabetes experiments shed some light on why ultrasound had this effect. We decided to zero in on a brain region called the hypothalamus , which controls many crucial automatic body functions, including metabolism, circadian rhythms, and body temperature. Our colleagues at GE Research started investigating by blocking the nerve signals that travel from the liver to the hypothalamus in two different ways—both cutting the nerves physically and using a local anesthetic. When we then applied FUS, we didn’t see the beneficial decrease in glucose levels. This result suggests that the ultrasound treatment works by changing glucose-sensing signals that travel from the liver to the brain—which in turn changes the commands the hypothalamus issues to the metabolic systems of the body, essentially telling them to lower glucose levels. The next steps in this research involve both technical development and clinical testing. Currently, administering FUS requires technical expertise, with a sonographer looking at ultrasound images, locating the target, and triggering the stimulation. But if FUS is to become a practical treatment for a chronic disease, we’ll need to make it usable by anyone and available as an at-home system. That could be a wearable device that uses ultrasound imaging to automatically locate the anatomical target and then delivers the FUS dose: All the patient would have to do is put on the device and turn it on. But before we get to that point, FUS treatment will have to be tested clinically in randomized controlled trials for people with obesity and diabetes. GE HealthCare recently partnered with Novo Nordisk to work on the clinical and product development of FUS in these areas. FUS for Cardiopulmonary Diseases FUS may also help with chronic cardiovascular diseases, many of which are associated with immune dysfunction and inflammation. We began with a disorder called pulmonary arterial hypertension , a rare but incurable disease in which blood pressure increases in the arteries within the lungs. At the start of our research, it wasn’t clear whether inflammation around the pulmonary arteries was a cause or a by-product of the disease, and whether targeting inflammation was a viable treatment. Our group was the first to try FUS of the spleen in order to reduce the inflammation associated with pulmonary hypertension in rats. The results, published last year, were very encouraging. We found that 12-minute FUS sessions reduced pulmonary pressure , improved heart function, and reduced lung inflammation in the animals in the experimental group (as compared to animals that received sham stimulation). What’s more, in the animals that received FUS, the progression of the disease slowed significantly even after the experiment ended, suggesting that this treatment could provide a lasting effect. One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation. This study was, to our knowledge, the first to successfully demonstrate an ultrasound-based therapy for any cardiopulmonary disease. And we’re eager to build on it. We’re next interested in studying whether FUS can help with congestive heart failure, a condition in which the heart can’t pump enough blood to meet the body’s needs. In the United States alone, more than 6 million people are living with heart failure, and that number could surpass 8 million by 2030. We know that inflammation plays a significant role in heart failure by damaging the heart’s muscle cells and reducing their elasticity. We plan to test FUS of the spleen in mice with the condition. If those tests are successful, we could move toward clinical testing in humans. The Future of Ultrasound Stimulation We have one huge advantage as we think about how to bring these results from the lab to the clinic: The basic hardware for ultrasound already exists, it’s already FDA approved, and it has a stellar safety record through decades of use. Our collaborators at GE have already experimented with modifying the typical ultrasound devices used for imaging so that they can be used for FUS treatments. Once we get to the point of optimizing FUS for clinical use, we’ll have to determine the best neuromodulation parameters. For instance, what are the right acoustic wavelengths and frequencies? Ultrasound imaging typically uses higher frequencies than FUS does, but human tissue absorbs more acoustic energy at higher frequencies than it does at lower frequencies. So to deliver a good dose of FUS, researchers are exploring a wide range of frequencies. We’ll also have to think about how long to transmit that ultrasound energy to make up a single pulse, what rate of pulses to use, and how long the treatment should be. In addition, we need to determine how long the beneficial effect of the treatment lasts. For some of the ailments that researchers are exploring, like FUS of the brain to treat chronic pain , a patient might be able to go to the doctor’s office once every three months for a dose. But for diseases associated with inflammation, a regular, several-times-per-week regimen might prove most effective, which would require at-home treatments. For home use to be possible, the wearable device would have to locate the targets automatically via ultrasound imaging. As vast databases already exist of human ultrasound images from the liver, spleen, and other organs, it seems feasible to train a machine-learning algorithm to detect targets automatically and in real time. One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation. A few startups are working on building such wearable devices, which could take the form of a belt or a vest. For example, the company SecondWave Systems , which has partnered with the University of Minnesota, in Minneapolis, has already conducted a small pilot study of its wearable device, trying it out on 13 people with rheumatoid arthritis and seeing positive outcomes. While it will be many years before FUS treatments are approved for clinical use, and likely still more years for wearable devices to be proven safe enough for home use, the path forward looks very promising. We believe that FUS and other forms of bioelectronic medicine offer a new paradigm for human health, one in which we reduce our reliance on pharmaceuticals and begin to speak directly to the body electric. This article appears in the July 2025 print issue as “Hacking the Nervous System With Ultrasound.”",
    "published": "Mon, 09 Jun 2025 13:00:03 +0000",
    "author": "Sangeeta S. Chavan",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Precise Nerve Stimulation Gives Relief Without Side Effects",
    "link": "https://spectrum.ieee.org/precision-vagus-nerve-stimulation",
    "summary": "The vagus nerve is a key communication line between the brain and organs like the heart and lungs—and stimulating it can ease conditions including epilepsy and arthritis. But this electrical therapy often hits the wrong neural fibers, causing side effects like coughing or voice changes. A new study finds that researchers can steer stimulation toward specific fibers and away from others by overlapping high-frequency currents inside the nerve. Tested in pigs, the technique boosted signals to the lungs while sparing the throat, reducing unwanted effects without sacrificing therapeutic impact. If the results hold up in humans, the approach could make vagus nerve therapies safer and more precise, says Stavros Zanos , a cardiologist and bioengineer at the Feinstein Institutes for Medical Research in Manhasset, N.Y. “It’s the basis for what we think is an improved way of stimulating the nerve,” he says. A prototype for human testing is now under development. Zanos and his colleagues reported their findings in May in Nature Communications . Vagus nerve stimulation (VNS) is already FDA-approved for epilepsy, depression, and stroke rehabilitation , and is under investigation for a range of other conditions. In recent weeks, researchers have reported encouraging findings for VNS in easing chronic cluster headaches and supporting rehabilitation in people with spinal cord injuries—and ongoing studies are probing its potential in conditions including inflammatory illnesses and heart conditions as well. The therapy works by sending mild electrical pulses through a device implanted in the neck, nudging the body’s internal circuits toward a more balanced state. But the vagus nerve is a complex bundle of many different fibers—around 100,000 in total—each heading to different organs. Current devices can’t easily tell one fiber from another, often lighting up off-target pathways and triggering side effects. The most common side effects are linked to unwanted activation of nerve fibers that control the voice box and throat. That is why researchers are now searching for more selective ways to stimulate the correct fibers, delivering benefits without the drawbacks. Turning Selectivity Into Strategy A range of more discriminating strategies have been explored , including tweaking the shape and timing of electrical pulses and using multi-contact electrodes to maneuver currents in specific directions. Some approaches aim to block the large fibers responsible for side effects, while others try to selectively activate smaller fibers linked to specific organ functions. But results have been mixed. Many techniques show partial success in animals, but few offer both the precision and reliability needed for clinical translation. Zanos is hopeful that his team’s new approach—which harnesses a technique called intermittent interferential current stimulation —will fare better. Instead of stimulating the whole nerve at once, the technique sends slightly offset high-frequency signals to different electrodes embedded in a cuff implanted around the vagus nerve in the neck. These signals then interact to produce localized interference, effectively steering activation toward chosen nerve fibers and away from others. “It’s a cool concept,” says Kip Ludwig , a biomedical engineer at the University of Wisconsin-Madison who was not involved in the study. “They’re using the spot interference to make it a bit harder to activate the stuff you don’t want—which is kind of paradigm” since researchers had long assumed that, at the point of maximal interference, nerve activation would be enhanced, not suppressed. Reducing Side Effects The researchers conducted experiments with pigs, which confirmed that targeted nerve activation is possible. By delivering two high-frequency, sinusoidal signals through different pairs of electrodes—one set at 20 kilohertz and the other at 22 kHz—they created a subtle interference pattern inside the vagal nerve bundle. Like a slow ripple in the ocean riding atop fast-moving swells, the combined signals produced a gentle 2-kHz wave that reshaped the nerve’s internal electric field. By adjusting the signal strength and electrode placement, the team could then navigate the stimulation zone as desired. As a proof of concept, the researchers selectively turned on fibers linked to the lungs, prompting the pigs to slow their breathing. And they did so without the stimulation spilling over into nearby fibers that control the throat and voice box, as shown by minimal muscle activity in those areas. In principle, targeting lung-related fibers in this way could help treat conditions like asthma or chronic anxiety in which controlled breathing plays a therapeutic role. But according to Zanos, the real promise of the technique lies in its flexibility, with the potential to improve VNS-based treatments across a wide range of conditions—anywhere specific fiber groups can be precisely targeted. “The same principle can be used for any organ,” he says, “as long as the fibers that project to that organ are also somewhat distinct.” The approach is still a ways off from human use. As co-author and computer scientist Vojkan Mihajlović points out, his health technology group at Imec —which developed the stimulation technique and created the integrated circuit to enable its precise delivery—still needs to fine-tune a range of parameters, from electrode placement to the exact frequencies and amplitudes of the stimulation currents. “There are knobs we can further turn to optimize the stimulation,” he says. But with a few more tweaks, vagus therapies might finally hit just the right nerve—selectively.",
    "published": "Wed, 04 Jun 2025 13:00:03 +0000",
    "author": "Elie Dolgin",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "A Pen With Magnetic Ink Could Test for Parkinson’s Disease",
    "link": "https://spectrum.ieee.org/parkinsons-disease-pen",
    "summary": "Parkinson’s disease can be difficult to diagnose , but one common symptom of the progressive neurodegenerative condition is small, frequent tremors in the hands. Now, with an eye toward screening and early detection of the disease, researchers have developed what they call a diagnostic pen to detect those hand motions. The pen does not write in the traditional sense. Instead, a flexible magnetic tip and ferrofluid ink convert movement into fluctuations in their magnetic field, taking advantage of what is known as the magnetoelastic effect . The magnetic flux produces an electrical current in a conductive coil built into the barrel of the pen. In a small pilot study, that electrical signal was used to train a convolutional neural network to accurately differentiate between the writing of patients with Parkinson’s disease and a healthy group. The diagnostic pen and human study were presented in Nature Chemical Engineering today. “While the underlying sensing mechanisms are well established, the true strength of this work lies in how the authors have ingeniously integrated them into a functional device,” says Pradeep Sharma , an engineer at the University of Houston who studies soft magnetic materials similar to the one used in the tip of the new stylus; he was not involved in the current research. Who Created the Diagnostic Pen? Because the device is capable of detecting small, high-frequency movements, it’s a good fit for examining hand tremors, says Gary Chen , lead author of the study and a Ph.D. candidate at the University of California, Los Angeles. The authors are primarily based in the bioengineering research group led by Jun Chen (no relation), which has been investigating uses for the magnetoelastic effect for around five years. “We view it as a very promising technology,” says Gary Chen, “but as we indicate in our paper, our current study has some shortcomings.” Chiefly, larger follow-up studies with a more diverse pool of subjects are necessary to answer questions about the device and its potential applications. In the pilot study, training data came from only two patients with Parkinson’s disease and 10 healthy control participants, and validation added an additional four participants, including one with Parkinson’s. In addition to validating early results, further research could also help determine if the pen is able to distinguish between Parkinson’s and other conditions with tremor symptoms, and whether it can identify different stages of the same disease. What’s more, the researchers want to study whether the subject’s native language or dominant handedness affect the results, which might be important for clinical applications. How Does the Diagnostic Pen Work? The new pen’s tip is made of small neodymium magnets mixed into Ecoflex , a brand of silicone rubber advertised for production of prosthetics and film props. The body contains a reservoir of ferrofluid “ink,” which is surrounded by a barrel with a built-in coil of conductive yarn. As a user draws or writes with the stylus, deformations in the tip change the magnetic field, and movement of the ferrofluid makes the pen sensitive to acceleration both across a writing surface or in the air. Minute magnetic fluctuations produce a current in the coil, and changes to that current were analyzed rather than the on-paper results of experimental writing or drawing tasks, as is commonly done in today’s neurological assessments. Participants were asked to perform several tasks, including drawing loops and writing letters. Normalized data was used to train several types of machine learning algorithms, and the best performing analysis came from a one-dimensional convolutional neural network, which reached over 96 percent accuracy in identifying subjects with Parkinson’s. Current fluctuations in testing were sometimes less than a microampere, and the study version of the pen connected to a current amplifier with a cable. Eventually, the group would like to transfer data wirelessly from pen to computer or smartphone, says Chen. Other Applications for Magnetoelastic Materials Soft magnetic materials similar to that used for the tip of the pen, sometimes called magnetorheological elastomers , are being investigated for a variety of uses, including how their properties change when exposed to an external magnetic field. The Jun Chen research group has also looked at using magnetoelastic materials for neck-worn patches for speech assistance and more general human-machine interfaces , among other applications. Earlier this year, a study estimated that there are around 12 million people living with Parkinson’s disease globally, a number that will double by 2050. Chen emphasizes the importance of larger-scale studies for evaluating the usefulness of the pen. “Admitting that does not compromise the promise,” he says, “though it may take many years or decades to finally get it delivered.”",
    "published": "Mon, 02 Jun 2025 15:00:04 +0000",
    "author": "Greg Uyeno",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "Giving Plants E-Tattoos Could Be Good for Their Health",
    "link": "https://spectrum.ieee.org/plant-health-monitoring-electronic-tattoo",
    "summary": "Imagine a future in which farmers can tell when plants are sick even before they start showing symptoms. That ability could save a lot of crops from disease and pests—and potentially save a lot of money as well. A team of researchers in Singapore and China have taken a step toward that possibility with their development of ultrathin electronic tattoos—dubbed e-tattoos—to study plant immune responses without the need for piercing, cutting, or bruising leaves. The e-tattoo is a silver nanowire film that attaches to the surface of plant leaves. It conducts a harmless alternating current—in the microampere range—to measure a plant’s electrochemical impedance to that current. That impedance is a telltale sign of the plant’s health. Lead author Tianyiyi He , an associate professor of the Shenzhen MSU-BIT University’s Artificial Intelligence Research Institute, says that a healthy plant has a characteristic impedance spectrum—it’s as unique to the plant as a person’s fingerprints. “If the plant is stressed or its cells are damaged, this spectrum changes in shape and magnitude. Different stressors—dehydration, immune response—cause different changes.” This is because plant cells, He explains, are like tiny chambers with fluids passing through them. The membranes of plant cells act like capacitors, resisting the flow of electrical current. “When cells break down—like in an immune response—the current flows more easily, and impedance drops,” He adds. Detecting Plant Stress Early with E-Tattoos Different problems yield different electrical responses: Dehydration, for example, looks different than an infection. Changes in a plant’s impedance spectrum means that something is not right—and by looking at where and how that spectrum changed, He’s team could spot what the problem was, up to three hours before physical symptoms started appearing. The researchers conducted the work in a controlled environment. He says that a lot more research is needed to help scientists spot a wider array of responses to stressors in the real-world environment. But this is a good step in that direction, says Eleni Stavrinidou, principal investigator in the electronic plants research group from Linköping University’s Laboratory of Organic Electronics in Sweden, who was not involved in the work. He’s team published its work on 4 April in Nature Communications . The team tested the film on lab-grown thale cress ( Arabidopsis thaliana ) for 14 days. They mixed the nanowires in water so that they could transfer smoothly to the plant, by simply dripping the mix onto the leaves. Then they applied the e-tattoo in two different positions—side by side on a single leaf and on opposite faces of a leaf—to see how the current would flow. Then, with a droplet of galistan (a liquid metal alloy composed of gallium , indium, and tin), they attached a copper wire with the diameter of a human hair to the e-tattoo’s surface to apply an AC current from a small generator. He’s team collected data every day to see how plants would react. Control plants showed a consistent spectrum over the course of two weeks, but plants that received immune-response stimulants (such as ethanol) or were wounded or dehydrated showed different patterns of electrical impedance spectra. He says liquid-carried silver nanowires worked better than other highly conductive metals such as copper or nickel because they were not soft enough to entirely “glue” to plants’ leaves and stay perfectly plastered even as the leaf bends or wrinkles. And in the case of thale cresses, they also have tricomas, tiny hairlike structures that usually protect and keep leaves from losing too much water. Tricomas, He explains, hinder perfect attachment since they make a leaf’s surface uneven—but silver nanowires managed to get around the problem in a better way than other materials. “Even the smallest gaps between the film and the leaf can mess with electrical impedance spectroscopy readings,” He says. The silver nanowire e-tattoo proved to be versatile, too. It also worked with coleus, polka-dot plants, and benth—a close relative to tobacco, field mustard, and sweet potato. The team noticed the material did not block sun rays, which means it did not interfere with photosynthesis. Advancements in Plant Impedance Spectroscopy This isn’t the first time tattoos or electrical impedance spectroscopy have been used for plants, says Stavrinidou. What’s new in the study, Stavrinidou says, “is the validation—they show this approach works on delicate plants like Arabidopsis and links clearly to immune responses.” Stavrinidou says that ensuring that impedance spectrum changes tell exactly what is wrong with a plant in an unknown scenario is still a challenge. “But this paper is a strong step in that direction.” At scale, the technique could be another tool to help farmers spot problems in their crops. But the technique will need improvement to get there, He says. Researchers can, for example, redesign the circuits to optimize them. “We can further shrink it to smaller sizes and add wireless communication to build IoT (Internet of Things) systems so we don’t have to link every plant to a wire. Everything is going to be wireless, connected, and transmitted to the cloud,” He says. To Stavrinidou, this work is a step toward a long-term goal: the development of sensors that correlate biological signals to physiological states—stress, disease, or growth—non-invasively. “As more of these studies are done, we’ll be able to map out what different impedance signals mean biologically. That opens the door to sensors that are not just diagnostic, but predictive—a game-changer for agriculture,” Stavrinidou says.",
    "published": "Mon, 02 Jun 2025 12:00:04 +0000",
    "author": "Meghie Rodrigues",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "IEEE Awardee’s Tech Prevents Chemotherapy-Induced Nerve Damage",
    "link": "https://spectrum.ieee.org/cooling-tech-nerve-damage",
    "summary": "Aishwarya Bandla tries to center her work around passion, people, and purpose, following the Japanese concept of ikigai , or a sense of purpose. For the IEEE senior member, that involves transforming patient care through innovative health technology. Bandla is developing a means to help prevent nerve damage in cancer patients resulting from chemotherapy treatment, a condition known as chemotherapy-induced peripheral neuropathy Chemotherapy is known to cause a variety of side effects including nausea, fatigue, and hair loss, according to the American Cancer Society . But one lesser-known effect is neuropathy, Bandla says. Aishwarya Bandla Employer: Paxman Coolers of Huddersfield, England Title: Clinical innovation manager Member grade: Senior member Alma maters: Anna University in Chennai, India, and the National University of Singapore in Queenstown Peripheral neuropathy nerve damage—which also can stem from diabetes, vitamin deficiencies, and other causes—affects mostly the tips of the patient’s hands and feet. Symptoms range from persistent tingling to excruciating pain. Currently there are no approved preventative measures for the condition; cancer patients try to manage it with painkillers or, in severe cases, reducing or stopping their chemotherapy, Bandla says. Bandla is the clinical innovation manager at Paxman Coolers , a medical equipment manufacturer headquartered in Huddersfield, England. She is developing a wearable device that cools a person’s limbs. Called the Paxman limb cryocompression system (PLCS), it’s designed to help prevent nerve damage from certain types of intravenous chemotherapy drugs. The cold temperature slows blood flow to the area, allowing less of the injected medication to reach the nerves there. Bandla, who is based in Singapore, is also a principal investigator at the N.1 Institute for Health , the National University of Singapore (NUS), and at the National University Cancer Institute of Singapore . An active IEEE volunteer, she follows ikigai in her work with the organization, she says, and she encourages other young professionals to do the same. She has overseen the launch of several career development and mentorship programs for IEEE Women in Engineering Singapore , IEEE Region 10 Women in Engineering , and IEEE Region 10 Young Professionals . “Being an IEEE member,” she says, “has helped me nurture my purpose in rallying my efforts toward creating meaningful impact.” For “her leadership in patient-centric health technology innovation and inspiring IEEE Young Professionals to drive meaningful change,” she is the recipient of this year’s IEEE Theodore W. Hissey Outstanding Young Professional Award . The award is sponsored by the IEEE Photonics and IEEE Power & Energy societies, as well as IEEE Young Professionals. “This recognition fuels me to continue the work IEEE is doing globally to make the world a better place,” she says. Engineering is a superpower Bandla had a difficult time deciding whether to pursue medicine or engineering as a career, she says, but she chose the latter because it’s “a superpower that can help you create things to make life better.” After earning her bachelor’s degree in electrical and electronics engineering in 2009 from Anna University , in Chennai, India, she joined software engineering company Infosys in Mysuru, India, as a technical consultant. She left three years later after being accepted into the neurotechnology doctoral program at NUS in Queenstown. Neurotechnology encompasses ways of directly engaging with the human brain and nervous system, including brain-computer interfaces, magnetic resonance imaging, and brain-wave monitors. Bandla conducted her research under biomedical engineer Nitish V. Thakor , who specializes in developing brain-monitoring technologies and neuroprostheses . The IEEE Life Fellow is a professor of biomedical engineering at Johns Hopkins University , in Baltimore. He also is director of the Singapore Institute for Neurotechnology , SINAPSE, a collaboration among six research universities including Johns Hopkins, NUS, and the University of Patras , in Greece. Under Thakor’s tutelage, Bandla began her work in developing the technology she is involved with today. Using technology to address nerve damage In 2012 Bandla and other researchers from Thakor’s lab met with neurologist Einar Wilder Smith and oncologist Raghav Sundar from National University Hospital in Kent Ridge, Singapore, to explore how the technology could help cancer patients with peripheral neuropathy. During chemotherapy, patients are injected with an individualized drug mixture that kills fast-dividing cells or prevents them from multiplying by damaging the cells’ DNA. But the mixture also can attack healthy cells and damage nervous-system structures, causing pain and sensitivity in the patient’s hands and feet, as explained in an article published in the International Journal of Molecular Sciences . In the meeting, the team learned about a scalp-cooling technology that helps prevent a different side effect: hair loss. A special cap is placed on the patient’s head to cool the scalp. Inspired by that cold cap, the team set out to develop similar technology for the hands and feet. But first, in 2014, the SINAPSE lab conducted a clinical trial with National University Hospital to see if cooling the limbs would help patients with peripheral neuropathy. Existing localized cryotherapy machines used for sports therapy—which circulate ice-cooled liquid to cool an area on the body, were tested on 15 chemotherapy patients at the hospital. The team found that patients could not comfortably tolerate temperatures below 22 °C during the three-hour treatment, Bandla says. “Being an IEEE member has helped me nurture my purpose in rallying my efforts toward creating meaningful impact.” She suggested conducting another clinical trial, this time testing cryocompression tools rather than cryotherapy ones. Cryocompression is used for sports therapy and rehab. It combines cooling and compression—which helps reduce swelling. In the second trial, the team found that patients could tolerate temperatures as low as 11 °C for three hours, Bandla says. The second trial ended in 2017. Bandla earned her Ph.D. that year but continued to work on the project as a SINAPSE research fellow. In 2018 the team members began another clinical study, testing if they could safely cool a patient’s scalp and limbs simultaneously to prevent multiple side effects at once. Throughout the five-year trial period, Bandla collected data to understand the best way to deliver cooling therapy that was safe, comfortable, and effective. The feedback she received from patients, caregivers, and the medical staff demonstrated a clear need for a device to use in the chemotherapy suite. After the pilot trials ended in 2019, the team began designing a device alongside Richard Paxman and his team at Paxman Coolers, who leveraged their expertise in cryotherapy for side-effect management. The portable PLCS connects to four insulated wraps, each containing a bladder filled with coolant. The wraps cover a patient’s forearms, hands, shins, and feet and include velcro flaps that can be adjusted for a better fit. The PLCS circulates the coolant through the wraps and powers the compression. It also keeps the coolant temperature at 11 °C. During every chemotherapy cycle, 30 minutes before the medication is administered, the wraps are placed on the patient’s forearms and shins to begin the cooling process. After the session ends, the device is used on the patient for 30 more minutes, Bandla says. The team was granted two U.S. patents for the PLCS. In 2022 Bandla joined Paxman as a research and development manager, and she was promoted to clinical innovation manager two years later. The impact her work has had keeps her motivated to continue, she says. The PLCS is being tested in a large-scale clinical trial in 25 U.S. hospitals in collaboration with the National Cancer Institute . Two years ago Bandla attended a social innovation camp for school students in India. Aishwarya Bandla Starting her IEEE volunteer journey Thakor introduced Bandla to IEEE. An active member of the IEEE Engineering in Medicine and Biology Society , he encourages his students to participate in its conferences and to publish papers in its journals. Bandla says volunteering with IEEE was a no-brainer for her. Her volunteerism began in 2012 with IEEE Women in Engineering Singapore. In 2019 she became its chair and launched the WIE Singapore Networking Night to help build camaraderie between the IEEE Singapore Section and technologists in industry, academia, and government. The annual event includes panel discussions. In 2021 Bandla joined the IEEE Region 10 Women in Engineering committee as the technical and Young Professionals lead. There she helped launch MentorHer , an eight-week program in which experts help their mentees design and implement a professional development plan. Bandla created the program’s framework. “After the pilot program was completed in 2021, we received nice feedback from participants,” she says. “Many people said they interacted with people they wouldn’t normally work with and enjoyed the experience.” In 2020 Bandla began participating in virtual events and conferences held by Region 10’s Young Professionals group as a speaker and panel moderator. Last year she became the chair. Guiding young professionals Volunteering for the YP group is special to her, she says, because she has been able to “build a community and help other young professionals become well-rounded leaders and decision-makers.” She helped develop the Career and Leadership Aid Program (CLAP) at the Region 10 Students, Young Professionals,Women in Engineering,Life Members Congress held in August in Tokyo. She introduced the concept of ikigai to young professionals by centering the event around it. The congress included what she calls a “human library” session. Ten IEEE members from different engineering fields were positioned around the meeting room, and attendees had an hour to learn about each of the “human books.” The group received positive feedback, with participants saying they enjoyed the focus on professional and leadership development. They said they liked how extraordinary the event was, in particular the “human library” session. Based on the success of the CLAP event, the team is building an IEEE Hive . The immersive professional development program is available for students and early career professionals at technical conferences and congresses around the world. The ability to make an impact, build a community, and connect with people resonates with her, Bandla says. “Volunteering with IEEE gives me so much energy!” she says. This article appears in the September 2025 print issue.",
    "published": "Fri, 30 May 2025 18:00:04 +0000",
    "author": "Joanna Goodrich",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  },
  {
    "title": "This E-Tattoo for Your Face Gauges Mental Strain",
    "link": "https://spectrum.ieee.org/electronic-tattoo",
    "summary": "Feeling stressed? Overworked? A new forehead-mounted electronic tattoo may soon offer real-time insights into your mental state. Researchers at the University of Texas at Austin have developed a wireless, ultrathin, wearable device that adheres to the skin like a temporary tattoo and monitors brain signals and eye movements to gauge mental strain. Think of it as a souped-up Oura ring for the face—and it might one day help pilots, surgeons, race-car drivers, and military personnel stay sharp under pressure. “In those kinds of high-stakes, high-demand tasks, we could have real-time monitoring and decoding of mental workloads,” says Nanshu Lu , a biomedical engineer at UT-Austin who co-led development of the forehead sensor. That kind of data, she says, could be used to adjust task assignments, reallocate personnel before errors occur, or even trigger alerts when someone’s cognitive burden reaches a critical threshold. Lu and her colleagues described the technology today in the journal Device . What Can E-Tattoos Be Used For? The new wearable builds on more than a decade of work by Lu’s lab to refine “electronic tattoos”— soft, skin-like devices that can track everything from blood pressure to alcohol intake without bulky hardware. Her team was among the first to demonstrate that ultrathin, stretchable electronics could adhere seamlessly to the skin, offering a comfortable and unobtrusive way to monitor the body’s electrical activity . Earlier versions were designed for applications such as heart monitoring , using chest-worn arrays of sensors to capture electrical and mechanical signals from the heart. But engineering a version for the forehead posed a fresh set of heady challenges. Lu’s team had to create motion-resistant electrodes that wouldn’t slip or lose signal quality due to facial expressions or sweat, yet would remain comfortable enough for long shifts, often under helmets or headsets. And the technology had to pick up subtle electrical activity emanating from the brain’s prefrontal lobe, the hub of reasoning, decision making, and information processing—signals that are far weaker than those generated by the heart. The solution: a postage-stamp-sized patch that sits just above and between the eyebrows, in the “third eye” position of the forehead. This central module houses the battery, while flexible electrodes stretch outward in a translucent circuit toward the temples, cheeks, and behind the ears. These electrodes are strategically positioned to detect shifts in visual gaze and to stabilize the signal. As in Lu’s past designs, the electrodes are printed onto carbon-doped polyurethane. But this latest iteration adds a soft, sticky coating that boosts signal fidelity and helps the device stay put, even through perspiration, prolonged wear, and pressure-packed situations. Testing the E-Tattoo By combining electroencephalography (EEG) and electrooculography (EOG), the device captures both brainwave activity and eye movement—two key markers of cognitive workload. A machine-learning algorithm then analyzes the incoming data, classifying whether the wearer is in a low or high mental-load state based on subtle shifts in neural and ocular patterns. In lab tests, volunteers performed memory and arithmetic tasks while wearing the contraption. The device reliably distinguished moments of mental ease from periods of strain, and it maintained accuracy even as participants moved their heads and blinked, underscoring the device’s potential for use in dynamic, real-world settings like operating rooms or cockpits. Still, true field-readiness will take more validation, particularly during activities that involve unpredictable or full-body motion, notes Yael Hanein , a physicist at Tel Aviv University in Israel and the co-founder of X-trodes , a wearable bioelectronics company. “There is a lot of work to do, but it’s a very nice step forward in establishing the properties and potential of this platform,” Hanein says. “The next step is really to move from the desktop and show you can walk with these things and still measure reliable EEG.” How the Forehead Device Compares In recent years, Lu and her colleagues had explored other approaches to real-time stress monitoring. A 2022 study introduced a palm-mounted e-tattoo that captured skin conductance and motion. And late last year, her team reported a scalp-printing method that allowed a biocompatible, conductive ink to record EEG signals through buzz-cut hair. Both approaches represented steps forward in comfort and wearability, but neither offered the level of integrated, multi-modal sensing packed into the new forehead device. The palm sensor tracked physiology but not brain activity, while the scalp ink recorded EEG but missed eye tracking. The latest design pulls double duty, capturing neural and ocular data in a lightweight patch designed for everyday wear —although i t may look like something out of Star Trek. “I very much like that this face tattoo measures a variety of biomarkers,” says Dmitry Kireev , a bioelectronics researcher from the University of Massachusetts at Amherst who wrote about e-tattoo sensors for the March 2025 issue of IEEE Spectrum . Is the E-Tattoo Fashionable or Functional? Kireev, who was not involved in the latest study, acknowledges that the circuit-laced patch isn’t exactly subtle—its look falls somewhere between sci-fi cosplay and cyberpunk spa day. But for him, that bionic flair is part of the device’s charm. “It’s something I would try,” he says. “The shape and form is kind of cool.” Mind you, not everyone will share Kireev’s fashion sense—which is why Lu and her team are working on systems with transparent electrodes and discreet, hairline-concealed hardware. Such cosmetic refinements could make the technology more workplace-friendly, especially in settings where bold facial markings would clash with the dress code, Lu notes. But even the current version, she argues, could be worth the small sacrifice in style if it keeps workers sharp—and safe—on the job. While facial ink is often dismissed as a “ job stopper ,” this e-tattoo might be the rare exception—raising performance, not eyebrows. This article appears in the July 2025 print issue as “A Mood Ring for Your Face.”",
    "published": "Thu, 29 May 2025 15:33:03 +0000",
    "author": "Elie Dolgin",
    "topic": "biomedical",
    "collected_at": "2025-10-08T14:03:21"
  }
]