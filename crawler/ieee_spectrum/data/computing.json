[
  {
    "title": "Noncontact Motion Sensor Brings Precision to Manufacturing",
    "link": "https://spectrum.ieee.org/aeva-motion-sensor-precision-manufacturing",
    "summary": "Aeva Technologies , a developer of lidar systems based in Mountain View, Calif., has unveiled the Aeva Eve 1V, a high-precision, noncontact motion sensor built on its frequency modulated continuous wave (FMCW) sensing technology. The company says that the Eve 1V measures an object’s motion with accuracy, repeatability, and reliability—all without ever making contact with the material. That last point is key for the Eve 1V’s intended environment: Industrial manufacturing . Today’s manufacturing lines are under pressure to deliver faster production, tighter tolerances, and zero defects, often while working with a wide variety of delicate materials. Traditional tactile tools such as measuring wheels and encoders can slip, wear out, and cause costly downtime. Many noncontact alternatives, while promising, are either too expensive or fall short in accuracy and reliability under real-world conditions, says Mina Rezk , cofounder and chief technology officer at Aeva. “Eve 1V was built to solve that exact gap: A compact, eye-safe, noncontact motion sensor that delivers submillimeter-per-second velocity accuracy without touching the material, so manufacturers can eliminate slippage errors, avoid material damage, and reduce maintenance-related downtime, enabling higher yield and more predictable operations,” Rezk says. Unlike traditional lidar that sends bursts of light and waits for those bursts to return to make measurements, FMCW continuously emits a low-power laser while sweeping its frequency. By comparing outgoing and returning signals, it detects frequency shifts that reveal both distance and velocity in real time. The additional measurement of an object’s velocity to its position in three-dimensional space makes FMCW a type of 4D lidar. Eve 1V is the second member of its Eve 1 family, following the launch of the Eve 1D earlier this year . The Eve 1D is a compact displacement sensor capable of detecting movement at the micrometer scale, roughly 1/100 the thickness of a human hair. “Together, Eve 1D and Eve 1V show how we can take the same FMCW perception platform and tailor it for different industrial needs: Eve 1D for distance measurement and vibration detection, and Eve 1V for precise velocity and length measurement,” Rezk says. Future applications could extend into robotics, logistics, and consumer health, where noncontact sensing may enable the detection of microvibrations on human skin for accurate pulse and blood-pressure readings. FMCW Lidar for Precision Manufacturing The company’s core FMCW architecture, originally developed for long-range 4D lidar for automobiles, can be adjusted through software and optics for highly precise motion sensing at close range in manufacturing, according to Rezk. This flexibility means the system can track extremely slow movements, down to fractions of a millimeter per second, in a factory setting, or it can monitor faster motion over longer distances in other applications. By avoiding physical contact, Eve 1V eliminates wear and tear, slippage, contamination, or the need for physical access to the part. “That delivers three practical advantages in a factory: One, maintenance-free operation with no measuring wheels to replace or recalibrate; two, material friendliness—you can measure delicate, soft, or textured surfaces without risk of damage, and three, operational robustness—no slippage errors and fewer stoppages for service,” Rezk says. Put together, that means more uptime, steady throughput, and less scrap, he adds. When measuring velocity, engineers often rely on one of three tools: encoders, laser velocimeters, or camera-based systems. Each has its strengths and its drawbacks. Traditional encoders are low-cost but can wear down over time. Laser-based velocity-measurement systems, while precise, tend to be large and expensive, making them difficult to implement widely. And camera-based approaches can work for certain inspection tasks, but they usually require markers, controlled lighting, and complex processing to measure speed accurately. Rezk says that the Eve 1V system offers a balance of these options. It provides precise and consistent velocity measurements without contacting material, making it compact, safe, and simple to install. Its outputs are comparable with existing encoder systems, and because it doesn’t rely on physical contact, it requires minimal maintenance. This approach helps cut down on wasted energy from slippage, eliminates the need for maintenance tied to parts that wear out, and ultimately lowers long-term operating costs—especially when compared with traditional contact-based systems or expensive laser options. This method avoids stitching together frame-by-frame comparisons and resists interference from sunlight, reflections, or ambient light. Built on silicon photonics, it scales from micrometer-level sensing to millimeter-level precision over longer ranges. The result is clean, repeatable data with minimal noise—outperforming legacy lidar and camera-based systems. Aeva is expecting to begin full production of the Eve 1V in early 2026. The Eve 1V reveal follows a recent partnership with LG Innotek , a components subsidiary of South Korea’s LG Group, under which Aeva will supply its Atlas Ultra 4D lidar for automobiles, with plans to expand the technology into consumer electronics, robotics, and industrial automation.",
    "published": "Tue, 07 Oct 2025 14:00:03 +0000",
    "author": "Kate Park",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "PsiQuantum Plans Quantum Supercomputer That Runs on Light",
    "link": "https://spectrum.ieee.org/psyquantum-supercomputer",
    "summary": "In an industry where timelines are often fuzzy and subject to revision, the quantum-computing company PsiQuantum has set itself an aggressive target. The California-based startup has committed to building a fault-tolerant quantum computer with roughly a million qubits by 2027. And the company is now in the process of assembling its first fully fledged prototype in a warehouse in California. Crossing the million-qubit threshold could start to realize quantum computing’s promise to revolutionize areas like materials science and chemistry. Founded by a quartet of academics from British universities in 2016, the company has raised US $1.7bn to build an optical quantum computer based on silicon photonics . Using photons to store and manipulate quantum information raises a very different set of challenges compared with those of “matter-based” approaches like superconducting qubits , trapped ions, or neutral atoms, says chief scientific officer and cofounder Pete Shadbolt . But the company is betting that by building on top of mature networking and photonics technology, it can reach scale ahead of its competitors. Realizing that vision has required PsiQuantum to make breakthroughs in materials science, develop bespoke cryogenics technology and industrialize the production of its photonic chips. But with the key components now in place, the company has started putting them together to build its “Alpha System” at a new facility in Milpitas, Calif., that will officially open later this year. “This system in California will be orders of magnitude more complex than any system we’ve tested previously,” says Shadbolt. “This is the first time that we’re building a networked system of large numbers of photon sources with real silicon and real [cryogenic] cabinets.” Planning for scale from the ground up One of the key factors that differentiates PsiQuantum from its competitors, says Shadbolt, is that the company has focused from the start on building a full-scale, fault-tolerant quantum computer. There was initially hope in the industry that smaller “noisy intermediate-scale quantum” (NISQ) computers could do useful work without error correction, but today there’s growing consensus true usefulness will become possible only with full fault tolerance. Shadbolt says PsiQauntum operated on this assumption from the start, and this drove its decision to focus on optical approaches. Reaching the millions of qubits necessary to implement error correction at scale requires you to solve four key challenges—cooling, control, connectivity, and manufacturability, he says—all of which are easier with photonics than competing hardware. Matter-based qubits are highly sensitive to temperature fluctuations and electromagnetic radiation, which means they need to be chilled to near-absolute zero using either dilution refrigerators or laser-based cooling systems. In contrast, photons are resistant to both heat and radiation, which means that in principle they can operate as qubits at room temperature. In practice, PsiQuantum’s hardware is still kept at cryogenic temperatures. The design relies on superconducting photon detectors that operate between 2 and 4 kelvins, but achieving these temperatures is much easier, says Shadbolt. While the dilution refrigerators required by superconducting qubits can house at most one or two chips, PsiQuantum has designed cryogenic cabinets the size of a server rack that can hold roughly 250. In the company’s new facility, three of these cabinets will be cooled by a cryoplant made by the engineering giant Linde . The cryogenic cabinet for PsiQuantum’s upcoming quantum supercomputer. Colby Macri/PsiQuantum Photons’ resistance to heat and radiation also makes it possible to pack control electronics close to the qubits, says Shadbolt, something that is proving much harder for matter-based approaches. And crucially, photons can be transmitted over standard telecom fiber, which makes networking chips together much simpler. The company recently demonstrated the ability to transmit qubits over 250 meters of fiber with 99.7 percent fidelity, says Shadbolt. Getting ready for mass manufacturing However, says Shadbolt, one of the biggest challenges for building a large-scale quantum computer is manufacturing. Most quantum computers are bespoke devices. But by building on top of mature silicon-photonics technology, PsiQuantum has been able to create a commercial fabrication process for its chips, detailed in a Nature paper earlier this year, in collaboration with Global Foundries. “The key insight that we were founded on is that you can’t change the semiconductor industry very much,” says Shadbolt. “If you want to make millions of devices at a high level of maturity, you need to leverage the trillion dollars in 50 years that’s gone into the semiconductor industry.” Getting the company’s chips production-ready wasn’t simple, as it incorporated novel designs and materials, including superconducting photon detectors and ultrafast optical switches. But Global Foundries is now churning out thousands of PsiQuantum’s chips at a commercial semiconductor fab in Malta, N.Y. While all the components have been independently tested, the system under construction in Milpitas will be the first true test of the company’s overall architecture. Shadbolt says he hopes to have the system cold by the end of the year, ready to start experiments by early 2026. Crucially, these experiments will not involve running quantum algorithms, says Mercedes Gimeno-Segovia , VP for system architecture. Companies like Google and IBM have used smaller prototypes to demonstrate quantum supremacy on toy problems, but Gimeno-Segovia says NISQ machines behave so differently from fault-tolerant ones that these kinds of experiments provide little insight. Instead, PsiQuantum’s Alpha System is designed to test whether the behavior of the system matches the predictions made by the company’s models, which will be crucial for designing future systems. “We’re not trying to impress anybody, frankly,” she adds. “What we’re trying to do is say, Do we understand the system that we’re building? And the telltale fact that tells us whether we do or not, is whether we can predict this behavior.” The trouble of flighty photons Simon Devitt , research director at the Centre for Quantum Software and Information at the University of Technology, Sydney, thinks PsiQuantum’s focus on skipping the NISQ regime and jumping straight to full fault tolerance is a good approach. But he points out that the company had little choice. PsiQuantum’s system relies on linear optics, where photon generation is inherently nondeterministic, says Devitt, which means gate operations fail roughly 25 to 50 percent of the time. PsiQuantum has come up with clever ways to reduce this number by running many photon-generation attempts and then picking out successful ones, something known as multiplexing. But this only partly solves the problem, and remaining gate failures must be dealt with by error correction. The inside of PsiQuantum’s new facility in Milpitas, Calif. Colby Macri/PsiQuantum This means it’s essentially impossible to run quantum algorithms on the hardware until fault tolerance is achieved, says Devitt. More importantly, he adds, it means a huge amount of the error-correction budget is used up fixing these gate failures. This leaves little leeway for other sources of error, such as detector inefficiencies or optical losses from coupling chips to fiber. “Photons are extremely easy to lose,” says Devitt. “So that’s really where a lot of the questions arise, as to whether or not they can get their devices working to the required accuracies so that they don’t overwhelm the error correction.” Optical loss is the biggest source of errors in PsiQuantum’s system after gate errors, says Devitt. Efforts to reduce this hinge on three key components—waveguides, photon detectors, and optical switches. Based on the data published in the company’s recent Nature paper or shared on X , he says the first two appear ready, but losses on the company’s switches are still too high. “The question is, is it a material-science limitation they’re hitting?” he adds. “Or is it just about purity and fabrication?” Shadbolt is confident it’s the latter. He sees no major hurdles; instead, it’s going to take thousands of small, incremental improvements to the design and geometry of the chips, alongside tweaks to the fabrication process. Optical loss is heavily impacted by the precision with which you can build components, Shadbolt adds, so being able to lean on Global Foundries’ world-leading tools and processes gives them a major advantage. “It’s very challenging to make these components with good enough performance, but we have a great track record of improving performance,” he says. “We know the steps that we’re going to take to close the remaining gap, and we have high confidence in our ability to do that.” Paul Smith-Goodson , principal analyst at Moor Insights & Strategy, thinks PsiQuantum has a realistic chance of meeting its lofty goals. While it still has a way to go in cutting losses, and a huge job ahead when it comes to integrating all these components at scale, he thinks the company is on track. For him, the bigger challenge may be financial rather than technical. Despite the massive sums the company has raised, this will cover only a few prototypes, and they will need to raise significantly more to build a full-scale machine. “It takes a lot of money to do what they’re doing,” he says.",
    "published": "Tue, 07 Oct 2025 12:00:04 +0000",
    "author": "Edd Gent",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Quantum Sensors Sidestep the Heisenberg Uncertainty Principle",
    "link": "https://spectrum.ieee.org/sidestep-heisenberg-uncertainty",
    "summary": "A cornerstone of quantum physics is uncertainty. Heisenberg’s uncertainty principle states that the more precisely you pinpoint the position of a particle, the less precisely you can know its momentum at the same time, and vice versa. However, a new study reveals that scientists have now discovered a way to sidestep this quantum trade-off. This could lead to next-generation quantum sensors that can simultaneously measure both the position and momentum of particles with unprecedented precision. “We take it for granted that the Heisenberg uncertainty principle is a fundamental law that cannot be broken,” says Tingrei Tan , a research fellow at the University of Sydney Nano Institute and School of Physics. “I want to be clear that we have not broken the Heisenberg uncertainty principle. But in certain cases, we can get around it.” Other kinds of uncertainty are known in quantum physics as well. For example, if an atom is excited to a higher orbital, one cannot precisely know both the energy and the duration of its excited state at the same time. Previously, scientists have taken advantages of these kinds of trade-offs to “ squeeze ” or reduce the uncertainty in the measurements of a given variable while increasing the uncertainty in the measurement of another variable the researchers can ignore. For example, researchers have shown they can make qubits—the key components in quantum computers—highly resistant to a common source of error known as bit flip , when a qubit’s state flips from 1 to 0 or the reverse, while making them more vulnerable to a different common source of error known as phase flip, when a qubit switches between one of two opposite phases. They make this trade-off because having just one common source of error to correct instead of two can drastically simplify quantum-computer design. Now Tan and his colleagues have found they can make a similar swap to sidestep Heisenberg’s uncertainty principle. “We are increasing the precision with which we can measure momentum and position at the same time within a small sensing range, while increasing the uncertainty with which we can measure those properties simultaneously outside of that sensing range,” Tan says. Since the kind of quantum sensing applications the researchers want to carry out with this new technique are roughly on the atomic scale anyhow, gaining precision on a tiny scale while losing it on larger ones is a worthwhile trade-off. It’s a bit like using a magnifying glass—you care about what you want to focus on under the lens, not around it. Inspiration from quantum computing In the new study, the researchers experimented with a single ytterbium ion held in place and controlled with electric and magnetic fields. They generated sets of specific patterns of vibrations in the ion known as grid states or Gottesman-Kitaev-Preskill states. Scientists have long investigated GKP states for quantum error-correction strategies in quantum computing . In the case of trapped ions, because of the way in which the patterns of vibrations are entangled together, any small disturbance would generate changes in these patterns, which quantum error-correction techniques can detect and account for in quantum computations. In the new study, by preparing grid states in the vibrations of a ytterbium ion, the researchers found they could simultaneously measure both its position and momentum with a precision beyond the standard quantum limit—the best achievable using only classical sensors. Tingrei Tan in the Sydney Nanoscience Hub at the University of Sydney, where he manages the Quantum Control Laboratory. Fiona Wolf/The University of Sydney “We are borrowing techniques from quantum error correction to do quantum sensing,” Tan says. Tan says a key application for this research is spectroscopy — the analysis of atoms or molecules based on the specific wavelengths of light they absorb or reflect, which finds wide use in medical research, navigation in environments where GPS does not work , and searches for dark matter . In general, however, “this opens up a whole new way to do precision measurements,” Tan says. “It was taken for granted that there were pairs of variables that you could not measure precisely at the same time, and our work provides a framework for how we can get around those limitations.” The scientists detailed their findings on 24 September in the journal Science Advances .",
    "published": "Mon, 06 Oct 2025 14:00:05 +0000",
    "author": "Charles Q. Choi",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "The Future of the Grid: Simulation-Driven Optimization",
    "link": "https://spectrum.ieee.org/multiphysics-simulation-power-grid",
    "summary": "This is a sponsored article brought to you by COMSOL . Simulation software is useful in the analysis of new designs for improving power grid resilience, ensuring efficient and reliable power distribution, and developing components that integrate alternative energy sources, such as nuclear fusion and renewables. The ability to simulate multiple physical phenomena in a unified modeling environment gives engineers a deeper understanding of how different components of the grid interact with and affect each other. For example, when designing the various components of grid infrastructure, such as transformers and transmission lines, multiphysics electromagnetic field analysis is essential for ensuring the safety of the surrounding individuals and environment. Understanding thermal behavior, another phenomenon involving multiple physics, is equally necessary for the design of grid components where heat dissipation and thermal stresses can significantly affect performance and lifespan. Structural and acoustics simulation, meanwhile, is used to predict and mitigate issues like transformer vibration and noise — an important practice for ensuring the longevity and reliability of grid components. Multiphysics simulation provides critical insight into the complex interactions at play within power grid components, enabling engineers to virtually test and optimize future grid designs. Electric breakdown and corona discharge analyses are particularly vital for high-voltage transmission lines, as such phenomena can compromise the performance of their insulation systems. Simulation allows development teams to predict where such events are likely to happen, enhancing the design of insulators and other components where the goal is to minimize energy loss and material degradation. As a real-world example, one leading manufacturer uses the COMSOL Multiphysics® simulation software software to develop magnetic couplings, a noncontact alternative to mechanical transmission that enables power transfer without the inherent friction-based limitations of continual contact. While the advantage of friction-free power transmission means that magnetic couplings have found applications in a broad range of technologies, including offshore wind turbines, these systems must be developed carefully to avoid degradation. By employing highly nonlinear hysteresis curves and applying its own material temperature dependences for magnetic loading, the manufacturer’s development team has successfully used multiphysics simulation to help prevent the permanent magnets from reaching critical temperatures, which can cause irreversible demagnetization and compromise the reliability of the designs. Additionally, due to the diverse nature of use cases for magnetic couplings, the company’s design engineers must be able to interchange shapes and materials of magnets to meet customer requirements without building costly and time-consuming prototypes — rendering multiphysics simulation a powerful approach for characterizing configurations, providing virtual prototypes of their designs, and ultimately reducing the price for customers while remaining vigilant on fine details. These examples show just a few of the ways that coupling multiple interacting physics within a single model can lead to successful simulation of real-world phenomena and thereby provide insights into current and future designs. Lightning strikes a tower’s shielded wires. The induced voltage on the three-phase conductors is computed using electromagnetic field analysis. COMSOL Improving Reliability with Digital Twins & Simulation Apps Engineering teams can also use simulation technology to create more efficient, effective, and sustainable power grids by creating digital twins. A digital twin contains a high-fidelity description of a physical product, device, or process — from the microscopic to the macroscopic level — that closely mirrors its real counterpart . For every application, the digital twin is continuously receiving information, ensuring an up-to-date and accurate representation. With this technology, grid operators and their equipment suppliers can predict which components are most likely to fail, enabling them to schedule maintenance and replacement more efficiently and thereby improving grid reliability. Digital twins can be made for equipment ranging from power sources including solar cells and wind turbines to power distribution systems and battery energy storage. An offshore wind farm where lightning strikes one of the turbine blades. The electric field on the turbine towers, seawater, and seabed is shown. COMSOL The most recent modeling and simulation technology provides power and energy companies with tools for creating digital twins in the form of standalone simulation apps, which significantly increases the number of users who have access to advanced simulation technology. By including only relevant functionality in a standalone simulation app, colleagues with no modeling and simulation experience can utilize this technology without needing guidance from the modeling specialist. Furthermore, the use of data-driven surrogate models in simulation apps enables near-instantaneous evaluation of what would otherwise be time-consuming simulations — which means that simulation technology can now be used in a real-world setting. Digital twins, in the form of standalone apps, bring the power of simulation to the field, where grid operators can utilize real-time performance information to ensure grid reliability. For instance, one organization that works with local power companies to analyze equipment maintenance and failure built a custom app based on a multiphysics model it had developed to predict cable faults and improve troubleshooting efficiency. While engineers have been utilizing simulation in labs for decades, cable failure occurs in the field, and onsite troubleshooting personnel are responsible for assessing these failure conditions. With this in mind, an engineer at the organization developed the simulation app using the Application Builder in COMSOL Multiphysics ®. Temperature distribution in a battery energy storage system (BESS). COMSOL The app features relevant parameters that troubleshooting personnel with no prior simulation experience can easily modify. Field technicians enter cable data and select the type of fault, which modifies the multiphysics model in real time, allowing the app to evaluate and output the data necessary to understand the condition that led to the fault. The app then produces a reported potential and electric field, which leads the technicians to an informed decision regarding whether they need to replace or repair the cable. Following the app’s successful deployment, the engineer who developed it stated, “The simulation app plays a key role in cable maintenance. It makes the work of our field technicians more efficient by empowering them to confidently assess and repair faults.” Routine physical tests of grid equipment cannot fully reflect conditions or determine failure types in many situations, as a large number of complex factors must be considered, such as cable structure and material, impurities in the cable, voltage fluctuation, and operating conditions and environments. As a result, simulation has proven to be indispensable in many cases for collecting accurate cable health assessments — and now in the form of custom apps, it is more accessible than ever. Generating Nuclear Solutions Simulation has also been heavily integrated into the design process of various components related to the nuclear industry. For example, simulation was used to help design generator circuit breakers (GCBs) for nuclear power plants. GCBs must be reliable and able to maintain performance even after long periods of inactivity. The COMSOL Multiphysics ® software can be used to improve the current-carrying capacity of the GCBs, which can offer protection from current surges and provide dependable electricity generation. The design of nuclear fusion machines like tokamaks has also benefitted from the use of simulation. These devices must be able to withstand high heat fluxes and plasma disruptions. COMSOL Multiphysics ® has been used to help engineers predict the effects of these problems and come up with design solutions, such as adding a structural support system that can help reduce stress and survive challenging conditions. Engineering the Grid of Tomorrow The development of next-generation power grid systems is a complex and dynamic process that requires safe, reliable, and affordable testing. Multiphysics simulation technology can play a major role in future innovations for this industry, enabling engineers to anticipate and analyze the complex interactions happening inside these devices while building upon the existing infrastructure to address the demands of modern-day consumption. COMSOL Multiphysics is a registered trademark of COMSOL AB.",
    "published": "Mon, 06 Oct 2025 10:00:05 +0000",
    "author": "Bjorn Sjodin",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Muscle-Bound Micromirrors Could Bring Lidar to More Cars",
    "link": "https://spectrum.ieee.org/mems-lidar",
    "summary": "Five years ago, Eric Aguilar was fed up. He had worked on lidar and other sensors for years at Tesla and Google X, but the technology always seemed too expensive and, more importantly, unreliable. He replaced the lidar sensors when they broke—which was all too often, and seemingly at random—and developed complex calibration methods and maintenance routines just to keep them functioning and the cars drivable. So, when he reached the end of his rope, he invented a more robust technology—what he calls the “most powerful micromachine ever made.” Aguilar and his team at startup Omnitron Sensors developed new microelectromechanical systems (MEMS) technology that he claims can produce more force per unit area than any other. By supplying new levels of power to micromirrors , the technology is capable of precisely steering lidar’s laser beams, even while weathering hazardous elements and the bumps and bangs of the open road. With chips under test by auto-industry customers, Omnitron is now modifying the technology to reduce the power consumed by AI data centers. Lidar, a scanning and detection system that uses lasers to determine how far away objects are, is often adopted by self-driving cars to find obstacles and navigate. Even as the market for lidar is expected to grow by 13.6 percent annually , lidar use in the automotive industry has remained relatively stagnant in recent years, Aguilar says, in part because the technology’s lifespan is so short. Vibration from bumpy roads and severe environmental conditions are the biggest reliability killers for automotive lidar, says Mo Li , who studies photonic systems at the University of Washington. The optical alignment within the lidar package atop self-driving cars is delicate—tremors from a poor paving job could physically alter where the mirrors sit in the housing, potentially misaligning the beam and causing the system to fail. Or temperature fluctuations could cause parts to expand or contract with the same unfortunate outcome, he explains. Aguilar wondered which part broke most often and found the culprit to be scanners, the parts responsible for angling small mirrors that direct the laser beam out of the system’s housing. He wanted to make scanners that could withstand the tough conditions lidar faces, and silicon flexures stood out as a solution. These structures act like springs and allow for meticulous control of the mirrors within lidar systems without wearing out, as the standard metal springs do, Aguilar claims. Designing a better chip Aguilar hoped the new material would be the answer to the problem that plagued him, but even silicon springs didn’t make lidar systems as robust as they needed to be to withstand the elements they faced. To make lidar even stronger, the team at Omnitron aimed to design a more powerful MEMS chip by increasing the amount of force the device can apply to control the mirrors in the lidar array. And they claim to have achieved it—their chip can exert 10 times as much force per unit area on an actuator that positions a micromirror or other sensor component as the current industry standard, they say. That extra force allows for extremely valuable control in fine adjustment. To reach this achievement, they had to dig deep—literally. Omnitron’s micromirrors steer lidar beams and could find use in data centers. Omnitron In this MEMS device, the mirror and its actuator are etched into a single silicon wafer. On its nonmirror end, the actuator is covered with tiny, closely spaced plates that fit between trenches in the wafer, like the interlocking teeth of two combs. To move the mirror, voltage is applied, and electrostatic forces angle the mirror into a specific position by moving the plates up and down within the trenches as the electric field pulls across the trench sidewalls . The force that can be used to move the mirror is limited by the ratio of depth to width of the trenches, called aspect ratio. Put simply, the deeper the trenches are, the more electrostatic force can be applied to an actuator, which leads to a higher range of motion for the sensor. But fabricating deep, narrow trenches is a difficult endeavor. Overcoming this limiting factor was a must for Aguilar. Aguilar says Omnitron was able to improve on the roughly 20:1 aspect ratio he notes is typical for MEMS (other experts say 30:1 or 40:1 is closer to average these days), reaching up to 100:1 through experimentation and prototyping in small university foundries across the United States “That’s really our core breakthrough,” Aguilar says. “It was through blood, sweat, tears, and frustration that we started this company.” The startup has secured over US $800 million in letters of intent from automotive partners, Aguilar says, and is two months into an 18-month plan to prove that it can produce its chips at full demand rate. Even after verifying production capabilities, the technology will have to face “very tough” safety testing for thousands of consecutive hours in realistic conditions, like vibrations, thermal cycles, and rain, before it can come to market, Li says. Saving power In the meantime, Omnitron is applying its technology to solve a different problem faced by a different industry. By 2030, AI data centers are expected to require around 945 terawatt-hours to function—more than the country of Japan consumes today. The problem is “the way data moves,” Aguilar says. When data is sent from one part of the data center to another, optical signals are converted into electrical signals, rerouted, and then turned back to optical signals to be sent on their way. This process, which takes place in systems called network switches, burns huge amounts of power. Google’s solution, called Apollo , is to keep the data packets in the form of optical signals for the duration of their travels, which yields a 40 percent power savings, the company claims . Apollo does so by using an array of mirrors to direct the data. Aguilar is planning to make the process even more efficient using dense arrays of Omnitron’s more-powerful mirrors. Doing so could quadruple the amount of data each network switch could route by increasing the number of channels in each switch from 126 to 441, Aguilar says. Omnitron is still early in its data-center implementation, so it’s not yet clear to what degree this technology can really improve on Google’s Apollo. However, following a “critical design review” in mid-September, “one of the world’s top AI hyperscalers has requested our mirrors for their next-generation switch,” Aguilar says. “This is proof that Omnitron solves a problem that even the biggest AI infrastructure companies can’t address in-house.” And there may be even more applications to come. Omnitron has received feelers from the defense industry, space companies, and groups interested in methane detection, says Aguilar. “It’s pretty cool seeing the people knock on our door for this, because I was just focusing on lidar,” he says.",
    "published": "Wed, 01 Oct 2025 15:00:03 +0000",
    "author": "Perri Thaler",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Array-Scale MUT Simulations Powered by the Cloud",
    "link": "https://events.bizzabo.com/764786",
    "summary": "Designing and optimizing ultrasound transducers—whether PMUTs or CMUTs—requires accuracy at scale. Yet traditional simulation approaches are often constrained to individual cells or limited structures, leaving important array-level effects poorly understood until expensive and time-consuming testing begins. This gap can lead to longer development cycles and higher risk of failed devices. In this webinar, we will introduce the improved approach: full array-scale MUT simulations with fully coupled multiphysics. By leveraging Quanscient’s cloud-native platform, engineers can model entire transducer arrays with all relevant physical interactions (electrical, mechanical, acoustic, and more) capturing system-level behaviors such as beam patterns and cross-talk that single-cell simulations miss. Cloud scalability also enables extensive design exploration. Through parallelization, users can run Monte Carlo analyses, parameter sweeps, and large-scale models in a fraction of the time, enabling rapid optimization and higher throughput in the design process. This not only accelerates R&D but ensures more reliable designs before fabrication. The session will feature real-world case examples with detailed insights of the methodology and key metrics. Attendees will gain practical understanding of how array-scale simulation can greatly improve MUT design workflows reducing reliance on costly prototypes, minimizing risk, and delivering better device performance. Join us to learn how array-scale MUT simulations in the cloud can improve MUT design accuracy, efficiency, and reliability. Register now for this free webinar!",
    "published": "Mon, 29 Sep 2025 14:47:06 +0000",
    "author": "Quanscient",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Configuring and Controlling Complex Test Equipment Setups for Silicon Device Test and Characterization",
    "link": "https://events.bizzabo.com/762929",
    "summary": "In this webinar, we will explore efficient, accurate, and scalable techniques for analog and mixed-signal device testing using reconfigurable test setups. As semiconductor devices grow more complex, engineers face the challenge of validating performance and catching edge cases under tight schedules. Test setups often include oscilloscopes, waveform generators, network analyzers, and more, potentially from different vendors with unique automation and configuration considerations. In order to keep pace with semiconductor validation requirements, multi-channel test setups designed for flexibility and performance can help engineers scale effectively. Register now for this free webinar!",
    "published": "Thu, 25 Sep 2025 14:16:46 +0000",
    "author": "Liquid Instruments",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Qualcomm’s New Chip Promises AI Agents in Your PC",
    "link": "https://spectrum.ieee.org/qualcomm-snapdragon-x2",
    "summary": "Qualcomm’s Snapdragon Summit, its annual conference held in Maui , Hawaii, took the wraps off the new generation of processors for PCs, Snapdragon X2. First introduced last year as part of Microsoft’s high-profile Copilot AI PC push, the processor lineup was notable because its CPU cores use the Arm instruction set instead of x 86. That initial Snapdragon X rollout was targeted at smaller and more portable Windows PCs. But f or its next move, Qualcomm is looking to expand its reach with a third-generation CPU and an upgraded neural processing unit (NPU), a one-two punch meant to accelerate AI workloads in mini PCs, all-in-one desktops, and high-performance laptops. “We’re going to bring AI everywhere,” says Qualcomm CEO Cristiano Amon. “Everything we’ve been talking about is starting to happen, and this dream of what’s going to happen with AI, it’s getting to the point where we see what’s going to happen at scale.” Keeping up with the Apples Oryon, Qualcomm’s CPU core in Snapdragon X, repeated a trick Apple pulled off with its introduction of Arm-based silicon in 2020. (The team that designed Oryon included a number of CPU architects who worked on Apple’s chips.) The first generation of the architecture placed an emphasis on efficiency and multicore performance. It also integrated an NPU, which is why it led Microsoft’s AI-focused Copilot Plus PCs . The third-gen Oryon architecture inside the Qualcomm Snapdragon X2 upped the core count from 12 to 18 for the top-end version, the Snapdragon X2 Elite Extreme. (The second generation was released only for smartphones.) But in a move that mirrors Intel and Apple’s architectures, it now combines two different kinds of CPU cores . (Qualcomm’s mobile CPUs also have two types of cores.) Unlike Apple and Intel, none of the cores in Snapdragon X2 are designed for low-performance workloads, Qualcomm stressed. Snapdragon X2 will pair what it calls “Performance” cores having multicore clock speeds up to 3.6 gigahertz (similar to those of the prior Snapdragon X) with new “Prime” cores that can reach multicore clock speeds up to 4.4 GHz, or up to 5 GHz in dual-core workloads. Why? Once again, it comes down to Apple. The world’s other major player in Arm chips for personal computing outpaced Qualcomm with the Apple M4, which achieves clock speeds up to 4.5 GHz and, in its 16-core top-tier M4 Max configuration . That left Snapdragon X a step behind in both single-core and multicore performance. Qualcomm believes that Snapdragon X2’s high clock speeds will give it the lead, though whether that’s the case remains to be seen. The first PCs with Snapdragon X2 aren’t expected until the first half of 2026, setting the stage for a showdown with Apple’s M5, which is expected to arrive in late 2025 or early 2026. More memory for more AI While Qualcomm and Apple butt heads on CPU performance, there’s another aspect of chip architecture where Qualcomm is ahead—the NPU. Competitors also have NPUs, which accelerate AI workloads , in their chips. But Qualcomm has staked the most on NPU performance. The first-gen Snapdragon X was quoted a NPU performance at 45 trillion operations per second (TOPS), and it claimed that for all chips in the product stack, not just the most expensive silicon. Snapdragon X2 will boost that metric to 80 TOPS. Snapdragon X2 also provides memory upgrades to improve AI workloads. The line of processors can support up to 128 gigabytes of advanced, low-power DRAM memory, LPDDR5x RAM. This is an upgrade from Snapdragon X, which topped out at 64 GB, and similar to AI-focused chips like AMD’s Ryzen AI Max. Memory bandwidth is also boosted from 135 GB/s to a maximum of 228 GB/s in the Snapdragon X2 Elite Extreme. Both metrics are important to generative-AI workloads. More memory makes it possible to load larger and more intelligent models, while added memory bandwidth improves the speed at which models can generate a response to a user’s prompt. A lot to offer, a lot to prove Qualcomm’s CEO speculates that personal computers will see a grand shift toward AI workloads. “As the AI can understand what we say, what we see, what we write…that becomes the new [user interface] of computers. The UI is human-centric and it gets processed where you are,” says Amon. Qualcomm’s NPU theoretically makes Snapdragon X2 a great choice for AI software. But it still has a lot to prove. While Amon’s keynote address in Maui imagined a future with numerous AI agents working on-device, the current reality is more modest. Qualcomm has Microsoft’s support and has a few software wins in specific features, like the AI-powered “magic mask” that removes unwanted objects from video in DaVinci Resolve Pro , a popular professional video editor. But such features remain exceptions to the rule, and Amon’s examples of how agentic AI might work were mostly hypothetical. An agentic AI assistant running on Qualcomm’s Snapdragon might one day be able to handle your calendar or pay your bills—but such an on-device agent doesn’t exist yet. The idea, then, appears to be “If you build it, they will come.” Qualcomm’s Oryon third-gen architecture and Snapdragon X2 chip imagines a future of personal computing with a focus on CPU and AI performance. But the company is still waiting to see if developers want to play ball.",
    "published": "Wed, 24 Sep 2025 23:41:54 +0000",
    "author": "Matthew S. Smith",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "The Top Programming Languages 2025",
    "link": "https://spectrum.ieee.org/top-programming-languages-2025",
    "summary": "Since 2013, we’ve been metaphorically peering over the shoulders of programmers to create our annual interactive rankings of the most popular programming languages. But fundamental shifts in how people are coding may not just make it harder to measure popularity, but could even make the concept itself irrelevant. And then things might get really weird. To see why, let’s start with this year’s rankings and a quick refresher of how we put this thing together. In the “ Spectrum ” default ranking, which is weighted with the interests of IEEE members in mind, we see that once again Python has the top spot, with the biggest change in the top five being JavaScript ’s drop from third place last year to sixth place this year. As JavaScript is often used to create web pages, and vibe coding is often used to create websites, this drop in the apparent popularity may be due to the effects of AI that we’ll dig into in a moment. But first to finish up with this year’s scores, in the “Jobs” ranking, which looks exclusively at what skills employers are looking for, we see that Python has also taken 1 st place, up from second place last year, though SQL expertise remains an incredibly valuable skill to have on your resume. Because we can’t literally look over the shoulders of everyone who codes, including kids hacking on Minecraft servers or academic researchers developing new architectures, we rely on proxies to measure popularity. We detail our methodology here , but the upshot is that we merge metrics from multiple sources to create our rankings. The metrics we choose publicly signal interest across a wide range of languages—Google search traffic, questions asked on Stack Exchange , mentions in research papers, activity on the GitHub open source code repository, and so on. For the first time ever, IEEE Spectrum is hosting an emergency interactive session to discuss whether AI is signalling the end of distinct programming languages as we know it. We’re bringing our expert team together with an audience of tech enthusiasts, developers, and industry leaders to discuss: How AI is rapidly changing the landscape of programming languages Why knowing the best languages might not be necessary in your career What skills you’ll need instead Join the conversation to explore 2025’s ranking of Top Programming Languages and get a glimpse into how AI will impact the ranking in 2026–or end it entirely. But programmers are turning away from many of these public expressions of interest. Rather than page through a book or search a website like Stack Exchange for answers to their questions, they’ll chat with an LLM like Claude or ChatGPT in a private conversation. And with an AI assistant like Cursor helping to write code, the need to pose questions in the first place is significantly decreased. For example, across the total set of languages evaluated in the TPL, the number of questions we saw posted per week on Stack Exchange in 2025 was just 22 percent of what it was in 2024. With less signal in publicly available metrics, it becomes harder to track popularity across a broad range of languages. This existential problem for our rankings can be tackled by searching for new metrics, or trying to survey programmers—in all their variety—directly. However, an even more fundamental problem is looming in the wings. Whether it’s a seasoned coder using an AI to handle the grunt work , or a neophyte vibe coding a complete web app, AI assistance means that programmers can concern themselves less and less with the particulars of any language. First details of syntax, then flow control and functions, and so on up the levels of how a program is put together—more and more is being left to the AI. Although code-writing LLM’s are still very much a work in progress , as they take over an increasing share of the work, programmers inevitably shift from being the kind of people willing to fight religious wars over whether source code should be indented by typing tabs or spaces to people who care less and less about what language is used. After all, the whole reason different computer languages exist is because given a particular challenge, it’s easier to express a solution in one language versus another. You wouldn’t control a washing machine using the R programming language , or conversely do a statistical analysis on large datasets using C . But it is technically possible to do both. A human might tear their hair out doing it, but LLMs have about as much hair as they do sentience. As long as there’s enough training data, they’ll generate code for a given prompt in any language you want. In practical terms, this means using one—any one—of today’s most popular general purpose programming languages. In the same way most developers today don’t pay much attention to the instruction sets and other hardware idiosyncrasies of the CPUs that their code runs on, which language a program is vibe coded in ultimately becomes a minor detail. Sure, there will always be some people who care, just as today there are nerds like me willing to debate the merits of writing for the Z80 versus the 6502 8-bit CPUs. But overall, the popularity of different computer languages could become as obscure a topic as the relative popularity of railway track gauges. One obvious long-term consequence to this is that it will become harder for new languages to emerge. Previously, new languages could emerge from individuals or small teams evangelizing their approach to potential contributors and users. Presentations, papers, demos, sample code and tutorials seeded new developer ecosystems. A single well-written book, like Leo Brodie’s Starting Forth or Brian Kernighan and Dennis Ritchies’ The C Programming Language , could make an enormous difference to a language’s popularity. But while a few samples and a tutorial can be enough material to jump-start adoption among programmers familiar with the ins and outs of hands-on coding, it’s not enough for today’s AIs. Humans build mental models that can extrapolate from relatively small amounts of data. LLMs rely on statistical probabilities, so the more data they can crunch, they better they are. Consequently programmers have noted that AIs give noticeably poorer results when trying to code in less-used languages. There are research efforts to make LLMs more universal coders , but that doesn’t really help new languages get off the ground. Fundamentally new languages grow because they are scratching some itch a programmer has. That itch can be as small as being annoyed at semicolons having to be placed after every statement, or as large as a philosophical argument about the purpose of computation . But if an AI is soothing our irritations with today’s languages, will any new ones ever reach the kind of critical mass needed to make an impact? Will the popularity of today’s languages remain frozen in time? What’s the future of programming languages? Before speculating further about the future, let’s touch base again where we are today. Modern high-level computer languages are really designed to do two things: create an abstraction layer that makes it easier to process data in a suitable fashion, and stop programmers from shooting themselves in the foot. The first objective has been around since the days of Fortran and Cobol , aimed at processing scientific and business data respectively. The second objective emerged later, spurred in no small part by Edgar Dijkstra’s 1968 paper “ Go To Statement Considered Harmful .” In this he argued for eliminating the ability for a programmer to make jumps to arbitrary points in their code. This restriction was to prevent so-called spaghetti code that makes it hard for a programmer to understand how a computer actually executes a given program. Instead, Dijkstra demanded that programmers bend to structural rules imposed by the language. Dijkstra’s argument ultimately won the day, and most modern languages do indeed minimize or eliminate Go Tos altogether in favor of structures like functions and other programmatic blocks. These structures don’t exist at the level of the CPU. If you look at the instruction sets for Arm, x86, or RISC-V processors, the flow of a program is controlled by just three types of machine code instructions. These are conditional jumps, unconditional jumps, and jumps with a trace stored (so you can call a subroutine and return to where you started). In other words, it’s Go Tos all the way down. Similarly, strict data types designed to label and protect data from incorrect use dissolve into anonymous bits flowing in and out of memory. So how much abstraction and anti-foot-shooting structure will a sufficiently-advanced coding AI really need? A hint comes from recent research in AI-assisted hardware design, such as Dall-EM , a generative AI developed at Princeton University used to create RF and electromagnetic filters. Designing these filters has always been something of a black art, involving the wrangling of complex electromagnetic fields as they swirl around little strips of metal. But Dall-EM can take in the desired inputs and outputs and spit out something that looks like a QR code. The results are something no human would ever design—but it works. Similarly, could we get our AIs to go straight from prompt to an intermediate language that could be fed into the interpreter or compiler of our choice? Do we need high-level languages at all in that future? True, this would turn programs into inscrutable black boxes, but they could still be divided into modular testable units for sanity and quality checks. And instead of trying to read or maintain source code, programmers would just tweak their prompts and generate software afresh. What’s the role of the programmer in a future without source code? Architecture design and algorithm selection would remain vital skills—for example, should a pathfinding program use a classic approach like the A* algorithm , or instead should it try to implement a new method? How should a piece of software be interfaced with a larger system? How should new hardware be exploited? In this scenario, computer science degrees, with their emphasis on fundamentals over the details of programming languages, rise in value over coding boot camps. Will there be a Top Programming Language in 2026? Right now, programming is going through the biggest transformation since compilers broke onto the scene in the early 1950s. Even if the predictions that much of AI is a bubble about to burst come true, the thing about tech bubbles is that there’s always some residual technology that survives. It’s likely that using LLMs to write and assist with code is something that’s going to stick. So we’re going to be spending the next 12 months figuring out what popularity means in this new age, and what metrics might be useful to measure. What do you think popularity should mean? What metrics do you think we should consider? Let us know in the comments below.",
    "published": "Tue, 23 Sep 2025 17:44:36 +0000",
    "author": "Stephen Cass",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Top Programming Languages Methodology 2025",
    "link": "https://spectrum.ieee.org/top-programming-languages-methodology-2025",
    "summary": "Our Top Programming Languages interactive tries to tackle the problem of estimating a language’s popularity by looking for proxy signals. We do this by constructing measures of popularity from a variety of data sources that we believe are good proxies for active interest for each programming language. In total, we identify 64 programming languages . We then weight each data source to create an overall index of popularity , excluding some of the lowest scorers. Below, we describe the sources of data we use to get the measures, and the weighting scheme we use to produce the overall indices. By popularity, we mean we are trying to rank languages that are in active use, including activity from maintaining legacy systems. We look at three different aspects of popularity: languages in active use among typical IEEE members and working software engineers (the “Spectrum” ranking), languages that are in demand by employers (the “Jobs” ranking), and languages that are in the zeitgeist (the “Trending” ranking). We gauged the popularity of languages using the following sources for a total of seven metrics (see below). We gathered the information for all metrics in July—August 2025. In the past we relied heavily on APIs to gather data from sources, but now the data is gathered manually to the difficulty of keeping up with API changes and terminations, and because many of the programming language’s names (C++, Scheme) collided with common terms found in research papers and job ads or were difficult for a search engine to parse. When a large number of search results made it impractical to resolve ambiguities by examining all of the results individually, we used a sample of each data source, and determined the relevant sample size based on estimating the true mean with 95 percent confidence. Not all data sources contain information for each programming language and we interpret this information as the programming language having “no hits” (that is, not being popular). The results from each metric are normalized to produce a relative popularity score between 0 and 1. Then the individual metrics are multiplied by a weight factor, combined, and the result renormalized to produce an aggregate popularity score. In aggregating metrics, we hope to compensate for statistical quirks that might distort a language’s popularity score in any particular source of data. Varying the weight factors allows us to create the different results for the Spectrum, Jobs, and Trending rankings. We fully acknowledge that, while these weights are subjective, they are based on our understanding of the sources and our prior coverage of software topics. Varying the weight factors allows us to emphasize different types of popularity and produce the different rankings. We then combined each weighted data source for each program and then renormalized the resulting frequency to produce an aggregate popularity score. The Top Programming Languages was originally created by data journalist Nick Diakopoulos . Our statistical methodology advisor is Hilary Wething , although all the actual data gathering and calculation is performed by us. Rankings are computed using R . Google Google is the leading search engine in the world, making it an ideal fit for estimating language popularity. We measured the number of hits for each language by searching on the template, “X programming language” (with quotation marks) and manually recorded the number of results that were returned by the search. We took the measurement in July 2025. We like this measure because it indicates the volume of online information resources about each programming language. Stack Overflow Stack Overflow is a popular site where programmers can ask questions about coding. We recorded the number of questions tagged to each program within the last week prior to our search (August 2025). For the Mathematica/Wolfram language, we relied on the sister “Stack” for the Mathematica platform and tallied the number of programming-related questions asked in the past week. These data were gathered manually. This measure indicates what programming languages are currently trending. IEEE Xplore Digital Library IEEE maintains a digital library with millions of conference and journal articles covering a wide array of scientific and engineering disciplines. We searched for journal, magazine, and early access articles that mention each of the languages in the template “X programming” for the 2025 year-to-date. For search results that returned thousands of articles, we identified the correct sample size for a 95 percent confidence interval (usually a little over 300) and pulled that number of articles. For each language we sampled, we identified the share of articles that utilize the programming language and then multiplied the total number of articles by this share to tally the likely total number of articles that reference a given programming language. We conducted this search in August 2025. This metric captures the prevalence of the different programming languages as used and referenced in engineering scholarship. IEEE Jobs Site We measured the demand for different programming languages in job postings on the IEEE Job Site . For search results that returned thousands of listings, we identified the correct sample size for a 95 percent confidence interval (usually around 300 results) and pulled that number of job listings to manually examine. For each language we sampled, we identified the share of listings that utilize the programming language and then multiplied the total number of job listings by this share to tally the likely total number of job listings that reference a given programming language. Additionally, because some of the languages we track could be ambiguous in plain text—such as Go, J, Ada, and R—we searched for job postings with those words in the job description and then manually examined the results, again sampling entries if the number of results was large. The search was conducted in August 2025. We like the IEEE Job Site for its large number of non-U.S. listings, making it an ideal to measure global popularity. Career Builder We measured the demand for different programming languages on the CareerBuilder job site. We searched for “Developer” jobs offered within the United States , as this is the most popular job title for programmers. We sampled 400 job ads and manually examined them to identify which languages employers mentioned in the postings. The search was conducted in August 2024. We like the career builder site to identify the popularity of programmer jobs among U.S.-based companies GitHub GitHub is a public repository for many volunteer-driven open-source software projects. We used Github’s listing of it’s top 50 programming languages, filtering out entries for things like Docker configuration scripts . The data cover the first quarter of 2025. This measured provides a strong indication what languages coders choose to work in when they have a personal choice. Trinity College Dublin Library The library of Trinity College Dublin is one of six legal deposit libraries in Ireland and the United Kingdom . A copy must be deposited with the library of any printed material published or distributed in Ireland, and on request any U.K. publisher or distributor must also deposit a book. We searched for all books published in the year to date that matched the names of programming languages and checked the results for false positives. The search was conducted in July 2025. We like this library collection because it represents a large and categorized sample of works, primarily in the English language. Discord Discord is popular chat-room platform where many programmers exchange information. We counted the number of tags that correspond to each language. In the case of languages that could also be names of nonprogramming topics (many nonprogramming-related topics also have dedicated Discord servers ; for example, “Julia” could refer to the programming language or the Sesame Street puppet ), results were manually examined. Disboard was searched in August 2025. Disboard lists many public discord servers and many young coders use the site, contributing a different demographic of coders.",
    "published": "Tue, 23 Sep 2025 17:44:19 +0000",
    "author": "Stephen Cass",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Tech Keeps Chatbots From Leaking Your Data",
    "link": "https://spectrum.ieee.org/homomorphic-encryption-llm",
    "summary": "Your chatbot might be leaky. According to recent reports , user conversations with AI chatbots such as OpenAI ’s ChatGPT and xAI’s Grok “have been exposed in search engine results.” Similarly, prompts on the Meta AI app may be appearing on a public feed . But what if those queries and chats can be protected, boosting privacy in the process? That’s what Duality , a company specializing in privacy-enhancing technologies, hopes to accomplish with its private large language model (LLM) inference framework. Behind the framework lies a technology called fully homomorphic encryption , or FHE, a cryptographic technique enabling computing on encrypted data without needing to decrypt it. Duality’s framework first encrypts a user prompt or query using FHE, then sends the encrypted query to an LLM. The LLM processes the query without decryption, generates an encrypted reply, and transmits it back to the user. “They can decrypt the results and get the benefit of running the LLM without actually revealing what was asked or what was responded,” says Kurt Rohloff , cofounder and chief technology officer at Duality. As a prototype, the framework supports only smaller models , particularly Google ’s BERT models. The team tweaked the LLMs to ensure compatibility with FHE, such as replacing some complex mathematical functions with their approximations for more efficient computation. Even with these slight alterations, however, the AI models operate just like a normal LLM would. “Whatever we do on the inference does not require retraining. In our approach, we still want to make sure that training happens the usual way, and it’s the inference that we essentially try to make more efficient,” says Yuriy Polyakov , vice president of cryptography at Duality. The Challenges of FHE LLM Inference FHE is considered a quantum-computer-proof encryption . Yet despite its high level of security, the cryptographic method can be slow. “Fully homomorphic encryption algorithms are heavily memory bound,” says Rashmi Agrawal , cofounder and chief technology officer at CipherSonic Labs , a company that spun out of her doctoral research at Boston University on accelerating homomorphic encryption. She explains that FHE relies on lattice-based cryptography , which is built on math problems around vectors in a grid. “Because of that lattice-based encryption scheme, you blow up the data size,” she adds. This results in huge ciphertexts (the encrypted version of your data) and keys requiring lots of memory. Another computational bottleneck entails an operation called bootstrapping, which is needed to periodically remove noise from ciphertexts, Agrawal says. “This particular operation is really expensive, and that is why FHE has been slow so far.” To overcome these challenges, the team at Duality is making algorithmic improvements to an FHE scheme known as CKKS (Cheon-Kim-Kim-Song) that’s well-suited for machine learning applications. “This scheme can work with large vectors of real numbers, and it achieves very high throughput,” says Polyakov. Part of those improvements involves integrating a recent advancement dubbed functional bootstrapping. “That allows us to do a very efficient homomorphic comparison operation of large vectors,” Polyakov adds. All of these implementations are available on OpenFHE , an open-source library that Duality contributes to and helps maintain. “This is a complicated and sophisticated problem that requires community effort. We’re making those tools available so that, together with the community, we can push the state of the art and enable inference for large language models,” says Polyakov. Hardware acceleration also plays a part in speeding up FHE for LLM inference, especially for bigger AI models. “They can be accelerated by two to three orders of magnitude using specialized hardware acceleration devices,” Polyakov says. Duality is building with this in mind and has added a hardware abstraction layer to OpenFHE for switching from a default CPU backend to swifter ones such as GPUs and application-specific integrated circuits ( ASICs ). Agrawal agrees that GPUs, as well as field-programmable gate arrays ( FPGAs ), are a good fit for FHE-protected LLM inference because they’re fast and connect to high-bandwidth memory. She adds that FPGAs in particular can be tailored for fully homomorphic encryption workloads. For Duality’s next steps, the team is progressing their private LLM inference framework from prototype to production. The company is also working on safeguarding other AI operations, including fine-tuning pretrained models on specialized data for specific tasks, as well as semantic search to uncover the context and meaning behind a search query rather than just using keywords. Encrypted LLMs Are the Future FHE forms part of a broader privacy-preserving toolbox for LLMs, alongside techniques such as differential privacy and confidential computing. Differential privacy introduces controlled noise or randomness to datasets, obscuring individual details while maintaining collective patterns. Meanwhile, confidential computing employs a trusted execution environment—a secure, isolated area within a CPU for processing sensitive data. Confidential computing has been around longer than the newer FHE technology, and Agrawal considers it as FHE’s “head-to-head competition.” However, she notes that confidential computing can’t support GPUs, making them an ill match for LLMs. “FHE is strongest when you need noninteractive end-to-end confidentiality because nobody is able to see your data anywhere in the whole process of computing,” Agrawal says. A fully encrypted LLM using FHE opens up a realm of possibilities. In health care , for instance, clinical results can be analyzed without revealing sensitive patient records. Financial institutions can check for fraud without disclosing bank account information. Enterprises can outsource computing to cloud environments without unveiling proprietary data. User conversations with AI assistants can be protected, too. “We’re entering into a renaissance of the applicability and usability of privacy technologies to enable secure data collaboration,” says Rohloff. “We all have data. We don’t necessarily have to choose between exposing our sensitive data and getting the best insights possible from that data.”",
    "published": "Tue, 23 Sep 2025 12:00:04 +0000",
    "author": "Rina Diane Caballar",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "AlphaEarth Provides New Ways to See, and Understand, Earth",
    "link": "https://spectrum.ieee.org/google-deepmind-alphaearth-foundations-ai",
    "summary": "Google DeepMind has debuted AlphaEarth Foundations, an AI model that treats Earth like a living dataset, tracking crop cycles, coastlines, urban expansion, melting ice, and much, much more. AlphaEarth weaves together disparate data streams, from satellite imagery and sensor data to geotagged Wikipedia entries, into a unified digital representation that scientists can probe to uncover patterns unfolding worldwide. AlphaEarth produces a 64-dimensional “embedding” for every 10-by-10-meter cell of the planet annually from 2017 to 2024, covering both raw imagery and the relationships present in the underlying data. An embedding is a dense numeric summary of a place’s key features, making locations directly comparable. This approach cuts storage needs sixteenfold while preserving fine spatial and temporal detail. Altogether, the system amounts to over 1.4 trillion embeddings per year. Detailed snapshots of year-round surface conditions will prove valuable in a wide range of fields, including planetary analysis, urban planning, ecosystem tracking, wildlife conservation, and wildfire risk management. Digital Embeddings of Earth A key challenge in building the model was handling the messy sprawl of geospatial data itself. Traditional satellites capture large volumes of information-rich images and measurements that can be difficult to connect and efficiently analyze. The AlphaEarth Foundations team told IEEE Spectrum that one limitation in Earth observation is the inherent irregularity and sparsity of the data. Unlike a continuous video feed, satellite data is a collection of intermittent snapshots with frequent gaps caused by factors like persistent cloud cover. To ensure consistent performance, the model needed a wide net of training data: A global sample of images covering more than 5 million locations acquired from the Google Earth Engine public data catalog, including optical imagery, radar, climate models, topographic maps, lidar, gravitational field strength, and surface temperature measurements. To enrich the dataset, the team also incorporated Wikipedia articles on landmarks and other features. That diversity makes the model’s representations more detailed, but still broad enough to be relevant across different regions and scientific tasks. In Ecuador, for example, embeddings enable analysts to see through persistent cloud cover, revealing agricultural plots in various development stages. “Given we were aiming to integrate this data into a unified digital representation to provide scientists with a more complete and consistent picture of our planet’s evolution, we had to grapple with petabytes of multisource, multiresolution imagery and other geospatial datasets,” says Chris Brown, a senior research engineer at Google DeepMind. The team first had to get data pipelines and modeling infrastructure to a place where working on petabyte scales was feasible. “We prioritized respecting the nuances of geospatial data, such as projections, unique sensor properties, and sensor-acquisition strategies, while ensuring the model and its outputs were robust and generally useful for a wide variety of applications,” says Brown. AlphaEarth Foundations consistently outperformed other featurization approaches. Christopher F. Brown, Michal R. Kazmierski, Valerie J. Pasquarella, et al. The team stresses that AlphaEarth isn’t a generative model but a self-supervised framework designed to provide compact summaries of patterns in existing data. They worked to mitigate training bias through stratified sampling, which involves training the model on millions of locations to ensure diverse geographies and ecosystems are represented. According to Emily Schechter , a senior product manager at Google Earth Engine, the team benchmarked AlphaEarth against both traditional approaches and other AI mapping systems across multiple time periods and tasks, such as estimating ground surface properties and tracking changes in how land is being used over time. The results, Schechter says, showed AlphaEarth consistently outperformed alternatives, even in situations where labeled data was scarce. In a paper posted in late July , Google DeepMind reported that AlphaEarth had a 23.9 percent lower error rate on average than competing approaches. The researchers noted that the identity of the next-best baseline varies by dataset and task, signaling inconsistent prior progress in the field. AlphaEarth, on the other hand, shows consistent gains even in historically difficult mapping scenarios. It’s also more effective at classifying data. When pulling embedding vectors from Earth Engine for a labeled set of sites, the model successfully classified 87 crop categories and land-cover types using only about 150 examples per class, something that usually demands thousands of labels. In other tasks DeepMind explored , AlphaEarth was able to detail intricate Antarctic terrain despite irregular satellite coverage, and to spot subtle shifts in Canadian farmland that are missing from standard imagery. “To the best of my knowledge, this is the largest scale effort of its kind to date in terms of training data, model context size, and integrated modalities,” Brown says. “There’s so much potential for this technology to be applied, in different ways and across different use cases. [...] We’ll continue working with our partners to find ways to make this most useful for people.” Unified Model for Earth Science While AlphaEarth Foundations shares some similarities with digital twins —virtual replicas of real-world environments—it functions more as a groundwork than a full twin. By turning Earth’s raw data into a flexible public format, it supports a range of specialized models and analyses to plug in on top without rebuilding the data pipeline each time. The satellite embedding dataset is available through the Earth Engine Data Catalog , which is free for noncommercial use. Google DeepMind has been running tests with over 50 organizations worldwide in the last year. Several universities and the Food and Agriculture Organization of the United Nations are already using the embeddings. AlphaEarth’s embedding fields provide dozens of different ways to understand parcels of the Earth’s surface. Google DeepMind; Google Earth Engine Schechter also pointed to examples such as the Brazilian nonprofit MapBiomas , which is now mapping environmental changes in the Amazon rainforest, and the Global Ecosystems Atlas, a program classifying unmapped ecosystems into shrublands, deserts, wetlands, and other categories. Beyond research, integration into the widely used spatial analytics platform CARTO puts AlphaEarth Foundations into the hands of insurers, telecommunications firms, and other users who can load the embeddings into their existing workflow to run risk models—like finding ZIP codes with environmental profiles resembling wildfire-prone areas—without API requests or extra storage.",
    "published": "Mon, 15 Sep 2025 14:00:03 +0000",
    "author": "Shannon Cuthrell",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Upgradable Laptop GPUs Have Arrived",
    "link": "https://spectrum.ieee.org/upgrade-laptop-gpu-framework-nvidia",
    "summary": "The GPU is among the most important silicon in a modern laptop. It’s often tapped for demanding tasks, including 3D graphics rendering, video encoding, and AI workloads. Yet despite its importance, swapping the GPU in a modern laptop is impossible: It’s soldered to a mainboard, and upgrades aren’t available. Until now. Framework, a small laptop maker focused on repairability, released a new upgradable graphics module late last month . It includes the Nvidia RTX 5070 GPU, fits in all existing Framework 16 laptops , and can be installed with a single screwdriver. “Upgradable graphics in a laptop is basically the holy grail,” says Nirav Patel , the CEO of Framework. “In the desktop world, graphics upgrades are among the most common behaviors […] it’s one of the core reasons to even buy or build a desktop PC. So we knew that was something we wanted to do.” Framework’s Swappable Graphics Module Upgrading laptop graphics isn’t entirely new. About a decade ago, a handful of manufacturers sold laptops compatible with Mobile PCIe Express (MXM), a compact PCI Express standard. But MXM never caught on. Card makers often ignored the specifications . MXM graphics modules designed for one laptop often didn’t fit another (even among laptops sold by the same company), and when they did fit, firmware complications meant it wasn’t guaranteed to work. Framework took a different approach. Instead of developing a standard for the industry, the company focused on building a swappable graphics module for a single laptop: the Framework 16. The module attaches to the laptop’s rear and connects via a custom interposer secured with screws, making it possible to swap the module in a few minutes. The first module launched in 2024 with AMD’s Radeon RX 7700S. It was the only GPU available, however, which meant it didn’t yet deliver upgradable graphics. Framework promised future upgrades, but users were skeptical. After all, MXM—a standard backed by many larger laptop makers—made similar promises and failed to deliver. Framework’s introduction of a new graphics module with Nvidia’s RTX 5070 fulfills its promise. The new module slots right into the Framework 16’s existing graphics bay, connects to the same custom interposer, and is secured with the same screws. Both graphics modules weigh 0.3 kilograms and look nearly identical. Even the internals are difficult to tell apart, though a close inspection will reveal subtle differences in the heat pipes and cooling fans. Patel says Framework and Nvidia began work on the RTX 5070 graphics module in 2024, shortly after the original graphics module was released, and partnered closely to make the upgrade a reality. “We were essentially the only laptop maker not shipping Nvidia at that point, so of course, they want to win every customer they can,” says Patel. “But also, they know the Framework 16 is unique and compelling, and solves a problem others haven’t been able to solve.” Framework picked Nvidia’s RTX 5070 for its second graphics module because the company felt it filled the power and performance “sw­eet spot” for an upgradable GPU. Introduced earlier this year, the RTX 5070 boasts Nvidia’s new Blackwell architecture and provides a considerable upgrade over the older AMD RX 7700S. But the RTX 5070 is also similar to the RX 7700S in thermals and physical size, making it a good fit for the existing graphics module design. While it’s fair to say the Nvidia GPU has a performance edge over the AMD option, the existing AMD graphics module remains relevant and will continue to be sold. AMD provides open-source drivers and is sometimes preferred in the Linux community, which is a key demographic for Framework—though Patel says Nvidia’s Linux driver support has “matured a lot” and shouldn’t be a problem “in the [distributions] that we recommend people use.” Introducing the 240-Watt USB-C Power Adapter The Nvidia RTX 5070 graphics module was the highlight of Framework’s recent announcement, but it came with a companion that also deserves the spotlight: a 240-watt USB-C power adapter, the first of its kind for a consumer laptop. Framework previously shipped the Framework 16 with an 180-W power adapter, which was already more capable than most (USB-C laptop adapters are typically 65 to 100 W). The 180-W adapter “is basically enough for most workloads,” Patel says, but in high-demand scenarios, power draw could rise higher. The 240-W adapter also brings new functionality. While the original AMD graphics module supported only display output over the included USB-C port, the RTX 5070 module can accept power input as well. Firmware work was required to make the port’s power delivery, display output, and data modes work seamlessly through the interposer that connects the graphics module to the laptop. So why haven’t other laptop makers introduced 240-W USB-C adapters for their own high-performance laptops? Part of the answer is supply-chain issues. The company moved production from China to Thailand to avoid potential tariff issues , though it continued working with the same manufacturing partner at a different facility. The company’s laptop assembly remains in Taiwan. Patel also says that designing a power-delivery system capable of handling the 240-W adapter required extra engineering effort. “For the first generation of Framework Laptop 16, the silicon didn’t exist for us to be able to create the [240-W] power adapter yet,” he says. “As soon as the technology was available, we went ahead and did that.”",
    "published": "Sat, 13 Sep 2025 13:00:03 +0000",
    "author": "Matthew S. Smith",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Machine Learning Tests Keep Getting Bigger",
    "link": "https://spectrum.ieee.org/mlperf-inference-51",
    "summary": "The machine learning field is moving fast, and the yardsticks used to measure its progress are having to race to keep up. A case in point: MLPerf, the biannual machine learning competition sometimes termed “the Olympics of AI,” has introduced three new benchmark tests, reflecting new directions in the field. “Lately, it has been very difficult trying to follow what happens in the field,” says Miro Hodak , an Advanced Micro Devices engineer and MLPerf Inference working-group cochair. “We see that the models are becoming progressively larger, and in the last two rounds we have introduced the largest models we’ve ever had.” The chips that tackled these new benchmarks came from the usual suspects—Nvidia, Arm, and Intel. Nvidia topped the charts, introducing its new Blackwell Ultra GPU, packaged in a GB300 rack-scale design. AMD put up a strong performance, introducing its latest MI325X GPUs. Intel proved that one can still do inference on CPUs with its Xeon submissions, but it also entered the GPU game with an Intel Arc Pro submission. New benchmarks Last round, MLPerf introduced its largest benchmark yet, a large language model based on Llama 3.1-403B. In this round, MLPerf topped itself yet again, introducing a benchmark based on the DeepSeek-R1 671B model—more than 1.5 as many parameters as the previous largest benchmark. As a reasoning model, DeepSeek-R1 goes through several steps of chain-of-thought prompting when approaching a query. This means that much more of the computation happens during inference than in normal LLM operation, making this benchmark even more challenging. Reasoning models are claimed to be the most accurate, making them the technique of choice for science, math, and complex programming queries. In addition to the largest LLM benchmark yet, MLPerf also introduced the smallest, based on Llama 3.1-8B. There is growing industry demand for low latency yet high-accuracy reasoning, explained Taran Iyengar, the MLPerf Inference task-force chair. Small LLMs can supply this, and they’re an excellent choice for tasks such as text summarization and edge applications. This brings the total count of LLM-based benchmarks to a confusing four. They include the new, smallest Llama 3.1-8B benchmark; a preexisting Llama 2-70B benchmark; last round’s introduction of the Llama 3.1-403B benchmark; and the largest, the new DeepSeek-R1 model. If nothing else, this signals that LLMs are not going anywhere. In addition to the myriad LLMs, this round of MLPerf Inference included a new voice-to-text model, based on Whisper-large-v3. This benchmark is a response to the growing number of voice-enabled applications, whether they’re smart devices or speech-based AI interfaces. The MLPerf Inference competition has two broad categories: “closed,” which requires using the reference neural-network model as-is without modifications, and “open,” where some modifications to the model are allowed. Within those, there are several subcategories related to how the tests are done and in what sort of infrastructure. We will focus on the “closed” data-center server results for the sake of sanity. Nvidia leads Surprising no one, the best performance per accelerator on each benchmark, at least in the server category, was achieved by an Nvidia GPU-based system. Nvidia also unveiled the Blackwell Ultra, topping the charts in the two largest benchmarks: Llama 3.1-405B and DeepSeek-R1 reasoning. Blackwell Ultra is a more-powerful iteration of the Blackwell architecture, featuring significantly more memory capacity, double the acceleration for attention layers, 1.5 times more AI compute, and faster memory and connectivity compared with the standard Blackwell. It is intended for larger AI workloads, like the two benchmarks it was tested on. In addition to the hardware improvements, Dave Salvator , director of accelerated computing products at Nvidia, attributes the success of Blackwell Ultra to two key changes. First, the use of Nvidia’s proprietary 4-bit floating-point number format , NVFP4 . “We can deliver comparable accuracy to formats like BF16,” Salvator says, while using a lot less computing power. The second is so-called disaggregated serving . The idea behind disaggregated serving is that there are two main parts to the inference workload: prefill, where the query (“Please summarize this report”) and its entire context window (the report) are loaded into the LLM, and generation/decoding, where the output is actually calculated. These two stages have different requirements. While prefill is compute heavy, generation/decoding is much more dependent on memory bandwidth. Salvator says that by assigning different groups of GPUs to the two different stages, Nvidia achieves a performance gain of nearly 50 percent. AMD close behind AMD’s newest accelerator chip, MI355X, launched in July. The company offered results only in the “open” category, where software modifications to the model are permitted. Like Blackwell Ultra, MI355X features 4-bit floating-point support, as well as expanded high-bandwidth memory. The MI355X beat its predecessor, the MI325X, in the open Llama 2.1-70B benchmark by a factor of 2.7, says Mahesh Balasubramanian , senior director of data-center GPU product marketing at AMD. AMD’s “closed” submissions included systems powered by AMD MI300X and MI325X GPUs. The more advanced MI325X computer performed similarly to those built with Nvidia H200s on the Llama 2-70b, the “mixture of experts” test, and image-generation benchmarks. This round also included the first hybrid submission, where both AMD MI300X and MI325X GPUs were used for the same inference task, the Llama 2-70b benchmark. The use of hybrid GPUs is important, because new GPUs are coming at a yearly cadence , and the older models, deployed en masse, are not going anywhere. Being able to spread workloads among different kinds of GPUs is an essential step. Intel enters the GPU game In the past, Intel has remained steadfast that one does not need a GPU to do machine learning. Indeed, submissions using Intel’s Xeon CPU still performed on par with the Nvidia L4 on the object-detection benchmark but trailed on the recommender-system benchmark. In this round, for the first time, an Intel GPU also made a showing. The Intel Arc Pro was first released in 2022. The MLPerf submission featured a graphics card called the MaxSun Intel Arc Pro B60 Dual 48G Turbo , which contains two GPUs and 48 gigabytes of memory. The system performed on par with Nvidia’s L40S on the small LLM benchmark and trailed it on the Llama 2-70b benchmark.",
    "published": "Wed, 10 Sep 2025 15:00:03 +0000",
    "author": "Dina Genkina",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "How Cryptocurrency Captured the Dream of the Decentralized Web",
    "link": "https://spectrum.ieee.org/web3-hardware-security",
    "summary": "The term Web3 was originally coined by Ethereum cofounder Gavin Wood as a secure, decentralized, peer-to-peer version of the Internet. The idea was to build an Internet based on blockchain technology and a peer-to-peer network, without the need for large data centers or third-party providers. These days, however, blockchain is most famous as the tool enabling cryptocurrencies . Most recently , the Trump administration has taken on a pro-cryptocurrency stance, boosting blockchain’s popularity and media prominence. Cryptography is central to the functioning of blockchains, whether for a decentralized Web or for cryptocurrencies. Every time a cryptocurrency transaction is initiated, all parties involved in the transaction need to securely prove that they agree to the transfer. This is done via a digital signature : a cryptographic protocol that generates a secret, private key that is unique to each user and a public key that the user shares. Then, the private key is used to generate a unique signature for each transaction. The public key can be used to verify that, indeed, the signature was created by the holder of the private key. In this way, Web3 in every incarnation relies heavily on cryptography. To learn more about the evolution of Web3, and cryptography’s role, we caught up with Riad Wahby , assistant professor of electrical and computer engineering at Carnegie Mellon University, in Pittsburgh, and a cofounder and CEO of hardware-backed Web3 security platform Cubist . Wahby explained what Web3 was meant to be, what it’s become, and how hardware-backed cryptography will enable its future. Web3 Began as a Response to What Came Before IEEE Spectrum : What is Web3? Riad Wahby: That’s the hardest question that you’re going to ask by far, because I don’t know how to answer it in a way that satisfies everyone. The term Web3 was coined around 2014, by people who looked at the way that the Web had developed. Web 1.0 was the first Web bubble, the dot-com bubble. Web 2.0, roughly speaking, is Google and Facebook and Microsoft and Apple and Netflix, and the like. And the perception, especially from folks who originally coined this term Web3, was that these companies had basically taken the Web in the wrong direction, because your privacy is gone, and you’re the product, so to speak. You use Gmail for free because Google is mining your emails to sell things like better advertising. Web3 was originally a reaction to this. Early proponents of Web3 basically said, “We don’t want that. We want to take back control of our stuff. I want to own my own data, and maybe cryptocurrencies and blockchains are the way there.” So that’s where the term originally came from. What does the term mean now? Wahby: Now it doesn’t mean anything like that at all. Now Web3 is the broader ecosystem around cryptocurrencies and blockchain-based technologies. And I think basically all of that revolutionary spirit has gone away in favor of building financial products and making a lot of money doing it. As far as I can tell, the term has really transformed from a reaction to a lack of privacy and a lack of sovereignty in my own data to “Hey, this is a technology that has something to do with blockchains.” Maybe you can buy some kind of speculative meme coin and make a bunch of money doing it. So I don’t know, maybe that took a dark turn at the end. That’s how things go. How are those two definitions connected? Wahby: Cryptography really fits into the revolutionary spirit, in the sense that the folks who want to cast off the chains of things like Google and Facebook, one of the tenets was—”The way that we’ll do that is we’ll build this technology that’s sort of amazing and that gives us all these great properties.” And they were going to do that using some advanced cryptographic technologies. This is the reason that there’s so many people who are cryptography researchers at universities that also are involved deeply in some kind of cryptocurrency. Because it’s like this is a sea change in the way that cryptography gets used in the world. Twenty years ago, it used to be that if you were working on really any kind of cryptography, regardless of how theoretical or how practical you intended it to be, you knew that there was not much of a chance that any of it was going to get really used in the world, unless it was extremely practical and extremely focused on solving some immediate problem. And it just used to be the case that people were extremely conservative about what kind of cryptography they used. Basically, everyone thought, “We don’t need any of this crazy stuff. That’s all theory. Nobody cares. The only stuff we need is what lets you connect to Amazon and safely buy stuff.” The rise of cryptocurrencies brought with it this whole shift in the way that cryptography gets deployed in the world, where now if you can come up with some interesting functionality that’s enabled by some advanced cryptography, probably somewhere somebody is going to try and turn that into a product that they can sell. Web3 Is Both Good and Bad for Cryptography What effect has this had on the cryptographic community? Wahby: It’s both good and bad. It’s good in that this means that there’s a lot of motivation to build interesting, cool stuff. And as a researcher in cryptography, I love it because it means that there’s tons more research money being poured into cryptography. That’s the good side. The bad side is that the reason that people were so conservative about deploying new cryptography is that it’s easy to get the security mechanism wrong. The default state of cryptography is [to assume everything is] broken. You have to be very, very careful that each change that you make isn’t returning your cryptography to the default state. I’m not saying that people in Web3 aren’t careful. They are. It’s just by the nature of things, since it’s a much faster time frame, there’s much more pressure to just push stuff into production. And I think the downside is that we have seen a little bit of brokenness. It’s hopefully not causing people to lose oodles of money. And I think the historical record bears this out: People lose oodles of money because other people are really dishonest, not because the cryptography is broken for the most part. But the cryptography can also be broken, and that can also be worrisome. But I’d say from the perspective of somebody who’s doing research in cryptography, the impact of Web3 on the cryptographic community has generally been a good thing. Now you’re focused on hardware security . Can you explain what that is? Wahby: Any cryptocurrency has this property that if I hold some token, and I want to send it to somebody else, the way that I do that is by producing a digital signature that says, I want to spend this token. The secret key is what lets you generate a signature. So if you have 10 E TH [cryptocurrency coins], and they’re all protected by this key, and somebody takes a copy of your key, then life is bad. With a digital signature key it could just be sitting on your hard drive, and then you get some malware, and now somebody has silently stolen your key. There have been these big, broadly targeted malware campaigns where millions and millions of people have all had their keys stolen. So now the criminals are just like sitting there counting up all the money that they’ve stolen, and there’s no reversing transactions, unlike at a bank. Here’s where hardware comes in. This is not really a Web3 technology, this is kind of old, good stuff. There are these devices called hardware security modules, and they’ve been used for multiple decades. This is a physical device, and this device can run certain cryptographic algorithms. And it knows enough that when you tell it, “Hey, please generate me a key,” it can generate you a key securely. And when you tell it, “Please give me a signature,” it can give you a signature securely. But the important thing is the way that it’s designed, the key never leaves this piece of hardware. It turns what was a piece of data into a physical object. And we know how to secure a physical object. You’re working on extending hardware security for more use cases. Can you explain what you’re doing? Wahby: There are two issues with the standard hardware security module. No. 1, you need more cryptography support, so you need to be able to apply digital signatures to transactions very quickly if you’re actively trading. And No. 2, you need a way of expressing that it’s not just a key that can generate any signature. It’s a key that also has attached to it some kind of policy that says these are the kinds of signatures that are okay to generate, and everything else is not allowed, to add extra security. These are the two directions that we have that our technology enables within traditional hardware security modules. We start with the security that’s provided by the traditional hardware security module, and we extend it using this, actually another piece of trusted hardware called the Trusted Execution Environment. We extend it to support the actual kinds of cryptography that are needed for Web3 and to support this rich programmable policy layer that lets you say, “This key is only intended for this specific kind of use,” or “anytime somebody tries to make a payment from this key, first I have to check whether the recipient is subject to sanctions,” or any other rule. So in the end, we have, not only a hardware security module, we have also this Trusted Execution Environment and this policy layer, and all this other cryptographic stuff that together gives us a hardware security module that’s really designed for the Web3 use case.",
    "published": "Mon, 08 Sep 2025 13:00:03 +0000",
    "author": "Dina Genkina",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Esoteric Languages Challenge Coders to Think Way Outside the Box",
    "link": "https://spectrum.ieee.org/esoteric-programming-languages-daniel-temkin",
    "summary": "Have you ever tried programming with a language that uses musical notation? What about a language that never runs programs the same way? What about a language where you write code with photographs? All exist, among many others, in the world of esoteric programming languages, and Daniel Temkin has written a forthcoming book covering 44 of them, some of which exist and are usable to some interpretation of the word “usable.” The book, Forty-Four Esolangs: The Art of Esoteric Code , is out on 23 September, published by MIT Press. I was introduced to Temkin’s work at the yearly Free and Open source Software Developer’s European Meeting ( FOSDEM ) event in Brussels in February. FOSDEM is typically full of strange and wonderful talks, where the open-source world gets to show its more unusual side. In Temkin’s talk, which I later described to a friend as “the most FOSDEM talk of 2025,” he demonstrated Valence , a programming language that uses eight ancient Greek measuring and numeric symbols . Temkin’s intention with Valence was to emulate the same ambiguity that human language has. This is the complete opposite of most programming languages, where syntax typically tries to be explicit and unambiguous. “Just as you could create an English sentence like, ‘Bob saw the group with the telescope,’ and you can’t quite be sure of whether it’s Bob who has the telescope and he’s seeing the group through it, or if it’s the group that has the telescope,” he says. “What if we wrote code that way so you could write something, and now you have two potential programs? One where Bob has a telescope and one where the group has a telescope.” How Esoteric Languages Spark Creativity Creating a language or an interpreter has often been the proving ground of many engineers and programmers, and esoteric languages are almost as old as nonesoteric ones. Temkin says his current effort has a lot to do with AI-generated code that seeks to do nothing but provide seemingly straight solutions to problems, removing any sense of creativity. Esoteric languages inherently make little sense and frequently serve little purpose, making them conceptually completely counter to AI-generated code and thus often not even understood by them—almost the code equivalent of wearing clothing to confuse facial recognition software . While the syntax of esoteric languages may be hard to understand, the actual programming stack is often wonderfully simple. Temkin believes that part of the appeal is also to explore the complexity of modern programming. “I come back a lot to an essay by Joseph Weizenbaum , the creator of the Eliza chatbot, about compulsiveness and code,” he says. “He described ‘the computer bomb,’ the person who writes code and becomes obsessed with getting everything perfect, but it doesn’t work the way they want. The computer is under their control. It’s doing what they’re telling it to do, but it’s not doing what they actually want it to do.” “So they make it more complicated, and then it works the way they want,” Temkin adds. “This is the classic bind in programming. We command the machine when we’re writing code, but how much control do we really have over what happens? I think that we’re now all used to the idea that much of what’s out there in terms of code is broken in some way.” Temkin explored the idea of control in his language Olympus , where the interpreter consists of a series of Greek gods, each of which will do specific things, but only if asked the right way. Temkin’s Olympus language includes an interpreter consisting of Greek gods, which must be asked to do things in the proper way. Daniel Temkin “One example regarding complicating our relationship with the machine and how much we’re in control is my language, Olympus, where code is written to please different Greek gods,” says Temkin. “The basic idea of the language is that you write in pseudo-natural language style, asking various Greek gods to construct code the way that you want it to be. It’s almost as if there’s a layer behind the code, which is the actual code. “You’re not actually writing the code,” Temkin adds. “You’re writing pleas to create that code, and you have to ask nicely. For example, if you call Zeus father of the gods, you can’t call him that again immediately because he doesn’t think you’re trying very hard.” “And then of course, to end a block of code, you have to call on Hades to collect the souls of all the unused variables. And so on,” Temkin says. The History of Esoteric Programming Languages Temkin continues a long-running tradition: esoteric languages date back to the early days of computing, with examples such as INTERCAL (1972), which had cryptic syntax, meaning coders often needed to plead with the compiler to run it. The scene gained momentum in 1993, with Wouter van Oortmerssen’s FALSE , in which most syntax maps to a single character. Despite this, FALSE is a Turing-complete language that allows creating programs as complex as any contemporary programming language. Its syntactical restrictions meant the compiler (which translates the syntax to machine-readable instructions) is only 1 kilobyte, compared to C++ compilers, which were generally hundreds of kilobytes. Exploring further, Chris Pressey wondered why code always had to be written from left to right and created Befunge in 1993. “It took the idea of the single-character commands and said if you’re going to have commands that are only one letter, why do we need to read it left to right?” says Temkin. “Why can’t we have code move a little bit to the right, then turn up, and then go off the page and come up off the bottom and so on?“ So Pressey decided to create a language that would be the most difficult language to build a compiler for,” Temkin continues. “I believe that was the original idea, allowing the code to turn in different directions and flow across the space.” Much of the mid-90s trend coincided with the rise of shareware , the demo scene , and the nascent days of the Internet, when it was necessary to program everything to be as small as possible to share it. “There’s definitely a lot of crossover between these things because they involve this kind of artistry, but also a kind of technical wizardry in showing, ‘Look how much I can do with this really minimal program,’ ” Temkin says. “What really interested me in esoteric languages specifically is the way that it’s community-based,” Temkin says. “If you make a language, it’s an invitation for other people to use the language. And when you make a language and somebody else shows you what’s possible to do with your language or discovers something new about it that you couldn’t have foreseen on your own.” One of Temkin’s esoteric languages uses a cuneiform script. Daniel Temkin You can play with many of Temkin’s languages on his website , as well as the Esoteric Languages Wiki , which raises the question: In the modern connected age, how does one create a shareable esoteric language? “It’s something that I’ve changed my attitude about over the years,” says Temkin. “Early on, I thought I had to write a serious compiler for my language. But now I think what’s really important is that people across different platforms and spaces can use it. So in general, I try to write everything in JavaScript when I can and have it run in the browser. If I don’t, then I tend to stick with Python as it has the largest user base. But I do get a little bored with those two languages.” “I realize there’s a certain irony there,” Temkin adds.",
    "published": "Thu, 04 Sep 2025 14:50:55 +0000",
    "author": "Chris Chinchilla",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Teach 5G Hands-On with TIMS Lab Experiments",
    "link": "https://content.knowledgehub.wiley.com/unravelling-5g-complexity-engaging-students-with-tims-powered-hands-on-education/",
    "summary": "Boost Student Comprehension in Telecoms with Interactive 5G Labs. Teaching complex 5G and telecommunications concepts can be challenging – students often struggle to connect theory with real-world applications. Traditional lecture-based methods may fail to engage, leaving gaps in understanding critical technologies like OFDM, channel coding, and signal modulation. The Telecommunications Instructional Modelling System (TIMS) bridges this gap by transforming abstract concepts into tangible, hands-on experiments. Designed for EE/ECE/EET educators, TIMS enables students to model 5G systems, measure real signals, and validate theory through interactive labs – boosting engagement and retention. Download this free whitepaper now!",
    "published": "Thu, 28 Aug 2025 16:10:47 +0000",
    "author": "Emona Instruments",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Does Computing Face a Lean Future?",
    "link": "https://spectrum.ieee.org/processor-efficiency-computing-lean-metric",
    "summary": "In July, a University of Michigan computer engineering professor put out a new idea for measuring the efficiency of a processor design . Todd Austin’s LEAN metric received both praise and skepticism, but even the critics understood the rationale: A lot of silicon is devoted to things that are not actually doing computing. For example, more than 95 percent of an Nvidia Blackwell GPU is designated for other tasks, Austin told IEEE Spec trum . It’s not like these parts aren’t doing important things, such as choosing the next instruction to execute, but Austin believes processor architectures can and should move toward designs that maximize computing and minimize everything else. Todd Austin Todd Austin is a professor of electrical engineering and computer science at the University of Michigan in Ann Arbor. What does the LEAN score measure? Todd Austin: LEAN stands for Logic Executing Actual Numbers. A score of 100 percent—an admittedly unreachable goal—would mean that every transistor is computing a number that contributes to the final results of a program. Less than 100 percent means that the design devotes silicon and power to inefficient computing and to logic that doesn’t do computing. What’s this other logic doing? Austin: If you look at how high-end architectures have been evolving, you can divide the design into two parts: the part that actually does the computation of the program and the part that decides what computation to do. The most successful designs are squeezing that “deciding what to do” part down as much as possible. Where is computing efficiency lost in today’s designs? Austin: The two losses that we experience in computation are precision loss and speculation loss. Precision loss means you’re using too many bits to do your computation. You see this trend in the GPU world. They’ve gone from 32-bit floating-point precision to 16-bit to 8-bit to even smaller. These are all trying to minimize precision loss in the computation. Speculation loss comes when instructions are hard to predict. [ Speculative execution is when the computer guesses what instruction will come next and starts working even before the instruction arrives.] Routinely, in a high-end CPU, you’ll see two [speculative] instruction results thrown away for every one that is usable. You’ve applied the metric to an Intel CPU, an Nvidia GPU, and Groq ’s AI inference chip. Find anything surprising? Austin: Yeah! The gap between the CPU and the GPU was a lot less than I thought it would be. The GPU was more than three times better than the CPU. But that was only 4.64 percent [devoted to efficient computing] versus 1.35 percent. For the Groq chip, it was 15.24 percent. There’s so much of these chips that’s not directly doing compute. What’s wrong with computing today that you felt like you needed to come up with this metric? Austin: I think we’re actually in a very good state. But it’s very apparent when you look at AI scaling trends that we need more compute, bigger access to memory, more memory bandwidth. And this comes around at the end of Moore’s Law . As a computer architect, if you want to create a better computer, you need to take the same 20 billion transistors and rearrange them in a way that is more valuable than the previous arrangement. I think that means we’re going to need leaner and leaner designs. This article appears in the September 2025 print issue as “ Todd Austin .”",
    "published": "Mon, 25 Aug 2025 14:00:03 +0000",
    "author": "Samuel K. Moore",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Learning More With Less",
    "link": "https://spectrum.ieee.org/africa-engineering-hardware",
    "summary": "My name is Engineer Bainomugisha. Yes, Engineer is my first name and also my career. My parents named me Engineer, and they recognized engineering traits in me from childhood, such as perseverance, resilience, and wanting to understand how things work. I grew up and spent my early years in a rural part of Uganda, more than 300 kilometers outside of Kampala, the capital city. As a young boy, I was always tinkering and hustling: I harvested old radio batteries to power lighting, created household utensils from wood, and herded animals and sold items to help the village make money. The Student & the Professor Two perspectives on engineering education in Africa Johnson I. Ejimanya is a one-man pony express. Walking the exhaust-fogged streets of Owerri, Nigeria, Ejimanya, the engineering dean of the Federal University of Technology, Owerri, carries with him a department’s worth of communications, some handwritten, others on disk. He’s delivering them to a man with a PC and an Internet connection who converts the missives into e-mails and downloads the responses. To Ejimanya, broadband means lugging a big bundle of printed e-mails back with him to the university, which despite being one of the country’s largest and most prestigious engineering schools, has no reliable means of connecting to the Internet. I met Ejimanya when I visited Nigeria in 2003 to report on how the SAT-3/WASC, the first undersea fiber-optic cable to connect West Africa to the world, was being used. (The passage above is from my February 2004 IEEE Spectrum article “ Surf Africa .”) Beyond the lack of computers and Internet access, I saw labs filled with obsolete technology from the 1960s. If students needed a computer or to get online, they went to an Internet cafe, their out-of-pocket costs a burden on them and their families. So is the situation any better 20-plus years on? The short answer is yes. But as computer science professor Engineer Bainomugisha and IEEE student member Oluwatosin Kolade attest in the following pages, there’s still a long way to go. Both men are engineers but at different stages of their academic journey: Bainomugisha went to college in the early 2000s and is now a computer science professor at Makerere University in Kampala, Uganda. Kolade is in his final semester as a mechanical engineering student at Obafemi Awolowo University in Ilé-Ifẹ̀, Nigeria. They describe the challenges they face and what they see as the path forward for a continent brimming with aspiring engineers but woefully short on the resources necessary for a robust education. —Harry Goldstein In high school, I studied physics, chemistry, maths, and biology. When I started studying at Makerere University , in Kampala, I joined the computer science program. This was in 2003. I had never interfaced with a computer before, and this was true for many of my classmates. The limited number of computers meant that student Internet cafés were common, requiring one to pay 500 Ugandan shillings (US $0.14) for 30 minutes. Access to programmable hardware was limited, with no access to microcontrollers or hardware manufacturing. Once I got the basic introduction to computer science, I was eager to build things with what was available to solve problems for the people around me. At the time, phones were very limited, and it was expensive to make calls, so SMS text messages were very popular. Students, the majority of whom didn’t own phones, needed some way to send texts without one. In my first year, I built a free Web-based SMS platform that allowed people to send messages easily. It quickly gained popularity among university students—a good outcome for my first “product.” After I graduated in 2006 with a bachelor’s degree in computer science, Professor Venansius Baryamureeba , then the dean of the faculty of computing and information technology at Makerere, inspired me to apply for graduate school in Belgium. I received a scholarship to pursue a master’s degree at Vrije Universiteit Brussel (Free University of Brussels). Related: What I Learned From a Janky Drone There, I encountered Arduino microcontroller boards for the first time. I witnessed undergraduate students using Arduino boards and sensors to implement embedded-systems projects, such as autonomous devices that could detect, identify, sense, and control their surroundings. I wondered how long it would take for universities in Africa to gain access to such hardware. After all, Arduino’s motto is “Empowering anyone to innovate,” but unfortunately, that empowerment had yet to reach sub-Saharan Africa. Fast forward to today, and the situation has drastically changed. Laptops are now widely available in Africa, Internet connectivity is faster, and smartphones and mobile Internet are common among computer science faculty and students. But the lag between the launch of a technology and its availability in Africa remains significant, as Oluwatosin Kolade’s story illustrates [see “ Lessons from a Janky Drone ,”]. Africa has immense potential for computer science and electronics engineering to address a wide range of challenges. Existing software solutions may be insufficient, and the public digital infrastructure may be lacking, so projects at the intersection of hardware and software could fill critical gaps. However, it is crucial for students to get better learning opportunities to interact with and build physical systems . There is a wide range of exciting applications in agriculture, transportation, education, and environmental monitoring, which is likely why Kolade’s engineering professor encouraged his team’s surveillance drone project despite the difficulties they encountered. Access to Hardware Remains a Bottleneck While the bottlenecks in hardware access for students and researchers in Africa have eased since my time as a student, obstacles persist. As Kolade attests, significant challenges exist in both scholastic funding and the supply chain. This hampers learning and places a large financial burden on young people. As Kolade explains, students must fund their undergraduate projects out of their own pockets, creating significant barriers for people with limited financial resources. The AirQo project [circuit boards shown here] gives students access to 3D printers, soldering stations, and basic sensor boards and components. Andrew Esiebo Electronics components must often be sourced from outside the continent, primarily from China, Europe, or the United States. While the number of online stores has increased, the time span from order to delivery can be several months. It is not uncommon for affordable shipping options to require 60 days or more, while faster delivery options can be several times more expensive than the hardware itself. Online shopping, while often necessary, presents an unavoidable complexity for students and faculty, especially if they have limited access to credit and debit cards. By contrast, students in Europe can receive their components within a week, allowing them to complete a hardware project and initiate new iterations before their counterparts in Africa even receive their hardware for initial building. What’s more, some vendors may choose not to ship to addresses in Africa due to transit risks coupled with real or perceived customs complexities. Customs and tax clearance procedures can indeed be burdensome, with import duties of up to 75 percent in some countries. While some countries in the region offer tax exemptions for educational resources, such exemptions are often difficult to obtain for individual components, or the procedures are unclear and cumbersome. Local vendors, mostly startups and tech hubs, are emerging, but they often lack sufficient stock and may not be able to fulfill bulk orders from educational institutions. Hardware Access Can Accelerate Education In light of these challenges, universities and students might be tempted to shift their focus to purely software projects or otherwise alter their priorities. However, this limits both education and innovation. Engineering projects that involve both hardware and software awaken students’ creativity and foster in-depth skills acquisition. Africa must seek viable solutions. University programs should increase their support of students by providing access to specialized makerspaces and fabrication hubs equipped with the necessary hardware and electronic components. The emergence of high-end makerspaces is encouraging, but the focus should be on providing essential components, such as sensors. Students can learn only so much in makerspaces that have 3D printers but no 3D-printing filament, or printed circuit board fabrication and assembly but no sensor components. Community groups and workshops focused on hardware projects can help address the accessibility challenges. These communities could tap into the global open-source hardware groups for education and research. Data Science Africa , a nonprofit that trains Africans in data science and machine learning, has run hardware sessions that could potentially be scaled to reach many more students. The emergence of research teams working on large-scale projects involving the development and deployment of hardware systems also presents opportunities for students and staff to access facilities and prototype quickly. Showcasing hardware projects from the continent and sharing lessons learned, successful or not, can inspire new projects. For example, at Makerere University—where I am now a computer science professor and the department chair—the AirQo project, which focuses on environmental sensing, provides access to key equipment, including 3D printers, soldering stations, and basic sensor boards and other electronic components. Despite the persistent challenges of supply-chain delays, import duties, and limited local vendors that continue to hamper access to hardware across African universities, the continent’s engineering students and educators are finding creative ways to build, innovate, and learn. From my own journey from rural Uganda to pioneering SMS platforms and the emergence of makerspaces and research projects like AirQo, to collaborative communities that connect local innovators with global open-source networks, Africa is steadily closing the technology gap. The question is no longer whether African students can compete in hardware innovation—it’s how quickly the world will recognize that some of tomorrow’s groundbreaking solutions are already being prototyped in labs from Kampala to Cape Town. They are being built by students like Oluwatosin Kolade, who learned to engineer solutions with whatever he could get his hands on. Imagine what they could do if they had access to the same resources I had in graduate school. African engineering potential is limitless, but to reach our full potential, we need access to technology that is more readily available in much of the world.",
    "published": "Wed, 20 Aug 2025 13:05:03 +0000",
    "author": "Engineer Bainomugisha",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "The Accidental Engineer Who Conjured Up Extended Reality",
    "link": "https://spectrum.ieee.org/engineer-conjured-up-extended-reality",
    "summary": "In the 1980s, people weren’t wearing head-mounted cameras, displays, or computers . Except for high school student Steve Mann , who regularly wore his homemade electronic computer vision system (seeing aid). Back then, Mann attracted stares, questions, suspicion, and sometimes hostility. But it didn’t stop him from refining the technology he developed. It now underlies augmented-reality eyeglasses —including those by Google and Magic Leap —that are used in operating rooms and industrial settings such as factories and warehouses. Steve Mann Employer: University of Toronto Job title: Professor of electrical and computer engineering, computer science, and forestry Member grade: Fellow Alma maters: McMaster University in Hamilton, Ontario; MIT Although head-mounted computers haven’t reached smartphone-level ubiquity, when Mann wears XR (eXtended Reality, something he and Charles Wyckoff invented at MIT in 1991 ) gear these days as a professor of electrical and computer engineering, computer science, and forestry at the University of Toronto , he doesn’t turn as many heads as he used to. In part because of his inventiveness and creativity, the IEEE Fellow was honored for his contributions to wearable computing and the concept of sousveillance —the practice of using personal recording devices to watch the watchers and invert traditional surveillance power structures—with this year’s IEEE Masaru Ibuka Consumer Technology Award. S ponsored by Sony , the award was bestowed by the IEEE Consumer Technology Society at the Consumer Electronics Show held in January in Las Vegas. Empowering people through wearable tech Mann is regarded as the “father of wearable computing .” Asked what he thinks about the moniker, he says it’s less about the title and more about empowering people to see the world—and themselves—in new ways. His research and systematic reimagining of how electronic devices can support and extend human abilities, especially vision, have yielded benefits for society. Among them are assisting the visually impaired with the ability to identify objects and enabling experts to remotely view what frontline workers see and then guide them from afar. His IEEE award came one month after he received the Lifeboat Foundation ’s Guardian Award , given to a scientist or public figure “who has warned of a future fraught with dangers and encouraged measures to prevent them.” The foundation is a nonprofit, nongovernmental organization dedicated to encouraging scientific advancements while helping humanity survive existential risks and possible misuse of increasingly powerful technologies including genetic engineering, nanotechnology, and robotics/AI. A natural-born tinkerer It stands to reason that Mann would become a leading tinkerer. His earliest memories are of welding with his grandfather and knitting with his grandmother—unusual hobbies for a typical 4-year-old, though not in Mann’s family. His father, who worked for a men’s clothing company, supplemented his income by buying and renovating houses, long before the concept of flipping houses became widespread. “We were always living in a house under construction,” Mann recalls. “I used to help my dad fix things when I was 4 or 5—hammer in my hand—normal stuff.” His grandfather, a refrigeration engineer, taught him how to weld. By age 6, he was wiring and building homemade radios. By the time he was 8, he had started a neighborhood repair business, fixing televisions and radios. “In a sense, preschool for me was learning engineering and science,” Mann says with a laugh. “I grew up putting together wood, metal, or fabric. I knew how to make things at a very young age.” Learning to see what others miss When Mann was 12 years old, his father brought home a broken oscillograph (an early version of the oscilloscope, used to display variations in voltage or current as visual waveforms). It turned out to be a defining moment in his life. Too impatient to accept that the waveform dot on the machine’s display moved only up and down instead of both vertically and horizontally, Mann invented a way to push its image through physical space. He placed the oscillograph—which he now keeps on a shelf in his laboratory—on a board mounted on roller skate wheels. He connected the device to a police radar and rolled it back and forth. When he realized the machine’s motion, combined with the dot’s vertical movement, created visible waveforms of the radar’s signals, as a function of space rather than time, he unknowingly made a revolutionary discovery. Later he would describe that merging of physical and virtual worlds as “extended reality”—a concept that underlies today’s AR and XR technologies. It wouldn’t be the last time Mann’s curiosity turned a problem into an opportunity. Decades later, on the main floor of his Toronto home, he co-founded InteraXon , the Toronto-based company behind the Muse brain-sensing headband , used to help people manage sleep, stress, and mental health. Mann shares legendary 1970s Xerox PARC researcher Alan Kay ’s belief that “ The best way to predict the future is to invent it .” Mann, however, adds: “Sometimes you invent it by simply refusing to accept the limitations of the present.” A member of MIT’s Media Lab In high school, Mann won several math competitions designed to challenge students at university level. In 1982 he enrolled in McMaster University , in Hamilton, Ontario, to pursue a degree in engineering physics (an interdisciplinary program that combines physics, mathematics, and engineering principles). As an undergraduate, Mann was already experimenting with early prototypes of wearable computers—head-mounted displays, body-worn cameras, and portable computing systems that predated mainstream mobile tech by decades. Mann [far right] sits alongside fellow MIT Media Lab graduate students, modeling the wearable computers or smart clothes they were developing as part of their Ph.D. research. Pam Berry/The Boston Globe/Getty Images He earned a bachelor’s degree in 1986. He continued his studies at McMaster to earn a second bachelor’s degree in electrical engineering in 1989, then a master’s degree in engineering in 1991. He then enrolled in a doctoral program at MIT , where he joined its renowned Media Lab , a hotbed for unconventional research blending technology, design, and the human experience. He formalized and expanded his ideas around wearable computing, wearable computer vision systems, and wearable AI. He also published some of the earliest academic papers that described the concept of sousveillance. He completed his Ph.D. in media arts and sciences in 1997. Mann’s doctoral research contributed foundational concepts and hardware that influenced future smart glasses and devices for life logging, the practice of creating a digital record of one’s daily life. He also helped blaze a trail for the fields of augmented reality and ubiquitous computing . Knitting passions into a unique academic career After completing his Ph.D., Mann returned to Canada and took a position at the University of Toronto as a professor of electrical and computer engineering in 1998. He says he is equally as fascinated by how technology interacts with the natural world as he is by how to remove barriers between the physical world and virtual world. His interests connect to what he calls “vironmentalism,” which regards technology as a boundary between our environment and our “vironment” (ourselves). This gives rise to his vision of “mersive” technologies that link humans not just to each other but also to the environment around them. “Go beyond [what’s covered at] school. Define yourself by what you love so much you’d do it [even if no teachers or managers were demanding it]. AI can replace a walking encyclopedia. It can’t replace passion.” “It’s advancing technology for humanity and Earth,” he says, riffing on IEEE’s mission statement. His guiding principle also explains his cross-appointment in the University of Toronto’s forestry department (now part of the Faculty of Architecture, Landscape, and Design)—an unusual entry on an electrical and computer engineering professor’s CV. IEEE and building community Prior to his groundbreaking doctoral work at MIT, Mann had already joined IEEE in 1988. He credits the organization with connecting him to pioneers like Simon Haykin , the radar visionary he met at McMaster while he was in high school. Haykin pushed him to dream big, he says. Mann has been active in the IEEE Computer and IEEE Consumer Technology societies. He has served as an organizer, session chair, and program committee member for IEEE conferences related to wearable computing and pervasive sensing. In 1997 he helped found the International Symposium on Wearable Computers , and numerous other wearable computing symposia, conferences, and events. He has given keynote talks and presented papers on topics including sousveillance, ubiquitous computing, and other humanistic aspects of technology at the IEEE International Symposium on Technology and Society and the IEEE International Conference on Pervasive Computing and Communications . His contributions include influential papers in IEEE journals, especially various IEEE Transactions and Computer Society magazines. Probably his most well-known paper is “ Wearable Computing .” Published in Computer magazine in October 1997, the seminal work outlined the structure and vision for wearable computing as a formal research field. He also contributed articles on sousveillance—exploring the intersection of technology, ethics, and human rights—in IEEE Technology and Society Magazine . He has collaborated with other IEEE members to develop frameworks for wearable computing standards , particularly around human-computer interfaces and privacy considerations. Forever the inventor Mann continues to teach, run his lab, and test new frontiers of wearable devices , smart clothing , and immersive environments . He’s still driven, he says, by the same forces that powered his backyard experiments as a child: curiosity and passion. For students who hope to follow in his footsteps, Mann’s advice is simple: “Go beyond [what’s covered at] school. Don’t define yourself by the classes you took or the jobs you had. Define yourself by what you love so much you’d do it “even if no teachers or managers were demanding it”. He adds that, “AI can replace a walking encyclopedia. It can’t replace passion.” Mann says he has no plans to retire. If anything, he says, his most productive years are yet to come. “I feel like I’m a late bloomer,” he says, chuckling at the irony. “I was fixing radios when I was 8, but my best work? That’s going to happen between 65 and 85.”",
    "published": "Tue, 19 Aug 2025 18:00:04 +0000",
    "author": "Willie D. Jones",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "The AI Agents of Tomorrow Need Data Integrity",
    "link": "https://spectrum.ieee.org/data-integrity",
    "summary": "Think of the Web as a digital territory with its own social contract. In 2014, Tim Berners-Lee called for a “Magna Carta for the Web” to restore the balance of power between individuals and institutions. This mirrors the original charter’s purpose: ensuring that those who occupy a territory have a meaningful stake in its governance. Web 3.0 —the distributed, decentralized Web of tomorrow—is finally poised to change the Internet’s dynamic by returning ownership to data creators. This will change many things about what’s often described as the “CIA triad” of digital security: confidentiality, integrity, and availability. Of those three features, data integrity will become of paramount importance. Ariane 5 Rocket (1996) Processing integrity f ailure A 64-bit velocity calculation was converted to a 16-bit output, causing an error called overflow. The corrupted data triggered catastrophic course corrections that forced the US $370 million rocket to self-destruct. When we have agency in digital spaces, we naturally maintain their integrity—protecting them from deterioration and shaping them with intention. But in territories controlled by distant platforms, where we’re merely temporary visitors, that connection frays. A disconnect emerges between those who benefit from data and those who bear the consequences of compromised integrity. Like homeowners who care deeply about maintaining the property they own, users in the Web 3.0 paradigm will become stewards of their personal digital spaces. This will be critical in a world where AI agents don’t just answer our questions but act on our behalf. These agents may execute financial transactions, coordinate complex workflows, and autonomously operate critical infrastructure, making decisions that ripple through entire industries. As digital agents become more autonomous and interconnected, the question is no longer whether we will trust AI but what that trust is built upon. In the new age we’re entering, the foundation isn’t intelligence or efficiency—it’s integrity. What Is Data Integrity? In information systems, integrity is the guarantee that data will not be modified without authorization, and that all transformations are verifiable throughout the data’s life cycle. While availability ensures that systems are running and confidentiality prevents unauthorized access, integrity focuses on whether information is accurate, unaltered, and consistent across systems and over time. NASA Mars Climate Orbiter (1999) Processing integrity f ailure Lockheed Martin’s software calculated thrust in pound-seconds, while NASA’s navigation software expected newton-seconds. The failure caused the $328 million spacecraft to burn up in the Mars atmosphere. It’s a new idea. The undo button, which prevents accidental data loss, is an integrity feature. So is the reboot process, which returns a computer to a known good state. Checksums are an integrity feature; so are verifications of network transmission. Without integrity, security measures can backfire. Encrypting corrupted data just locks in errors. Systems that score high marks for availability but spread misinformation just become amplifiers of risk. All IT systems require some form of data integrity, but the need for it is especially pronounced in two areas today. First: Internet of Things devices interact directly with the physical world, so corrupted input or output can result in real-world harm. Second: AI systems are only as good as the integrity of the data they’re trained on, and the integrity of their decision-making processes. If that foundation is shaky, the results will be too. Integrity manifests in four key areas. The first, input integrity, concerns the quality and authenticity of data entering a system. When this fails, consequences can be severe. In 2021, Facebook’s global outage was triggered by a single mistaken command—an input error missed by automated systems. Protecting input integrity requires robust authentication of data sources, cryptographic signing of sensor data, and diversity in input channels for cross-validation. The second issue is processing integrity, which ensures that systems transform inputs into outputs correctly. In 2003, the U.S.–Canada blackout affected 55 million people when a control-room process failed to refresh properly, resulting in damages exceeding US $6 billion. Safeguarding processing integrity means formally verifying algorithms, cryptographically protecting models, and monitoring systems for anomalous behavior. Microsoft’s Tay Chatbot (2016) Processing integrity f ailure Released on Twitter, Microsoft ’s AI chatbot was vulnerable to a “repeat after me” command, which meant it would echo any offensive content fed to it. Storage integrity covers the correctness of information as it’s stored and communicated. In 2023, the Federal Aviation Administration was forced to halt all U.S. departing flights because of a corrupted database file. Addressing this risk requires cryptographic approaches that make any modification computationally infeasible without detection, distributed storage systems to prevent single points of failure, and rigorous backup procedures. Finally, contextual integrity addresses the appropriate flow of information according to the norms of its larger context. It’s not enough for data to be accurate; it must also be used in ways that respect expectations and boundaries. For example, if a smart speaker listens in on casual family conversations and uses the data to build advertising profiles, that action would violate the expected boundaries of data collection. Preserving contextual integrity requires clear data-governance policies, principles that limit the use of data to its intended purposes, and mechanisms for enforcing information-flow constraints. As AI systems increasingly make critical decisions with reduced human oversight, all these dimensions of integrity become critical. The Need for Integrity in Web 3.0 As the digital landscape has shifted from Web 1.0 to Web 2.0 and now evolves toward Web 3.0, we’ve seen each era bring a different emphasis in the CIA triad of confidentiality, integrity, and availability. Boeing 737 MAX (2018) Input integrity f ailure Faulty sensor data caused a n automated flight-control system to repeatedly push the airplane’s nose down, leading to a fatal crash. Returning to our home metaphor: When simply having shelter is what matters most, availability takes priority—the house must exist and be functional. Once that foundation is secure, confidentiality becomes important—you need locks on your doors to keep others out. Only after these basics are established do you begin to consider integrity, to ensure that what’s inside the house remains trustworthy, unaltered, and consistent over time. Web 1.0 of the 1990s prioritized making information available. Organizations digitized their content, putting it out there for anyone to access. In Web 2.0, the Web of today, platforms for e-commerce, social media, and cloud computing prioritize confidentiality, as personal data has become the Internet’s currency. Somehow, integrity was largely lost along the way. In our current Web architecture, where control is centralized and removed from individual users, the concern for integrity has diminished. The massive social media platforms have created environments where no one feels responsible for the truthfulness or quality of what circulates. SolarWinds Supply-Chain Attack (2020) Storage integrity f ailure Russian hackers compromised the process that SolarWinds used to package its software, injecting malicious code that was distributed to 18,000 customers, including nine federal agencies. The hack remained undetected for 14 months. Web 3.0 is poised to change this dynamic by returning ownership to the data owners. This is not speculative; it’s already emerging. For example, ActivityPub , the protocol behind decentralized social networks like Mastodon , combines content sharing with built-in attribution. Tim Berners-Lee’s Solid protocol restructures the Web around personal data pods with granular access controls. These technologies prioritize integrity through cryptographic verification that proves authorship, decentralized architectures that eliminate vulnerable central authorities, machine-readable semantics that make meaning explicit—structured data formats that allow computers to understand participants and actions, such as “Alice performed surgery on Bob”—and transparent governance where rules are visible to all. As AI systems become more autonomous, communicating directly with one another via standardized protocols, these integrity controls will be essential for maintaining trust. Why Data Integrity Matters in AI For AI systems, integrity is crucial in four domains. The first is decision quality. With AI increasingly contributing to decision-making in health care, justice, and finance, the integrity of both data and models’ actions directly impact human welfare. Accountability is the second domain. Understanding the causes of failures requires reliable logging, audit trails, and system records. ChatGPT Data Leak (2023) Storage integrity f ailure A bug in OpenAI’s ChatGPT mixed different users’ conversation histories. Users suddenly had other people’s chats appear in their interfaces with no way to prove the conversations weren’t theirs. The third domain is the security relationships between components. Many authentication systems rely on the integrity of identity information and cryptographic keys. If these elements are compromised, malicious agents could impersonate trusted systems, potentially creating cascading failures as AI agents interact and make decisions based on corrupted credentials. Finally, integrity matters in our public definitions of safety. Governments worldwide are introducing rules for AI that focus on data accuracy, transparent algorithms, and verifiable claims about system behavior. Integrity provides the basis for meeting these legal obligations. The importance of integrity only grows as AI systems are entrusted with more critical applications and operate with less human oversight. While people can sometimes detect integrity lapses, autonomous systems may not only miss warning signs—they may exponentially increase the severity of breaches. Without assurances of integrity, organizations will not trust AI systems for important tasks, and we won’t realize the full potential of AI. How to Build AI Systems With Integrity Imagine an AI system as a home we’re building together. The integrity of this home doesn’t rest on a single security feature but on the thoughtful integration of many elements: solid foundations, well-constructed walls, clear pathways between rooms, and shared agreements about how spaces will be used. Midjourney Bias (2023) Contextual integrity failure Users discovered that the AI image generator often produced biased images of people, such as showing white men as CEOs regardless of the prompt. The AI tool didn’t accurately reflect the context requested by the users. We begin by laying the cornerstone: cryptographic verification . Digital signatures ensure that data lineage is traceable, much like a title deed proves ownership. Decentralized identifiers act as digital passports, allowing components to prove identity independently. When the front door of our AI home recognizes visitors through their own keys rather than through a vulnerable central doorman, we create resilience in the architecture of trust. Formal verification methods enable us to mathematically prove the structural integrity of critical components, ensuring that systems can withstand pressures placed upon them—especially in high-stakes domains where lives may depend on an AI’s decision. Just as a well-designed home creates separate spaces, trustworthy AI systems are built with thoughtful compartmentalization. We don’t rely on a single barrier but rather layer them to limit how problems in one area might affect others. Just as a kitchen fire is contained by fire doors and independent smoke alarms, training data is separated from the AI’s inferences and output to limit the impact of any single failure or breach. Throughout this AI home, we build transparency into the design: The equivalent of large windows that allow light into every corner is clear pathways from input to output. We install monitoring systems that continuously check for weaknesses, alerting us before small issues become catastrophic failures. Prompt Injection Attacks (2023–2024) Input integrity failure Attackers embedded hidden prompts in emails, documents, and websites that hijacked AI assistants, causing them to treat malicious instructions as legitimate commands. But a home isn’t just a physical structure, it’s also the agreements we make about how to live within it. Our governance frameworks act as these shared understandings. Before welcoming new residents, we provide them with certification standards. Just as landlords conduct credit checks, we conduct integrity assessments to evaluate newcomers. And we strive to be good neighbors, aligning our community agreements with broader societal expectations. Perhaps most important, we recognize that our AI home will shelter diverse individuals with varying needs. Our governance structures must reflect this diversity, bringing many stakeholders to the table. A truly trustworthy system cannot be designed only for its builders but must serve anyone authorized to eventually call it home. That’s how we’ll create AI systems worthy of trust: not by blindly believing in their perfection but because we’ve intentionally designed them with integrity controls at every level. A Challenge of Language Unlike other properties of security, like “available” or “private,” we don’t have a common adjective form for “integrity.” This makes it hard to talk about it. It turns out that there is a word in English: “integrous.” The Oxford English Dictionary recorded the word used in the mid-1600s but now declares it obsolete . CrowdStrike Outage (2024) Processing integrity failure A faulty software update from CrowdStrike caused 8.5 million Windows computers worldwide to crash—grounding flights, shutting down hospitals, and disrupting banks. The update, which contained a software logic error, hadn’t gone through full testing protocols. CrowdStrike Outage (2024) Processing integrity failure A faulty software update from CrowdStrike caused 8.5 million Windows computers worldwide to crash—grounding flights, shutting down hospitals, and disrupting banks. The update, which contained a software logic error, hadn’t gone through full testing protocols. We believe that the word needs to be revived. We need the ability to describe a system with integrity. We must be able to talk about integrous systems design. The Road Ahead Ensuring integrity in AI presents formidable challenges. As models grow larger and more complex, maintaining integrity without sacrificing performance becomes difficult. Integrity controls often require computational resources that can slow systems down—particularly challenging for real-time applications. Another concern is that emerging technologies like quantum computing threaten current cryptographic protections . Additionally, the distributed nature of modern AI—which relies on vast ecosystems of libraries, frameworks, and services—presents a large attack surface. Beyond technology, integrity depends heavily on social factors. Companies often prioritize speed to market over robust integrity controls. Development teams may lack specialized knowledge for implementing these controls, and may find it particularly difficult to integrate them into legacy systems. And while some governments have begun establishing regulations for aspects of AI, we need worldwide alignment on governance for AI integrity. Voice-Clone Scams (2024) Input and processing integrity failure Scammers used AI-powered voice-cloning tools to mimic the voices of victims’ family members, tricking people into sending money. These scams succeeded because neither phone systems nor victims identified the AI-generated voice as fake. Addressing these challenges requires sustained research into verifying and enforcing integrity, as well as recovering from breaches. Priority areas include fault-tolerant algorithms for distributed learning, verifiable computation on encrypted data, techniques that maintain integrity despite adversarial attacks, and standardized metrics for certification. We also need interfaces that clearly communicate integrity status to human overseers. As AI systems become more powerful and pervasive, the stakes for integrity have never been higher. We are entering an era where machine-to-machine interactions and autonomous agents will operate with reduced human oversight and make decisions with profound impacts. The good news is that the tools for building systems with integrity already exist. What’s needed is a shift in mind-set: from treating integrity as an afterthought to accepting that it’s the core organizing principle of AI security. The next era of technology will be defined not by what AI can do, but by whether we can trust it to know or especially to do what’s right. Integrity—in all its dimensions—will determine the answer.",
    "published": "Mon, 18 Aug 2025 13:00:05 +0000",
    "author": "Bruce Schneier",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Is the World Adopting Post-Quantum Cryptography Fast Enough?",
    "link": "https://spectrum.ieee.org/post-quantum-cryptography-standards-nist",
    "summary": "A year ago today, the National Institute of Standards and Technology (NIST) published the first-ever official standard for post-quantum cryptography (PQC) algorithms. The standard was a result of a 2022 memorandum from the Biden administration that requires federal agencies to transition to PQC-based security by 2035. Cryptography relies on math problems that are nearly impossible to solve, but easy to check if a solution is correct. Armed with such math problems, only the holder of a secret key can check their solution and get access to the secret data. Today, most online cryptography relies on one of two such algorithms: either RSA or elliptic-curve cryptography . The cause for concern is that quantum computers, if a large enough one is ever built, would make easy work of the “hard” problems underlying current cryptographic methods. Luckily, there are other math problems that appear to be equally hard for quantum computers and their existing classical counterparts. That’s the basis of post-quantum cryptography : cryptography that’s secure against hypothetical quantum computers. With the mathematics behind PQC ironed out, and standards in hand, the work of adoption is now underway. This is no easy feat: every computer, laptop, smartphone, self-driving car, or IoT device will have to fundamentally change the way they run cryptography. Ali El Kaafarani is a research fellow at the Oxford Mathematical Institute who contributed to the development of NIST’s PQC standards. He also founded a company, PQShield , to help bring post-quantum cryptography into the real world by assisting original equipment manufacturers in implementing the new protocols. He spoke with IEEE Spectrum about how adoption is going and whether the new standards will be implemented in time to beat the looming threat of quantum computers. What has changed in the industry since the NIST PQC standards came out? Ali El Kaafarani PQShield Ali El Kaafarani : Before the standards came out, a lot of people were not talking about it at all, in the spirit of “If it’s working, don’t touch it.” Once the standards were published, the whole story changed, because now it’s not hypothetical quantum hype, it’s a compliance issue. There are standards published by the U.S. government. There are deadlines for the adoption. And the 2035 [deadline] came together with the publication from [the National Security Agency] and was adopted in formal legislation that passed Congress, and therefore there is no way around it. Now it’s a compliance issue. Before, people used to ask us, “When do you think we’re going to have a quantum computer?” I don’t know when we’re going to have a quantum computer. But that’s the issue, because we’re talking about a risk that can materialize any time. Some other, more intelligent people who have access to a wider range of information decided in 2015 to categorize quantum computing as a real threat. So this year was a transformational year, because the question went from “Why do we need it?” to “How are we going to use it?” And the whole supply chain started looking into who’s going to do what, from chip design to the network security layer, to the critical national infrastructure, to build up a post-quantum-enabled network security kit. Challenges in PQC Implementation What are some of the difficulties of implementing the NIST standards? El Kaafarani: You have the beautiful math, you have the algorithms from NIST, but you also have the wild west of cybersecurity. That infrastructure goes from the smallest sensors and car keys, etc., to the largest server sitting there and trying to crunch hundreds of thousands of transactions per second, each with different security requirements, each with different energy consumption requirements. Now that is a different problem. That’s not a mathematical problem, that’s an implementation problem. This is where you need a company like PQShield, where we gather hardware engineers, and firmware engineers, and software engineers, and mathematicians, and everyone else around them to actually say, “What can we do with this particular use case?” Cryptography is the backbone of cybersecurity infrastructure, and worse than that, it’s the invisible piece that nobody cares about until it breaks. If it’s working, nobody touches it. They only talk about it when there’s a breach, and then they try to fix things. In the end, they usually put Band-Aids on it. That’s normal, because enterprises can’t sell the security feature to the customers. They were just using it when governments force them, like when there’s a compliance issue. And now it’s a much bigger problem, as someone is telling them, “You know what, all the cryptography that you’ve been using for the past 15 years, 20 years, you need to change it, actually.” Are there security concerns for the PQC algorithm implementations? El Kaafarani: Well, we haven’t done it before. It hasn’t been battle-tested. And now what we’re saying is, “Hey, AMD and the rest of the hardware or semiconductor world, go and put all those new algorithms in hardware, and trust us, they’re going to work fine, and then nobody’s going to be able to hack them and extract the key.” That’s not easy, right? Nobody has the guts to say this. That’s why, at PQShield, we have vulnerability teams that are trying to break our own designs, separately from those teams who are designing things. You have to do this. You need to be one step ahead of attackers. That’s all you need to do, and that’s all you can do, because you can’t say, “Okay, I’ve got something that is secure. Nobody can break it.” If you say that, you’re going to eat a humble pie in 10 years’ time, because maybe someone will come up with a way to break it. You need to just do this continuous innovation and continuous security testing for your products. Because PQC is new, we still haven’t seen all the creativity of attackers trying to bypass the beautiful mathematics and come up with those creative and nasty side-channel attacks that just laugh at the mathematics. For example, some attacks look at the energy consumption the algorithm is taking on your laptop, and they extract the key from the differences in energy consumption. Or there are timing attacks that look at how long it takes for you to encrypt the same message 100 times and how that’s changing, and they can actually extract the key. So there are different ways to attack algorithms there, and that’s not new. We just don’t have billions of these devices in our hands now that have post-quantum cryptography that people have tested. Progress in PQC Adoption How would you say adoption has been going so far? El Kaafarani: The fact that a lot of companies only started when the standards were published, it puts us in a position where there are some that are well advanced in their thoughts and their processes and their adoption, and there are others that are totally new to it because they were not paying attention, and they were just kicking the can down the road. The majority of those who were kicking the can down the road are the ones that don’t sit high up in the supply chain, because they felt like it’s someone else’s responsibility. But they didn’t understand that they had to influence their suppliers when it comes to their requirements and timelines and integration and so many things that they have to prepare. This is what’s going on now: A lot of them are doing a lot of work. Now, those who sit high up in the supply chain, quite a few of them have made great progress and started embedding post-quantum cryptography designs into new products and are trying to work out a way to upgrade products that are already on the ground. I don’t think that we’re in a great place, where everyone is doing what they’re supposed to be doing. That’s not the case. But I think that from last year, when many people were asking “When do you think we’re going to have a quantum computer?” and are now asking “How can I be compliant? Where do you think I should start? And how can I evaluate where the infrastructure to understand where the most valuable assets are, and how can I protect them? What influence can I exercise on my suppliers?” I think huge progress has been made. Is it enough? It’s never enough in security. Security is damn difficult. It’s a multidisciplinary topic. There are two types of people: those who love to build security products, and those who would love to break them. We’re trying to get most of those who love to break them onto the right side of history so that they can make products stronger rather than actually making existing ones vulnerable for exploitation. Do you think we’re going to make it by 2035? El Kaafarani: I think that the majority of our infrastructure should be post-quantum secure by 2035, and that’s a good thing. That’s a good thought to have. Now, what happens if quantum computers happen to become reality before that? That’s a good topic for a TV series or for a movie. What happens when most secrets are readable? People are not thinking hard enough about it. I don’t think that anyone has an answer for that.",
    "published": "Wed, 13 Aug 2025 15:01:19 +0000",
    "author": "Dina Genkina",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Meta’s New Bracelet Reads Hand Gestures",
    "link": "https://spectrum.ieee.org/meta-wristband-interface",
    "summary": "Imagine the ability to control machines with your mind instead of having to type on a keyboard or click on a mouse. Now Facebook’s parent company, Meta, is aiming for the next best thing—a new wristband that can, with the help of AI, infer electrical commands sent from the brain to muscles and convert them into computer signals, all in a noninvasive way. Although experts doubt it will replace keyboards and mice for traditional computing, it might have new uses for a wide range of applications, such as wearable interfaces for mobile devices, or thought-controlled assistive technologies for people with disabilities. The bracelet from Reality Labs at Meta uses metal contacts placed against the skin to detect electrical signals from muscles—a technique known as surface electromyography (sEMG)—which are generated in response to commands from the brain. The highly sensitive new system transmits this data to a computer using Bluetooth to help it recognize gestures such as pointing and pinching in real time, findings detailed in Nature on 23 July. The bracelet is not a direct interface with the brain . “It is not a mind-reading system. It cannot make you act in a different way as imposed by your will; it does not connect you to other people neurally; it does not predict your intentions,” says Dario Farina , chair in neurorehabilitation engineering at Imperial College, London, who did not take part in Meta’s research but has tested the technology. (Meta was unable to make anyone available for comment as of press time.) How AI Enables Meta’s Wristband Previous “neuromotor” devices, such as the discontinued Myo armband , also sought to use sEMG for computer interfaces. A key challenge these earlier devices faced was how they each needed time-consuming personalized calibration for each user to account for differences in human anatomy and behavior. See how the device detects thumb swipes, finger taps, and handwriting gestures. Reality Labs at Meta In contrast, Meta says its bracelet can essentially work off the shelf. The key was training deep-learning artificial intelligence systems on data from more than 6,000 paid volunteers who wore the device. This generated models that could accurately interpret user input across different people without requiring individual calibration, says Joe Paradiso , head of the Responsive Environments Research Group at the MIT Media Lab, who did not participate in this study. “The amount of information decoded with this device is very large, far larger than any previous attempt,” Farina says. “The device can recognize handwriting, for example, which would have not been conceivable before.” The wristband uses surface electromyography to noninvasively measure electrical activity of the muscles that control hand gestures. Reality Labs at Meta A possible concern with using this wristband is how users might not want every hand motion interpreted as input—say, if they had to scratch an itch, or pick up a glass of water. However, the device is trained to recognize only certain commands, while ignoring other gestures. “A potential user of the device can perform any activity of daily living without the risk to activate the device, and yet control the device with the specific gestures for which the device has been trained,” Farina says. In tests, volunteers who had never previously tried the bracelet could use handwriting gestures to input text at 20.9 words per minute. (In comparison, mobile phone keyboard typing speeds average roughly 36 words per minute.) Given this interface speed, “I doubt most computer users would rush out to buy this wristband if it becomes commercially available,” says Eben Kirksey , a professor of anthropology at the University of Oxford who studies the interplay of science, technology, and society. “Most people type at around 40 words a minute, and highly skilled typists can bang out upwards of 100 words a minute. Since this new wearable device only enables users to write 20 words a minute, I doubt many people will want to take the time to learn how to use this new way to interface with computers.” Instead, the new study argues the bracelet might prove useful in scenarios where keyboards or mice would be limiting, such as mobile and wearable applications. “It’s not a keyboard replacement. It’s something else, and I think things like this are needed for the computational paradigms that are coming,” Paradiso says. For example, when it comes to the virtual reality and augmented reality glasses that Meta, Google, and others have pursued, interfaces previously envisioned for these devices include vision-based trackers that track the motions of hands held up in front of users , Paradiso says. However, the fatigue that can set in with this “gorilla arm” approach limits long-term use, and “the need to use space in front of you has its drawbacks,” Paradiso notes. Because this new bracelet interprets electrical signals instead of motion, it could instead let users keep their hands to their sides and interact with devices using subtle finger motions. “Interfaces like these are needed for the everywhere-computing eyewear that’s emerging,” Paradiso says. “My favorite scenario is on a crowded train with my head-worn display catching up with the news, or communicating with friends. You just nudge it around with your hands by your side. Going for a walk, and so on—same deal.” Applications for Accessibility The researchers also suggest the wristband could help people with disabilities better interact with computers, particularly individuals with reduced mobility, muscle weakness, finger amputations, paralysis, and more. “Some members of the disabled community have difficulty typing on conventional keyboards, or using computer mice,” says Kirksey, who did not take part in this research. “This device offers a new option that might help some members of this community, who have very specific bodily challenges, interface with computers.” For such applications, a virtue of the new bracelet is that it is relatively easy to don and doff. In contrast, brain-computer interfaces (BCIs), which also rely on electrical signals from the brain, often require invasive brain surgery. Non-invasive BCIs do exist, such as ones that apply electrodes onto the scalp , “but a wristband is more ergonomic than a skullcap,” Paradiso says. However, many questions remain before this new technology might help people with disabilities. “This type of wearable assumes typical limb shape and fine motor control,” says Solomon Romney , formerly the head of Microsoft’s Inclusive Design Lab. “As a limb-different person, I am always looking for ways to move activities to my limblet rather than continue to overload my typical hand. How easily can it be adjusted to fit nontypical limbs and/or musculature? How does it filter tremors? How effectively would it work for someone with no hands to wear on their ankle?” Ultimately, “the main obstacle is large-scale deployment,” Farina says. “To be distributed to millions of individuals, the system needs to be robust across different anatomies and be used with minimal error rate. To match the error rate of a mouse or keyboard in a vast human population is certainly a huge challenge.”",
    "published": "Wed, 13 Aug 2025 13:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Large Hadron Collider Poised to Receive Major Tech Upgrade",
    "link": "https://spectrum.ieee.org/lhc-radiation-chip",
    "summary": "This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Deep in the belly of the Large Hadron Collider (LHC), about 400 million particle collisions are happening in a single second. But as the LHC undergoes upgrades and becomes the High Luminosity-LHC, the number of collisions will increase to an astounding 1.5 billion collisions or more per second. Capturing all these events via detectors and analyzing the staggering amount of data created from each experiment is no easy feat. Fortunately, a team of scientists has been working for years to create a chip that is capable of digitally examining all 1.5 billion of these collisions in the blink of an eye. Their new chip is described in a study published 28 May in IEEE Open Journal of the Solid-State Circuits Society . The large hadron collider’s hefty computing requirements The LHC, a massive underground facility straddling the border of France and Switzerland, has been smashing particles together since 2008, revealing critical insights into the fundamental laws of physics. However, the system needs a break about every decade to undergo maintenance and technical upgrades. In anticipation of these upgrades, researchers at Columbia University, in New York City, including Peter Kinget in the electrical engineering department, have been designing two specialized chips with collaborators at the University of Texas at Austin. Both chips are designed for the LHC’s ATLAS detector, which investigates a wide range of physics phenomena, from the Higgs boson to extra dimensions and particles that could make up dark matter . The massive detector—at 46 meters long and 25 meters high—is lined with tens of thousands of specialized chips to record collision events. The first chip designed by Kinget and his colleagues is called a “trigger” analog-to-digital converter (ADC) chip. It’s helpful for sifting through the immense amounts of data—roughly 60 petabytes of raw data—created upon particle collisions. Kinget says the ATLAS detector is like a giant 3D camera taking pictures at a very, very high rate. Meanwhile, the trigger system is quickly screening these “pictures,” searching for events that may be useful for further analysis. The trigger system tells the detector to disregard the data that is not of interest, while saving the data of interest. The finalized trigger ADC was incorporated into the ATLAS detector during the last shutdown, which ended in 2022. More recently, they finished designing and testing a second, higher resolution ADC that reads out the signals from the detector and converts it to digital data for analysis. The challenge, however, is that the ADC must be able to cope with the extreme amount of radiation that’s created upon particle collisions. “This environment around this beam is one of the most intense environments you can imagine,” says Kinget. “As a result…very intense radiation is being generated.” Rui Xu, a Ph.D. student in Kinget’s lab who helped design the second ADC chip, says the amount of radiation the chip sees over its lifetime in the HL-LHC is similar to what a satellite would experience after eight years in high orbit around Earth. Designing a radiation-proof chip Unfortunately, this intense radiation can hinder the chip’s ability to record data accurately. Digital data is conveyed through a series of ones and zeroes. But as the ADC converts electrical signals into this digital format, upset caused by radiation could cause a one to be recorded as a zero, or vice versa, essentially corrupting the data. Therefore, the researchers created their ADC using a technique that provides a triple-check measure to ensure that the digital data has been converted and stored correctly. The chances that the data would be corrupted three times is very unlikely, Kinget points out. After designing their ADC, the researchers subjected it to radiation using medical equipment at a hospital in Boston to evaluate how it performs under intense radiation and estimate how it would perform in the LHC. The results suggest the chip is up to the task, and it is now being readied for integration and installation in the next LHC upgrade scheduled to start in 2026. The stakes are high for the chips to work. As Kinget notes, hardware upgrades come only every decade, so there’s a lot of pressure on engineers and scientists to have their technology working flawlessly at the time of installation. But the payoff will be high if the chip performs well—allowing more than a billion collisions per second to be detected. Kinget emphasizes that this research has been the result of a great deal of collaboration, not just with experts within Columbia University, such as physicists John Parsons and Gustaaf Brooijmans, but also physicist Tim Andeen and electrical engineer Nan Sun at the University of Texas at Austin and many teams from around the world that have made the LHC work possible. As of 2022, more than 5,500 scientists across 42 countries have contributed to ATLAS alone. “It’s been nice for us to be part of that,” says Kinget.",
    "published": "Sun, 10 Aug 2025 13:00:02 +0000",
    "author": "Michelle Hampson",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Chips With Neural Tissue Aim to Make AI More Energy Efficient",
    "link": "https://spectrum.ieee.org/biochip-organoid-intelligence-ai-processor",
    "summary": "As generative AI systems advance, so too does their appetite for energy. Training and running large language models consumes vast amounts of electricity. AI’s energy demand is projected to double in the next five years, gobbling up 3 percent of total global electricity consumption. But what if AI chips could function more like the human brain, processing complex tasks with minimal energy? A growing chorus of scientists and engineers believes that the key might lie in organoid intelligence . AI enthusiasts were introduced to the concept of brain-inspired chips in July at the United Nations’ AI for Good Summit in Geneva. There, David Gracias , a professor of chemical and biomolecular engineering at Johns Hopkins University, in Baltimore, gave a talk discussing the latest research he’s led on biochips and their applications to AI. Focused on nanotech, intelligent systems, and bioengineering, Gracias’s research team is among the first to build a functioning biochip that combines neural organoids with advanced hardware, enabling chips to run on and interact with living tissue. Organoid intelligence is an emerging field that blends lab-grown neurons with machine learning to create a new form of computing. (The term organoid intelligence was coined by a group of Johns Hopkins researchers that includes Thomas Hartung .) The neurons, called organoids, are more specifically three-dimensional clusters of lab-grown brain cells that mimic neural structures and functions. Some researchers believe that so-called biochips—organoid systems that integrate living brain cells into hardware—have the potential to outstrip silicon-based processors like CPUs and GPUs in both efficiency and adaptability. If the process is commercialized, experts say biochips could potentially reduce the staggering energy demands of today’s AI systems while enhancing their learning capabilities. “This is an exploration of an alternate way to form computers,” Gracias says. How Do Biochips Mimic the Brain? Traditional chips have long been confined to two-dimensional layouts, which can limit how signals flow through the system. This paradigm is starting to shift, as chipmakers are now developing 3D chip architectures to increase their devices’ processing power. Similarly, biochips are designed to emulate the brain’s own three-dimensional structure. The human brain can support neurons with up to 200,000 connections—levels of interconnectivity that Gracias says flat silicon chips can’t achieve. This spatial complexity allows biochips to transmit signals across multiple axes, which could enable more efficient information processing. Gracias’s team developed a 3D electroencephalogram (EEG) shell that wraps around an organoid, enabling richer stimulation and recording than conventional flat electrodes. This cap conforms to the organoid’s curved surface, creating a better interface for stimulating and recording electrical activity. To train organoids, the team uses reinforcement learning. Electrical pulses are applied to targeted regions. When the resulting neural activity matches a desired pattern, it’s reinforced with dopamine, the brain’s natural reward chemical. Over time, the organoid learns to associate certain stimuli with outcomes. Once a pattern is learned, it can be used to control physical actions, such as steering a miniature robot car through strategically placed electrodes. This demonstrates neuromodulation—the ability to produce predictable responses from the organoid. These consistent reactions lay the groundwork for more advanced functions, such as stimulus discrimination, which is essential for applications like facial recognition, decision-making, and generalized AI inference. Gracias’s team is in the early stages of developing miniature self-driving cars controlled by biochips: A proof of concept that the system can act as a controller. This experimental work suggests future roles in robotics, prosthetics, and bio-integrated implants that communicate with human tissue. These systems also hold promise in disease modeling and drug testing. Gracias’s group is developing organoids that mimic neurological diseases like Parkinson’s. By observing how these diseased tissues respond to various drugs, researchers can test new treatments in a dish rather than relying solely on animal models. They can also uncover potential mechanisms of cognitive impairment that current AI systems fail to simulate. Because these chips are alive, they require constant care: temperature regulation, nutrient feeding, and waste removal. Gracias’s team has kept integrated biochips alive and functional for up to a month with continuous monitoring. Fred Jordan [left] and Martin Kutter are the founders of FinalSpark, a Swiss startup developing biochips that the company claims can store data in living neurons. FinalSpark Challenges in Scaling Biochip Technology Yet significant challenges remain. Biochips are fragile and high maintenance, and current systems depend on bulky lab equipment. Scaling them down for practical use will require biocompatible materials and technologies that can autonomously manage life-supporting functions. Neural latency, signal noise, and the scalability of neuron training also present hurdles for real-time AI inference. “There are a lot of biological and hardware questions,” Gracias says. Meanwhile, some companies are testing the waters. Swiss startup FinalSpark claims its biochip can store data in living neurons—a milestone it calls a “bio bit,” says Ewelina Kurtys , a scientist and strategic advisor at the company. This step suggests biological systems could one day perform core computing functions traditionally handled by silicon hardware. FinalSpark aims to develop remote-accessible bioservers for general computing in about a decade. The goal is to match digital processors in performance while being exponentially more energy efficient. “The biggest challenge is programming neurons, as we need to figure out a totally new way of doing this,” Kurtys says. Still, transitioning from the lab to industry will require more than just technical breakthroughs. ”We have enough funding to keep the lab running,” Gracias says. “But for the research to take off, more funding is needed from Silicon Valley.” Whether biochips will augment or replace silicon remains to be seen. But as AI systems demand more and more power, the idea of chips that think—and sip energy—like brains is becoming increasingly attractive. For Gracias, that technology could be shipped to market sooner than we think. “I don’t see any major showstoppers on the way to implementing this,” he says.",
    "published": "Sat, 09 Aug 2025 13:00:02 +0000",
    "author": "Aaron Mok",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Two-Factor Authentication Just Got Simpler",
    "link": "https://spectrum.ieee.org/two-factor-authentication-sandia-labs",
    "summary": "Two-factor authentication is a cornerstone of modern digital security, protecting banking, email, and many other kinds of accounts worldwide. Now scientists at Sandia National Laboratories in Albuquerque have developed a new, simpler form of two-factor authentication that, unlike conventional methods, does not require generating authentication codes based on the current time. They say it could help bring two-factor authentication to many new types of devices that currently cannot support it, including drones, remote sensors, farm equipment and industrial control systems. Two-factor authentication (TFA) is a security routine that requires both a password and an additional, temporary code to log into an account, typically sent via text, email, or an authenticator app. This kind of authentication is more difficult to crack than one that uses only a password because it requires a combination of different types of information. Usually, TFA generates these codes based on the current time . Banks might get that from their servers, while remote devices often get it from GPS . All kinds of applications could, in principle, use TFA, such as smart electric meters that require users to log in to change their settings. However, many devices lack the processing power, network bandwidth, or GPS connection to support it, leaving them vulnerable to potential cyberattacks, says Chris Jenkins , a cybersecurity researcher at Sandia. Moreover, as simple as TFA might seem, it often requires a complex set of transactions behind the scenes. For example, authentication codes from banks often come from third-party vendors, which in turn rely on telecom providers to send codes to phones, Jenkins says. Simplifying Two-Factor Authentication Jenkins and his colleagues have devised a simpler variation of TFA that does not require a time stamp and can work directly between two devices without third parties or extensive telecom infrastructures. They suggest it could enable a device as basic as a thermostat to generate its own authentication code. Chris Jenkins, a Sandia cybersecurity researcher, developed a new, simpler two-factor authentication method that does not depend on the current time to generate an additional verification code. Craig Fritz “When this work first started, it was focused on military weapons systems , which can be in GPS-denied environments ,” Jenkins says. “So we wanted TFA or something similar where knowing the time wasn’t going to be a requirement.” Instead of using the current time, the new TFA variant uses a random number generator . “Nothing about TFA in and of itself requires using time,” Jenkins says. “It was just easy, when implementing TFA systems, to use time to generate one-time passwords because of the infrastructure that was already in place.” This new work “doesn’t imply that we should abandon current TFA,” says Eric Vugrin , a senior cybersecurity scientist at Sandia. “It’s just that current TFA does not work for everything.” The new system uses minimal computing resources, and so may prove ideal for devices designed to minimize size, weight, and power use. Such electronics typically lack the kind of processing power needed to run complex security software, Jenkins says. “Our system can be used in resource-constrained devices,” Jenkins says. “Conventional TFA is always generating new codes as time passes—say, every minute or so. Our system performs the computation once. So for systems that might, say, save energy by waking up just once a day, they don’t have to burn energy every minute performing computations. They can just do everything up front.” That said, “there’s no reason our system can’t be used for all kinds of applications,” Jenkins says. Jenkins says that Sandia has a copyright on the code, so anyone interested in using it for TFA would have to go through Sandia’s Licensing and Tech Transfer department to discuss licensing it or establishing a Cooperative Research and Development Agreement . The scientists found their new system resisted machine-learning-based attacks . In the future, they hope to make it more robust with codes that can be updated dynamically during authentication. “When hackers steal passwords, those secrets don’t change if leaked,” Jenkins says. “So we’re looking for ways to update those secrets in the event that databases get compromised.”",
    "published": "Tue, 05 Aug 2025 12:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Mammalian Cell-Evolution Machine May Bring New Vaccines",
    "link": "https://spectrum.ieee.org/proteus-mammalian-ai",
    "summary": "Evolution naturally produces the fittest living things for a given environment, but labs can speed up that process to uncover how cells respond to specific pressures. These directed evolution experiments typically rely on genetically simple organisms like bacteria or yeast, but a new system pushes that paradigm into mammalian cells—notoriously difficult targets for new drugs and therapeutics. Scientists at the University of Sydney’s Neely Lab and the Centenary Institute spent three years building a virus-inspired biological machine, PROTEUS (PROTein Evolution Using Selection), an open-source platform that could enable more effective gene-editing tools, mRNA-based medicines with greater specificity, and therapeutic proteins engineered to avoid side effects. Each cycle takes 24 hours of diversification, selection, and amplification, shortening the protein evolution process from months or years to just weeks. “PROTEUS can produce mammalian-specific adaptations that we wouldn’t have predicted or evolved in other systems,” says University of Sydney research fellow Christopher Denes , one of the study’s authors. The team evolved a transcription factor—a protein that activates the expression of a gene target—with mutations only beneficial in mammalian cells, not bacteria. “The general method of directed evolution is cyclic, where we generate diversity in a gene of interest, select for fitness against a challenge, and amplify successful genetic variants to repeat the process. This is all done with the goal of directing a gene towards a desired outcome,” says Denes. “PROTEUS enables all three steps in a single round of evolution. We repeat these cycles over and over, sometimes playing with the intensity of selective pressure to really challenge our protein.” In May, the team published a study in the open-access journal Nature Communications , making their data and sequences public as an academic resource. How PROTEUS Works PROTEUS resembles biological artificial intelligence , a relatively new field combining bioengineering with machine learning principles to create dynamic biological systems. But Denes conceptualizes PROTEUS more as a “biological trial-and-error machine” that adapts to the user’s demands. Unlike AI, the system doesn’t need a defined problem or goal to evolve. It naturally generates genetic diversity, thanks to its foundation in error-prone RNA viruses. “Think of this as similar to how SARS-CoV-2 [the virus that causes COVID-19 ] adapted and evolved through variants from Alpha through to Delta and now Omicron, but the selection pressure applied by our genetic problem filters out any ‘bad’ variants, amplifying the good,” Denes says. “In the next round, these good variants can continue to become better or might even introduce a single bad mutation that cancels out the benefit.” This tube contains 15 billion virus-like vesicles—small sacs of noninfectious particles replicated in a lab. Each carries a mutated gene. Tian Du PROTEUS starts by engineering a single gene sequence for the protein the researcher wants to evolve into the genome of a virus. The genome is then introduced into mammalian cells with a shell-like packaging element to produce virus-like vesicles (VLVs), small sacs of noninfectious particles. The VLVs are then added to cells that carry a defined synthetic circuit, or a genetic problem the protein has been tasked to solve. “Within 24 hours, these VLVs copy their genome in readiness to grow, but this copying step frequently makes mistakes, introducing mutations along the gene we want to evolve. Our gene is converted into protein and then challenged within cells by the genetic problem at hand,” Denes explains. “If it succeeds in solving this puzzle, it’ll produce more of itself. We’ve effectively linked together protein fitness with VLV survival, pretty much enforcing the survival of the fittest.” Across hundreds of thousands to millions of cells, Denes says PROTEUS can quickly produce millions of possible genetic variants—all selected for in parallel. It can also be scaled for more cells, providing broader diversity. Why Are Mammalian Cells So Difficult? Many proteins evolved in typical directed evolution environments—yeast or bacteria—don’t translate well to human cells. Mammalian cells are tricky evolution hosts. They grow slowly, contain large genomes, and regulate protein behavior through unpredictable “post-translational modifications”—diverse chemical changes that occur after a protein is built from mRNA. That complexity makes conventional tools too imprecise to isolate and manipulate their genetic material. Mammalian directed evolution also requires massive cell-population sizes to cover all the potential mutations necessary to test a desired variant, says Kate Adamala , an associate professor in the University of Minnesota’s Department of Genetics, Cell Biology, and Development. “Here, [with PROTEUS] they managed to circumvent mammalian cells’ natural intolerance to high mutation rates, which is important because if you want to test a lot of variants, you need a huge mutation rate.” As a general-purpose synthetic biologist practicing artificial evolution herself, though not with mammalian cells, Adamala was excited about the findings. “I’ve been following it because I’m looking for ways to make artificial evolution more biomedically applicable, and that means we have to start poking at complex mammalian systems,” Adamala says. “If my lab had PROTEUS, I’d start with membrane proteins, because that’s a huge area and incredibly attractive from a human health perspective and as a foundational research question of understanding how drugs interact with membranes.” This is the predicted 3D structure of an evolved protein, featuring mutation sites [red], displaced functional groups [blue], and the drug-binding area [green circle]. Alexander Cole, Christopher Denes, Cesar Moreno, et al. Many existing diseases and drugs are related to membrane proteins, like opioids, weight-loss medications, and viral resistance. “There are a lot of targets on a membrane’s surface, and evolving those proteins has been a pain in the lower back because they’re difficult to work with and existing processes are slow,” Adamala says. Through the not-for-profit plasmid repository Addgene, PROTEUS will be available to any lab worldwide with the appropriate infrastructure, virology training, and skills in molecular biology. According to Denes, the evolution process costs a few thousand dollars and is cost-effective in campaigns for multiple genes in parallel. PROTEUS’s lead researchers filed a provisional patent application in Australia. Denes says the team is exploring commercialization pathways. Neely Lab is interested in applying the technology to gene editing, having previously used CRISPR to understand the mechanisms behind how venoms cause cell death and pain, and how proteins bind to cells, including the crucial spike protein in SARS-CoV-2 . “Evolved CRISPR tools would be really valuable for both research and medicine,” Denes says, citing a recent study that used CRISPR-based gene editing in a human baby to treat a rare genetic disease within eight months from diagnosis at birth. “The gene editor they used was actually derived through a bacteria-based directed-evolution method, so there’s huge potential in evolving these editors further in mammalian cells with PROTEUS to produce proteins with boosted activity in human disease treatment,” Denes adds.",
    "published": "Sun, 27 Jul 2025 13:00:02 +0000",
    "author": "Shannon Cuthrell",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Startup Claims up to 100x Better Embedded Computing Efficiency",
    "link": "https://spectrum.ieee.org/efficient-computer-dataflow-architecture",
    "summary": "There’s a growing need for CPUs that can live life on the edge. That is, computing for a long time embedded in hard-to-get-to places and surviving on battery power or energy they can scrounge from the environment. Frustrated with inherent inefficiencies in the architecture of ultralow-power microprocessors , the founders of startup Efficient Computer decided to reinvent the general-purpose processor from the ground up for energy efficiency. “We’re doing something that has the capability of a CPU but is one or two orders of magnitude more efficient,” says cofounder Brandon Lucia . The result, the Electron E1 and its accompanying compiler, is now heading to developers and early partners. According to Lucia, the C -programmable processor is delivering between 10- and 100-fold better efficiency than commercial ultralow-power CPUs on typical embedded systems tasks, like performing a fast Fourier transform on sensor data or doing convolutions for machine learning. The key innovation was to invent an architecture that can lay out any program’s instructions spatially on a chip rather than delivering them sequentially from memory as is done now in processors that follow the von Neumann architecture , says Lucia. A Fabric for Dataflow The von Neumann architecture has dominated computing for decades. It basically takes in an instruction from memory that tells the processor what to do with data—add it to something, flip it around, whatever—and puts the result in memory. Then it picks the next instruction, and the next, and so on. It sounds simple, but it actually comes with a lot of overhead. “Several billion times per second, you’re pulling an instruction in from memory. That operation costs some energy,” says Lucia. Additionally, to prevent the process from stalling, modern CPUs have to guess at what instruction comes next, requiring logic called branch prediction and still more overhead. Instead, the E1 maps out the sequence of instructions as a spatial pathway through which data moves. Fundamentally, the E1 is an array of “tiles.” Each is like a stripped-down processor core—capable of performing a set of instructions but lacking instruction fetching, branch prediction, and other overhead. The tiles are linked together in a specially designed, programmable network. The E1’s compiler, called the effcc Compiler , reads the program, which can be written in C or other common languages and platforms, and assigns each instruction in the program to a tile. It then sets up the network so that data enters one tile, is processed, and the result becomes the input to the next tile all in the right sequence to run the program. When the sequence branches, such as when the program encounters an if/then/else, so too does the spatial pattern of tiles. “It’s like a switch track in a railroad,” says Lucia. “There have been other dataflow-style architectures,” Lucia notes. Google’s TPUs and Amazon’s Inferentia chips, for example, are designed around a dataflow architecture called a systolic array. But systolic arrays and other dataflow efforts are restricted to a subset of all the possible data paths software might demand, Lucia says. In contrast, the E1’s network fabric allows any arbitrary path a program could ask for. Critical to that is the fabric’s ability to support so-called arbitrary recurrences, such as the “while loop.” (Think: “while the light is red, depress the brake.”) Such loops require a feedback path. “It turns out that’s harder than it seems when you first look at it,” says Lucia. The E1 fabric can carry values around the feedback paths in a way that allows for general purpose computing. “A lot of other dataflow architectures don’t do general purpose because they couldn’t crack that nut.… It took us years to get it right.” According to Efficient Computer, the E1 consumes less energy than two competing ARM processors at three common tasks: matrix multiplication for machine learning, the fast Fourier transform, and convolution for computer vision. Efficient Computer According to University of Michigan computer science and engineering professor Todd Austin , chips like the E1 are a good example of an efficient architecture, because they minimize parts of the silicon engaged in things that are not purely computation, such as fetching instructions, temporarily stashing data, and checking if a network route is in use. Lucia’s team “is doing a lot of clever work to allow you to get extremely low power for general purpose computing,” says Rakesh Kumar , a computer architect at the University of Illinois Urbana-Champaign. The challenge for the startup will be economics, he predicts. “Ultralow-power companies have had a hard time because of strong competition in low-power, very cheap microcontrollers. The key challenge is in identifying a new capability” and getting customers to pay for it.",
    "published": "Thu, 24 Jul 2025 13:00:03 +0000",
    "author": "Samuel K. Moore",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "IEEE Makes Strides to Improve Online Safety for Kids",
    "link": "https://spectrum.ieee.org/online-safety-kids-ieee-standard",
    "summary": "Kids are using the Internet at a younger age and spending more time online, according to UNICEF . One in three Internet users worldwide is younger than 18 , it reports. An increasing amount of harmful online practices and content is impacting children mentally, emotionally, and physically, as well as posing a risk to their safety and security. This has led many governments and regulatory agencies around the world to introduce legislation to reduce addictive online features and to take steps to restrict youngsters’ access through various means, including by mandating that online sites use systems to verify a user’s age . These systems can give parents peace of mind, knowing the website has taken measures to deactivate addictive algorithmic mechanisms and to present content geared for children and is safe for them to use. To help developers build age-appropriate websites, the IEEE Standards Association has released two guidelines: one for the process of designing age-appropriate sites for children and the other for systems to verify the user’s age online. IEEE SA also launched a related certification program. Upholding children’s rights and designing with age in mind The IEEE 2089-2021 Standard for an Age Appropriate Digital Services Framework Based on the 5Rights Principles for Children provides practical steps to qualify online products and services for children. Introduced in 2021, the standard requires systems to present information in an age-appropriate way and to uphold the rights established for youngsters in the U.N. Convention on the Rights of the Child . The follow-up IEEE 2089.1-2024 , Standard for Online Age Verification, provides a framework for designing, specifying, evaluating, and deploying age-verification systems. Launched last year, IEEE 2089.1 includes requirements for privacy protection, data security, and information management specific to the age-assurance process. The standard also provides procedures for verifying a user’s age or age range with a high degree of accuracy. Age verification helps organizations present age-appropriate content, ensures their marketing materials align with regulations designed to protect minors, and builds trust with parents, says Jon Labrador , director of conformity assessment for IEEE SA. Indonesia used key provisions from the two IEEE standards to inform its child-protection regulation , which was signed into law in May. It is the first age-appropriate design regulation in Asia , according to the 5Rights Foundation , an international NGO working for a rights-respecting digital world for children. The act mandates that online service providers prioritize kids’ best interests in their platform design and practices. The collaboration is “an example of IEEE putting its mission into practice in a concrete way to support society’s and children’s needs, where IEEE’s global perspective and technical knowledge combined with the Indonesian authorities’ dedication to achieve a successful outcome,” Sophia Muirhead , IEEE executive director and chief operating officer, said in a news release. “Together,” Muirhead said, “we enabled the development of a practical, forward-looking legal framework and regulation that will make the digital space safer for the next generation in Indonesia and, we hope, inspire similar actions worldwide.” Age verification system certification IEEE SA this year launched its Online Age Verification Certification Program , which assesses systems to make sure they conform to the IEEE 2089.1 framework. “This certification program ensures that age verification systems used by organizations conform to requirements in the standard, thereby ensuring that access to their products and services are made after proper verification,” says Ravi Subramaniam , senior director for the IEEE SA Business, Product Development and Marketing group. “It says that this organization prioritizes the rights and needs of children—designing tools that ensure their safety, privacy, autonomy and agency.” Six indicators are used to rate the systems: Accuracy. The degree to which the verification process determines the user’s age. Frequency of assurance. How often does the website verify the user’s age? Counter-fraud measures. The mechanisms used to prevent fraudulent attempts to bypass the verification process. Authenticity. Is the proof-of-age documentation provided by the user genuine? Frequency of authenticity. How often is the user’s claimed age verified? Birth date. A specific indicator that determines the level of confidence in the accuracy of the birthday provided, using varying levels of rigor based on the age-verification method. The IEEE SA commissioned the Age Check Certification Scheme , a British provider of age-verification services, to conduct tests that lead to IEEE certification. “As the digital world continues to evolve, a reliable, secure, and globally recognized framework for verifying the age of online users is essential ,” Tony Allen , founder and chief executive of Age Check, said in a news release . “We are proud to be working with the IEEE Standards Association to advance the cause of online safety for the world’s youth.” To learn more about the certification program, fill out this form . Promoting safer online environments around the world IEEE also has been involved in recommending website designs with children in mind. Konstantinos Karachalios , an advisor to Muirhead and former managing director of IEEE SA, moderated a panel discussion featuring Greece’s prime minister and the 5Rights Foundation president at a global forum on AI held this month in Athens. Regulation on the design of online systems is gaining ground, according to a news release summarizing the discussion. “The models and the algorithms [of the systems] are designed to promote addition,” Prime Minister Kyriakos Mitsotakis said. He described the current situation as an “unprecedented global experiment with the mental health of our children and teenagers.” Greece recently launched a strategy to protect minors online , drawing on IEEE standards and expertise, and leading to the Greek government proposing a series of informed measures for an age-appropriate regulatory framework, which also includes a system for age verification and content filtering. Baroness Beeban Kidron , 5Rights president and a member of the U.K. House of Lords, agreed with Mitsotakis. The digital environment for children has been designed to be addictive, she said, but it can be reengineered to be safer. “This is a 100 percent engineered world,” Kidron said. “We have to design the world to be appropriate for children, like we design other environments.” She said regulations should focus on how systems are designed and developed, specifically through effective default settings instead of relying on reactive tools or parental controls.",
    "published": "Tue, 22 Jul 2025 18:00:03 +0000",
    "author": "Kathy Pretz",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  },
  {
    "title": "Quantinuum Claims Key Step Toward Scaling Up Quantum Computers",
    "link": "https://spectrum.ieee.org/quantinuum-fault-tolerant-quantum-computing",
    "summary": "Quantum computers theoretically could rapidly find answers to problems that regular computers would take eons to solve, but they have to first overcome their error-prone nature. Now quantum computing firm Quantinuum says its machines can, for the first time, run all the operations needed to answer otherwise intractable questions in an error-compensating way. Quantum computers perform calculations using components known as qubits , which are highly unstable in nature. Present-day, state-of-the-art quantum computers typically suffer roughly one error every 1,000 operations . In contrast, many practical applications for quantum computing demand error rates lower by a billionfold or more. To move past the current era of noisy intermediate-scale quantum computing , scientists aim to compensate for high error rates by spreading quantum information across many redundant qubits. These quantum error correction strategies would help quantum computers detect and correct mistakes. In these schemes, a cluster of “physical” qubits altogether behave as one low-error “logical” qubit, serving as the foundation of a fault-tolerant quantum computer . Once a quantum error correction code is running, a quantum computer can then link qubits together to carry out elementary operations known as quantum gates. These come in two flavors—so-called Clifford gates, which classical computers can simulate, and non-Clifford gates, which they cannot. A so-called universal quantum computer that can run both kinds of gates is theoretically capable of performing computations far beyond the capabilities of conventional supercomputers . However, although some groups have performed quantum gates using quantum error correction, Quantinuum notes those displayed error rates near 10 percent, too high for practical use. In addition, non-Clifford gates require qubits with special properties known as magic states. Previous research found ways to prepare magic states from noisy qubits, but the number of qubits needed to create usable magic states “was thought to be really enormous,” says Shival Dasu , an advanced physicist at Quantinuum. Scaling Up Quantum Computers Is Becoming More Feasible Recent work revealed “overheads for some quantum error correction codes are coming down faster than others,” Dasu says. “We managed to design a really efficient magic-state production protocol.” Quantinuum’s H1 trap holds 20 ytterbium ions, which act as error-prone physical qubits. The team used eight to create a so-called magic state, which they used to perform quantum operations that were less error-prone than their physical constituents. Quantinuum In a new study, Dasu and his colleagues experimented with Quantinuum’s H1-1 processor . The device uses 20 qubits made from electrically trapped ytterbium ions. The researchers showed they could not only prepare two magic states from just eight physical qubits, but also perform a two-qubit non-Clifford gate with a logical error rate of about one mistake per 5,000 operations, an error rate approaching one-tenth that of its physical one. “This is the first time a quantum circuit was run using a universal quantum gate set and showed a higher accuracy with encoding than without it,” Dasu says. The scientists prepared these magic states with an error rate of just seven mistakes per 100,000 operations, about 10 times better than any previously reported work. Their simulations also suggested they could reach just six errors per 10 billion operations on a larger-scale version of their quantum computer, and five per 100 trillion operations as they continue to improve their hardware. “Our simulations suggest we can use roughly 40 physical qubits to create one very-high-fidelity magic-state qubit,” Dasu says. “That overhead looks pretty reasonable.” In comparison, “when it comes to the previous state-of-the-art work I’m aware of, magic states would take about 10 times more qubits,” says David Hayes , director of computational design and theory at Quantinuum. Code Switching Brings Advantages In another study, Quantinuum researchers and their colleagues switched a quantum processor back and forth from one quantum error correction code to another. “One reason to do this is because, for instance, it’s harder to do some quantum gates in one code than in others,” Hayes says. “The idea with code switching is to jump back and forth between codes to perform gates that are easy for them.” Such code switching is not practical for all codes and quantum-computing architectures. “What’s special about our architecture is there is this all-to-all connectivity between our qubits,” Hayes says. “So you can imagine two codes, one requiring qubits living in a 2D geometry, the other in a 3D space, and if your qubits are all laid out locked in a plane, switching between these codes is impossible. With our architecture, it’s possible.” In experiments with Quantinuum’s H2-1 processor , which uses 56 qubits made from electrically trapped ytterbium ions, the researchers showed they could switch between one code that was more efficient at producing magic states and another that was better at performing quantum gates. “We’re finding more and more evidence that all-to-all connectivity can bring down the resource requirements for quantum computing,” Hayes says. Quantinuum says these new findings mark a key advance in the company’s goal of a scalable universal fault-tolerant quantum computer by 2029. “Once you get quantum error correction going, you can push connectivity between qubits pretty high,” Hayes says. “How many ions can ultimately be connected together on a chip? Maybe thousands of qubits, maybe hundreds of thousands. We’re talking with foundries right now for bigger and bigger chips to house more and more qubits.” Quantinuum also needs photonic integrated chips “with lasers to manipulate the qubits,” Hayes says. “That technology is less mature, but we’re making agreements with the University of New Mexico and Los Alamos and Sandia National Labs to help advance photonics as quickly as possible.” The scientists detailed their findings online on 26 June in two studies on the ArXiv preprint server. This story was updated on 12 July, 2025 to correct a quote from Shival Dasu. This article appears in the September 2025 print issue as “ Quantinuum Demonstrates Quantum Trick for Massive Scale-Up .”",
    "published": "Thu, 10 Jul 2025 13:00:03 +0000",
    "author": "Charles Q. Choi",
    "topic": "computing",
    "collected_at": "2025-10-08T14:03:20"
  }
]