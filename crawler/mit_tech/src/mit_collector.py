import os
import json
import time
from rss_parser import parse_feed
from shared.utils import save_json, get_data_dir
from shared.logger_config import logger
from topic_collector import fetch_topic
from shared.bloom_filter import BloomFilter          # âœ… RedisBloom
from shared.hash_utils import generate_hash_id       # âœ… ê³ ìœ  í•´ì‹œ ìƒì„±ê¸°

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = get_data_dir("mit_tech")

GREEN = "\033[92m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
CYAN = "\033[96m"
RESET = "\033[0m"

# âœ… RedisBloom ì´ˆê¸°í™”
bloom = BloomFilter(host="localhost", port=6379)
bloom.ensure_filter("mit_tech")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# feeds.json ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_feeds():
    """MIT Tech Review RSS/Topic URL ëª©ë¡ ë¡œë“œ"""
    path = os.path.join(BASE_DIR, "../config/feeds.json")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RSS + API í”¼ë“œ ì¤‘ë³µ í•„í„°ë§ ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def filter_new_articles(topic: str, articles: list):
    """
    ê° ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ RedisBloom ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ì œê±° ìˆ˜í–‰.
    ê³ ìœ  í•´ì‹œ ID(link + title)ë¥¼ ìƒì„±í•˜ì—¬ ì¤‘ë³µ í•„í„°ë§í•¨.
    """
    new_articles = []
    duplicate_count = 0

    for article in articles:
        link = article.get("link", "")
        title = article.get("title", "")
        if not link or not title:
            continue

        # âœ… ê³ ìœ  í•´ì‹œ ìƒì„±
        article_hash = generate_hash_id(link, title)

        # âœ… RedisBloom ì¤‘ë³µ ì²´í¬
        if not bloom.add(f"mit_tech:{topic}", article_hash):
            duplicate_count += 1
            continue

        # í•´ì‹œë¥¼ idë¡œ ì¶”ê°€
        article["id"] = article_hash
        new_articles.append(article)

    return new_articles, duplicate_count

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•œ ì‚¬ì´í´ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_cycle():
    """MIT Tech Review í”¼ë“œ ë° í† í”½ í¬ë¡¤ë§ 1íšŒ ì‹¤í–‰"""
    feeds = load_feeds()
    success_topics, empty_topics = [], []
    duplicate_stats = {}

    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")
    print(f"{CYAN}MIT Tech Review Feed Parsing | {time.strftime('%Y-%m-%d %H:%M:%S')}{RESET}")
    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")

    # ------------------------------------------------------
    # â‘  ë©”ì¸ í”¼ë“œ
    # ------------------------------------------------------
    try:
        articles = parse_feed("main", feeds["main"])
        articles, dup_count = filter_new_articles("main", articles)
        duplicate_stats["main"] = dup_count

        if articles:
            save_json(articles, os.path.join(DATA_DIR, "main.json"))
            success_topics.append(f"main({len(articles)})")
        else:
            empty_topics.append("main")
    except Exception as e:
        logger.error(f"[main] RSS feed error: {e}")
        empty_topics.append("main")

    # ------------------------------------------------------
    # â‘¡ RSS ê¸°ë°˜ í† í”½ í”¼ë“œ
    # ------------------------------------------------------
    for topic, url in feeds["topics"].items():
        time.sleep(1)
        try:
            articles = parse_feed(topic, url)
            articles, dup_count = filter_new_articles(topic, articles)
            duplicate_stats[topic] = dup_count

            if articles:
                save_json(articles, os.path.join(DATA_DIR, f"{topic}.json"))
                success_topics.append(f"{topic}({len(articles)})")
            else:
                empty_topics.append(topic)
        except Exception as e:
            logger.error(f"[{topic}] RSS feed error: {e}")
            empty_topics.append(topic)

    # ------------------------------------------------------
    # â‘¢ API ê¸°ë°˜ í† í”½ (business, climate-change)
    # ------------------------------------------------------
    for topic in ["business", "climate-change"]:
        time.sleep(1)
        try:
            articles = fetch_topic(topic, max_pages=5)
            articles, dup_count = filter_new_articles(topic, articles)
            duplicate_stats[topic] = dup_count

            if articles:
                save_json(articles, os.path.join(DATA_DIR, f"{topic}.json"))
                success_topics.append(f"{topic}({len(articles)})")
            else:
                empty_topics.append(topic)
        except Exception as e:
            logger.error(f"[{topic}] API ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            empty_topics.append(topic)

    # ------------------------------------------------------
    # ê²°ê³¼ ì¶œë ¥
    # ------------------------------------------------------
    print(f"{GREEN}âœ” Success".ljust(15) + f"{', '.join(success_topics) or '-'}{RESET}")
    print(f"{YELLOW}âš  Empty".ljust(15) + f"{', '.join(empty_topics) or '-'}{RESET}")
    print(f"{CYAN}ðŸ§  Duplicates{RESET}".ljust(15))
    for topic, count in duplicate_stats.items():
        if count > 0:
            print(f"   {topic}: {YELLOW}{count}{RESET} ì¤‘ë³µ ê±´ìˆ˜")

    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")
    print(f"âœ… Completed | {GREEN}{len(success_topics)} ì„±ê³µ{RESET} | {YELLOW}{len(empty_topics)} ì‹¤íŒ¨{RESET}")
    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")

    return {"success": success_topics, "empty": empty_topics, "duplicates": duplicate_stats}
