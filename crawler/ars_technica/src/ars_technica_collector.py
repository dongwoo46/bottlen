import os
import json
import time
import feedparser
from bs4 import BeautifulSoup
from shared.utils import save_json, get_data_dir
from shared.logger_config import logger
from shared.bloom_filter import BloomFilter
from shared.hash_utils import generate_hash_id  # âœ… í•´ì‹œ ìœ í‹¸ ì‚¬ìš©

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_DIR = os.path.join(BASE_DIR, "../config")
DATA_DIR = get_data_dir("ars_technica")

GREEN = "\033[92m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
CYAN = "\033[96m"
RESET = "\033[0m"

# âœ… RedisBloom ì´ˆê¸°í™”
bloom = BloomFilter(host="localhost", port=6379)
bloom.ensure_filter("ars_technica")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# feeds.json ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_feeds():
    path = os.path.join(CONFIG_DIR, "feeds.json")
    with open(path, "r", encoding="utf-8") as f:
        feeds = json.load(f)
        return feeds.get("ars_technica", {})


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HTML ì •ì œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_html(raw_html: str) -> str:
    if not raw_html:
        return ""
    soup = BeautifulSoup(raw_html, "html.parser")
    text = soup.get_text(separator=" ", strip=True)
    return " ".join(text.split())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RSS íŒŒì‹±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_feed(topic: str, url: str):
    """Ars Technica RSS íŒŒì‹± + RedisBloom ì¤‘ë³µì œê±°"""
    try:
        feed = feedparser.parse(url)
        if not feed.entries:
            logger.warning(f"[{topic}] RSS í”¼ë“œ ë¹„ì–´ìžˆìŒ: {url}")
            return [], 0

        articles = []
        duplicate_count = 0

        for entry in feed.entries:
            link = entry.get("link", "")
            title = entry.get("title", "")
            if not link or not title:
                continue

            # âœ… í•´ì‹œ ìƒì„± (link + title)
            article_hash = generate_hash_id(link, title)

            # âœ… Bloom Filter ì¤‘ë³µ ì²´í¬
            if not bloom.add(f"ars_technica:{topic}", article_hash):
                duplicate_count += 1
                continue

            # ìš”ì•½
            summary_raw = entry.get("summary", "")
            summary_text = clean_html(summary_raw)

            # ë³¸ë¬¸ ì¼ë¶€
            content_html = ""
            if "content" in entry and entry.content:
                content_html = entry.content[0].get("value", "")
            elif "summary_detail" in entry:
                content_html = entry.summary_detail.get("value", "")
            content_text = clean_html(content_html)

            # ëŒ€í‘œ ì´ë¯¸ì§€
            image_url = None
            if "media_content" in entry:
                image_url = entry.media_content[0].get("url")

            # ì¹´í…Œê³ ë¦¬
            categories = [tag.term for tag in entry.get("tags", [])] if "tags" in entry else []

            # ìž‘ì„±ìž
            author = entry.get("author", "")
            if not author and "dc_creator" in entry:
                author = entry.get("dc_creator")

            articles.append({
                "id": article_hash,  # âœ… ê³ ìœ  ID í¬í•¨
                "title": title,
                "link": link,
                "summary": summary_text,
                "content": content_text,
                "published": entry.get("published", ""),
                "author": author,
                "categories": categories,
                "image_url": image_url,
                "topic": topic,
                "collected_at": time.strftime("%Y-%m-%dT%H:%M:%S")
            })

        return articles, duplicate_count

    except Exception as e:
        logger.error(f"[{topic}] RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
        return [], 0


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìˆ˜ì§‘ ë£¨í”„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_cycle():
    feeds = load_feeds()
    success_topics, empty_topics = [], []
    duplicate_stats = {}

    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")
    print(f"{CYAN}Ars Technica Feed Parsing | {time.strftime('%Y-%m-%d %H:%M:%S')}{RESET}")
    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")

    # main í”¼ë“œ
    try:
        main_articles, dup_count = parse_feed("main", feeds.get("main", ""))
        duplicate_stats["main"] = dup_count
        if main_articles:
            save_json(main_articles, os.path.join(DATA_DIR, "main.json"))
            success_topics.append(f"main({len(main_articles)})")
        else:
            empty_topics.append("main")
    except Exception as e:
        logger.error(f"[main] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        empty_topics.append("main")

    # ë‚˜ë¨¸ì§€ í† í”½ í”¼ë“œ
    topics = feeds.get("topics", {})
    for topic, url in topics.items():
        time.sleep(1)
        try:
            articles, dup_count = parse_feed(topic, url)
            duplicate_stats[topic] = dup_count
            if articles:
                save_json(articles, os.path.join(DATA_DIR, f"{topic}.json"))
                success_topics.append(f"{topic}({len(articles)})")
            else:
                empty_topics.append(topic)
        except Exception as e:
            logger.error(f"[{topic}] RSS feed error: {e}")
            empty_topics.append(topic)

    # ê²°ê³¼ ì¶œë ¥
    print(f"{GREEN}âœ” Success".ljust(15) + f"{', '.join(success_topics) or '-'}{RESET}")
    print(f"{YELLOW}âš  Empty".ljust(15) + f"{', '.join(empty_topics) or '-'}{RESET}")
    print(f"{CYAN}ðŸ§  Duplicates{RESET}".ljust(15))
    for topic, count in duplicate_stats.items():
        if count > 0:
            print(f"   {topic}: {YELLOW}{count}{RESET} ì¤‘ë³µ ê±´ìˆ˜")

    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")
    print(f"âœ… Completed | {GREEN}{len(success_topics)} ì„±ê³µ{RESET} | {YELLOW}{len(empty_topics)} ì‹¤íŒ¨{RESET}")
    print(f"{BLUE}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{RESET}")

    return {"success": success_topics, "empty": empty_topics, "duplicates": duplicate_stats}
